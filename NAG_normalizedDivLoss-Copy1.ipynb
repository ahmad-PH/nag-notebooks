{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ahmad-PH/nag-notebooks/blob/master/NAG_tripletLossExperiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqeZpz16do4y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up-to-date.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os; import subprocess\n",
    "\n",
    "def detect_env():\n",
    "    return 'colab' if 'content' in os.listdir('/') else 'IBM'\n",
    "  \n",
    "def run_shell_command(cmd):\n",
    "  p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "  print(str(p.communicate()[0], 'utf-8'))\n",
    "  \n",
    "if detect_env() == 'colab': root_folder = '/content'\n",
    "elif detect_env() == 'IBM' : root_folder = '/root/Derakhshani/adversarial'\n",
    "python_files_path = root_folder + '/nag-public'\n",
    "\n",
    "if os.path.isdir(python_files_path):\n",
    "  initial_dir = os.getcwd()\n",
    "  os.chdir(python_files_path)\n",
    "  run_shell_command('git pull')\n",
    "  os.chdir(initial_dir)\n",
    "else:\n",
    "  os.chdir('/root/Derakhshani/adversarial')\n",
    "  run_shell_command('git clone https://github.com/ahmad-PH/nag-public.git')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(python_files_path + '/NAG-11May-beforeDenoiser')\n",
    "\n",
    "from nag_util import *\n",
    "import nag_util\n",
    "from environment import *\n",
    "from visualization import *\n",
    "\n",
    "env = create_env()\n",
    "env.setup(cuda_index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Ev7jcRKoARg"
   },
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.imports import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import sys; import os; import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralRelu(nn.Module):\n",
    "  def __init__(self, leak=None, sub=None, maxv=None):\n",
    "    super().__init__()\n",
    "    self.leak,self.sub,self.maxv = leak,sub,maxv\n",
    "\n",
    "  def forward(self, x): \n",
    "    x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n",
    "    if self.sub is not None: x.sub_(self.sub)\n",
    "    if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "    return x\n",
    "  \n",
    "class deconv_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k_size = (4,4), s = (2,2), pad = (1,1), b = True, activation = True):\n",
    "        super(deconv_layer, self).__init__()\n",
    "\n",
    "        self.CT2d = nn.ConvTranspose2d(in_channels = in_ch,\n",
    "                                  out_channels = out_ch,\n",
    "                                  kernel_size = k_size,\n",
    "                                  stride = s, \n",
    "                                  padding = pad,\n",
    "                                  bias = b)\n",
    "        self.BN2d = nn.BatchNorm2d(out_ch)\n",
    "        \n",
    "        self.activation = activation\n",
    "        if self.activation:\n",
    "            self.relu = GeneralRelu(0, 0.2, 5)\n",
    "        \n",
    "        self.weight_init()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.activation:\n",
    "            return self.relu(self.BN2d(self.CT2d(input)))\n",
    "        else:\n",
    "            return self.BN2d(self.CT2d(input))\n",
    "\n",
    "    def weight_init(self):\n",
    "        self.CT2d.weight.data.normal_(mean = 0, std = 0.02)\n",
    "        self.CT2d.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Gen(nn.Module):\n",
    "#   def __init__(self, z_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "#     super(Gen, self).__init__()\n",
    "\n",
    "#     self.bs = None\n",
    "#     self.z_dim = z_dim\n",
    "#     self.gf_dim = gf_dim\n",
    "#     self.y_dim = y_dim\n",
    "#     self.df_dim = df_dim\n",
    "#     self.image_shape = image_shape\n",
    "\n",
    "#     self.z_ = nn.Linear(self.z_dim, self.gf_dim * 7 * 4 * 4, bias=True)\n",
    "#     self.z_.bias.data.fill_(0)\n",
    "#     self.BN_ = nn.BatchNorm2d(self.gf_dim * 7)\n",
    "\n",
    "#     self.half = max(self.gf_dim // 2, 1) \n",
    "#     self.quarter = max(self.gf_dim // 4, 1)\n",
    "#     self.eighth = max(self.gf_dim // 8, 1)\n",
    "#     # sixteenth = max(self.gf_dim // 16, 1)\n",
    "    \n",
    "#     self.CT2d_1 = deconv_layer(self.gf_dim * 8, self.gf_dim * 4, k_size = (5,5), pad = (2,2))\n",
    "#     self.CT2d_2 = deconv_layer(self.gf_dim * 5, self.gf_dim * 2)    \n",
    "#     self.CT2d_3 = deconv_layer(self.gf_dim * 2 + self.half, self.gf_dim * 1)\n",
    "#     self.CT2d_4 = deconv_layer(self.gf_dim * 1 + self.quarter, self.gf_dim * 1)\n",
    "#     self.CT2d_5 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "#     self.CT2d_6 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "#     self.CT2d_7 = deconv_layer(self.gf_dim * 1 + self.eighth, 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "#   def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "#     h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "#     h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "#     output = deconv_layer(h_input)\n",
    "#     assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "#             \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "#             \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "#     return output\n",
    "  \n",
    "#   def forward_z(self, z):\n",
    "#     self.bs = z.shape[0]\n",
    "    \n",
    "#     h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "#     assert h0.shape[2:] == (4, 4), \"Unexpected shape, it shoud be (4,4)\"\n",
    "\n",
    "#     h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "#     h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "#     h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "#     h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "#     h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "#     h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "#     h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "#     ksi = 10.0\n",
    "#     output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "#     # this coeff scales the output to be appropriate for images that are \n",
    "#     # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "#     # interval)\n",
    "#     return output_coeff * torch.tanh(h7)\n",
    "  \n",
    "# #   # blind-selection\n",
    "#   def forward(self, inputs):\n",
    "#     self.bs = inputs.shape[0]\n",
    "\n",
    "#     benign_preds_onehot = arch(inputs)\n",
    "#     benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "    \n",
    "#     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "#     for i in range(self.bs):\n",
    "#       random_label = self.randint(0,1000, exclude = benign_preds[i].item())\n",
    "#       z[i][random_label] = 1.\n",
    "    \n",
    "#     z_out = self.forward_z(z)\n",
    "    \n",
    "#     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "\n",
    "# #   #second-best selection: made validation so much worse\n",
    "# #   def forward(self, inputs):\n",
    "# #     self.bs = inputs.shape[0]\n",
    "\n",
    "# #     benign_preds_onehot = arch(inputs)\n",
    "# #     target_preds = torch.topk(benign_preds_onehot, 2, dim = 1).indices[:, 1:]\n",
    "    \n",
    "# #     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "# #     for i in range(self.bs):\n",
    "# #       z[i][target_preds[i]] = 1.\n",
    "    \n",
    "# #     z_out = self.forward_z(z)\n",
    "    \n",
    "# #     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "  \n",
    "# #    def forward(self, inputs):\n",
    "# #     self.bs = inputs.shape[0]\n",
    "\n",
    "# #     benign_preds_onehot = arch(inputs)\n",
    "# #     benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "    \n",
    "# #     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "# #     random_label = self.randint(0,1000, exclude = benign_preds.tolist())\n",
    "# #     for i in range(self.bs):\n",
    "# #       z[i][random_label] = 1.\n",
    "    \n",
    "# #     z_out = self.forward_z(z)\n",
    "    \n",
    "# #     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "  \n",
    "#   @staticmethod\n",
    "#   def randint(low, high, exclude):\n",
    "#     temp = np.random.randint(low, high - 1)\n",
    "#     if temp == exclude:\n",
    "#       temp = temp + 1\n",
    "#     return temp\n",
    "  \n",
    "#   def forward_single_z(self, z):\n",
    "#     return self.forward_z(z[None]).squeeze()\n",
    "\n",
    "#     def generate_single_noise(self):\n",
    "#       z = torch.empty(self.z_dim).uniform_(-1,1).cuda()\n",
    "#       return self.forward_single_z(z)    \n",
    "  \n",
    "#   def make_triplet_samples(self, z, margin, r2, r3):\n",
    "#     positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "#     negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "#     return positive_sample, negative_sample\n",
    "\n",
    "#   def random_vector_surface(self, shape, r = 1.):\n",
    "#     mat = torch.randn(size=shape).cuda()\n",
    "#     norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "#     return (mat/norm) * r\n",
    "\n",
    "  \n",
    "#   def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "#     fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "#     fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "#     fraction.unsqueeze_(-1)\n",
    "#     return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "#   def make_z(self, in_shape):\n",
    "#     return torch.empty(in_shape).cuda().uniform_(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-targeted Gen\n",
    "class Gen(nn.Module):\n",
    "  def __init__(self, z_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "    super(Gen, self).__init__()\n",
    "\n",
    "    self.bs = None\n",
    "    self.z_dim = z_dim\n",
    "    self.gf_dim = gf_dim\n",
    "    self.y_dim = y_dim\n",
    "    self.df_dim = df_dim\n",
    "    self.image_shape = image_shape\n",
    "\n",
    "    self.z_ = nn.Linear(self.z_dim, self.gf_dim * 7 * 4 * 4, bias=True)\n",
    "    self.z_.bias.data.fill_(0)\n",
    "    self.BN_ = nn.BatchNorm2d(self.gf_dim * 7)\n",
    "\n",
    "    self.half = max(self.gf_dim // 2, 1) \n",
    "    self.quarter = max(self.gf_dim // 4, 1)\n",
    "    self.eighth = max(self.gf_dim // 8, 1)\n",
    "    # sixteenth = max(self.gf_dim // 16, 1)\n",
    "\n",
    "    self.CT2d_1 = deconv_layer(self.gf_dim * 8, self.gf_dim * 4, k_size = (5,5), pad = (2,2))\n",
    "    self.CT2d_2 = deconv_layer(self.gf_dim * 5, self.gf_dim * 2)    \n",
    "    self.CT2d_3 = deconv_layer(self.gf_dim * 2 + self.half, self.gf_dim * 1)\n",
    "    self.CT2d_4 = deconv_layer(self.gf_dim * 1 + self.quarter, self.gf_dim * 1)\n",
    "    self.CT2d_5 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "    self.CT2d_6 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "    self.CT2d_7 = deconv_layer(self.gf_dim * 1 + self.eighth, 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "  def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "    h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "    h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "    output = deconv_layer(h_input)\n",
    "    assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "            \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "            \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "    return output\n",
    "  \n",
    "  def forward_z(self, z):\n",
    "    self.bs = z.shape[0]\n",
    "    \n",
    "    h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "    assert h0.shape[2:] == (4, 4), \"Non-expected shape, it shoud be (4,4)\"\n",
    "\n",
    "    h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "    h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "    h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "    h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "    h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "    h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "    h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "    ksi = 10.0\n",
    "    output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "    # this coeff scales the output to be appropriate for images that are \n",
    "    # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "    # interval)\n",
    "    return output_coeff * torch.tanh(h7)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    self.bs = inputs.shape[0]\n",
    "    z = inputs.new_empty([self.bs, self.z_dim]).uniform_(-1,1).cuda()\n",
    "    p, n = self.make_triplet_samples(z, 0.1, 0.1, 2.)\n",
    "    \n",
    "    z_out = self.forward_z(z)\n",
    "    p_out = self.forward_z(p)\n",
    "    n_out = self.forward_z(n)\n",
    "    \n",
    "    return z_out, p_out, n_out, inputs, z\n",
    "#     return z_out, None, None, inputs\n",
    "  \n",
    "  def forward_single_z(self, z):\n",
    "    return self.forward_z(z[None]).squeeze()\n",
    "  \n",
    "  def generate_single_noise(self):\n",
    "    z = torch.empty(self.z_dim).uniform_(-1,1).cuda()\n",
    "    return self.forward_single_z(z)\n",
    "           \n",
    "  \n",
    "  def make_triplet_samples(self, z, margin, r2, r3):\n",
    "    positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "    negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "    return positive_sample, negative_sample\n",
    "\n",
    "  def random_vector_surface(self, shape, r = 1.):\n",
    "    mat = torch.randn(size=shape).cuda()\n",
    "    norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "    return (mat/norm) * r\n",
    "\n",
    "  \n",
    "  def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "    fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "    fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "    fraction.unsqueeze_(-1)\n",
    "    return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "  def make_z(self, in_shape):\n",
    "    return torch.empty(in_shape).cuda().uniform_(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def js_distance(x1, x2):\n",
    "  m = 0.5 * (x1 + x2)\n",
    "  return 0.5 * (F.kl_div(x1, m) + F.kl_div(x2, m))\n",
    "\n",
    "def kl_distance(x1, x2):\n",
    "  inp = torch.log(x1)\n",
    "  target = x2\n",
    "  return F.kl_div(inp, target, reduction='batchmean')\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  x1 = tensorify(x1)\n",
    "  x2 = tensorify(x2)\n",
    "  x1 = x1 / torch.sum(x1)\n",
    "  x2 = x2 / torch.sum(x2)\n",
    "  return kl_distance(x1[None], x2[None])\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  if not isinstance(x1, torch.Tensor): x1 = torch.tensor(x1)\n",
    "  if not isinstance(x2, torch.Tensor): x2 = torch.tensor(x2)\n",
    "  x1 = x1 * 100. / torch.sum(x1)\n",
    "  x2 = x2 * 100. / torch.sum(x2)\n",
    "  return torch.norm(x1 - x2, 2)\n",
    "\n",
    "def distance_from_uniform(x):\n",
    "  return distrib_distance(x, [1.] * len(x))\n",
    "\n",
    "def wasserstein_distance(x1, x2):\n",
    "  return torch.mean(x1 - x2)\n",
    "\n",
    "def l1_distance(x1, x2):\n",
    "  return F.l1_loss(x1, x2)\n",
    "\n",
    "def l2_distance(x1, x2):\n",
    "  return F.mse_loss(x1 * 10, x2 * 10)\n",
    "\n",
    "def mse_loss(x1, x2):\n",
    "  return F.mse_loss(x1, x2)\n",
    "\n",
    "def cos_distance(x1, x2, dim = 1):\n",
    "  return -1 * torch.mean(F.cosine_similarity(x1, x2, dim=dim))\n",
    "\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, distance_func, margin):\n",
    "  # max distance when using l1_distance is 2\n",
    "  # max distacne when using l2-distance is sqrt(2)\n",
    "#   print(\"anchor: \", anchor.min(), anchor.max())\n",
    "  ap_dist = distance_func(anchor, positive)\n",
    "  an_dist = distance_func(anchor, negative)\n",
    "\n",
    "  triplet_loss.call_count += 1\n",
    "  if triplet_loss.call_count % 200 == 0 : #and anchor.shape[1] == 1000:\n",
    "#     print(\"a: \", end=\"\"); print_big_vector(anchor[0])\n",
    "#     print(\"p: \", end=\"\"); print_big_vector(positive[0])\n",
    "#     print(\"n: \", end=\"\"); print_big_vector(negative[0])\n",
    "    print(\"func:{}, ap_dist: {}, an_dist: {}\".format(distance_func.__name__, ap_dist, an_dist))\n",
    "    \n",
    "  return torch.mean(F.relu(ap_dist - an_dist + margin))\n",
    "\n",
    "triplet_loss.call_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def diversity_loss(embeddings, z_s):\n",
    "#   size = z_s.shape[0]\n",
    "#   result = 0\n",
    "#   for i in range(size):\n",
    "#     for j in range(i+1, size):\n",
    "# #       a = F.cosine_similarity(embeddings[i], embeddings[j], dim = 0)\n",
    "# #       b = torch.norm(z_s[i] - z_s[j], 2, dim = 0)\n",
    "# #       print('embeddings: ')\n",
    "# #       print_big_vector(embeddings[i])\n",
    "# #       print_big_vector(embeddings[j])\n",
    "# #       print(f'a: {a}, b:{b}, multiple: {a*b}')\n",
    "#       result += F.cosine_similarity(embeddings[i], embeddings[j], dim = 0) * \\\n",
    "#                 torch.norm(z_s[i] - z_s[j], 2, dim = 0)\n",
    "#   n_pairs = (size * (size - 1)) / 2\n",
    "#   mean = result / n_pairs \n",
    "# #   print(f'result {result}, n_pairs {n_pairs}, mean {mean}')\n",
    "#   return mean\n",
    "  \n",
    "\n",
    "# normalized with shuffling\n",
    "def diversity_loss(embeddings, deranged_embeddings, z_s, deranged_z_s):\n",
    "    cos_distance = 1 - F.cosine_similarity(embeddings, deranged_embeddings)\n",
    "#     z_distance = torch.norm(z_s - deranged_z_s, 2, dim = 1)\n",
    "    z_distance = 1 - F.cosine_similarity(z_s, deranged_z_s)\n",
    "    epsilon = 1e-10\n",
    "    normalized_distance = cos_distance / (z_distance + epsilon)\n",
    "    theta = 10.\n",
    "    loss = -1 * torch.min(normalized_distance, torch.tensor(theta).cuda())\n",
    "    \n",
    "    diversity_loss.count += 1\n",
    "    if diversity_loss.count % 50 == 0:\n",
    "      print('\\ncos dist: {}\\n z_dist: {}\\n normalized: {}\\nloss: {}\\n\\n'.format(\n",
    "        cos_distance, z_distance, normalized_distance,loss))\n",
    "    return torch.mean(loss)\n",
    "  \n",
    "diversity_loss.count = 0\n",
    "  \n",
    "# # normalized with shuffling\n",
    "# def diversity_loss(embeddings, z_s):\n",
    "#     deranged_embeddings, deranged_z_s = derange(embeddings, z_s)\n",
    "#     cos_similarity = F.cosine_similarity(embeddings, deranged_embeddings)\n",
    "#     z_distance = torch.norm(z_s - deranged_z_s, dim = 1)\n",
    "#     max_possible_z_distance = 6.3246\n",
    "#     return torch.mean(cos_similarity * (z_distance/max_possible_z_distance))\n",
    "\n",
    "\n",
    "# def diversity_loss(input, target):\n",
    "# #   return -1 * torch.mean(torch.pow(f_x_a-f_x_s,2))\n",
    "#   if input.shape[0] != batch_size:\n",
    "#     print(\"input shape: \", input.shape)\n",
    "#     print(\"target shape: \", target.shape, \"\\n\\n\")\n",
    "#   return torch.mean(F.cosine_similarity(\n",
    "#     input.view([batch_size, -1]),\n",
    "#     target.view([batch_size, -1]), \n",
    "#   ))\n",
    "\n",
    "def fool_loss(input, target):\n",
    "  true_class = torch.argmax(target, dim=1).view(-1,1).long().cuda()\n",
    "  target_probabilities = input.gather(1, true_class)\n",
    "  epsilon = 1e-10\n",
    "  result =  torch.mean(-1 * torch.log(1 - target_probabilities + epsilon))\n",
    "  \n",
    "  fool_loss.call_count += 1\n",
    "  if fool_loss.call_count % 200 == 0:\n",
    "    print(\"target probs {}, loss: {}: \".format(target_probabilities, result))\n",
    "    \n",
    "  return result\n",
    "\n",
    "fool_loss.call_count = 0\n",
    "\n",
    "# def fool_loss(model_output, target_labels):\n",
    "#   target_labels = target_labels.view(-1, 1).long().cuda()\n",
    "#   target_probabilities = model_output.gather(1, target_labels)\n",
    "#   epsilon = 1e-10\n",
    "#   # highest possible fool_loss is - log(1e-10) == 23\n",
    "#   result = torch.mean(-1 * torch.log(target_probabilities + epsilon))\n",
    "  \n",
    "#   global fool_loss_count\n",
    "#   fool_loss_count += 1\n",
    "#   if fool_loss_count % 20 == 0:\n",
    "#     print(\"target probs {}, loss: {}: \".format(target_probabilities, result))\n",
    "  \n",
    "#   return result\n",
    "\n",
    "# # targeted \n",
    "# def validation(gen_output, target):\n",
    "#   perturbations, _, _, clean_images, _, _ = gen_output\n",
    "#   return validation_(perturbations, clean_images)\n",
    "\n",
    "def validation(gen_output, target):\n",
    "  perturbations, _, _, clean_images, _ = gen_output\n",
    "  return validation_(perturbations, clean_images)\n",
    "\n",
    "unfooled_histogram = np.array([0.] * 1000)\n",
    "valid_cnt = 0\n",
    "\n",
    "def validation_(perturbations, clean_images):\n",
    "  perturbed_images = clean_images + perturbations\n",
    "  benign_preds = torch.argmax(arch(clean_images), 1)\n",
    "  adversary_preds = torch.argmax(arch(perturbed_images), 1)\n",
    "\n",
    "#   print('benign, adversary, unfooled')\n",
    "#   print(benign_preds)\n",
    "#   print(adversary_preds)\n",
    "\n",
    "#   is_unfooled = (benign_preds == adversary_preds)\n",
    "#   for i , unfooled in enumerate(is_unfooled):\n",
    "#     if unfooled == 1:\n",
    "#       unfooled_histogram[benign_preds[i]] += 1\n",
    "  \n",
    "#   global valid_cnt\n",
    "#   valid_cnt += 1\n",
    "#   if valid_cnt % 10 == 0:\n",
    "#     indexed = [(i, u) for i, u in enumerate(unfooled_histogram)]\n",
    "#     summarized = list(filter(lambda x: x[1] > 0.0, indexed))\n",
    "#     percent = [(i, 100. * u / np.sum(unfooled_histogram)) for i, u in summarized]\n",
    "#     print('\\nhist: ')\n",
    "#     print(sorted(summarized, key=lambda x: x[1], reverse = True))\n",
    "#     print('\\npercent: ')\n",
    "#     print(sorted(percent, key =lambda x: x[1], reverse = True))\n",
    "#     print('\\n')\n",
    "    \n",
    "  return (benign_preds != adversary_preds).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FeatureLoss(nn.Module):\n",
    "#     def __name__(self):\n",
    "#       return \"feature_loss\"\n",
    "  \n",
    "#     def __init__(self, dis, layers, layer_weights):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # define generator here \n",
    "#         self.dis = dis\n",
    "#         self.diversity_layers = layers\n",
    "#         self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "#         self.weights = layer_weights\n",
    "#         self.metric_names = [\"fool_loss\"] #+ [f\"div_loss_{i}\" for i in range(len(layers))] #maybe Gram\n",
    "# #         self.triplet_hooks = hook_outputs([arch.m.features[4]], detach=False)\n",
    "    \n",
    "#     def make_features(self, x, clone=False):\n",
    "#         y = self.dis(x)\n",
    "#         return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "  \n",
    "#     def forward(self, inp, target):\n",
    "#       sigma_B, _, _, X_B, B_Y, z = inp\n",
    "\n",
    "#       X_A = X_B + sigma_B\n",
    "# #       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "#       A_Y, A_feat = self.make_features(X_A)\n",
    "# #       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "#       chosen_labels = z.argmax(dim=1)\n",
    "#       fooling_loss =  fool_loss(A_Y, chosen_labels)\n",
    "\n",
    "# #       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "# #       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "# #       self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "# #       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "#       self.losses = [fooling_loss]\n",
    "#       self.metrics = dict(zip(self.metric_names, [fooling_loss]))\n",
    "\n",
    "#       return sum(self.losses)\n",
    "  \n",
    "#     def add_perturbation_shuffled(self, inp, perturbation):\n",
    "# #       j = torch.randperm(inp.shape[0])\n",
    "#         j = derangement(inp.shape[0])\n",
    "#         return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derange(*args):\n",
    "  if len(args) == 0: raise ValueError('shuffle function needs atleast one argument')\n",
    "  deranged_indexes = derangement(args[0].shape[0])\n",
    "  if not all([args[0].shape[0] == arg.shape[0] for arg in args]): \n",
    "    raise ValueError('inputs to shuffle must all have the same 0th dimension')\n",
    "  return [arg[deranged_indexes] for arg in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-targeted FeatureLoss\n",
    "class FeatureLoss(nn.Module):\n",
    "    def __name__(self):\n",
    "      return \"feature_loss\"\n",
    "  \n",
    "    def __init__(self, dis, layers, layer_weights):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dis = dis\n",
    "        self.diversity_layers = layers\n",
    "        self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "        self.weights = layer_weights\n",
    "        \n",
    "#         self.metric_names = [\"fool_loss\"] + [f\"div_loss_{i}\" for i in range(len(layers))] + ['triplet_loss']# Maybe Gram\n",
    "#         self.metric_names = [\"div_loss\"] + ['triplet_loss']# Maybe Gram\n",
    "        self.metric_names = [\"fool_loss\"] + ['div_loss']# Maybe Gram\n",
    "#         self.metric_names = [\"div_loss\"]\n",
    "        self.triplet_weight = 1.\n",
    "        self.div_weight = 1.\n",
    "        self.fooling_weight = 1.\n",
    "    \n",
    "    def make_features(self, x, clone=False):\n",
    "        y = self.dis(x)\n",
    "        return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "  \n",
    "    # contrastive loss\n",
    "    def forward(self, inp, target):\n",
    "        sigma_B, sigma_pos, sigma_neg, X_B, z_B = inp\n",
    "\n",
    "        deranged_perturbations, deranged_z_s = derange(sigma_B, z_B)\n",
    "        \n",
    "        X_A = X_B + sigma_B\n",
    "        X_S = X_B + deranged_perturbations\n",
    "        X_A_pos = X_B + sigma_pos\n",
    "        X_A_neg = X_B + sigma_neg\n",
    "        \n",
    "        B_Y, _ = self.make_features(X_B)\n",
    "        A_Y, A_feat = self.make_features(X_A)\n",
    "        _, S_feat = self.make_features(X_S)\n",
    "        pos_softmax, _ = self.make_features(X_A_pos)\n",
    "        neg_softmax, _ = self.make_features(X_A_neg)\n",
    "        \n",
    "        raw_fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "        weighted_fooling_loss = self.fooling_weight * raw_fooling_loss\n",
    "      \n",
    "        raw_diversity_loss = diversity_loss(A_feat[0], S_feat[0], z_B, deranged_z_s)\n",
    "#         raw_diversity_loss = diversity_loss(A_feat[0], S_feat[0])\n",
    "        weighted_diversity_loss = raw_diversity_loss * self.div_weight\n",
    "        \n",
    "#         raw_triplet_loss = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "#         weighted_triplet_loss = raw_triplet_loss * self.triplet_weight\n",
    "    \n",
    "        self.losses = [weighted_diversity_loss]\n",
    "        raw_losses = [raw_diversity_loss]\n",
    "  \n",
    "#         self.losses = weighted_diversity_losses + [weighted_triplet_loss]\n",
    "#         raw_losses = raw_diversity_losses + [raw_triplet_loss]\n",
    "        \n",
    "        self.losses = [weighted_fooling_loss] + [weighted_diversity_loss] #+ [weighted_triplet_loss]\n",
    "        raw_losses = [raw_fooling_loss] + [raw_diversity_loss] #+ [raw_triplet_loss]\n",
    "        \n",
    "#         self.losses = [fooling_loss] + [weighted_triplet_loss]\n",
    "#         self.metrics = dict(zip(self.metric_names, [fooling_loss] + [raw_triplet_loss]))\n",
    "      \n",
    "        if len(self.metric_names) != len(raw_losses):\n",
    "          raise Exception(\"length of metric names unequals length of losses\")\n",
    "        \n",
    "        self.metrics = dict(zip(self.metric_names, raw_losses))\n",
    "        return sum(self.losses)\n",
    "  \n",
    "  \n",
    "  \n",
    "# #     triplet loss\n",
    "#     def forward(self, inp, target):\n",
    "#         sigma_B, sigma_pos, sigma_neg, X_B, _ = inp\n",
    "\n",
    "#         X_A = self.add_perturbation(X_B, sigma_B) \n",
    "#         X_A_pos = self.add_perturbation(X_B, sigma_pos)\n",
    "#         X_A_neg = self.add_perturbation(X_B, sigma_neg) \n",
    "#         X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "        \n",
    "# #         B_Y, _ = self.make_features(X_B)\n",
    "#         A_Y, A_feat = self.make_features(X_A)\n",
    "# #         _, S_feat = self.make_features(X_S)\n",
    "#         pos_softmax, _ = self.make_features(X_A_pos)\n",
    "#         neg_softmax, _ = self.make_features(X_A_neg)\n",
    "        \n",
    "# #         raw_fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "# #         weighted_fooling_loss = self.fooling_weight * raw_fooling_loss\n",
    "      \n",
    "#         raw_diversity_losses = [diversity_loss(a_f, s_f, sigma_B, ) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "#         weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "#         raw_triplet_loss = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "#         weighted_triplet_loss = raw_triplet_loss * self.triplet_weight\n",
    "    \n",
    "#         self.losses = weighted_diversity_losses + [weighted_triplet_loss]\n",
    "#         raw_losses = raw_diversity_losses + [raw_triplet_loss]\n",
    "        \n",
    "# #         self.losses = [weighted_fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss]\n",
    "# #         raw_losses = [raw_fooling_loss] + raw_diversity_losses + [raw_triplet_loss]\n",
    "        \n",
    "# #         self.losses = [fooling_loss] + [weighted_triplet_loss]\n",
    "# #         self.metrics = dict(zip(self.metric_names, [fooling_loss] + [raw_triplet_loss]))\n",
    "      \n",
    "#         if len(self.metric_names) != len(raw_losses):\n",
    "#           raise Exception(\"length of metric names unequals length of losses\")\n",
    "        \n",
    "#         self.metrics = dict(zip(self.metric_names, raw_losses))\n",
    "#         return sum(self.losses)\n",
    "\n",
    "\n",
    "#     #use two types of triplet losses\n",
    "#     def forward(self, inp, target):\n",
    "#       sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "#       X_A = self.add_perturbation(X_B, sigma_B) \n",
    "#       X_A_pos = self.add_perturbation(X_B, sigma_pos)\n",
    "#       X_A_neg = self.add_perturbation(X_B, sigma_neg) \n",
    "\n",
    "#       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "#       B_Y, _ = self.make_features(X_B)\n",
    "#       A_Y, A_feat = self.make_features(X_A)\n",
    "#       _, S_feat = self.make_features(X_S)\n",
    "#       pos_softmax, _ = self.make_features(X_A_pos)\n",
    "#       neg_softmax, _ = self.make_features(X_A_neg)\n",
    "\n",
    "#       fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "#       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "#       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "      \n",
    "#       raw_triplet_loss_sm = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "#       weighted_triplet_loss_sm = raw_triplet_loss_sm * self.triplet_weight_sm\n",
    "      \n",
    "#       raw_triplet_loss_noise = triplet_loss(sigma_B, sigma_pos, sigma_neg, l2_distance, 5.)\n",
    "#       weighted_triplet_loss_noise = raw_triplet_loss_noise * self.triplet_weight_noise\n",
    "\n",
    "#       self.losses = [fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss_sm, weighted_triplet_loss_noise] \n",
    "#       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses + [weighted_triplet_loss_sm, weighted_triplet_loss_noise]))\n",
    "\n",
    "#       return sum(self.losses)\n",
    "\n",
    "#     # just fooling and diversity\n",
    "#     def forward(self, inp, target):\n",
    "#       sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "#       X_A = self.add_perturbation(X_B, sigma_B) \n",
    "\n",
    "#       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "#       B_Y, _ = self.make_features(X_B)\n",
    "#       A_Y, A_feat = self.make_features(X_A)\n",
    "#       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "#       fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "#       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "#       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "#       self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "#       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "#       return sum(self.losses)\n",
    "  \n",
    " \n",
    "    def add_perturbation_shuffled(self, inp, perturbation):\n",
    "      j = derangement(inp.shape[0])\n",
    "      return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def produce_summary(root_folder, n_files):\n",
    "  def writeline(file, values, fmt_string):\n",
    "    file.write(', '.join(fmt_string.format(v) for v in values) + '\\n')\n",
    "  \n",
    "  last_rows = []\n",
    "  for i in range(n_files):\n",
    "    prefix = '/root/Derakhshani/adversarial/textual_notes/CSVs'\n",
    "    df = pd.read_csv(\"{}/{}/{}.csv\".format(prefix, root_folder, i))\n",
    "    last_rows.append(df.iloc[-1][1:-1].values.tolist())\n",
    "  \n",
    "  last_rows = np.array(last_rows)\n",
    "  \n",
    "  labels = list(df.columns[1:-1])\n",
    "  means = np.mean(last_rows, axis=0).tolist()\n",
    "  outfile = open('{}/{}/summary.txt'.format(prefix, root_folder), 'w+')\n",
    "  outfile.write('means: \\n')\n",
    "  writeline(outfile, labels, '{: >11}')\n",
    "  writeline(outfile, means, '{: >11.3}')\n",
    "  outfile.write('\\n')\n",
    "      \n",
    "  operations = []\n",
    "  for column in df.columns[1:-1]:\n",
    "    if column in ['train_loss', 'valid_loss', 'fool_loss', 'triplet_loss'] or column[:8] == 'div_loss':\n",
    "      operations.append('min')\n",
    "    elif column in ['validation', 'div_metric']:\n",
    "      operations.append('max')\n",
    "    else:\n",
    "      raise ValueError('column {} is not recognized'.format(column))\n",
    "    \n",
    "  results = []\n",
    "  indexes = []\n",
    "  \n",
    "  for i in range(len(operations)):\n",
    "    values = last_rows[:, i]\n",
    "    if operations[i] == 'max': operation = np.max\n",
    "    elif operations[i] == 'min': operation = np.min\n",
    "    result = operation(values)\n",
    "    results.append(result)\n",
    "    indexes.append(values.tolist().index(result))\n",
    "  \n",
    "  outfile.write('bests: \\n')\n",
    "  writeline(outfile, labels, '{: >11}')\n",
    "  writeline(outfile, operations, '{: >11}')\n",
    "  writeline(outfile, results, '{: >11.3}')\n",
    "  writeline(outfile, indexes, '{: >11}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils import dir_util \n",
    "\n",
    "def investigate_initial_settings(n_settings, n_epochs, lr, wd, results_dir):\n",
    "  os.mkdir(env.get_csv_dir() + results_dir)\n",
    "  os.mkdir(env.get_models_dir() + results_dir)\n",
    "  \n",
    "  for setting_ind in range(n_settings):\n",
    "    print(f\"investigation no: {setting_ind}\")\n",
    "    learn = None; gen = None; gc.collect()\n",
    "    gen = Gen(z_dim = z_dim)\n",
    "    init_cnn(gen, True)\n",
    "    div_metric = DiversityMetric(10, 95)\n",
    "    \n",
    "    tmp_csv_filename =  env.temp_csv_path + '/' + results_dir + '/' + str(setting_ind)\n",
    "    csv_logger = partial(ImmediateCSVLogger, filename=tmp_csv_filename)\n",
    "    \n",
    "    learn = Learner(data, gen, loss_func = feat_loss, metrics=[validation, div_metric], \n",
    "                    model_dir = env.get_learner_models_dir(), callback_fns=[LossMetrics, csv_logger])\n",
    "    div_metric.set_learner(learn)\n",
    "    \n",
    "    saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=model.__name__ + \"-best\")\n",
    "    saver_every_epoch = SaveModelCallback(learn, every='epoch', name=model.__name__)\n",
    "\n",
    "    learn.fit(n_epochs, lr=lr, wd = wd, callbacks=[saver_best, saver_every_epoch])\n",
    "    \n",
    "    shutil.copyfile(tmp_csv_filename + \".csv\", env.get_csv_dir() + results_dir + '/' + str(setting_ind) + '.csv')\n",
    "    \n",
    "    model_dest = env.get_models_dir() + results_dir + '/' + str(setting_ind)\n",
    "    os.mkdir(model_dest)\n",
    "    dir_util.copy_tree(env.data_path/env.get_learner_models_dir(), model_dest)\n",
    "    shutil.rmtree(env.data_path/env.get_learner_models_dir())  \n",
    "    \n",
    "  produce_summary(results_dir, n_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_perturbations(learn, n_perturbations):\n",
    "  initial_training_mode = learn.model.training\n",
    "  gen = learn.model.eval()\n",
    "  perturbations = [gen.generate_single_noise() for _ in range(n_perturbations)]\n",
    "  learn.model.train(initial_training_mode)  \n",
    "  return perturbations\n",
    "\n",
    "def compute_prediction_histogram(learn, perturbation, verbose=False):\n",
    "  pred_hist = [0] * 1000\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 and verbose: print (\"at batch no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbation[None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      pred_hist[pred] += 1\n",
    "  return pred_hist\n",
    "\n",
    "\n",
    "def compute_mean_prediction_histogram(learn, perturbations, verbose=False):\n",
    "  pred_hist = torch.tensor([0] * 1000).detach_()\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 5 == 0 and verbose: print(f\"at batch no {batch_no}\")\n",
    "    for j, perturbation in enumerate(perturbations):\n",
    "      perturbed_batch = batch + perturbation[None]\n",
    "      preds = arch(perturbed_batch).argmax(1)\n",
    "      for pred in preds:\n",
    "        pred_hist[pred] += 1\n",
    "  pred_hist = pred_hist.float() / len(perturbations)\n",
    "  return pred_hist.tolist()\n",
    "\n",
    "\n",
    "def classes_needed_to_reach(percentage, hist):\n",
    "  hist_sum = np.sum(hist)\n",
    "  indexed_hist = [(i, hist_element) for i,hist_element in  \n",
    "                          enumerate(hist)]\n",
    "  sorted_hist = sorted(indexed_hist, key=lambda x: x[1], reverse = True)\n",
    "  \n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = sorted_hist[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / hist_sum) * 100.\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, sorted_hist\n",
    "\n",
    "def diversity(learn, n_perturbations, percentage = 95, verbose = True):\n",
    "  pred_histogram = compute_mean_prediction_histogram(\n",
    "      learn, generate_perturbations(learn, n_perturbations), verbose\n",
    "  )\n",
    "  print(\"finished creating the prediction histogram\")\n",
    "\n",
    "  return classes_needed_to_reach(95, pred_histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiversityMetric(Callback):\n",
    "  def __init__(self, n_perturbations, percentage):\n",
    "    super().__init__()\n",
    "    self.name = \"div_metric\"\n",
    "    self.average_over = 4\n",
    "    self.n_perturbations = n_perturbations\n",
    "    self.percentage = percentage\n",
    "    self.learn = None\n",
    "  \n",
    "  def set_learner(self, learn):\n",
    "    self.learn = learn\n",
    "    \n",
    "  def on_epoch_begin(self, **kwargs):\n",
    "    global learn\n",
    "    self.perturbations_list = [generate_perturbations(self.learn, self.n_perturbations) \\\n",
    "                          for _ in range(self.average_over)]\n",
    "    self.pred_hist_list = [torch.tensor([0] * 1000).detach_() for _ in range(self.average_over)]\n",
    "    \n",
    "  def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "    images = last_output[3]; assert(images.shape[1:] == (3,224, 224))\n",
    "    for perturbations, pred_hist in zip(self.perturbations_list, self.pred_hist_list):\n",
    "      for j, perturbation in enumerate(perturbations):\n",
    "        perturbed_batch = images + perturbation[None]\n",
    "        preds = arch(perturbed_batch).argmax(1)\n",
    "        for pred in preds:\n",
    "          pred_hist[pred] += 1\n",
    "  \n",
    "  def on_epoch_end(self, last_metrics, **kwargs):\n",
    "    for i in range(len(self.pred_hist_list)):\n",
    "      self.pred_hist_list[i] = (self.pred_hist_list[i].float() / self.n_perturbations).tolist()\n",
    "    \n",
    "    div_metric_list = [classes_needed_to_reach(self.percentage, pred_hist)[0] \\\n",
    "                          for pred_hist in self.pred_hist_list]\n",
    "    div_metric = np.mean(div_metric_list)\n",
    "    return add_metrics(last_metrics, div_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tltucTv2ep9-"
   },
   "outputs": [],
   "source": [
    "# mode = \"sanity_check\"\n",
    "# mode = \"normal\"\n",
    "mode = \"div_metric_calc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50\n",
    "# model = models.resnet152\n",
    "# model = models.vgg16_bn\n",
    "# model = torchvision.models.googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SO1h55obXzOv",
    "outputId": "54414cc5-84d5-4f45-ecab-87374a58dd33"
   },
   "outputs": [],
   "source": [
    "if mode == \"normal\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "elif mode == \"sanity_check\":\n",
    "  env.load_dataset('dataset_sanity_check_small', 'dataset_sanity_check_small')  \n",
    "  env.set_data_path('dataset_sanity_check_small')\n",
    "elif mode == \"div_metric_calc\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "  env.load_test_dataset(str(env.data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "koaQZmjMom7w"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "gpu_flag = True\n",
    "nag_util.batch_size = batch_size; nag_util.gpu_flag = gpu_flag;\n",
    "tfms = get_transforms(do_flip=False, max_rotate=0)\n",
    "data = (ImageList.from_folder(env.data_path)\n",
    "        .split_by_folder(valid=('test' if mode == 'div_metric_calc' else 'valid'))\n",
    "        .label_from_folder()\n",
    "        .transform(tfms, size=224)\n",
    "        .databunch(bs=batch_size, num_workers=1)\n",
    "        .normalize(imagenet_stats))\n",
    "\n",
    "# data.show_batch(rows=2, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDBkRV8yovwV"
   },
   "outputs": [],
   "source": [
    "z_dim = 10\n",
    "arch = SoftmaxWrapper(model(pretrained=True).cuda().eval())\n",
    "nag_util.arch = arch\n",
    "requires_grad(arch, False)\n",
    "\n",
    "# vgg:\n",
    "# layers = []\n",
    "# blocks = [i-1 for i,o in enumerate(children(arch.features)) if isinstance(o, nn.MaxPool2d)]\n",
    "# layers = [arch.features[i] for i in blocks]\n",
    "# layer_weights = [1] * len(layers)\n",
    "\n",
    "layers = [\n",
    "    arch.softmax\n",
    "]\n",
    "\n",
    "layer_weights = [1.] * len(layers)\n",
    "\n",
    "# inception:\n",
    "# layers = [\n",
    "#     arch.Conv2d_1a_3x3,\n",
    "#     arch.Mixed_6e,\n",
    "#     arch.Mixed_7a,\n",
    "#     arch.fc    \n",
    "# ]\n",
    "# layer_weights = [1.0/4.0] * len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qd9gXUy_ovww"
   },
   "outputs": [],
   "source": [
    "feat_loss = FeatureLoss(arch, layers, layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfZKdYD2MSdi"
   },
   "outputs": [],
   "source": [
    "# env.save_filename = 'vgg16_35' \n",
    "env.save_filename = 'resnet50_68x'\n",
    "# env.save_filename = 'vgg16_32'\n",
    "\n",
    "if Path(env.get_csv_path() + '.csv').exists(): raise FileExistsError(\"csv_path already exists\")\n",
    "if Path(env.get_models_path()).exists(): raise FileExistsError(\"models_path already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9J20CBLS8S9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models_directory returned is:  models/300\n"
     ]
    }
   ],
   "source": [
    "learn = None; gen = None; gc.collect()\n",
    "csv_logger = partial(ImmediateCSVLogger, filename= env.temp_csv_path + '/' + env.save_filename)\n",
    "gen = Gen(z_dim=z_dim)\n",
    "init_cnn(gen, True)\n",
    "div_metric = DiversityMetric(10, 95)\n",
    "\n",
    "learn = Learner(data, gen, loss_func = feat_loss, model_dir = env.get_learner_models_dir(),\n",
    "                metrics=[validation, div_metric], callback_fns=[LossMetrics, csv_logger])\n",
    "\n",
    "div_metric.set_learner(learn)\n",
    "\n",
    "# learn = Learner(data, Gen(z_dim=10), loss_func = feat_loss, metrics=[validation], callback_fns=LossMetrics, opt_func = optim.SGD)\n",
    "# learn = Learner(data, Gen(z_dim=z_dim), loss_func = feat_loss, metrics=[validation], callback_fns=[LossMetrics, DiversityWeightsScheduler])\n",
    "\n",
    "# load_starting_point(learn, model.__name__, z_dim)\n",
    "# random_seed(42, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wOZYzOHDEdB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GeneralRelu. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type FeatureLoss. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Gen. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type deconv_layer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (9000 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "n02454379,n02454379,n02454379,n02454379,n02454379\n",
       "Path: /root/Derakhshani/adversarial/datasets/dataset;\n",
       "\n",
       "Valid: LabelList (50000 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "n02454379,n02454379,n02454379,n02454379,n02454379\n",
       "Path: /root/Derakhshani/adversarial/datasets/dataset;\n",
       "\n",
       "Test: None, model=Gen(\n",
       "  (z_): Linear(in_features=10, out_features=7168, bias=True)\n",
       "  (BN_): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (CT2d_1): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (BN2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_2): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(320, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_3): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(160, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_4): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(80, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_5): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_6): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_7): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (BN2d): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FeatureLoss(\n",
       "  (dis): SoftmaxWrapper(\n",
       "    (m): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax()\n",
       "  )\n",
       "), metrics=[<function validation at 0x7fcd9003cae8>, DiversityMetric\n",
       "n_perturbations: 10\n",
       "percentage: 95], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/Derakhshani/adversarial/datasets/dataset'), model_dir='models/300', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), <class 'fastai.callbacks.loss_metrics.LossMetrics'>, functools.partial(<class 'nag_util.ImmediateCSVLogger'>, filename='/root/Derakhshani/adversarial/temp/resnet50_68x')], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): Linear(in_features=10, out_features=7168, bias=True)\n",
       "  (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): GeneralRelu()\n",
       "  (5): ConvTranspose2d(320, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): GeneralRelu()\n",
       "  (8): ConvTranspose2d(160, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): GeneralRelu()\n",
       "  (11): ConvTranspose2d(80, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): GeneralRelu()\n",
       "  (14): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (16): GeneralRelu()\n",
       "  (17): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): GeneralRelu()\n",
       "  (20): ConvTranspose2d(72, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (21): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")], add_time=True, silent=None)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !cp \"/content/gdrive/My Drive/DL/models/vgg16_12-last.pth\"  \"/content/\"\n",
    "# learn.load('/content/vgg16_12-last')\n",
    "\n",
    "# load_filename = 'resnet50-11_39'\n",
    "# load_filename = 'resnet50_startpoint_0'\n",
    "load_filename = 'resnet50_60/resnet50_60_79'\n",
    "# load_filename = 'investigate_resnet50_7/7/resnet50_3'\n",
    "# load_filename = 'vgg16_34/vgg16_34_9'\n",
    "# load_filename = 'vgg16_12-last'\n",
    "\n",
    "learn.load('/root/Derakhshani/adversarial/models/' + load_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the selected settings are : \n",
      "\tmode: div_metric_calc \n",
      "\tnetw-under-attack: resnet50 \n",
      "\tload filename: resnet50_60/resnet50_60_79 \n",
      "      \tsave filename: resnet50_68x\n",
      "\tmetric names: ['fool_loss', 'div_loss']\n",
      "\n",
      "please MAKE SURE that the config is correct.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  load_filename\n",
    "except NameError:\n",
    "  load_filename = None\n",
    "\n",
    "print(\"the selected settings are : \")\n",
    "print('''\\tmode: {} \\n\\tnetw-under-attack: {} \\n\\tload filename: {} \n",
    "      \\tsave filename: {}\\n\\tmetric names: {}\\n'''.format(\n",
    "      mode, model.__name__, load_filename , env.save_filename, feat_loss.metric_names\n",
    "))\n",
    "print(\"please MAKE SURE that the config is correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find(1e-6, 100)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dir = 'investigate_vgg16_0'\n",
    "# investigate_initial_settings(7, 3, lr = 1e-2, wd = 0.0, results_dir = results_dir)\n",
    "# shutil.rmtree(env.get_models_dir() + results_dir)\n",
    "# shutil.rmtree(env.get_csv_dir() + results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WeightTuner(LearnerCallback):\n",
    "#   def __init__(self, learn: Learner):\n",
    "#     super().__init__(learn)\n",
    "#     self.fooling_weight = learn.loss_func.fooling_weight\n",
    "    \n",
    "#   def on_epoch_begin(self, **kwargs):\n",
    "#     fooling_rate = \n",
    "#     print(\"by how much to increase the fooling_weight? (current value: {})\".format(fooling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoolingWeightScheduler(LearnerCallback):\n",
    "  def __init__(self, learn: Learner, fooling_loss_index):\n",
    "    super().__init__(learn)\n",
    "    self.fooling_loss_index = fooling_loss_index\n",
    "    self.weights_history = []\n",
    "    self.fooling_loss_history = []\n",
    "  \n",
    "  def on_epoch_end(self, last_metrics, **kwargs):\n",
    "    # history keeping\n",
    "    self.weights_history.append((kwargs['epoch'], self.learn.loss_func.fooling_weight))\n",
    "    \n",
    "    # the actualy functionality\n",
    "    fooling_loss = last_metrics[self.fooling_loss_index]\n",
    "    if fooling_loss < -2 or fooling_loss > 2:\n",
    "      raise ValueError('fooling loss is outside the range [-2, 2]. the fooling index is probably wrong.')\n",
    "    \n",
    "    self.fooling_loss_history.append(fooling_loss)\n",
    "    \n",
    "    if len(self.weights_history) < 2:\n",
    "      return\n",
    "    \n",
    "    if self.fooling_loss_history[-1] > self.fooling_loss_history[-2]:\n",
    "      self.learn.loss_func.fooling_weight += 0.3    \n",
    "      print('fooling weight increased to {} at the end of epoch {}'.format(\n",
    "        self.learn.loss_func.fooling_weight, kwargs['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='11' class='' max='80', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      13.75% [11/80 1:02:06<6:29:35]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>div_metric</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>div_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.666423</td>\n",
       "      <td>0.849319</td>\n",
       "      <td>0.548000</td>\n",
       "      <td>775.000000</td>\n",
       "      <td>1.404034</td>\n",
       "      <td>-0.554715</td>\n",
       "      <td>05:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.376944</td>\n",
       "      <td>0.589454</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>651.750000</td>\n",
       "      <td>1.075393</td>\n",
       "      <td>-0.485939</td>\n",
       "      <td>05:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.163024</td>\n",
       "      <td>0.139828</td>\n",
       "      <td>0.737000</td>\n",
       "      <td>639.750000</td>\n",
       "      <td>0.586197</td>\n",
       "      <td>-0.446369</td>\n",
       "      <td>05:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.044765</td>\n",
       "      <td>0.205146</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>418.250000</td>\n",
       "      <td>0.615546</td>\n",
       "      <td>-0.410399</td>\n",
       "      <td>05:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.103054</td>\n",
       "      <td>0.268080</td>\n",
       "      <td>0.765000</td>\n",
       "      <td>336.500000</td>\n",
       "      <td>0.525779</td>\n",
       "      <td>-0.415433</td>\n",
       "      <td>05:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.164518</td>\n",
       "      <td>0.232296</td>\n",
       "      <td>0.776000</td>\n",
       "      <td>325.250000</td>\n",
       "      <td>0.515474</td>\n",
       "      <td>-0.437820</td>\n",
       "      <td>05:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.118574</td>\n",
       "      <td>0.236519</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>315.750000</td>\n",
       "      <td>0.532435</td>\n",
       "      <td>-0.455646</td>\n",
       "      <td>05:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.164487</td>\n",
       "      <td>0.374958</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>323.250000</td>\n",
       "      <td>0.496082</td>\n",
       "      <td>-0.418773</td>\n",
       "      <td>05:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.149940</td>\n",
       "      <td>0.307777</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>0.486388</td>\n",
       "      <td>-0.470443</td>\n",
       "      <td>05:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.234321</td>\n",
       "      <td>0.336037</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>316.250000</td>\n",
       "      <td>0.483661</td>\n",
       "      <td>-0.437821</td>\n",
       "      <td>05:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.168844</td>\n",
       "      <td>0.366615</td>\n",
       "      <td>0.782000</td>\n",
       "      <td>312.500000</td>\n",
       "      <td>0.513291</td>\n",
       "      <td>-0.454650</td>\n",
       "      <td>05:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='302' class='' max='562', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      53.74% [302/562 02:11<01:53 0.2865]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cos dist: tensor([9.4222e-01, 1.1514e-01, 1.4918e-02, 9.0092e-01, 0.0000e+00, 9.9912e-01,\n",
      "        6.8545e-06, 2.0630e-01, 1.4759e-01, 1.6093e-05, 7.0453e-05, 1.3433e-02,\n",
      "        3.3414e-02, 6.5373e-03, 4.4232e-01, 1.0651e-04], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([0.6435, 1.3517, 0.7942, 1.1761, 1.2265, 1.0322, 0.8930, 0.3955, 0.9954,\n",
      "        1.2487, 1.1668, 0.8753, 0.9585, 0.3846, 1.3378, 0.4925],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([1.4641e+00, 8.5187e-02, 1.8784e-02, 7.6602e-01, 0.0000e+00, 9.6798e-01,\n",
      "        7.6756e-06, 5.2164e-01, 1.4828e-01, 1.2888e-05, 6.0380e-05, 1.5346e-02,\n",
      "        3.4860e-02, 1.6999e-02, 3.3064e-01, 2.1629e-04], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-1.4641e+00, -8.5187e-02, -1.8784e-02, -7.6602e-01, -0.0000e+00,\n",
      "        -9.6798e-01, -7.6756e-06, -5.2164e-01, -1.4828e-01, -1.2888e-05,\n",
      "        -6.0380e-05, -1.5346e-02, -3.4860e-02, -1.6999e-02, -3.3064e-01,\n",
      "        -2.1629e-04], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "cos dist: tensor([0.8432, 0.5392, 0.9999, 0.0380, 0.0390, 0.9998, 0.0673, 0.4863, 0.9999,\n",
      "        0.0166, 0.1275, 0.0127, 0.0038, 0.0341, 0.3297, 0.9999],\n",
      "       device='cuda:0', grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([0.2471, 0.7308, 0.9839, 1.1043, 0.8767, 1.4066, 0.9184, 0.9076, 0.9923,\n",
      "        1.0847, 1.5033, 1.7183, 0.7659, 1.0334, 1.5982, 1.0902],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([3.4117, 0.7378, 1.0163, 0.0344, 0.0445, 0.7108, 0.0733, 0.5358, 1.0076,\n",
      "        0.0153, 0.0848, 0.0074, 0.0050, 0.0330, 0.2063, 0.9172],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "loss: tensor([-3.4117, -0.7378, -1.0163, -0.0344, -0.0445, -0.7108, -0.0733, -0.5358,\n",
      "        -1.0076, -0.0153, -0.0848, -0.0074, -0.0050, -0.0330, -0.2063, -0.9172],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "cos dist: tensor([3.0717e-01, 6.1658e-01, 8.6691e-01, 1.2648e-04, 5.2519e-03, 1.0887e-02,\n",
      "        1.6600e-03, 9.3123e-01, 9.4259e-01, 1.2985e-02, 2.0239e-01, 2.8289e-01,\n",
      "        1.0000e+00, 6.0534e-04, 3.1074e-01, 5.8514e-01], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([0.5704, 0.5960, 0.2294, 0.4826, 1.1397, 1.0958, 0.4826, 1.4301, 0.8303,\n",
      "        0.6652, 0.8303, 0.9250, 1.0260, 0.9358, 1.0260, 1.0226],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([5.3856e-01, 1.0345e+00, 3.7783e+00, 2.6207e-04, 4.6081e-03, 9.9350e-03,\n",
      "        3.4397e-03, 6.5117e-01, 1.1352e+00, 1.9520e-02, 2.4375e-01, 3.0581e-01,\n",
      "        9.7467e-01, 6.4685e-04, 3.0287e-01, 5.7221e-01], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-5.3856e-01, -1.0345e+00, -3.7783e+00, -2.6207e-04, -4.6081e-03,\n",
      "        -9.9350e-03, -3.4397e-03, -6.5117e-01, -1.1352e+00, -1.9520e-02,\n",
      "        -2.4375e-01, -3.0581e-01, -9.7467e-01, -6.4685e-04, -3.0287e-01,\n",
      "        -5.7221e-01], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "target probs tensor([[1.4104e-01],\n",
      "        [8.7182e-01],\n",
      "        [7.7893e-04],\n",
      "        [5.2224e-01],\n",
      "        [9.9947e-03],\n",
      "        [2.8668e-01],\n",
      "        [9.7159e-01],\n",
      "        [8.2132e-01],\n",
      "        [9.7178e-02],\n",
      "        [6.4472e-01],\n",
      "        [1.8559e-05],\n",
      "        [3.7528e-04],\n",
      "        [2.3130e-01],\n",
      "        [2.2765e-01],\n",
      "        [4.5423e-01],\n",
      "        [8.1765e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.7839415669441223: \n",
      "\n",
      "cos dist: tensor([0.6811, 0.5254, 0.9937, 0.0987, 0.4770, 0.1798, 0.0016, 0.9999, 0.3109,\n",
      "        0.1060, 0.2161, 0.0071, 0.0966, 0.0195, 0.2959, 0.9220],\n",
      "       device='cuda:0', grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([1.5382, 1.1127, 0.9740, 1.3295, 1.6141, 1.6336, 1.6336, 0.9699, 0.9140,\n",
      "        0.9770, 1.3860, 0.9334, 0.8931, 1.0896, 0.9112, 0.7182],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([4.4279e-01, 4.7222e-01, 1.0202e+00, 7.4259e-02, 2.9553e-01, 1.1004e-01,\n",
      "        9.5537e-04, 1.0310e+00, 3.4016e-01, 1.0845e-01, 1.5590e-01, 7.5788e-03,\n",
      "        1.0815e-01, 1.7870e-02, 3.2471e-01, 1.2838e+00], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-4.4279e-01, -4.7222e-01, -1.0202e+00, -7.4259e-02, -2.9553e-01,\n",
      "        -1.1004e-01, -9.5537e-04, -1.0310e+00, -3.4016e-01, -1.0845e-01,\n",
      "        -1.5590e-01, -7.5788e-03, -1.0815e-01, -1.7870e-02, -3.2471e-01,\n",
      "        -1.2838e+00], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "cos dist: tensor([0.7399, 0.9997, 0.8856, 0.9788, 0.9999, 0.1129, 0.2247, 0.9888, 0.0015,\n",
      "        0.0051, 0.1672, 0.9571, 0.9934, 0.9998, 0.9861, 0.9749],\n",
      "       device='cuda:0', grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([0.7697, 1.5055, 0.6506, 0.8588, 1.1214, 0.5173, 1.2504, 1.5055, 0.9815,\n",
      "        1.4786, 0.8329, 1.6111, 0.9053, 1.3983, 0.4973, 1.5543],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([9.6128e-01, 6.6407e-01, 1.3612e+00, 1.1397e+00, 8.9165e-01, 2.1836e-01,\n",
      "        1.7968e-01, 6.5679e-01, 1.4897e-03, 3.4175e-03, 2.0074e-01, 5.9407e-01,\n",
      "        1.0973e+00, 7.1499e-01, 1.9828e+00, 6.2725e-01], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-9.6128e-01, -6.6407e-01, -1.3612e+00, -1.1397e+00, -8.9165e-01,\n",
      "        -2.1836e-01, -1.7968e-01, -6.5679e-01, -1.4897e-03, -3.4175e-03,\n",
      "        -2.0074e-01, -5.9407e-01, -1.0973e+00, -7.1499e-01, -1.9828e+00,\n",
      "        -6.2725e-01], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "cos dist: tensor([8.7611e-01, 9.8547e-01, 9.8658e-01, 6.1584e-01, 6.0105e-04, 2.7130e-02,\n",
      "        6.8700e-01, 9.9888e-01, 4.9122e-01, 1.7881e-07, 7.1489e-01, 8.9920e-01,\n",
      "        6.9401e-01, 4.4907e-03, 9.4539e-04, 9.9975e-01], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([1.2224, 1.3534, 0.6110, 0.3449, 1.2628, 0.7071, 0.6163, 0.8562, 0.8280,\n",
      "        0.7857, 0.4912, 0.7962, 1.0955, 0.4372, 0.7857, 1.0003],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([7.1671e-01, 7.2814e-01, 1.6148e+00, 1.7857e+00, 4.7596e-04, 3.8370e-02,\n",
      "        1.1148e+00, 1.1667e+00, 5.9327e-01, 2.2759e-07, 1.4553e+00, 1.1294e+00,\n",
      "        6.3353e-01, 1.0271e-02, 1.2033e-03, 9.9945e-01], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-7.1671e-01, -7.2814e-01, -1.6148e+00, -1.7857e+00, -4.7596e-04,\n",
      "        -3.8370e-02, -1.1148e+00, -1.1667e+00, -5.9327e-01, -2.2759e-07,\n",
      "        -1.4553e+00, -1.1294e+00, -6.3353e-01, -1.0271e-02, -1.2033e-03,\n",
      "        -9.9945e-01], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "cos dist: tensor([9.9240e-01, 6.0118e-01, 4.0894e-03, 9.9997e-01, 1.7343e-01, 9.3862e-01,\n",
      "        6.4749e-01, 8.4686e-01, 5.9605e-08, 5.3644e-05, 7.9612e-01, 9.5464e-01,\n",
      "        9.0404e-01, 1.1145e-01, 9.9996e-01, 9.9991e-01], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([1.3836, 0.5870, 1.1152, 0.2248, 0.5789, 1.0059, 0.9554, 0.8289, 0.8779,\n",
      "        0.8018, 1.1857, 0.8307, 0.5832, 1.1840, 1.1891, 1.5863],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([7.1727e-01, 1.0241e+00, 3.6671e-03, 4.4492e+00, 2.9961e-01, 9.3311e-01,\n",
      "        6.7771e-01, 1.0216e+00, 6.7894e-08, 6.6906e-05, 6.7143e-01, 1.1491e+00,\n",
      "        1.5501e+00, 9.4128e-02, 8.4097e-01, 6.3035e-01], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-7.1727e-01, -1.0241e+00, -3.6671e-03, -4.4492e+00, -2.9961e-01,\n",
      "        -9.3311e-01, -6.7771e-01, -1.0216e+00, -6.7894e-08, -6.6906e-05,\n",
      "        -6.7143e-01, -1.1491e+00, -1.5501e+00, -9.4128e-02, -8.4097e-01,\n",
      "        -6.3035e-01], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "target probs tensor([[7.9498e-03],\n",
      "        [2.7288e-03],\n",
      "        [1.2846e-05],\n",
      "        [1.7708e-02],\n",
      "        [8.7816e-07],\n",
      "        [9.2191e-01],\n",
      "        [3.5336e-04],\n",
      "        [2.5673e-01],\n",
      "        [2.0240e-01],\n",
      "        [2.2301e-01],\n",
      "        [1.9653e-01],\n",
      "        [3.9421e-01],\n",
      "        [9.9433e-01],\n",
      "        [3.1133e-01],\n",
      "        [3.3798e-01],\n",
      "        [3.0931e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.6501689553260803: \n",
      "\n",
      "cos dist: tensor([9.8748e-01, 8.3681e-01, 9.2781e-01, 5.0375e-01, 3.2534e-01, 6.3586e-01,\n",
      "        3.1439e-02, 4.7503e-01, 7.9230e-01, 3.6743e-02, 2.8715e-03, 5.2135e-02,\n",
      "        2.3842e-06, 1.6231e-01, 9.6690e-01, 6.8016e-02], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([0.7269, 0.8271, 1.2427, 0.6933, 1.3878, 0.9069, 0.9475, 1.0938, 0.9471,\n",
      "        0.4490, 1.2410, 1.1526, 1.4996, 1.1053, 1.4314, 0.8972],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([1.3584e+00, 1.0118e+00, 7.4662e-01, 7.2655e-01, 2.3443e-01, 7.0113e-01,\n",
      "        3.3180e-02, 4.3428e-01, 8.3658e-01, 8.1833e-02, 2.3137e-03, 4.5232e-02,\n",
      "        1.5899e-06, 1.4685e-01, 6.7549e-01, 7.5806e-02], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-1.3584e+00, -1.0118e+00, -7.4662e-01, -7.2655e-01, -2.3443e-01,\n",
      "        -7.0113e-01, -3.3180e-02, -4.3428e-01, -8.3658e-01, -8.1833e-02,\n",
      "        -2.3137e-03, -4.5232e-02, -1.5899e-06, -1.4685e-01, -6.7549e-01,\n",
      "        -7.5806e-02], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cos dist: tensor([7.9735e-01, 4.4107e-06, 6.4648e-01, 3.0521e-01, 9.9939e-01, 0.0000e+00,\n",
      "        8.4168e-01, 8.5209e-01, 9.6248e-01, 6.8239e-01, 8.8704e-02, 9.9838e-01,\n",
      "        9.4351e-01, 6.5811e-01, 1.8770e-01, 1.2028e-02], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([1.2127, 1.2407, 0.7520, 0.7349, 1.0257, 0.6127, 1.3500, 0.9232, 1.5620,\n",
      "        0.8296, 0.9273, 1.8418, 1.4737, 1.0208, 1.0208, 1.8418],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([6.5749e-01, 3.5552e-06, 8.5973e-01, 4.1529e-01, 9.7434e-01, 0.0000e+00,\n",
      "        6.2349e-01, 9.2296e-01, 6.1620e-01, 8.2257e-01, 9.5656e-02, 5.4206e-01,\n",
      "        6.4022e-01, 6.4468e-01, 1.8387e-01, 6.5302e-03], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-6.5749e-01, -3.5552e-06, -8.5973e-01, -4.1529e-01, -9.7434e-01,\n",
      "        -0.0000e+00, -6.2349e-01, -9.2296e-01, -6.1620e-01, -8.2257e-01,\n",
      "        -9.5656e-02, -5.4206e-01, -6.4022e-01, -6.4468e-01, -1.8387e-01,\n",
      "        -6.5302e-03], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "cos dist: tensor([0.7767, 0.8676, 0.9895, 0.9983, 0.9926, 0.8982, 0.4989, 0.4334, 0.9946,\n",
      "        0.9999, 0.0112, 0.9975, 0.9997, 0.9731, 0.9972, 0.0040],\n",
      "       device='cuda:0', grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([0.6866, 0.8192, 0.7648, 1.1507, 0.5663, 0.5943, 0.8771, 1.0067, 0.8516,\n",
      "        0.6216, 0.8396, 0.8207, 1.4618, 1.0339, 0.6216, 0.7976],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([1.1312, 1.0590, 1.2938, 0.8676, 1.7529, 1.5113, 0.5687, 0.4305, 1.1679,\n",
      "        1.6087, 0.0134, 1.2154, 0.6839, 0.9413, 1.6042, 0.0050],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "loss: tensor([-1.1312, -1.0590, -1.2938, -0.8676, -1.7529, -1.5113, -0.5687, -0.4305,\n",
      "        -1.1679, -1.6087, -0.0134, -1.2154, -0.6839, -0.9413, -1.6042, -0.0050],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "cos dist: tensor([0.9835, 0.6933, 1.0000, 1.0000, 0.9326, 0.1728, 0.9906, 0.3928, 0.8513,\n",
      "        0.3944, 0.9274, 0.9482, 0.8691, 0.9411, 0.5050, 0.9998],\n",
      "       device='cuda:0', grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([0.6039, 0.8366, 0.7477, 1.0391, 0.9651, 0.6469, 1.4623, 0.7527, 0.7092,\n",
      "        0.7237, 0.7439, 1.3289, 1.2278, 1.3026, 0.9322, 0.8895],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([1.6285, 0.8286, 1.3374, 0.9623, 0.9663, 0.2672, 0.6774, 0.5219, 1.2005,\n",
      "        0.5450, 1.2466, 0.7135, 0.7078, 0.7225, 0.5417, 1.1240],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "loss: tensor([-1.6285, -0.8286, -1.3374, -0.9623, -0.9663, -0.2672, -0.6774, -0.5219,\n",
      "        -1.2005, -0.5450, -1.2466, -0.7135, -0.7078, -0.7225, -0.5417, -1.1240],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "target probs tensor([[1.9399e-05],\n",
      "        [1.2362e-03],\n",
      "        [1.0256e-01],\n",
      "        [5.8685e-01],\n",
      "        [5.7000e-03],\n",
      "        [9.9841e-01],\n",
      "        [4.6285e-07],\n",
      "        [6.1003e-03],\n",
      "        [2.5152e-05],\n",
      "        [1.3848e-01],\n",
      "        [1.8128e-02],\n",
      "        [2.0155e-01],\n",
      "        [9.9851e-01],\n",
      "        [7.0380e-01],\n",
      "        [5.0030e-01],\n",
      "        [9.9958e-01]], device='cuda:0'), loss: 1.5028785467147827: \n",
      "\n",
      "cos dist: tensor([1.1727e-02, 6.9055e-02, 9.8945e-01, 1.0276e-02, 4.6353e-01, 3.5763e-07,\n",
      "        9.9997e-01, 2.1162e-01, 4.0237e-02, 5.5076e-01, 9.5711e-01, 5.0343e-01,\n",
      "        2.6882e-05, 3.3498e-04, 9.5841e-01, 5.9605e-07], device='cuda:0')\n",
      " z_dist: tensor([0.7864, 0.8549, 0.6069, 0.8640, 1.4524, 1.4986, 0.6754, 1.0092, 0.7054,\n",
      "        1.2383, 0.9320, 1.1882, 0.9588, 0.8580, 1.1867, 1.3555],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([1.4913e-02, 8.0779e-02, 1.6304e+00, 1.1894e-02, 3.1915e-01, 2.3864e-07,\n",
      "        1.4807e+00, 2.0968e-01, 5.7041e-02, 4.4479e-01, 1.0269e+00, 4.2370e-01,\n",
      "        2.8036e-05, 3.9043e-04, 8.0765e-01, 4.3972e-07], device='cuda:0')\n",
      "loss: tensor([-1.4913e-02, -8.0779e-02, -1.6304e+00, -1.1894e-02, -3.1915e-01,\n",
      "        -2.3864e-07, -1.4807e+00, -2.0968e-01, -5.7041e-02, -4.4479e-01,\n",
      "        -1.0269e+00, -4.2370e-01, -2.8036e-05, -3.9043e-04, -8.0765e-01,\n",
      "        -4.3972e-07], device='cuda:0')\n",
      "\n",
      "\n",
      "list of metrics:  [705, 785, 802, 808]\n",
      "Better model found at epoch 0 with validation value: 0.5479999780654907.\n",
      "\n",
      "cos dist: tensor([0.0000e+00, 5.9276e-02, 9.7242e-01, 3.4151e-01, 4.0643e-01, 8.4298e-01,\n",
      "        1.0000e+00, 9.0204e-01, 6.1442e-01, 9.7522e-01, 9.4181e-01, 1.0265e-01,\n",
      "        4.1723e-07, 9.8017e-01, 9.9892e-01, 9.9569e-01], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([0.8030, 0.8495, 1.4394, 1.3114, 0.8070, 0.9202, 1.6564, 1.0468, 1.5317,\n",
      "        1.4436, 0.9642, 1.1890, 1.7383, 0.6340, 0.9354, 0.9266],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([0.0000e+00, 6.9780e-02, 6.7559e-01, 2.6041e-01, 5.0364e-01, 9.1606e-01,\n",
      "        6.0371e-01, 8.6171e-01, 4.0114e-01, 6.7554e-01, 9.7678e-01, 8.6333e-02,\n",
      "        2.4003e-07, 1.5459e+00, 1.0679e+00, 1.0745e+00], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-0.0000e+00, -6.9780e-02, -6.7559e-01, -2.6041e-01, -5.0364e-01,\n",
      "        -9.1606e-01, -6.0371e-01, -8.6171e-01, -4.0114e-01, -6.7554e-01,\n",
      "        -9.7678e-01, -8.6333e-02, -2.4003e-07, -1.5459e+00, -1.0679e+00,\n",
      "        -1.0745e+00], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "cos dist: tensor([9.9134e-01, 1.4156e-04, 1.0868e-01, 9.6909e-01, 1.1444e-02, 2.2523e-01,\n",
      "        6.3837e-05, 9.2926e-01, 9.3578e-02, 6.0434e-01, 1.5916e-01, 7.5680e-04,\n",
      "        5.7536e-01, 3.2717e-04, 9.8298e-01, 8.7858e-02], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([0.8777, 1.1499, 0.7071, 1.2836, 0.3914, 1.5438, 0.8912, 0.1186, 0.5736,\n",
      "        0.9598, 0.8362, 1.1721, 1.5645, 0.8469, 1.6509, 1.2350],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([1.1295e+00, 1.2310e-04, 1.5370e-01, 7.5501e-01, 2.9240e-02, 1.4590e-01,\n",
      "        7.1632e-05, 7.8327e+00, 1.6315e-01, 6.2967e-01, 1.9033e-01, 6.4566e-04,\n",
      "        3.6775e-01, 3.8630e-04, 5.9542e-01, 7.1142e-02], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-1.1295e+00, -1.2310e-04, -1.5370e-01, -7.5501e-01, -2.9240e-02,\n",
      "        -1.4590e-01, -7.1632e-05, -7.8327e+00, -1.6315e-01, -6.2967e-01,\n",
      "        -1.9033e-01, -6.4566e-04, -3.6775e-01, -3.8630e-04, -5.9542e-01,\n",
      "        -7.1142e-02], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "target probs tensor([[1.9039e-04],\n",
      "        [8.5821e-03],\n",
      "        [1.1901e-02],\n",
      "        [4.3382e-02],\n",
      "        [1.7546e-03],\n",
      "        [1.1539e-04],\n",
      "        [1.4022e-02],\n",
      "        [3.2562e-05],\n",
      "        [7.0437e-01],\n",
      "        [9.3037e-06],\n",
      "        [3.1835e-03],\n",
      "        [1.4886e-02],\n",
      "        [9.6191e-06],\n",
      "        [6.4736e-05],\n",
      "        [1.1300e-02],\n",
      "        [4.7661e-06]], device='cuda:0'), loss: 0.08308935910463333: \n",
      "\n",
      "cos dist: tensor([0.9974, 0.9671, 0.0243, 0.1057, 0.9716, 0.7877, 0.4254, 0.9675, 0.0233,\n",
      "        0.9648, 0.9939, 0.0027, 0.8124, 0.0029, 0.2668, 0.0562],\n",
      "       device='cuda:0')\n",
      " z_dist: tensor([0.9611, 1.2774, 0.7506, 1.2587, 1.0718, 1.1406, 1.0912, 1.4868, 0.9157,\n",
      "        0.9626, 1.1853, 0.8901, 1.2244, 0.7170, 1.3211, 0.6064],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([1.0378, 0.7571, 0.0324, 0.0840, 0.9065, 0.6906, 0.3898, 0.6507, 0.0255,\n",
      "        1.0023, 0.8385, 0.0030, 0.6635, 0.0041, 0.2020, 0.0926],\n",
      "       device='cuda:0')\n",
      "loss: tensor([-1.0378, -0.7571, -0.0324, -0.0840, -0.9065, -0.6906, -0.3898, -0.6507,\n",
      "        -0.0255, -1.0023, -0.8385, -0.0030, -0.6635, -0.0041, -0.2020, -0.0926],\n",
      "       device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "cos dist: tensor([0.2297, 0.8460, 0.1876, 0.2076, 0.7260, 0.0071, 0.1606, 0.7818],\n",
      "       device='cuda:0')\n",
      " z_dist: tensor([0.7533, 0.8287, 1.1612, 1.5097, 1.1432, 1.4084, 1.4349, 1.6709],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([0.3049, 1.0208, 0.1615, 0.1375, 0.6351, 0.0051, 0.1119, 0.4679],\n",
      "       device='cuda:0')\n",
      "loss: tensor([-0.3049, -1.0208, -0.1615, -0.1375, -0.6351, -0.0051, -0.1119, -0.4679],\n",
      "       device='cuda:0')\n",
      "\n",
      "\n",
      "list of metrics:  [329, 298, 328, 310]\n",
      "Better model found at epoch 9 with validation value: 0.7860000133514404.\n",
      "\n",
      "cos dist: tensor([1.1510e-01, 9.3324e-03, 8.3114e-01, 9.8926e-01, 9.6268e-01, 7.9811e-05,\n",
      "        1.1190e-01, 6.8401e-01, 7.9567e-01, 8.2986e-01, 8.7656e-01, 2.5807e-02,\n",
      "        3.5892e-01, 3.2168e-03, 9.0712e-04, 4.1056e-01], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([0.7274, 1.0802, 0.9883, 1.2027, 1.3106, 0.7402, 1.2795, 0.8752, 0.8997,\n",
      "        1.4109, 0.8860, 0.6767, 1.1457, 1.2563, 0.7781, 0.8358],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([1.5823e-01, 8.6393e-03, 8.4094e-01, 8.2254e-01, 7.3454e-01, 1.0782e-04,\n",
      "        8.7450e-02, 7.8152e-01, 8.8441e-01, 5.8818e-01, 9.8939e-01, 3.8137e-02,\n",
      "        3.1327e-01, 2.5605e-03, 1.1658e-03, 4.9122e-01], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-1.5823e-01, -8.6393e-03, -8.4094e-01, -8.2254e-01, -7.3454e-01,\n",
      "        -1.0782e-04, -8.7450e-02, -7.8152e-01, -8.8441e-01, -5.8818e-01,\n",
      "        -9.8939e-01, -3.8137e-02, -3.1327e-01, -2.5605e-03, -1.1658e-03,\n",
      "        -4.9122e-01], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cos dist: tensor([2.3465e-01, 9.8366e-01, 1.7436e-03, 8.7191e-01, 2.2531e-01, 9.7885e-01,\n",
      "        1.4592e-01, 2.3842e-07, 2.7087e-02, 7.4104e-01, 4.6226e-01, 1.1280e-01,\n",
      "        3.1539e-01, 9.9408e-01, 7.9546e-01, 5.1750e-01], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([0.7955, 1.0988, 0.4801, 1.1186, 0.4111, 1.4164, 1.1326, 1.3804, 0.8060,\n",
      "        0.9915, 1.3311, 0.7893, 0.5712, 0.8924, 0.5530, 0.8649],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([2.9498e-01, 8.9518e-01, 3.6317e-03, 7.7946e-01, 5.4802e-01, 6.9109e-01,\n",
      "        1.2884e-01, 1.7272e-07, 3.3608e-02, 7.4737e-01, 3.4728e-01, 1.4291e-01,\n",
      "        5.5218e-01, 1.1140e+00, 1.4384e+00, 5.9834e-01], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-2.9498e-01, -8.9518e-01, -3.6317e-03, -7.7946e-01, -5.4802e-01,\n",
      "        -6.9109e-01, -1.2884e-01, -1.7272e-07, -3.3608e-02, -7.4737e-01,\n",
      "        -3.4728e-01, -1.4291e-01, -5.5218e-01, -1.1140e+00, -1.4384e+00,\n",
      "        -5.9834e-01], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "target probs tensor([[1.8780e-06],\n",
      "        [7.8259e-01],\n",
      "        [6.7318e-05],\n",
      "        [9.5123e-01],\n",
      "        [3.6228e-01],\n",
      "        [1.7681e-03],\n",
      "        [1.6431e-01],\n",
      "        [9.2490e-01],\n",
      "        [5.1381e-03],\n",
      "        [4.5304e-04],\n",
      "        [2.8972e-06],\n",
      "        [6.9862e-01],\n",
      "        [5.0368e-05],\n",
      "        [1.1768e-04],\n",
      "        [1.7018e-01],\n",
      "        [6.4829e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5728074908256531: \n",
      "\n",
      "cos dist: tensor([9.0640e-02, 3.1745e-04, 5.7036e-01, 7.4029e-05, 1.6289e-01, 2.8216e-01,\n",
      "        4.6861e-01, 1.1570e-01, 5.2613e-02, 7.2304e-02, 5.6249e-02, 2.1897e-02,\n",
      "        2.9572e-02, 9.7844e-01, 3.7237e-01, 3.3259e-05], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([0.8014, 0.9518, 0.4632, 1.0434, 0.7530, 1.3430, 0.9211, 0.9241, 0.8295,\n",
      "        0.9722, 0.8541, 0.3442, 0.9921, 1.0938, 1.1920, 1.2727],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([1.1311e-01, 3.3352e-04, 1.2313e+00, 7.0948e-05, 2.1632e-01, 2.1010e-01,\n",
      "        5.0873e-01, 1.2521e-01, 6.3424e-02, 7.4369e-02, 6.5860e-02, 6.3624e-02,\n",
      "        2.9808e-02, 8.9451e-01, 3.1239e-01, 2.6133e-05], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-1.1311e-01, -3.3352e-04, -1.2313e+00, -7.0948e-05, -2.1632e-01,\n",
      "        -2.1010e-01, -5.0873e-01, -1.2521e-01, -6.3424e-02, -7.4369e-02,\n",
      "        -6.5860e-02, -6.3624e-02, -2.9808e-02, -8.9451e-01, -3.1239e-01,\n",
      "        -2.6133e-05], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "cos dist: tensor([9.4492e-01, 4.6729e-02, 9.6126e-01, 9.2115e-01, 9.2875e-01, 5.7269e-03,\n",
      "        1.0092e-01, 9.8769e-03, 9.7385e-01, 9.3362e-01, 5.4397e-02, 2.3842e-07,\n",
      "        1.9050e-02, 7.9779e-01, 8.3844e-01, 9.4026e-01], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([0.6596, 0.4877, 1.0807, 1.1275, 0.7933, 0.8910, 0.9702, 1.0769, 1.2464,\n",
      "        0.6983, 0.2608, 0.8729, 1.2174, 0.7566, 1.4015, 0.9856],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([1.4325e+00, 9.5813e-02, 8.8949e-01, 8.1695e-01, 1.1707e+00, 6.4274e-03,\n",
      "        1.0401e-01, 9.1719e-03, 7.8135e-01, 1.3369e+00, 2.0856e-01, 2.7313e-07,\n",
      "        1.5648e-02, 1.0544e+00, 5.9824e-01, 9.5399e-01], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-1.4325e+00, -9.5813e-02, -8.8949e-01, -8.1695e-01, -1.1707e+00,\n",
      "        -6.4274e-03, -1.0401e-01, -9.1719e-03, -7.8135e-01, -1.3369e+00,\n",
      "        -2.0856e-01, -2.7313e-07, -1.5648e-02, -1.0544e+00, -5.9824e-01,\n",
      "        -9.5399e-01], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "cos dist: tensor([4.3694e-01, 6.9474e-01, 8.8595e-01, 9.8876e-01, 5.0138e-03, 4.2174e-03,\n",
      "        2.3679e-02, 1.0916e-01, 9.7752e-06, 7.9868e-01, 5.8696e-01, 4.7724e-01,\n",
      "        3.2562e-03, 9.9690e-01, 3.7449e-01, 8.0185e-01], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([0.7194, 1.2277, 0.9095, 1.1908, 0.9568, 1.0147, 1.6370, 0.1959, 1.2847,\n",
      "        0.8102, 0.8408, 0.8284, 0.8189, 0.5262, 0.6571, 1.1807],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([6.0739e-01, 5.6590e-01, 9.7410e-01, 8.3030e-01, 5.2402e-03, 4.1562e-03,\n",
      "        1.4465e-02, 5.5736e-01, 7.6090e-06, 9.8578e-01, 6.9808e-01, 5.7609e-01,\n",
      "        3.9765e-03, 1.8947e+00, 5.6993e-01, 6.7912e-01], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-6.0739e-01, -5.6590e-01, -9.7410e-01, -8.3030e-01, -5.2402e-03,\n",
      "        -4.1562e-03, -1.4465e-02, -5.5736e-01, -7.6090e-06, -9.8578e-01,\n",
      "        -6.9808e-01, -5.7609e-01, -3.9765e-03, -1.8947e+00, -5.6993e-01,\n",
      "        -6.7912e-01], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "cos dist: tensor([0.1560, 0.0276, 0.0824, 0.0090, 0.0939, 0.6313, 0.0776, 0.0197, 0.4991,\n",
      "        0.9502, 0.6981, 0.9850, 0.1365, 0.0872, 0.1753, 0.7061],\n",
      "       device='cuda:0', grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([1.0025, 1.1834, 0.2712, 0.7078, 0.9113, 1.5730, 1.5378, 0.3778, 1.5730,\n",
      "        0.6778, 1.5747, 0.4819, 0.7448, 1.0400, 0.7495, 1.1350],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([0.1556, 0.0233, 0.3039, 0.0127, 0.1030, 0.4013, 0.0505, 0.0522, 0.3173,\n",
      "        1.4019, 0.4433, 2.0440, 0.1833, 0.0839, 0.2339, 0.6222],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "loss: tensor([-0.1556, -0.0233, -0.3039, -0.0127, -0.1030, -0.4013, -0.0505, -0.0522,\n",
      "        -0.3173, -1.4019, -0.4433, -2.0440, -0.1833, -0.0839, -0.2339, -0.6222],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "target probs tensor([[3.3726e-01],\n",
      "        [1.8204e-04],\n",
      "        [1.7633e-04],\n",
      "        [9.9026e-01],\n",
      "        [8.3893e-01],\n",
      "        [2.3745e-02],\n",
      "        [7.1399e-04],\n",
      "        [8.7924e-03],\n",
      "        [9.7695e-05],\n",
      "        [1.0423e-03],\n",
      "        [1.7436e-05],\n",
      "        [3.0462e-05],\n",
      "        [1.4017e-03],\n",
      "        [1.9580e-03],\n",
      "        [3.1474e-04],\n",
      "        [3.7112e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.4317733943462372: \n",
      "\n",
      "cos dist: tensor([1.4903e-01, 4.9103e-03, 2.5636e-02, 1.3562e-03, 6.7139e-04, 9.2351e-01,\n",
      "        5.7824e-02, 9.4854e-01, 5.8060e-03, 8.4420e-01, 9.8330e-01, 8.0834e-01,\n",
      "        9.6366e-01, 8.7780e-01, 9.6342e-01, 4.2567e-01], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([0.5598, 1.0850, 0.3069, 0.9350, 0.7649, 1.1849, 1.2415, 0.8890, 1.3437,\n",
      "        0.7691, 1.0005, 1.3541, 1.1849, 1.3574, 0.9827, 1.0037],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([2.6622e-01, 4.5255e-03, 8.3523e-02, 1.4505e-03, 8.7769e-04, 7.7942e-01,\n",
      "        4.6575e-02, 1.0670e+00, 4.3210e-03, 1.0976e+00, 9.8277e-01, 5.9697e-01,\n",
      "        8.1331e-01, 6.4669e-01, 9.8042e-01, 4.2412e-01], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-2.6622e-01, -4.5255e-03, -8.3523e-02, -1.4505e-03, -8.7769e-04,\n",
      "        -7.7942e-01, -4.6575e-02, -1.0670e+00, -4.3210e-03, -1.0976e+00,\n",
      "        -9.8277e-01, -5.9697e-01, -8.1331e-01, -6.4669e-01, -9.8042e-01,\n",
      "        -4.2412e-01], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "cos dist: tensor([0.0485, 0.9534, 0.5984, 0.0031, 0.3109, 0.9826, 0.0344, 0.7689, 0.8320,\n",
      "        0.0448, 0.0211, 0.9999, 0.0021, 0.9004, 0.0736, 0.0013],\n",
      "       device='cuda:0', grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([1.5180, 1.1747, 0.4683, 0.7539, 1.0491, 0.5297, 1.0571, 0.6699, 0.8880,\n",
      "        1.3323, 0.4050, 0.5110, 1.2968, 0.8796, 1.1423, 0.4683],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([3.1927e-02, 8.1158e-01, 1.2779e+00, 4.0834e-03, 2.9638e-01, 1.8549e+00,\n",
      "        3.2499e-02, 1.1478e+00, 9.3696e-01, 3.3628e-02, 5.2057e-02, 1.9565e+00,\n",
      "        1.5885e-03, 1.0237e+00, 6.4388e-02, 2.7084e-03], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-3.1927e-02, -8.1158e-01, -1.2779e+00, -4.0834e-03, -2.9638e-01,\n",
      "        -1.8549e+00, -3.2499e-02, -1.1478e+00, -9.3696e-01, -3.3628e-02,\n",
      "        -5.2057e-02, -1.9565e+00, -1.5885e-03, -1.0237e+00, -6.4388e-02,\n",
      "        -2.7084e-03], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "cos dist: tensor([6.4766e-01, 2.1858e-03, 9.6126e-01, 9.4826e-01, 9.7856e-01, 8.5478e-01,\n",
      "        2.9649e-03, 9.6775e-01, 7.5794e-01, 9.5389e-01, 2.6077e-01, 3.7907e-01,\n",
      "        3.3975e-05, 8.5198e-01, 1.7671e-01, 9.0743e-01], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([0.7337, 1.2717, 1.5692, 1.1793, 1.4128, 1.1525, 0.3361, 1.4371, 0.4862,\n",
      "        1.3751, 1.3811, 1.0995, 0.5124, 0.4801, 0.7887, 0.8913],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([8.8269e-01, 1.7188e-03, 6.1259e-01, 8.0406e-01, 6.9264e-01, 7.4165e-01,\n",
      "        8.8208e-03, 6.7342e-01, 1.5589e+00, 6.9369e-01, 1.8881e-01, 3.4477e-01,\n",
      "        6.6309e-05, 1.7748e+00, 2.2404e-01, 1.0181e+00], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-8.8269e-01, -1.7188e-03, -6.1259e-01, -8.0406e-01, -6.9264e-01,\n",
      "        -7.4165e-01, -8.8208e-03, -6.7342e-01, -1.5589e+00, -6.9369e-01,\n",
      "        -1.8881e-01, -3.4477e-01, -6.6309e-05, -1.7748e+00, -2.2404e-01,\n",
      "        -1.0181e+00], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cos dist: tensor([8.5350e-01, 7.2649e-01, 9.0271e-02, 9.4920e-01, 2.2835e-02, 4.3553e-04,\n",
      "        2.9490e-01, 4.9548e-01, 5.3151e-01, 8.7264e-01, 9.5562e-01, 9.4277e-01,\n",
      "        7.9930e-01, 9.3760e-01, 1.1592e-01, 9.6381e-01], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([1.4510, 0.6412, 1.7875, 0.7996, 0.8527, 0.4763, 0.7946, 0.9967, 1.1698,\n",
      "        1.0534, 1.3023, 1.5104, 1.0712, 1.6669, 1.1166, 0.9498],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([5.8820e-01, 1.1330e+00, 5.0502e-02, 1.1871e+00, 2.6778e-02, 9.1433e-04,\n",
      "        3.7113e-01, 4.9710e-01, 4.5434e-01, 8.2839e-01, 7.3379e-01, 6.2417e-01,\n",
      "        7.4615e-01, 5.6248e-01, 1.0382e-01, 1.0148e+00], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-5.8820e-01, -1.1330e+00, -5.0502e-02, -1.1871e+00, -2.6778e-02,\n",
      "        -9.1433e-04, -3.7113e-01, -4.9710e-01, -4.5434e-01, -8.2839e-01,\n",
      "        -7.3379e-01, -6.2417e-01, -7.4615e-01, -5.6248e-01, -1.0382e-01,\n",
      "        -1.0148e+00], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "target probs tensor([[6.0776e-01],\n",
      "        [3.9367e-03],\n",
      "        [4.5054e-03],\n",
      "        [9.7302e-07],\n",
      "        [5.5007e-01],\n",
      "        [7.2871e-05],\n",
      "        [1.1589e-03],\n",
      "        [2.9763e-02],\n",
      "        [3.3008e-02],\n",
      "        [1.5413e-03],\n",
      "        [6.3540e-06],\n",
      "        [2.2730e-05],\n",
      "        [8.6505e-01],\n",
      "        [4.1215e-04],\n",
      "        [3.8975e-01],\n",
      "        [7.7213e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.3616076707839966: \n",
      "\n",
      "cos dist: tensor([1.8309e-02, 8.0720e-01, 6.3860e-01, 9.6247e-01, 1.0410e-01, 2.6574e-01,\n",
      "        1.5221e-02, 6.2833e-01, 1.2337e-02, 4.4200e-01, 9.1706e-01, 8.2962e-02,\n",
      "        4.0346e-04, 8.0595e-02, 9.8846e-01, 3.9059e-04], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([0.5149, 1.5214, 0.9759, 1.0876, 0.6862, 0.6161, 0.1361, 0.9661, 1.4408,\n",
      "        1.3919, 0.9661, 1.1191, 1.1046, 0.7397, 1.5445, 1.3679],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([3.5555e-02, 5.3056e-01, 6.5440e-01, 8.8497e-01, 1.5170e-01, 4.3136e-01,\n",
      "        1.1183e-01, 6.5040e-01, 8.5626e-03, 3.1756e-01, 9.4927e-01, 7.4132e-02,\n",
      "        3.6527e-04, 1.0895e-01, 6.3999e-01, 2.8555e-04], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-3.5555e-02, -5.3056e-01, -6.5440e-01, -8.8497e-01, -1.5170e-01,\n",
      "        -4.3136e-01, -1.1183e-01, -6.5040e-01, -8.5626e-03, -3.1756e-01,\n",
      "        -9.4927e-01, -7.4132e-02, -3.6527e-04, -1.0895e-01, -6.3999e-01,\n",
      "        -2.8555e-04], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "cos dist: tensor([9.1111e-01, 6.5604e-01, 2.7198e-01, 9.4721e-01, 2.8222e-01, 1.4967e-04,\n",
      "        9.8390e-01, 5.6124e-01, 9.8080e-01, 7.1831e-01, 1.7661e-01, 1.2523e-01,\n",
      "        8.7263e-01, 9.9474e-01, 3.1132e-01, 1.0729e-06], device='cuda:0')\n",
      " z_dist: tensor([1.3417, 0.6595, 1.2988, 1.4885, 0.8682, 0.3861, 0.9625, 1.1957, 1.4900,\n",
      "        0.6261, 1.3387, 0.9000, 1.5179, 1.0391, 1.2694, 0.6861],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([6.7905e-01, 9.9481e-01, 2.0942e-01, 6.3637e-01, 3.2505e-01, 3.8767e-04,\n",
      "        1.0222e+00, 4.6936e-01, 6.5828e-01, 1.1473e+00, 1.3193e-01, 1.3915e-01,\n",
      "        5.7488e-01, 9.5735e-01, 2.4525e-01, 1.5636e-06], device='cuda:0')\n",
      "loss: tensor([-6.7905e-01, -9.9481e-01, -2.0942e-01, -6.3637e-01, -3.2505e-01,\n",
      "        -3.8767e-04, -1.0222e+00, -4.6936e-01, -6.5828e-01, -1.1473e+00,\n",
      "        -1.3193e-01, -1.3915e-01, -5.7488e-01, -9.5735e-01, -2.4525e-01,\n",
      "        -1.5636e-06], device='cuda:0')\n",
      "\n",
      "\n",
      "list of metrics:  [333, 301, 320, 296]\n",
      "fooling weight increased to 1.9000000000000001 at the end of epoch 10\n",
      "\n",
      "cos dist: tensor([9.9310e-02, 7.8082e-06, 9.5074e-01, 6.2565e-01, 2.1829e-01, 3.3570e-01,\n",
      "        8.0227e-01, 7.7139e-01, 5.4745e-01, 9.5152e-01, 7.9841e-01, 3.0344e-02,\n",
      "        3.5616e-01, 5.3886e-01, 5.4092e-01, 3.8855e-01], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([0.9891, 1.0880, 0.5311, 1.5358, 1.0864, 1.0643, 0.6474, 1.2510, 1.2786,\n",
      "        1.5556, 0.9917, 0.9390, 0.8645, 0.9415, 1.0005, 0.5745],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([1.0041e-01, 7.1767e-06, 1.7901e+00, 4.0737e-01, 2.0092e-01, 3.1541e-01,\n",
      "        1.2392e+00, 6.1662e-01, 4.2816e-01, 6.1166e-01, 8.0509e-01, 3.2316e-02,\n",
      "        4.1199e-01, 5.7233e-01, 5.4062e-01, 6.7637e-01], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-1.0041e-01, -7.1767e-06, -1.7901e+00, -4.0737e-01, -2.0092e-01,\n",
      "        -3.1541e-01, -1.2392e+00, -6.1662e-01, -4.2816e-01, -6.1166e-01,\n",
      "        -8.0509e-01, -3.2316e-02, -4.1199e-01, -5.7233e-01, -5.4062e-01,\n",
      "        -6.7637e-01], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "cos dist: tensor([9.9673e-01, 6.8597e-01, 4.7646e-01, 9.8734e-01, 4.5893e-02, 9.9327e-01,\n",
      "        3.5526e-01, 1.2517e-06, 8.0210e-02, 1.5826e-01, 9.5702e-01, 7.1825e-03,\n",
      "        7.6447e-01, 8.9720e-01, 1.2971e-01, 9.9921e-01], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([1.1681, 0.4143, 1.0525, 1.7043, 0.9236, 0.8723, 1.1399, 0.9787, 1.0238,\n",
      "        0.3706, 1.0572, 0.7038, 1.2278, 1.2385, 0.9785, 0.8945],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([8.5327e-01, 1.6555e+00, 4.5271e-01, 5.7931e-01, 4.9688e-02, 1.1386e+00,\n",
      "        3.1166e-01, 1.2789e-06, 7.8344e-02, 4.2704e-01, 9.0527e-01, 1.0205e-02,\n",
      "        6.2266e-01, 7.2443e-01, 1.3256e-01, 1.1170e+00], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-8.5327e-01, -1.6555e+00, -4.5271e-01, -5.7931e-01, -4.9688e-02,\n",
      "        -1.1386e+00, -3.1166e-01, -1.2789e-06, -7.8344e-02, -4.2704e-01,\n",
      "        -9.0527e-01, -1.0205e-02, -6.2266e-01, -7.2443e-01, -1.3256e-01,\n",
      "        -1.1170e+00], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "target probs tensor([[6.4746e-05],\n",
      "        [2.4199e-04],\n",
      "        [1.2935e-03],\n",
      "        [2.5423e-03],\n",
      "        [7.9148e-01],\n",
      "        [3.5370e-01],\n",
      "        [3.6678e-01],\n",
      "        [8.8705e-01],\n",
      "        [8.1356e-05],\n",
      "        [2.3127e-02],\n",
      "        [8.5872e-02],\n",
      "        [4.2759e-04],\n",
      "        [3.6895e-01],\n",
      "        [3.6422e-03],\n",
      "        [9.8600e-01],\n",
      "        [1.1043e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5933274030685425: \n",
      "\n",
      "cos dist: tensor([1.8955e-02, 3.6900e-01, 3.5565e-03, 2.4457e-01, 5.6253e-03, 1.2006e-01,\n",
      "        3.8154e-01, 2.7478e-05, 1.9226e-03, 1.3847e-01, 4.6657e-03, 6.7591e-01,\n",
      "        2.9375e-01, 8.6128e-01, 8.3447e-06, 2.0948e-01], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([0.8588, 0.9847, 0.6621, 1.1315, 0.2177, 1.0982, 0.6440, 0.6927, 0.9669,\n",
      "        0.9686, 1.6791, 1.1348, 0.5700, 1.2814, 0.9974, 0.9054],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([2.2070e-02, 3.7472e-01, 5.3716e-03, 2.1615e-01, 2.5838e-02, 1.0932e-01,\n",
      "        5.9246e-01, 3.9669e-05, 1.9884e-03, 1.4297e-01, 2.7786e-03, 5.9564e-01,\n",
      "        5.1536e-01, 6.7214e-01, 8.3660e-06, 2.3138e-01], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-2.2070e-02, -3.7472e-01, -5.3716e-03, -2.1615e-01, -2.5838e-02,\n",
      "        -1.0932e-01, -5.9246e-01, -3.9669e-05, -1.9884e-03, -1.4297e-01,\n",
      "        -2.7786e-03, -5.9564e-01, -5.1536e-01, -6.7214e-01, -8.3660e-06,\n",
      "        -2.3138e-01], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "cos dist: tensor([2.3603e-05, 5.9219e-01, 7.2729e-01, 2.5514e-02, 3.0607e-04, 2.0693e-01,\n",
      "        8.4376e-02, 1.6153e-01, 3.2656e-03, 1.6143e-02, 8.7201e-02, 7.4262e-01,\n",
      "        1.4223e-02, 9.4065e-01, 2.4565e-02, 6.2323e-04], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([0.1623, 1.0690, 0.9659, 0.8376, 0.9396, 0.2216, 0.5372, 0.9160, 0.9533,\n",
      "        0.3189, 0.2846, 1.1553, 1.0240, 0.5372, 0.9078, 0.6692],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([1.4541e-04, 5.5395e-01, 7.5294e-01, 3.0462e-02, 3.2574e-04, 9.3384e-01,\n",
      "        1.5705e-01, 1.7635e-01, 3.4255e-03, 5.0616e-02, 3.0635e-01, 6.4277e-01,\n",
      "        1.3890e-02, 1.7509e+00, 2.7061e-02, 9.3125e-04], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-1.4541e-04, -5.5395e-01, -7.5294e-01, -3.0462e-02, -3.2574e-04,\n",
      "        -9.3384e-01, -1.5705e-01, -1.7635e-01, -3.4255e-03, -5.0616e-02,\n",
      "        -3.0635e-01, -6.4277e-01, -1.3890e-02, -1.7509e+00, -2.7061e-02,\n",
      "        -9.3125e-04], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "cos dist: tensor([0.5074, 0.0711, 0.6019, 0.9850, 0.4895, 0.9781, 0.4853, 0.4294, 0.6640,\n",
      "        0.6301, 0.0084, 0.0905, 0.0130, 0.8806, 0.1261, 0.0481],\n",
      "       device='cuda:0', grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([1.3463, 0.9886, 1.3460, 0.7785, 1.6021, 1.5206, 1.0878, 0.3196, 0.9654,\n",
      "        1.1891, 0.4823, 0.7452, 0.8088, 0.7186, 1.1748, 1.3210],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([0.3769, 0.0719, 0.4472, 1.2652, 0.3055, 0.6432, 0.4461, 1.3438, 0.6878,\n",
      "        0.5299, 0.0175, 0.1214, 0.0161, 1.2255, 0.1073, 0.0364],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "loss: tensor([-0.3769, -0.0719, -0.4472, -1.2652, -0.3055, -0.6432, -0.4461, -1.3438,\n",
      "        -0.6878, -0.5299, -0.0175, -0.1214, -0.0161, -1.2255, -0.1073, -0.0364],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cos dist: tensor([9.5179e-01, 9.4505e-02, 4.9764e-01, 1.6749e-05, 8.0547e-02, 9.9185e-01,\n",
      "        6.2275e-01, 9.7269e-01, 1.4852e-02, 4.3393e-03, 5.1402e-01, 4.4129e-02,\n",
      "        6.6848e-01, 5.1023e-01, 8.6080e-01, 1.3566e-02], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      " z_dist: tensor([0.7373, 0.8505, 0.8300, 1.0434, 0.4072, 0.9149, 0.8486, 0.9514, 0.5350,\n",
      "        0.5542, 1.0135, 0.6526, 0.7390, 0.3740, 1.0240, 0.8823],\n",
      "       device='cuda:0')\n",
      " normalized: tensor([1.2909e+00, 1.1112e-01, 5.9953e-01, 1.6052e-05, 1.9778e-01, 1.0841e+00,\n",
      "        7.3385e-01, 1.0224e+00, 2.7761e-02, 7.8293e-03, 5.0717e-01, 6.7622e-02,\n",
      "        9.0460e-01, 1.3644e+00, 8.4064e-01, 1.5377e-02], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor([-1.2909e+00, -1.1112e-01, -5.9953e-01, -1.6052e-05, -1.9778e-01,\n",
      "        -1.0841e+00, -7.3385e-01, -1.0224e+00, -2.7761e-02, -7.8293e-03,\n",
      "        -5.0717e-01, -6.7622e-02, -9.0460e-01, -1.3644e+00, -8.4064e-01,\n",
      "        -1.5377e-02], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN SITE\n",
    "\n",
    "if mode == \"sanity_check\":\n",
    "  print(\"\\n\\n\\nWARNING: you are training on a sanity_check dataset.\\n\\n\\n\\n\")\n",
    "\n",
    "saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=env.save_filename + \"-best\")\n",
    "saver_every_epoch = SaveModelCallback(learn, every='epoch', name=env.save_filename)\n",
    "fooling_weight_scheduler = FoolingWeightScheduler(learn, fooling_loss_index = 3)\n",
    "\n",
    "# with Hooks(gen, append_stats_normal) as hooks:\n",
    "#   learn.fit(1, lr=5e-03, wd = 0., callbacks=[saver_best, saver_every_epoch])\n",
    "  \n",
    "# learn.fit(10, lr=1e-2, wd = 0., callbacks=[saver_best, saver_every_epoch])\n",
    "learn.fit(80, lr=1e-3, wd = 0., callbacks=[saver_best, saver_every_epoch, fooling_weight_scheduler])\n",
    "\n",
    "# # learn.fit(70, lr=1e-02, wd = 0.001, callbacks=[saver_best, saver_every_epoch])\n",
    "\n",
    "# learn.fit(60, lr=1e-2, wd = 0.001, callbacks=[saver_best, saver_every_epoch])\n",
    "\n",
    "# for i in range(10):\n",
    "#   learn.fit_one_cycle(7, wd = 0.,max_lr=1., div_factor = 1000.) \n",
    "  \n",
    "# learn.fit_one_cycle(5, max_lr=2e-2) #used for vgg-19-bn\n",
    "# learn.fit_one_cycle(5, max_lr=3e-3) # used for resnet50\n",
    "\n",
    "shutil.copyfile(env.temp_csv_path + '/' + env.save_filename + \".csv\", env.get_csv_path() + '.csv')\n",
    "shutil.copytree(env.data_path/env.get_learner_models_dir(), env.get_models_path())\n",
    "shutil.rmtree(env.data_path/env.get_learner_models_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram\n",
    "fig, axes = plt.subplots(len(hooks),1, figsize=(30,12))\n",
    "for ax,h in zip(axes.flatten(), hooks):\n",
    "  ax.imshow(get_hist(h), origin='lower')\n",
    "  ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mean and std\n",
    "fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "for h in hooks:\n",
    "  ms, ss, _ = h.stats\n",
    "  ax0.plot(ms[:100])\n",
    "  ax1.plot(ss[:100])\n",
    "plt.legend(range(len(hooks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "for h in hooks:\n",
    "  ms, ss, _ = h.stats\n",
    "  ax0.plot(ms)\n",
    "  ax1.plot(ss)\n",
    "plt.legend(range(len(hooks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero precentage:\n",
    "fig,axes = plt.subplots(len(hooks),1, figsize=(30,30))\n",
    "for ax,h in zip(axes.flatten(), hooks):\n",
    "    ax.plot(get_min(h))\n",
    "    ax.set_ylim(0,1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MO2fZ-hSSUzJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# z1 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "# z2 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "z1 = torch.tensor([0.8, -0.5] * 5).cuda()\n",
    "z2 = torch.tensor([-1.] * 10).cuda()\n",
    "print(\"z1: \", z1)\n",
    "print(\"z2: \", z2)\n",
    "print(\"distance: \", torch.norm(z1-z2,p=2))\n",
    "model = learn.model.eval()\n",
    "\n",
    "z_s = interpolate(z1, z2, 0.1)\n",
    "print(len(z_s))\n",
    "\n",
    "for i,z in enumerate(z_s):\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n",
    "  #img.save('./pics/' + str(i) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea : have 200 noises (1 for each class), then start iterating the dataset, and for each image, randomly apply one noise and record the result\n",
    "def targeted_diversity(learn, n_perturbations = 200, percentage = 95):\n",
    "  model = learn.model.eval()\n",
    "\n",
    "  one_hot_conditions = [torch.empty(z_dim).uniform_(0,1).cuda().detach() for _ in range(n_perturbations)]\n",
    "#   for i in range(z_dim):\n",
    "#     one_hot_conditions[i][i] = 1.\n",
    "\n",
    "  perturbations = [model.forward_single_z(z) for z in one_hot_conditions]\n",
    "\n",
    "  hist = [0.] * 1000\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 : print(\"at batch_no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbations[np.random.randint(0,len(perturbations))][None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      hist[pred] += 1\n",
    "\n",
    "  pred_histogram_sum = np.sum(hist)\n",
    "  indexed_pred_histogram = [(i, hist_element) for i,hist_element in  \n",
    "                            enumerate(hist)]\n",
    "\n",
    "  indexed_pred_histogram.sort(key=lambda x: x[1], reverse = True)\n",
    "\n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = indexed_pred_histogram[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / pred_histogram_sum) * 100.\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, indexed_pred_histogram\n",
    "\n",
    "def targeted_diversity_average(learn, n_perturbations = 200, percentage = 95, average_over = 4):\n",
    "  results = []\n",
    "  for i in range(average_over):\n",
    "    n, _ = targeted_diversity(learn, n_perturbations, percentage)\n",
    "    print(f'done with the {i}th calculation: {n}')\n",
    "    results.append(n)\n",
    "  return np.mean(results)\n",
    "\n",
    "def diversity_average(learn, n_perturbations = 10, percentage = 95, average_over = 4):\n",
    "  results = []\n",
    "  for i in range(average_over):\n",
    "    n, _ = diversity(learn, n_perturbations, percentage, verbose = False)\n",
    "    print(f'done with the {i}th calculation: {n}')\n",
    "    results.append(n)\n",
    "  return np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for n_pert in range(10, 110, 10):\n",
    "  n = targeted_diversity_average(learn, n_pert, 95, 4)\n",
    "  print(f'result for n_pert: {n_pert} is {n}')\n",
    "  results.append(n)\n",
    "\n",
    "x = list(range(10, 110, 10))\n",
    "plt.ylim(0, 500)\n",
    "plt.plot(x, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 5\n",
      "at batch no 10\n",
      "at batch no 15\n",
      "at batch no 20\n",
      "at batch no 25\n",
      "at batch no 30\n",
      "at batch no 35\n",
      "at batch no 40\n",
      "at batch no 45\n",
      "at batch no 50\n",
      "at batch no 55\n",
      "at batch no 60\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(224,\n",
       " [(721, 110.50),\n",
       "  (520, 91.60),\n",
       "  (971, 86.60),\n",
       "  (669, 62.10),\n",
       "  (431, 43.10),\n",
       "  (750, 39.50),\n",
       "  (414, 36.20),\n",
       "  (556, 35.60),\n",
       "  (588, 31.30),\n",
       "  (61, 28.60),\n",
       "  (411, 24.40),\n",
       "  (828, 20.50),\n",
       "  (904, 11.60),\n",
       "  (39, 10.80),\n",
       "  (709, 9.30),\n",
       "  (599, 9.20),\n",
       "  (489, 8.40),\n",
       "  (794, 8.40),\n",
       "  (614, 8.00),\n",
       "  (84, 6.50),\n",
       "  (770, 5.40),\n",
       "  (790, 4.80),\n",
       "  (581, 4.60),\n",
       "  (419, 4.50),\n",
       "  (490, 4.50),\n",
       "  (801, 3.70),\n",
       "  (824, 3.70),\n",
       "  (549, 3.60),\n",
       "  (604, 3.60),\n",
       "  (60, 3.50),\n",
       "  (401, 3.50),\n",
       "  (955, 3.40),\n",
       "  (570, 3.30),\n",
       "  (879, 3.30),\n",
       "  (516, 3.20),\n",
       "  (815, 3.00),\n",
       "  (893, 2.80),\n",
       "  (837, 2.70),\n",
       "  (906, 2.70),\n",
       "  (907, 2.70),\n",
       "  (987, 2.70),\n",
       "  (621, 2.40),\n",
       "  (56, 2.30),\n",
       "  (580, 2.30),\n",
       "  (48, 2.20),\n",
       "  (55, 2.20),\n",
       "  (806, 2.20),\n",
       "  (953, 2.10),\n",
       "  (108, 2.00),\n",
       "  (464, 2.00),\n",
       "  (67, 1.90),\n",
       "  (872, 1.90),\n",
       "  (858, 1.80),\n",
       "  (575, 1.70),\n",
       "  (805, 1.70),\n",
       "  (96, 1.60),\n",
       "  (572, 1.60),\n",
       "  (646, 1.60),\n",
       "  (711, 1.60),\n",
       "  (46, 1.50),\n",
       "  (62, 1.50),\n",
       "  (406, 1.50),\n",
       "  (496, 1.50),\n",
       "  (506, 1.50),\n",
       "  (641, 1.50),\n",
       "  (762, 1.50),\n",
       "  (857, 1.50),\n",
       "  (151, 1.40),\n",
       "  (408, 1.40),\n",
       "  (518, 1.40),\n",
       "  (523, 1.40),\n",
       "  (791, 1.40),\n",
       "  (973, 1.40),\n",
       "  (509, 1.30),\n",
       "  (754, 1.30),\n",
       "  (781, 1.30),\n",
       "  (864, 1.30),\n",
       "  (871, 1.30),\n",
       "  (68, 1.20),\n",
       "  (300, 1.20),\n",
       "  (417, 1.20),\n",
       "  (612, 1.20),\n",
       "  (656, 1.20),\n",
       "  (850, 1.20),\n",
       "  (868, 1.20),\n",
       "  (892, 1.20),\n",
       "  (91, 1.10),\n",
       "  (128, 1.10),\n",
       "  (412, 1.10),\n",
       "  (424, 1.10),\n",
       "  (443, 1.10),\n",
       "  (445, 1.10),\n",
       "  (468, 1.10),\n",
       "  (591, 1.10),\n",
       "  (738, 1.10),\n",
       "  (783, 1.10),\n",
       "  (788, 1.10),\n",
       "  (865, 1.10),\n",
       "  (898, 1.10),\n",
       "  (981, 1.10),\n",
       "  (7, 1.00),\n",
       "  (25, 1.00),\n",
       "  (33, 1.00),\n",
       "  (57, 1.00),\n",
       "  (105, 1.00),\n",
       "  (115, 1.00),\n",
       "  (118, 1.00),\n",
       "  (120, 1.00),\n",
       "  (123, 1.00),\n",
       "  (124, 1.00),\n",
       "  (163, 1.00),\n",
       "  (242, 1.00),\n",
       "  (247, 1.00),\n",
       "  (254, 1.00),\n",
       "  (260, 1.00),\n",
       "  (274, 1.00),\n",
       "  (281, 1.00),\n",
       "  (290, 1.00),\n",
       "  (304, 1.00),\n",
       "  (306, 1.00),\n",
       "  (314, 1.00),\n",
       "  (316, 1.00),\n",
       "  (342, 1.00),\n",
       "  (360, 1.00),\n",
       "  (376, 1.00),\n",
       "  (392, 1.00),\n",
       "  (398, 1.00),\n",
       "  (429, 1.00),\n",
       "  (457, 1.00),\n",
       "  (474, 1.00),\n",
       "  (488, 1.00),\n",
       "  (498, 1.00),\n",
       "  (507, 1.00),\n",
       "  (528, 1.00),\n",
       "  (533, 1.00),\n",
       "  (538, 1.00),\n",
       "  (566, 1.00),\n",
       "  (582, 1.00),\n",
       "  (637, 1.00),\n",
       "  (661, 1.00),\n",
       "  (694, 1.00),\n",
       "  (746, 1.00),\n",
       "  (787, 1.00),\n",
       "  (816, 1.00),\n",
       "  (819, 1.00),\n",
       "  (826, 1.00),\n",
       "  (923, 1.00),\n",
       "  (934, 1.00),\n",
       "  (937, 1.00),\n",
       "  (944, 1.00),\n",
       "  (982, 1.00),\n",
       "  (992, 1.00),\n",
       "  (997, 1.00),\n",
       "  (1, 0.90),\n",
       "  (83, 0.90),\n",
       "  (90, 0.90),\n",
       "  (186, 0.90),\n",
       "  (292, 0.90),\n",
       "  (307, 0.90),\n",
       "  (334, 0.90),\n",
       "  (387, 0.90),\n",
       "  (393, 0.90),\n",
       "  (453, 0.90),\n",
       "  (476, 0.90),\n",
       "  (526, 0.90),\n",
       "  (619, 0.90),\n",
       "  (620, 0.90),\n",
       "  (645, 0.90),\n",
       "  (703, 0.90),\n",
       "  (716, 0.90),\n",
       "  (734, 0.90),\n",
       "  (741, 0.90),\n",
       "  (768, 0.90),\n",
       "  (847, 0.90),\n",
       "  (889, 0.90),\n",
       "  (15, 0.80),\n",
       "  (109, 0.80),\n",
       "  (301, 0.80),\n",
       "  (492, 0.80),\n",
       "  (545, 0.80),\n",
       "  (697, 0.80),\n",
       "  (705, 0.80),\n",
       "  (753, 0.80),\n",
       "  (786, 0.80),\n",
       "  (116, 0.70),\n",
       "  (205, 0.70),\n",
       "  (218, 0.70),\n",
       "  (219, 0.70),\n",
       "  (293, 0.70),\n",
       "  (369, 0.70),\n",
       "  (410, 0.70),\n",
       "  (482, 0.70),\n",
       "  (530, 0.70),\n",
       "  (539, 0.70),\n",
       "  (552, 0.70),\n",
       "  (562, 0.70),\n",
       "  (576, 0.70),\n",
       "  (725, 0.70),\n",
       "  (796, 0.70),\n",
       "  (803, 0.70),\n",
       "  (814, 0.70),\n",
       "  (829, 0.70),\n",
       "  (854, 0.70),\n",
       "  (963, 0.70),\n",
       "  (8, 0.60),\n",
       "  (40, 0.60),\n",
       "  (63, 0.60),\n",
       "  (94, 0.60),\n",
       "  (164, 0.60),\n",
       "  (176, 0.60),\n",
       "  (355, 0.60),\n",
       "  (433, 0.60),\n",
       "  (440, 0.60),\n",
       "  (483, 0.60),\n",
       "  (485, 0.60),\n",
       "  (517, 0.60),\n",
       "  (625, 0.60),\n",
       "  (643, 0.60),\n",
       "  (737, 0.60),\n",
       "  (748, 0.60),\n",
       "  (822, 0.60),\n",
       "  (42, 0.50),\n",
       "  (86, 0.50),\n",
       "  (87, 0.50),\n",
       "  (97, 0.50),\n",
       "  (235, 0.50),\n",
       "  (308, 0.50),\n",
       "  (407, 0.50),\n",
       "  (444, 0.50),\n",
       "  (465, 0.50),\n",
       "  (472, 0.50),\n",
       "  (532, 0.50),\n",
       "  (606, 0.50),\n",
       "  (651, 0.50),\n",
       "  (684, 0.50),\n",
       "  (691, 0.50),\n",
       "  (735, 0.50),\n",
       "  (905, 0.50),\n",
       "  (52, 0.40),\n",
       "  (291, 0.40),\n",
       "  (294, 0.40),\n",
       "  (318, 0.40),\n",
       "  (327, 0.40),\n",
       "  (347, 0.40),\n",
       "  (396, 0.40),\n",
       "  (425, 0.40),\n",
       "  (459, 0.40),\n",
       "  (478, 0.40),\n",
       "  (618, 0.40),\n",
       "  (676, 0.40),\n",
       "  (692, 0.40),\n",
       "  (701, 0.40),\n",
       "  (722, 0.40),\n",
       "  (751, 0.40),\n",
       "  (779, 0.40),\n",
       "  (808, 0.40),\n",
       "  (817, 0.40),\n",
       "  (820, 0.40),\n",
       "  (852, 0.40),\n",
       "  (939, 0.40),\n",
       "  (0, 0.30),\n",
       "  (18, 0.30),\n",
       "  (37, 0.30),\n",
       "  (45, 0.30),\n",
       "  (75, 0.30),\n",
       "  (92, 0.30),\n",
       "  (102, 0.30),\n",
       "  (189, 0.30),\n",
       "  (230, 0.30),\n",
       "  (231, 0.30),\n",
       "  (313, 0.30),\n",
       "  (319, 0.30),\n",
       "  (336, 0.30),\n",
       "  (375, 0.30),\n",
       "  (409, 0.30),\n",
       "  (497, 0.30),\n",
       "  (505, 0.30),\n",
       "  (515, 0.30),\n",
       "  (527, 0.30),\n",
       "  (547, 0.30),\n",
       "  (565, 0.30),\n",
       "  (595, 0.30),\n",
       "  (609, 0.30),\n",
       "  (629, 0.30),\n",
       "  (636, 0.30),\n",
       "  (706, 0.30),\n",
       "  (728, 0.30),\n",
       "  (732, 0.30),\n",
       "  (743, 0.30),\n",
       "  (809, 0.30),\n",
       "  (836, 0.30),\n",
       "  (843, 0.30),\n",
       "  (878, 0.30),\n",
       "  (915, 0.30),\n",
       "  (946, 0.30),\n",
       "  (984, 0.30),\n",
       "  (998, 0.30),\n",
       "  (9, 0.20),\n",
       "  (17, 0.20),\n",
       "  (19, 0.20),\n",
       "  (47, 0.20),\n",
       "  (72, 0.20),\n",
       "  (76, 0.20),\n",
       "  (107, 0.20),\n",
       "  (138, 0.20),\n",
       "  (144, 0.20),\n",
       "  (157, 0.20),\n",
       "  (159, 0.20),\n",
       "  (171, 0.20),\n",
       "  (207, 0.20),\n",
       "  (249, 0.20),\n",
       "  (275, 0.20),\n",
       "  (321, 0.20),\n",
       "  (340, 0.20),\n",
       "  (381, 0.20),\n",
       "  (383, 0.20),\n",
       "  (397, 0.20),\n",
       "  (423, 0.20),\n",
       "  (437, 0.20),\n",
       "  (441, 0.20),\n",
       "  (481, 0.20),\n",
       "  (495, 0.20),\n",
       "  (555, 0.20),\n",
       "  (584, 0.20),\n",
       "  (607, 0.20),\n",
       "  (616, 0.20),\n",
       "  (622, 0.20),\n",
       "  (627, 0.20),\n",
       "  (632, 0.20),\n",
       "  (638, 0.20),\n",
       "  (668, 0.20),\n",
       "  (671, 0.20),\n",
       "  (674, 0.20),\n",
       "  (679, 0.20),\n",
       "  (717, 0.20),\n",
       "  (727, 0.20),\n",
       "  (772, 0.20),\n",
       "  (802, 0.20),\n",
       "  (830, 0.20),\n",
       "  (832, 0.20),\n",
       "  (867, 0.20),\n",
       "  (870, 0.20),\n",
       "  (881, 0.20),\n",
       "  (886, 0.20),\n",
       "  (920, 0.20),\n",
       "  (996, 0.20),\n",
       "  (999, 0.20),\n",
       "  (24, 0.10),\n",
       "  (28, 0.10),\n",
       "  (31, 0.10),\n",
       "  (50, 0.10),\n",
       "  (64, 0.10),\n",
       "  (74, 0.10),\n",
       "  (77, 0.10),\n",
       "  (82, 0.10),\n",
       "  (89, 0.10),\n",
       "  (95, 0.10),\n",
       "  (98, 0.10),\n",
       "  (99, 0.10),\n",
       "  (113, 0.10),\n",
       "  (131, 0.10),\n",
       "  (132, 0.10),\n",
       "  (134, 0.10),\n",
       "  (141, 0.10),\n",
       "  (145, 0.10),\n",
       "  (146, 0.10),\n",
       "  (155, 0.10),\n",
       "  (195, 0.10),\n",
       "  (199, 0.10),\n",
       "  (224, 0.10),\n",
       "  (229, 0.10),\n",
       "  (236, 0.10),\n",
       "  (258, 0.10),\n",
       "  (263, 0.10),\n",
       "  (273, 0.10),\n",
       "  (282, 0.10),\n",
       "  (284, 0.10),\n",
       "  (289, 0.10),\n",
       "  (302, 0.10),\n",
       "  (311, 0.10),\n",
       "  (315, 0.10),\n",
       "  (317, 0.10),\n",
       "  (330, 0.10),\n",
       "  (331, 0.10),\n",
       "  (346, 0.10),\n",
       "  (348, 0.10),\n",
       "  (351, 0.10),\n",
       "  (353, 0.10),\n",
       "  (358, 0.10),\n",
       "  (361, 0.10),\n",
       "  (363, 0.10),\n",
       "  (364, 0.10),\n",
       "  (365, 0.10),\n",
       "  (388, 0.10),\n",
       "  (389, 0.10),\n",
       "  (395, 0.10),\n",
       "  (399, 0.10),\n",
       "  (420, 0.10),\n",
       "  (430, 0.10),\n",
       "  (438, 0.10),\n",
       "  (442, 0.10),\n",
       "  (446, 0.10),\n",
       "  (447, 0.10),\n",
       "  (450, 0.10),\n",
       "  (452, 0.10),\n",
       "  (454, 0.10),\n",
       "  (463, 0.10),\n",
       "  (480, 0.10),\n",
       "  (491, 0.10),\n",
       "  (502, 0.10),\n",
       "  (508, 0.10),\n",
       "  (535, 0.10),\n",
       "  (541, 0.10),\n",
       "  (553, 0.10),\n",
       "  (558, 0.10),\n",
       "  (559, 0.10),\n",
       "  (579, 0.10),\n",
       "  (593, 0.10),\n",
       "  (602, 0.10),\n",
       "  (603, 0.10),\n",
       "  (605, 0.10),\n",
       "  (611, 0.10),\n",
       "  (626, 0.10),\n",
       "  (633, 0.10),\n",
       "  (639, 0.10),\n",
       "  (644, 0.10),\n",
       "  (653, 0.10),\n",
       "  (654, 0.10),\n",
       "  (655, 0.10),\n",
       "  (672, 0.10),\n",
       "  (680, 0.10),\n",
       "  (696, 0.10),\n",
       "  (700, 0.10),\n",
       "  (710, 0.10),\n",
       "  (712, 0.10),\n",
       "  (719, 0.10),\n",
       "  (723, 0.10),\n",
       "  (729, 0.10),\n",
       "  (736, 0.10),\n",
       "  (742, 0.10),\n",
       "  (745, 0.10),\n",
       "  (749, 0.10),\n",
       "  (757, 0.10),\n",
       "  (758, 0.10),\n",
       "  (764, 0.10),\n",
       "  (765, 0.10),\n",
       "  (778, 0.10),\n",
       "  (782, 0.10),\n",
       "  (792, 0.10),\n",
       "  (799, 0.10),\n",
       "  (813, 0.10),\n",
       "  (823, 0.10),\n",
       "  (825, 0.10),\n",
       "  (833, 0.10),\n",
       "  (839, 0.10),\n",
       "  (842, 0.10),\n",
       "  (853, 0.10),\n",
       "  (862, 0.10),\n",
       "  (863, 0.10),\n",
       "  (873, 0.10),\n",
       "  (874, 0.10),\n",
       "  (880, 0.10),\n",
       "  (884, 0.10),\n",
       "  (890, 0.10),\n",
       "  (896, 0.10),\n",
       "  (897, 0.10),\n",
       "  (899, 0.10),\n",
       "  (900, 0.10),\n",
       "  (910, 0.10),\n",
       "  (917, 0.10),\n",
       "  (918, 0.10),\n",
       "  (927, 0.10),\n",
       "  (938, 0.10),\n",
       "  (950, 0.10),\n",
       "  (952, 0.10),\n",
       "  (954, 0.10),\n",
       "  (985, 0.10),\n",
       "  (2, 0.00),\n",
       "  (3, 0.00),\n",
       "  (4, 0.00),\n",
       "  (5, 0.00),\n",
       "  (6, 0.00),\n",
       "  (10, 0.00),\n",
       "  (11, 0.00),\n",
       "  (12, 0.00),\n",
       "  (13, 0.00),\n",
       "  (14, 0.00),\n",
       "  (16, 0.00),\n",
       "  (20, 0.00),\n",
       "  (21, 0.00),\n",
       "  (22, 0.00),\n",
       "  (23, 0.00),\n",
       "  (26, 0.00),\n",
       "  (27, 0.00),\n",
       "  (29, 0.00),\n",
       "  (30, 0.00),\n",
       "  (32, 0.00),\n",
       "  (34, 0.00),\n",
       "  (35, 0.00),\n",
       "  (36, 0.00),\n",
       "  (38, 0.00),\n",
       "  (41, 0.00),\n",
       "  (43, 0.00),\n",
       "  (44, 0.00),\n",
       "  (49, 0.00),\n",
       "  (51, 0.00),\n",
       "  (53, 0.00),\n",
       "  (54, 0.00),\n",
       "  (58, 0.00),\n",
       "  (59, 0.00),\n",
       "  (65, 0.00),\n",
       "  (66, 0.00),\n",
       "  (69, 0.00),\n",
       "  (70, 0.00),\n",
       "  (71, 0.00),\n",
       "  (73, 0.00),\n",
       "  (78, 0.00),\n",
       "  (79, 0.00),\n",
       "  (80, 0.00),\n",
       "  (81, 0.00),\n",
       "  (85, 0.00),\n",
       "  (88, 0.00),\n",
       "  (93, 0.00),\n",
       "  (100, 0.00),\n",
       "  (101, 0.00),\n",
       "  (103, 0.00),\n",
       "  (104, 0.00),\n",
       "  (106, 0.00),\n",
       "  (110, 0.00),\n",
       "  (111, 0.00),\n",
       "  (112, 0.00),\n",
       "  (114, 0.00),\n",
       "  (117, 0.00),\n",
       "  (119, 0.00),\n",
       "  (121, 0.00),\n",
       "  (122, 0.00),\n",
       "  (125, 0.00),\n",
       "  (126, 0.00),\n",
       "  (127, 0.00),\n",
       "  (129, 0.00),\n",
       "  (130, 0.00),\n",
       "  (133, 0.00),\n",
       "  (135, 0.00),\n",
       "  (136, 0.00),\n",
       "  (137, 0.00),\n",
       "  (139, 0.00),\n",
       "  (140, 0.00),\n",
       "  (142, 0.00),\n",
       "  (143, 0.00),\n",
       "  (147, 0.00),\n",
       "  (148, 0.00),\n",
       "  (149, 0.00),\n",
       "  (150, 0.00),\n",
       "  (152, 0.00),\n",
       "  (153, 0.00),\n",
       "  (154, 0.00),\n",
       "  (156, 0.00),\n",
       "  (158, 0.00),\n",
       "  (160, 0.00),\n",
       "  (161, 0.00),\n",
       "  (162, 0.00),\n",
       "  (165, 0.00),\n",
       "  (166, 0.00),\n",
       "  (167, 0.00),\n",
       "  (168, 0.00),\n",
       "  (169, 0.00),\n",
       "  (170, 0.00),\n",
       "  (172, 0.00),\n",
       "  (173, 0.00),\n",
       "  (174, 0.00),\n",
       "  (175, 0.00),\n",
       "  (177, 0.00),\n",
       "  (178, 0.00),\n",
       "  (179, 0.00),\n",
       "  (180, 0.00),\n",
       "  (181, 0.00),\n",
       "  (182, 0.00),\n",
       "  (183, 0.00),\n",
       "  (184, 0.00),\n",
       "  (185, 0.00),\n",
       "  (187, 0.00),\n",
       "  (188, 0.00),\n",
       "  (190, 0.00),\n",
       "  (191, 0.00),\n",
       "  (192, 0.00),\n",
       "  (193, 0.00),\n",
       "  (194, 0.00),\n",
       "  (196, 0.00),\n",
       "  (197, 0.00),\n",
       "  (198, 0.00),\n",
       "  (200, 0.00),\n",
       "  (201, 0.00),\n",
       "  (202, 0.00),\n",
       "  (203, 0.00),\n",
       "  (204, 0.00),\n",
       "  (206, 0.00),\n",
       "  (208, 0.00),\n",
       "  (209, 0.00),\n",
       "  (210, 0.00),\n",
       "  (211, 0.00),\n",
       "  (212, 0.00),\n",
       "  (213, 0.00),\n",
       "  (214, 0.00),\n",
       "  (215, 0.00),\n",
       "  (216, 0.00),\n",
       "  (217, 0.00),\n",
       "  (220, 0.00),\n",
       "  (221, 0.00),\n",
       "  (222, 0.00),\n",
       "  (223, 0.00),\n",
       "  (225, 0.00),\n",
       "  (226, 0.00),\n",
       "  (227, 0.00),\n",
       "  (228, 0.00),\n",
       "  (232, 0.00),\n",
       "  (233, 0.00),\n",
       "  (234, 0.00),\n",
       "  (237, 0.00),\n",
       "  (238, 0.00),\n",
       "  (239, 0.00),\n",
       "  (240, 0.00),\n",
       "  (241, 0.00),\n",
       "  (243, 0.00),\n",
       "  (244, 0.00),\n",
       "  (245, 0.00),\n",
       "  (246, 0.00),\n",
       "  (248, 0.00),\n",
       "  (250, 0.00),\n",
       "  (251, 0.00),\n",
       "  (252, 0.00),\n",
       "  (253, 0.00),\n",
       "  (255, 0.00),\n",
       "  (256, 0.00),\n",
       "  (257, 0.00),\n",
       "  (259, 0.00),\n",
       "  (261, 0.00),\n",
       "  (262, 0.00),\n",
       "  (264, 0.00),\n",
       "  (265, 0.00),\n",
       "  (266, 0.00),\n",
       "  (267, 0.00),\n",
       "  (268, 0.00),\n",
       "  (269, 0.00),\n",
       "  (270, 0.00),\n",
       "  (271, 0.00),\n",
       "  (272, 0.00),\n",
       "  (276, 0.00),\n",
       "  (277, 0.00),\n",
       "  (278, 0.00),\n",
       "  (279, 0.00),\n",
       "  (280, 0.00),\n",
       "  (283, 0.00),\n",
       "  (285, 0.00),\n",
       "  (286, 0.00),\n",
       "  (287, 0.00),\n",
       "  (288, 0.00),\n",
       "  (295, 0.00),\n",
       "  (296, 0.00),\n",
       "  (297, 0.00),\n",
       "  (298, 0.00),\n",
       "  (299, 0.00),\n",
       "  (303, 0.00),\n",
       "  (305, 0.00),\n",
       "  (309, 0.00),\n",
       "  (310, 0.00),\n",
       "  (312, 0.00),\n",
       "  (320, 0.00),\n",
       "  (322, 0.00),\n",
       "  (323, 0.00),\n",
       "  (324, 0.00),\n",
       "  (325, 0.00),\n",
       "  (326, 0.00),\n",
       "  (328, 0.00),\n",
       "  (329, 0.00),\n",
       "  (332, 0.00),\n",
       "  (333, 0.00),\n",
       "  (335, 0.00),\n",
       "  (337, 0.00),\n",
       "  (338, 0.00),\n",
       "  (339, 0.00),\n",
       "  (341, 0.00),\n",
       "  (343, 0.00),\n",
       "  (344, 0.00),\n",
       "  (345, 0.00),\n",
       "  (349, 0.00),\n",
       "  (350, 0.00),\n",
       "  (352, 0.00),\n",
       "  (354, 0.00),\n",
       "  (356, 0.00),\n",
       "  (357, 0.00),\n",
       "  (359, 0.00),\n",
       "  (362, 0.00),\n",
       "  (366, 0.00),\n",
       "  (367, 0.00),\n",
       "  (368, 0.00),\n",
       "  (370, 0.00),\n",
       "  (371, 0.00),\n",
       "  (372, 0.00),\n",
       "  (373, 0.00),\n",
       "  (374, 0.00),\n",
       "  (377, 0.00),\n",
       "  (378, 0.00),\n",
       "  (379, 0.00),\n",
       "  (380, 0.00),\n",
       "  (382, 0.00),\n",
       "  (384, 0.00),\n",
       "  (385, 0.00),\n",
       "  (386, 0.00),\n",
       "  (390, 0.00),\n",
       "  (391, 0.00),\n",
       "  (394, 0.00),\n",
       "  (400, 0.00),\n",
       "  (402, 0.00),\n",
       "  (403, 0.00),\n",
       "  (404, 0.00),\n",
       "  (405, 0.00),\n",
       "  (413, 0.00),\n",
       "  (415, 0.00),\n",
       "  (416, 0.00),\n",
       "  (418, 0.00),\n",
       "  (421, 0.00),\n",
       "  (422, 0.00),\n",
       "  (426, 0.00),\n",
       "  (427, 0.00),\n",
       "  (428, 0.00),\n",
       "  (432, 0.00),\n",
       "  (434, 0.00),\n",
       "  (435, 0.00),\n",
       "  (436, 0.00),\n",
       "  (439, 0.00),\n",
       "  (448, 0.00),\n",
       "  (449, 0.00),\n",
       "  (451, 0.00),\n",
       "  (455, 0.00),\n",
       "  (456, 0.00),\n",
       "  (458, 0.00),\n",
       "  (460, 0.00),\n",
       "  (461, 0.00),\n",
       "  (462, 0.00),\n",
       "  (466, 0.00),\n",
       "  (467, 0.00),\n",
       "  (469, 0.00),\n",
       "  (470, 0.00),\n",
       "  (471, 0.00),\n",
       "  (473, 0.00),\n",
       "  (475, 0.00),\n",
       "  (477, 0.00),\n",
       "  (479, 0.00),\n",
       "  (484, 0.00),\n",
       "  (486, 0.00),\n",
       "  (487, 0.00),\n",
       "  (493, 0.00),\n",
       "  (494, 0.00),\n",
       "  (499, 0.00),\n",
       "  (500, 0.00),\n",
       "  (501, 0.00),\n",
       "  (503, 0.00),\n",
       "  (504, 0.00),\n",
       "  (510, 0.00),\n",
       "  (511, 0.00),\n",
       "  (512, 0.00),\n",
       "  (513, 0.00),\n",
       "  (514, 0.00),\n",
       "  (519, 0.00),\n",
       "  (521, 0.00),\n",
       "  (522, 0.00),\n",
       "  (524, 0.00),\n",
       "  (525, 0.00),\n",
       "  (529, 0.00),\n",
       "  (531, 0.00),\n",
       "  (534, 0.00),\n",
       "  (536, 0.00),\n",
       "  (537, 0.00),\n",
       "  (540, 0.00),\n",
       "  (542, 0.00),\n",
       "  (543, 0.00),\n",
       "  (544, 0.00),\n",
       "  (546, 0.00),\n",
       "  (548, 0.00),\n",
       "  (550, 0.00),\n",
       "  (551, 0.00),\n",
       "  (554, 0.00),\n",
       "  (557, 0.00),\n",
       "  (560, 0.00),\n",
       "  (561, 0.00),\n",
       "  (563, 0.00),\n",
       "  (564, 0.00),\n",
       "  (567, 0.00),\n",
       "  (568, 0.00),\n",
       "  (569, 0.00),\n",
       "  (571, 0.00),\n",
       "  (573, 0.00),\n",
       "  (574, 0.00),\n",
       "  (577, 0.00),\n",
       "  (578, 0.00),\n",
       "  (583, 0.00),\n",
       "  (585, 0.00),\n",
       "  (586, 0.00),\n",
       "  (587, 0.00),\n",
       "  (589, 0.00),\n",
       "  (590, 0.00),\n",
       "  (592, 0.00),\n",
       "  (594, 0.00),\n",
       "  (596, 0.00),\n",
       "  (597, 0.00),\n",
       "  (598, 0.00),\n",
       "  (600, 0.00),\n",
       "  (601, 0.00),\n",
       "  (608, 0.00),\n",
       "  (610, 0.00),\n",
       "  (613, 0.00),\n",
       "  (615, 0.00),\n",
       "  (617, 0.00),\n",
       "  (623, 0.00),\n",
       "  (624, 0.00),\n",
       "  (628, 0.00),\n",
       "  (630, 0.00),\n",
       "  (631, 0.00),\n",
       "  (634, 0.00),\n",
       "  (635, 0.00),\n",
       "  (640, 0.00),\n",
       "  (642, 0.00),\n",
       "  (647, 0.00),\n",
       "  (648, 0.00),\n",
       "  (649, 0.00),\n",
       "  (650, 0.00),\n",
       "  (652, 0.00),\n",
       "  (657, 0.00),\n",
       "  (658, 0.00),\n",
       "  (659, 0.00),\n",
       "  (660, 0.00),\n",
       "  (662, 0.00),\n",
       "  (663, 0.00),\n",
       "  (664, 0.00),\n",
       "  (665, 0.00),\n",
       "  (666, 0.00),\n",
       "  (667, 0.00),\n",
       "  (670, 0.00),\n",
       "  (673, 0.00),\n",
       "  (675, 0.00),\n",
       "  (677, 0.00),\n",
       "  (678, 0.00),\n",
       "  (681, 0.00),\n",
       "  (682, 0.00),\n",
       "  (683, 0.00),\n",
       "  (685, 0.00),\n",
       "  (686, 0.00),\n",
       "  (687, 0.00),\n",
       "  (688, 0.00),\n",
       "  (689, 0.00),\n",
       "  (690, 0.00),\n",
       "  (693, 0.00),\n",
       "  (695, 0.00),\n",
       "  (698, 0.00),\n",
       "  (699, 0.00),\n",
       "  (702, 0.00),\n",
       "  (704, 0.00),\n",
       "  (707, 0.00),\n",
       "  (708, 0.00),\n",
       "  (713, 0.00),\n",
       "  (714, 0.00),\n",
       "  (715, 0.00),\n",
       "  (718, 0.00),\n",
       "  (720, 0.00),\n",
       "  (724, 0.00),\n",
       "  (726, 0.00),\n",
       "  (730, 0.00),\n",
       "  (731, 0.00),\n",
       "  (733, 0.00),\n",
       "  (739, 0.00),\n",
       "  (740, 0.00),\n",
       "  (744, 0.00),\n",
       "  (747, 0.00),\n",
       "  (752, 0.00),\n",
       "  (755, 0.00),\n",
       "  (756, 0.00),\n",
       "  (759, 0.00),\n",
       "  (760, 0.00),\n",
       "  (761, 0.00),\n",
       "  (763, 0.00),\n",
       "  (766, 0.00),\n",
       "  (767, 0.00),\n",
       "  (769, 0.00),\n",
       "  (771, 0.00),\n",
       "  (773, 0.00),\n",
       "  (774, 0.00),\n",
       "  (775, 0.00),\n",
       "  (776, 0.00),\n",
       "  (777, 0.00),\n",
       "  (780, 0.00),\n",
       "  (784, 0.00),\n",
       "  (785, 0.00),\n",
       "  (789, 0.00),\n",
       "  (793, 0.00),\n",
       "  (795, 0.00),\n",
       "  (797, 0.00),\n",
       "  (798, 0.00),\n",
       "  (800, 0.00),\n",
       "  (804, 0.00),\n",
       "  (807, 0.00),\n",
       "  (810, 0.00),\n",
       "  (811, 0.00),\n",
       "  (812, 0.00),\n",
       "  (818, 0.00),\n",
       "  (821, 0.00),\n",
       "  (827, 0.00),\n",
       "  (831, 0.00),\n",
       "  (834, 0.00),\n",
       "  (835, 0.00),\n",
       "  (838, 0.00),\n",
       "  (840, 0.00),\n",
       "  (841, 0.00),\n",
       "  (844, 0.00),\n",
       "  (845, 0.00),\n",
       "  (846, 0.00),\n",
       "  (848, 0.00),\n",
       "  (849, 0.00),\n",
       "  (851, 0.00),\n",
       "  (855, 0.00),\n",
       "  (856, 0.00),\n",
       "  (859, 0.00),\n",
       "  (860, 0.00),\n",
       "  (861, 0.00),\n",
       "  (866, 0.00),\n",
       "  (869, 0.00),\n",
       "  (875, 0.00),\n",
       "  (876, 0.00),\n",
       "  (877, 0.00),\n",
       "  (882, 0.00),\n",
       "  (883, 0.00),\n",
       "  (885, 0.00),\n",
       "  (887, 0.00),\n",
       "  (888, 0.00),\n",
       "  (891, 0.00),\n",
       "  (894, 0.00),\n",
       "  (895, 0.00),\n",
       "  (901, 0.00),\n",
       "  (902, 0.00),\n",
       "  (903, 0.00),\n",
       "  (908, 0.00),\n",
       "  (909, 0.00),\n",
       "  (911, 0.00),\n",
       "  (912, 0.00),\n",
       "  (913, 0.00),\n",
       "  (914, 0.00),\n",
       "  (916, 0.00),\n",
       "  (919, 0.00),\n",
       "  (921, 0.00),\n",
       "  (922, 0.00),\n",
       "  (924, 0.00),\n",
       "  (925, 0.00),\n",
       "  (926, 0.00),\n",
       "  (928, 0.00),\n",
       "  (929, 0.00),\n",
       "  (930, 0.00),\n",
       "  (931, 0.00),\n",
       "  (932, 0.00),\n",
       "  (933, 0.00),\n",
       "  (935, 0.00),\n",
       "  (936, 0.00),\n",
       "  (940, 0.00),\n",
       "  (941, 0.00),\n",
       "  (942, 0.00),\n",
       "  (943, 0.00),\n",
       "  (945, 0.00),\n",
       "  (947, 0.00),\n",
       "  (948, 0.00),\n",
       "  (949, 0.00),\n",
       "  (951, 0.00),\n",
       "  (956, 0.00),\n",
       "  (957, 0.00),\n",
       "  (958, 0.00),\n",
       "  (959, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (962, 0.00),\n",
       "  (964, 0.00),\n",
       "  (965, 0.00),\n",
       "  (966, 0.00),\n",
       "  (967, 0.00),\n",
       "  (968, 0.00),\n",
       "  (969, 0.00),\n",
       "  (970, 0.00),\n",
       "  (972, 0.00),\n",
       "  (974, 0.00),\n",
       "  (975, 0.00),\n",
       "  (976, 0.00),\n",
       "  (977, 0.00),\n",
       "  (978, 0.00),\n",
       "  (979, 0.00),\n",
       "  (980, 0.00),\n",
       "  (983, 0.00),\n",
       "  (986, 0.00),\n",
       "  (988, 0.00),\n",
       "  (989, 0.00),\n",
       "  (990, 0.00),\n",
       "  (991, 0.00),\n",
       "  (993, 0.00),\n",
       "  (994, 0.00),\n",
       "  (995, 0.00)])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 5\n",
      "at batch no 10\n",
      "at batch no 15\n",
      "at batch no 20\n",
      "at batch no 25\n",
      "at batch no 30\n",
      "at batch no 35\n",
      "at batch no 40\n",
      "at batch no 45\n",
      "at batch no 50\n",
      "at batch no 55\n",
      "at batch no 60\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(180,\n",
       " [(971, 177.60),\n",
       "  (721, 106.60),\n",
       "  (669, 94.00),\n",
       "  (556, 56.00),\n",
       "  (520, 49.70),\n",
       "  (588, 34.00),\n",
       "  (431, 30.80),\n",
       "  (828, 29.80),\n",
       "  (61, 26.20),\n",
       "  (750, 20.90),\n",
       "  (414, 19.90),\n",
       "  (794, 14.50),\n",
       "  (411, 14.10),\n",
       "  (599, 12.80),\n",
       "  (39, 11.60),\n",
       "  (904, 11.40),\n",
       "  (709, 7.30),\n",
       "  (84, 6.80),\n",
       "  (604, 6.50),\n",
       "  (570, 6.20),\n",
       "  (801, 6.20),\n",
       "  (490, 6.10),\n",
       "  (489, 5.70),\n",
       "  (770, 5.00),\n",
       "  (837, 4.20),\n",
       "  (581, 4.00),\n",
       "  (955, 3.90),\n",
       "  (48, 3.80),\n",
       "  (60, 3.40),\n",
       "  (790, 2.80),\n",
       "  (907, 2.80),\n",
       "  (879, 2.40),\n",
       "  (973, 2.40),\n",
       "  (56, 2.30),\n",
       "  (572, 2.20),\n",
       "  (893, 2.10),\n",
       "  (987, 2.10),\n",
       "  (108, 2.00),\n",
       "  (419, 2.00),\n",
       "  (806, 2.00),\n",
       "  (621, 1.90),\n",
       "  (55, 1.80),\n",
       "  (516, 1.80),\n",
       "  (591, 1.80),\n",
       "  (124, 1.70),\n",
       "  (788, 1.70),\n",
       "  (872, 1.70),\n",
       "  (68, 1.60),\n",
       "  (401, 1.60),\n",
       "  (464, 1.60),\n",
       "  (953, 1.60),\n",
       "  (575, 1.50),\n",
       "  (614, 1.50),\n",
       "  (711, 1.50),\n",
       "  (62, 1.40),\n",
       "  (67, 1.40),\n",
       "  (118, 1.40),\n",
       "  (641, 1.40),\n",
       "  (865, 1.40),\n",
       "  (300, 1.30),\n",
       "  (518, 1.30),\n",
       "  (646, 1.30),\n",
       "  (892, 1.30),\n",
       "  (151, 1.20),\n",
       "  (406, 1.20),\n",
       "  (443, 1.20),\n",
       "  (468, 1.20),\n",
       "  (496, 1.20),\n",
       "  (580, 1.20),\n",
       "  (619, 1.20),\n",
       "  (815, 1.20),\n",
       "  (868, 1.20),\n",
       "  (306, 1.10),\n",
       "  (476, 1.10),\n",
       "  (651, 1.10),\n",
       "  (738, 1.10),\n",
       "  (754, 1.10),\n",
       "  (762, 1.10),\n",
       "  (791, 1.10),\n",
       "  (864, 1.10),\n",
       "  (7, 1.00),\n",
       "  (33, 1.00),\n",
       "  (57, 1.00),\n",
       "  (91, 1.00),\n",
       "  (96, 1.00),\n",
       "  (105, 1.00),\n",
       "  (120, 1.00),\n",
       "  (123, 1.00),\n",
       "  (128, 1.00),\n",
       "  (163, 1.00),\n",
       "  (186, 1.00),\n",
       "  (242, 1.00),\n",
       "  (247, 1.00),\n",
       "  (254, 1.00),\n",
       "  (260, 1.00),\n",
       "  (274, 1.00),\n",
       "  (281, 1.00),\n",
       "  (290, 1.00),\n",
       "  (292, 1.00),\n",
       "  (293, 1.00),\n",
       "  (314, 1.00),\n",
       "  (316, 1.00),\n",
       "  (334, 1.00),\n",
       "  (376, 1.00),\n",
       "  (392, 1.00),\n",
       "  (429, 1.00),\n",
       "  (445, 1.00),\n",
       "  (453, 1.00),\n",
       "  (474, 1.00),\n",
       "  (485, 1.00),\n",
       "  (488, 1.00),\n",
       "  (498, 1.00),\n",
       "  (506, 1.00),\n",
       "  (507, 1.00),\n",
       "  (526, 1.00),\n",
       "  (528, 1.00),\n",
       "  (533, 1.00),\n",
       "  (566, 1.00),\n",
       "  (625, 1.00),\n",
       "  (637, 1.00),\n",
       "  (661, 1.00),\n",
       "  (694, 1.00),\n",
       "  (746, 1.00),\n",
       "  (753, 1.00),\n",
       "  (783, 1.00),\n",
       "  (787, 1.00),\n",
       "  (796, 1.00),\n",
       "  (816, 1.00),\n",
       "  (826, 1.00),\n",
       "  (857, 1.00),\n",
       "  (858, 1.00),\n",
       "  (898, 1.00),\n",
       "  (923, 1.00),\n",
       "  (934, 1.00),\n",
       "  (937, 1.00),\n",
       "  (944, 1.00),\n",
       "  (982, 1.00),\n",
       "  (992, 1.00),\n",
       "  (997, 1.00),\n",
       "  (15, 0.90),\n",
       "  (25, 0.90),\n",
       "  (115, 0.90),\n",
       "  (301, 0.90),\n",
       "  (360, 0.90),\n",
       "  (393, 0.90),\n",
       "  (440, 0.90),\n",
       "  (482, 0.90),\n",
       "  (523, 0.90),\n",
       "  (612, 0.90),\n",
       "  (83, 0.80),\n",
       "  (90, 0.80),\n",
       "  (304, 0.80),\n",
       "  (307, 0.80),\n",
       "  (342, 0.80),\n",
       "  (398, 0.80),\n",
       "  (412, 0.80),\n",
       "  (457, 0.80),\n",
       "  (576, 0.80),\n",
       "  (645, 0.80),\n",
       "  (656, 0.80),\n",
       "  (691, 0.80),\n",
       "  (716, 0.80),\n",
       "  (734, 0.80),\n",
       "  (803, 0.80),\n",
       "  (819, 0.80),\n",
       "  (847, 0.80),\n",
       "  (1, 0.70),\n",
       "  (37, 0.70),\n",
       "  (45, 0.70),\n",
       "  (46, 0.70),\n",
       "  (87, 0.70),\n",
       "  (176, 0.70),\n",
       "  (235, 0.70),\n",
       "  (530, 0.70),\n",
       "  (532, 0.70),\n",
       "  (538, 0.70),\n",
       "  (539, 0.70),\n",
       "  (562, 0.70),\n",
       "  (735, 0.70),\n",
       "  (748, 0.70),\n",
       "  (768, 0.70),\n",
       "  (781, 0.70),\n",
       "  (850, 0.70),\n",
       "  (871, 0.70),\n",
       "  (889, 0.70),\n",
       "  (939, 0.70),\n",
       "  (8, 0.60),\n",
       "  (52, 0.60),\n",
       "  (86, 0.60),\n",
       "  (189, 0.60),\n",
       "  (219, 0.60),\n",
       "  (355, 0.60),\n",
       "  (387, 0.60),\n",
       "  (492, 0.60),\n",
       "  (509, 0.60),\n",
       "  (547, 0.60),\n",
       "  (582, 0.60),\n",
       "  (620, 0.60),\n",
       "  (697, 0.60),\n",
       "  (705, 0.60),\n",
       "  (728, 0.60),\n",
       "  (779, 0.60),\n",
       "  (805, 0.60),\n",
       "  (963, 0.60),\n",
       "  (102, 0.50),\n",
       "  (291, 0.50),\n",
       "  (294, 0.50),\n",
       "  (396, 0.50),\n",
       "  (408, 0.50),\n",
       "  (433, 0.50),\n",
       "  (441, 0.50),\n",
       "  (444, 0.50),\n",
       "  (483, 0.50),\n",
       "  (552, 0.50),\n",
       "  (629, 0.50),\n",
       "  (643, 0.50),\n",
       "  (676, 0.50),\n",
       "  (684, 0.50),\n",
       "  (741, 0.50),\n",
       "  (752, 0.50),\n",
       "  (814, 0.50),\n",
       "  (47, 0.40),\n",
       "  (109, 0.40),\n",
       "  (205, 0.40),\n",
       "  (218, 0.40),\n",
       "  (336, 0.40),\n",
       "  (369, 0.40),\n",
       "  (397, 0.40),\n",
       "  (409, 0.40),\n",
       "  (410, 0.40),\n",
       "  (417, 0.40),\n",
       "  (459, 0.40),\n",
       "  (472, 0.40),\n",
       "  (515, 0.40),\n",
       "  (632, 0.40),\n",
       "  (698, 0.40),\n",
       "  (725, 0.40),\n",
       "  (829, 0.40),\n",
       "  (836, 0.40),\n",
       "  (854, 0.40),\n",
       "  (906, 0.40),\n",
       "  (981, 0.40),\n",
       "  (0, 0.30),\n",
       "  (40, 0.30),\n",
       "  (42, 0.30),\n",
       "  (116, 0.30),\n",
       "  (157, 0.30),\n",
       "  (159, 0.30),\n",
       "  (231, 0.30),\n",
       "  (327, 0.30),\n",
       "  (375, 0.30),\n",
       "  (407, 0.30),\n",
       "  (508, 0.30),\n",
       "  (517, 0.30),\n",
       "  (545, 0.30),\n",
       "  (555, 0.30),\n",
       "  (654, 0.30),\n",
       "  (680, 0.30),\n",
       "  (786, 0.30),\n",
       "  (822, 0.30),\n",
       "  (824, 0.30),\n",
       "  (897, 0.30),\n",
       "  (28, 0.20),\n",
       "  (63, 0.20),\n",
       "  (76, 0.20),\n",
       "  (92, 0.20),\n",
       "  (94, 0.20),\n",
       "  (107, 0.20),\n",
       "  (164, 0.20),\n",
       "  (171, 0.20),\n",
       "  (249, 0.20),\n",
       "  (289, 0.20),\n",
       "  (318, 0.20),\n",
       "  (347, 0.20),\n",
       "  (353, 0.20),\n",
       "  (363, 0.20),\n",
       "  (383, 0.20),\n",
       "  (399, 0.20),\n",
       "  (424, 0.20),\n",
       "  (425, 0.20),\n",
       "  (447, 0.20),\n",
       "  (481, 0.20),\n",
       "  (505, 0.20),\n",
       "  (605, 0.20),\n",
       "  (607, 0.20),\n",
       "  (655, 0.20),\n",
       "  (671, 0.20),\n",
       "  (706, 0.20),\n",
       "  (719, 0.20),\n",
       "  (722, 0.20),\n",
       "  (732, 0.20),\n",
       "  (743, 0.20),\n",
       "  (751, 0.20),\n",
       "  (808, 0.20),\n",
       "  (817, 0.20),\n",
       "  (820, 0.20),\n",
       "  (830, 0.20),\n",
       "  (843, 0.20),\n",
       "  (886, 0.20),\n",
       "  (910, 0.20),\n",
       "  (918, 0.20),\n",
       "  (932, 0.20),\n",
       "  (18, 0.10),\n",
       "  (19, 0.10),\n",
       "  (31, 0.10),\n",
       "  (66, 0.10),\n",
       "  (75, 0.10),\n",
       "  (97, 0.10),\n",
       "  (155, 0.10),\n",
       "  (199, 0.10),\n",
       "  (213, 0.10),\n",
       "  (230, 0.10),\n",
       "  (283, 0.10),\n",
       "  (308, 0.10),\n",
       "  (309, 0.10),\n",
       "  (319, 0.10),\n",
       "  (321, 0.10),\n",
       "  (430, 0.10),\n",
       "  (495, 0.10),\n",
       "  (541, 0.10),\n",
       "  (586, 0.10),\n",
       "  (595, 0.10),\n",
       "  (609, 0.10),\n",
       "  (633, 0.10),\n",
       "  (668, 0.10),\n",
       "  (703, 0.10),\n",
       "  (727, 0.10),\n",
       "  (736, 0.10),\n",
       "  (737, 0.10),\n",
       "  (792, 0.10),\n",
       "  (848, 0.10),\n",
       "  (867, 0.10),\n",
       "  (878, 0.10),\n",
       "  (887, 0.10),\n",
       "  (905, 0.10),\n",
       "  (938, 0.10),\n",
       "  (984, 0.10),\n",
       "  (2, 0.00),\n",
       "  (3, 0.00),\n",
       "  (4, 0.00),\n",
       "  (5, 0.00),\n",
       "  (6, 0.00),\n",
       "  (9, 0.00),\n",
       "  (10, 0.00),\n",
       "  (11, 0.00),\n",
       "  (12, 0.00),\n",
       "  (13, 0.00),\n",
       "  (14, 0.00),\n",
       "  (16, 0.00),\n",
       "  (17, 0.00),\n",
       "  (20, 0.00),\n",
       "  (21, 0.00),\n",
       "  (22, 0.00),\n",
       "  (23, 0.00),\n",
       "  (24, 0.00),\n",
       "  (26, 0.00),\n",
       "  (27, 0.00),\n",
       "  (29, 0.00),\n",
       "  (30, 0.00),\n",
       "  (32, 0.00),\n",
       "  (34, 0.00),\n",
       "  (35, 0.00),\n",
       "  (36, 0.00),\n",
       "  (38, 0.00),\n",
       "  (41, 0.00),\n",
       "  (43, 0.00),\n",
       "  (44, 0.00),\n",
       "  (49, 0.00),\n",
       "  (50, 0.00),\n",
       "  (51, 0.00),\n",
       "  (53, 0.00),\n",
       "  (54, 0.00),\n",
       "  (58, 0.00),\n",
       "  (59, 0.00),\n",
       "  (64, 0.00),\n",
       "  (65, 0.00),\n",
       "  (69, 0.00),\n",
       "  (70, 0.00),\n",
       "  (71, 0.00),\n",
       "  (72, 0.00),\n",
       "  (73, 0.00),\n",
       "  (74, 0.00),\n",
       "  (77, 0.00),\n",
       "  (78, 0.00),\n",
       "  (79, 0.00),\n",
       "  (80, 0.00),\n",
       "  (81, 0.00),\n",
       "  (82, 0.00),\n",
       "  (85, 0.00),\n",
       "  (88, 0.00),\n",
       "  (89, 0.00),\n",
       "  (93, 0.00),\n",
       "  (95, 0.00),\n",
       "  (98, 0.00),\n",
       "  (99, 0.00),\n",
       "  (100, 0.00),\n",
       "  (101, 0.00),\n",
       "  (103, 0.00),\n",
       "  (104, 0.00),\n",
       "  (106, 0.00),\n",
       "  (110, 0.00),\n",
       "  (111, 0.00),\n",
       "  (112, 0.00),\n",
       "  (113, 0.00),\n",
       "  (114, 0.00),\n",
       "  (117, 0.00),\n",
       "  (119, 0.00),\n",
       "  (121, 0.00),\n",
       "  (122, 0.00),\n",
       "  (125, 0.00),\n",
       "  (126, 0.00),\n",
       "  (127, 0.00),\n",
       "  (129, 0.00),\n",
       "  (130, 0.00),\n",
       "  (131, 0.00),\n",
       "  (132, 0.00),\n",
       "  (133, 0.00),\n",
       "  (134, 0.00),\n",
       "  (135, 0.00),\n",
       "  (136, 0.00),\n",
       "  (137, 0.00),\n",
       "  (138, 0.00),\n",
       "  (139, 0.00),\n",
       "  (140, 0.00),\n",
       "  (141, 0.00),\n",
       "  (142, 0.00),\n",
       "  (143, 0.00),\n",
       "  (144, 0.00),\n",
       "  (145, 0.00),\n",
       "  (146, 0.00),\n",
       "  (147, 0.00),\n",
       "  (148, 0.00),\n",
       "  (149, 0.00),\n",
       "  (150, 0.00),\n",
       "  (152, 0.00),\n",
       "  (153, 0.00),\n",
       "  (154, 0.00),\n",
       "  (156, 0.00),\n",
       "  (158, 0.00),\n",
       "  (160, 0.00),\n",
       "  (161, 0.00),\n",
       "  (162, 0.00),\n",
       "  (165, 0.00),\n",
       "  (166, 0.00),\n",
       "  (167, 0.00),\n",
       "  (168, 0.00),\n",
       "  (169, 0.00),\n",
       "  (170, 0.00),\n",
       "  (172, 0.00),\n",
       "  (173, 0.00),\n",
       "  (174, 0.00),\n",
       "  (175, 0.00),\n",
       "  (177, 0.00),\n",
       "  (178, 0.00),\n",
       "  (179, 0.00),\n",
       "  (180, 0.00),\n",
       "  (181, 0.00),\n",
       "  (182, 0.00),\n",
       "  (183, 0.00),\n",
       "  (184, 0.00),\n",
       "  (185, 0.00),\n",
       "  (187, 0.00),\n",
       "  (188, 0.00),\n",
       "  (190, 0.00),\n",
       "  (191, 0.00),\n",
       "  (192, 0.00),\n",
       "  (193, 0.00),\n",
       "  (194, 0.00),\n",
       "  (195, 0.00),\n",
       "  (196, 0.00),\n",
       "  (197, 0.00),\n",
       "  (198, 0.00),\n",
       "  (200, 0.00),\n",
       "  (201, 0.00),\n",
       "  (202, 0.00),\n",
       "  (203, 0.00),\n",
       "  (204, 0.00),\n",
       "  (206, 0.00),\n",
       "  (207, 0.00),\n",
       "  (208, 0.00),\n",
       "  (209, 0.00),\n",
       "  (210, 0.00),\n",
       "  (211, 0.00),\n",
       "  (212, 0.00),\n",
       "  (214, 0.00),\n",
       "  (215, 0.00),\n",
       "  (216, 0.00),\n",
       "  (217, 0.00),\n",
       "  (220, 0.00),\n",
       "  (221, 0.00),\n",
       "  (222, 0.00),\n",
       "  (223, 0.00),\n",
       "  (224, 0.00),\n",
       "  (225, 0.00),\n",
       "  (226, 0.00),\n",
       "  (227, 0.00),\n",
       "  (228, 0.00),\n",
       "  (229, 0.00),\n",
       "  (232, 0.00),\n",
       "  (233, 0.00),\n",
       "  (234, 0.00),\n",
       "  (236, 0.00),\n",
       "  (237, 0.00),\n",
       "  (238, 0.00),\n",
       "  (239, 0.00),\n",
       "  (240, 0.00),\n",
       "  (241, 0.00),\n",
       "  (243, 0.00),\n",
       "  (244, 0.00),\n",
       "  (245, 0.00),\n",
       "  (246, 0.00),\n",
       "  (248, 0.00),\n",
       "  (250, 0.00),\n",
       "  (251, 0.00),\n",
       "  (252, 0.00),\n",
       "  (253, 0.00),\n",
       "  (255, 0.00),\n",
       "  (256, 0.00),\n",
       "  (257, 0.00),\n",
       "  (258, 0.00),\n",
       "  (259, 0.00),\n",
       "  (261, 0.00),\n",
       "  (262, 0.00),\n",
       "  (263, 0.00),\n",
       "  (264, 0.00),\n",
       "  (265, 0.00),\n",
       "  (266, 0.00),\n",
       "  (267, 0.00),\n",
       "  (268, 0.00),\n",
       "  (269, 0.00),\n",
       "  (270, 0.00),\n",
       "  (271, 0.00),\n",
       "  (272, 0.00),\n",
       "  (273, 0.00),\n",
       "  (275, 0.00),\n",
       "  (276, 0.00),\n",
       "  (277, 0.00),\n",
       "  (278, 0.00),\n",
       "  (279, 0.00),\n",
       "  (280, 0.00),\n",
       "  (282, 0.00),\n",
       "  (284, 0.00),\n",
       "  (285, 0.00),\n",
       "  (286, 0.00),\n",
       "  (287, 0.00),\n",
       "  (288, 0.00),\n",
       "  (295, 0.00),\n",
       "  (296, 0.00),\n",
       "  (297, 0.00),\n",
       "  (298, 0.00),\n",
       "  (299, 0.00),\n",
       "  (302, 0.00),\n",
       "  (303, 0.00),\n",
       "  (305, 0.00),\n",
       "  (310, 0.00),\n",
       "  (311, 0.00),\n",
       "  (312, 0.00),\n",
       "  (313, 0.00),\n",
       "  (315, 0.00),\n",
       "  (317, 0.00),\n",
       "  (320, 0.00),\n",
       "  (322, 0.00),\n",
       "  (323, 0.00),\n",
       "  (324, 0.00),\n",
       "  (325, 0.00),\n",
       "  (326, 0.00),\n",
       "  (328, 0.00),\n",
       "  (329, 0.00),\n",
       "  (330, 0.00),\n",
       "  (331, 0.00),\n",
       "  (332, 0.00),\n",
       "  (333, 0.00),\n",
       "  (335, 0.00),\n",
       "  (337, 0.00),\n",
       "  (338, 0.00),\n",
       "  (339, 0.00),\n",
       "  (340, 0.00),\n",
       "  (341, 0.00),\n",
       "  (343, 0.00),\n",
       "  (344, 0.00),\n",
       "  (345, 0.00),\n",
       "  (346, 0.00),\n",
       "  (348, 0.00),\n",
       "  (349, 0.00),\n",
       "  (350, 0.00),\n",
       "  (351, 0.00),\n",
       "  (352, 0.00),\n",
       "  (354, 0.00),\n",
       "  (356, 0.00),\n",
       "  (357, 0.00),\n",
       "  (358, 0.00),\n",
       "  (359, 0.00),\n",
       "  (361, 0.00),\n",
       "  (362, 0.00),\n",
       "  (364, 0.00),\n",
       "  (365, 0.00),\n",
       "  (366, 0.00),\n",
       "  (367, 0.00),\n",
       "  (368, 0.00),\n",
       "  (370, 0.00),\n",
       "  (371, 0.00),\n",
       "  (372, 0.00),\n",
       "  (373, 0.00),\n",
       "  (374, 0.00),\n",
       "  (377, 0.00),\n",
       "  (378, 0.00),\n",
       "  (379, 0.00),\n",
       "  (380, 0.00),\n",
       "  (381, 0.00),\n",
       "  (382, 0.00),\n",
       "  (384, 0.00),\n",
       "  (385, 0.00),\n",
       "  (386, 0.00),\n",
       "  (388, 0.00),\n",
       "  (389, 0.00),\n",
       "  (390, 0.00),\n",
       "  (391, 0.00),\n",
       "  (394, 0.00),\n",
       "  (395, 0.00),\n",
       "  (400, 0.00),\n",
       "  (402, 0.00),\n",
       "  (403, 0.00),\n",
       "  (404, 0.00),\n",
       "  (405, 0.00),\n",
       "  (413, 0.00),\n",
       "  (415, 0.00),\n",
       "  (416, 0.00),\n",
       "  (418, 0.00),\n",
       "  (420, 0.00),\n",
       "  (421, 0.00),\n",
       "  (422, 0.00),\n",
       "  (423, 0.00),\n",
       "  (426, 0.00),\n",
       "  (427, 0.00),\n",
       "  (428, 0.00),\n",
       "  (432, 0.00),\n",
       "  (434, 0.00),\n",
       "  (435, 0.00),\n",
       "  (436, 0.00),\n",
       "  (437, 0.00),\n",
       "  (438, 0.00),\n",
       "  (439, 0.00),\n",
       "  (442, 0.00),\n",
       "  (446, 0.00),\n",
       "  (448, 0.00),\n",
       "  (449, 0.00),\n",
       "  (450, 0.00),\n",
       "  (451, 0.00),\n",
       "  (452, 0.00),\n",
       "  (454, 0.00),\n",
       "  (455, 0.00),\n",
       "  (456, 0.00),\n",
       "  (458, 0.00),\n",
       "  (460, 0.00),\n",
       "  (461, 0.00),\n",
       "  (462, 0.00),\n",
       "  (463, 0.00),\n",
       "  (465, 0.00),\n",
       "  (466, 0.00),\n",
       "  (467, 0.00),\n",
       "  (469, 0.00),\n",
       "  (470, 0.00),\n",
       "  (471, 0.00),\n",
       "  (473, 0.00),\n",
       "  (475, 0.00),\n",
       "  (477, 0.00),\n",
       "  (478, 0.00),\n",
       "  (479, 0.00),\n",
       "  (480, 0.00),\n",
       "  (484, 0.00),\n",
       "  (486, 0.00),\n",
       "  (487, 0.00),\n",
       "  (491, 0.00),\n",
       "  (493, 0.00),\n",
       "  (494, 0.00),\n",
       "  (497, 0.00),\n",
       "  (499, 0.00),\n",
       "  (500, 0.00),\n",
       "  (501, 0.00),\n",
       "  (502, 0.00),\n",
       "  (503, 0.00),\n",
       "  (504, 0.00),\n",
       "  (510, 0.00),\n",
       "  (511, 0.00),\n",
       "  (512, 0.00),\n",
       "  (513, 0.00),\n",
       "  (514, 0.00),\n",
       "  (519, 0.00),\n",
       "  (521, 0.00),\n",
       "  (522, 0.00),\n",
       "  (524, 0.00),\n",
       "  (525, 0.00),\n",
       "  (527, 0.00),\n",
       "  (529, 0.00),\n",
       "  (531, 0.00),\n",
       "  (534, 0.00),\n",
       "  (535, 0.00),\n",
       "  (536, 0.00),\n",
       "  (537, 0.00),\n",
       "  (540, 0.00),\n",
       "  (542, 0.00),\n",
       "  (543, 0.00),\n",
       "  (544, 0.00),\n",
       "  (546, 0.00),\n",
       "  (548, 0.00),\n",
       "  (549, 0.00),\n",
       "  (550, 0.00),\n",
       "  (551, 0.00),\n",
       "  (553, 0.00),\n",
       "  (554, 0.00),\n",
       "  (557, 0.00),\n",
       "  (558, 0.00),\n",
       "  (559, 0.00),\n",
       "  (560, 0.00),\n",
       "  (561, 0.00),\n",
       "  (563, 0.00),\n",
       "  (564, 0.00),\n",
       "  (565, 0.00),\n",
       "  (567, 0.00),\n",
       "  (568, 0.00),\n",
       "  (569, 0.00),\n",
       "  (571, 0.00),\n",
       "  (573, 0.00),\n",
       "  (574, 0.00),\n",
       "  (577, 0.00),\n",
       "  (578, 0.00),\n",
       "  (579, 0.00),\n",
       "  (583, 0.00),\n",
       "  (584, 0.00),\n",
       "  (585, 0.00),\n",
       "  (587, 0.00),\n",
       "  (589, 0.00),\n",
       "  (590, 0.00),\n",
       "  (592, 0.00),\n",
       "  (593, 0.00),\n",
       "  (594, 0.00),\n",
       "  (596, 0.00),\n",
       "  (597, 0.00),\n",
       "  (598, 0.00),\n",
       "  (600, 0.00),\n",
       "  (601, 0.00),\n",
       "  (602, 0.00),\n",
       "  (603, 0.00),\n",
       "  (606, 0.00),\n",
       "  (608, 0.00),\n",
       "  (610, 0.00),\n",
       "  (611, 0.00),\n",
       "  (613, 0.00),\n",
       "  (615, 0.00),\n",
       "  (616, 0.00),\n",
       "  (617, 0.00),\n",
       "  (618, 0.00),\n",
       "  (622, 0.00),\n",
       "  (623, 0.00),\n",
       "  (624, 0.00),\n",
       "  (626, 0.00),\n",
       "  (627, 0.00),\n",
       "  (628, 0.00),\n",
       "  (630, 0.00),\n",
       "  (631, 0.00),\n",
       "  (634, 0.00),\n",
       "  (635, 0.00),\n",
       "  (636, 0.00),\n",
       "  (638, 0.00),\n",
       "  (639, 0.00),\n",
       "  (640, 0.00),\n",
       "  (642, 0.00),\n",
       "  (644, 0.00),\n",
       "  (647, 0.00),\n",
       "  (648, 0.00),\n",
       "  (649, 0.00),\n",
       "  (650, 0.00),\n",
       "  (652, 0.00),\n",
       "  (653, 0.00),\n",
       "  (657, 0.00),\n",
       "  (658, 0.00),\n",
       "  (659, 0.00),\n",
       "  (660, 0.00),\n",
       "  (662, 0.00),\n",
       "  (663, 0.00),\n",
       "  (664, 0.00),\n",
       "  (665, 0.00),\n",
       "  (666, 0.00),\n",
       "  (667, 0.00),\n",
       "  (670, 0.00),\n",
       "  (672, 0.00),\n",
       "  (673, 0.00),\n",
       "  (674, 0.00),\n",
       "  (675, 0.00),\n",
       "  (677, 0.00),\n",
       "  (678, 0.00),\n",
       "  (679, 0.00),\n",
       "  (681, 0.00),\n",
       "  (682, 0.00),\n",
       "  (683, 0.00),\n",
       "  (685, 0.00),\n",
       "  (686, 0.00),\n",
       "  (687, 0.00),\n",
       "  (688, 0.00),\n",
       "  (689, 0.00),\n",
       "  (690, 0.00),\n",
       "  (692, 0.00),\n",
       "  (693, 0.00),\n",
       "  (695, 0.00),\n",
       "  (696, 0.00),\n",
       "  (699, 0.00),\n",
       "  (700, 0.00),\n",
       "  (701, 0.00),\n",
       "  (702, 0.00),\n",
       "  (704, 0.00),\n",
       "  (707, 0.00),\n",
       "  (708, 0.00),\n",
       "  (710, 0.00),\n",
       "  (712, 0.00),\n",
       "  (713, 0.00),\n",
       "  (714, 0.00),\n",
       "  (715, 0.00),\n",
       "  (717, 0.00),\n",
       "  (718, 0.00),\n",
       "  (720, 0.00),\n",
       "  (723, 0.00),\n",
       "  (724, 0.00),\n",
       "  (726, 0.00),\n",
       "  (729, 0.00),\n",
       "  (730, 0.00),\n",
       "  (731, 0.00),\n",
       "  (733, 0.00),\n",
       "  (739, 0.00),\n",
       "  (740, 0.00),\n",
       "  (742, 0.00),\n",
       "  (744, 0.00),\n",
       "  (745, 0.00),\n",
       "  (747, 0.00),\n",
       "  (749, 0.00),\n",
       "  (755, 0.00),\n",
       "  (756, 0.00),\n",
       "  (757, 0.00),\n",
       "  (758, 0.00),\n",
       "  (759, 0.00),\n",
       "  (760, 0.00),\n",
       "  (761, 0.00),\n",
       "  (763, 0.00),\n",
       "  (764, 0.00),\n",
       "  (765, 0.00),\n",
       "  (766, 0.00),\n",
       "  (767, 0.00),\n",
       "  (769, 0.00),\n",
       "  (771, 0.00),\n",
       "  (772, 0.00),\n",
       "  (773, 0.00),\n",
       "  (774, 0.00),\n",
       "  (775, 0.00),\n",
       "  (776, 0.00),\n",
       "  (777, 0.00),\n",
       "  (778, 0.00),\n",
       "  (780, 0.00),\n",
       "  (782, 0.00),\n",
       "  (784, 0.00),\n",
       "  (785, 0.00),\n",
       "  (789, 0.00),\n",
       "  (793, 0.00),\n",
       "  (795, 0.00),\n",
       "  (797, 0.00),\n",
       "  (798, 0.00),\n",
       "  (799, 0.00),\n",
       "  (800, 0.00),\n",
       "  (802, 0.00),\n",
       "  (804, 0.00),\n",
       "  (807, 0.00),\n",
       "  (809, 0.00),\n",
       "  (810, 0.00),\n",
       "  (811, 0.00),\n",
       "  (812, 0.00),\n",
       "  (813, 0.00),\n",
       "  (818, 0.00),\n",
       "  (821, 0.00),\n",
       "  (823, 0.00),\n",
       "  (825, 0.00),\n",
       "  (827, 0.00),\n",
       "  (831, 0.00),\n",
       "  (832, 0.00),\n",
       "  (833, 0.00),\n",
       "  (834, 0.00),\n",
       "  (835, 0.00),\n",
       "  (838, 0.00),\n",
       "  (839, 0.00),\n",
       "  (840, 0.00),\n",
       "  (841, 0.00),\n",
       "  (842, 0.00),\n",
       "  (844, 0.00),\n",
       "  (845, 0.00),\n",
       "  (846, 0.00),\n",
       "  (849, 0.00),\n",
       "  (851, 0.00),\n",
       "  (852, 0.00),\n",
       "  (853, 0.00),\n",
       "  (855, 0.00),\n",
       "  (856, 0.00),\n",
       "  (859, 0.00),\n",
       "  (860, 0.00),\n",
       "  (861, 0.00),\n",
       "  (862, 0.00),\n",
       "  (863, 0.00),\n",
       "  (866, 0.00),\n",
       "  (869, 0.00),\n",
       "  (870, 0.00),\n",
       "  (873, 0.00),\n",
       "  (874, 0.00),\n",
       "  (875, 0.00),\n",
       "  (876, 0.00),\n",
       "  (877, 0.00),\n",
       "  (880, 0.00),\n",
       "  (881, 0.00),\n",
       "  (882, 0.00),\n",
       "  (883, 0.00),\n",
       "  (884, 0.00),\n",
       "  (885, 0.00),\n",
       "  (888, 0.00),\n",
       "  (890, 0.00),\n",
       "  (891, 0.00),\n",
       "  (894, 0.00),\n",
       "  (895, 0.00),\n",
       "  (896, 0.00),\n",
       "  (899, 0.00),\n",
       "  (900, 0.00),\n",
       "  (901, 0.00),\n",
       "  (902, 0.00),\n",
       "  (903, 0.00),\n",
       "  (908, 0.00),\n",
       "  (909, 0.00),\n",
       "  (911, 0.00),\n",
       "  (912, 0.00),\n",
       "  (913, 0.00),\n",
       "  (914, 0.00),\n",
       "  (915, 0.00),\n",
       "  (916, 0.00),\n",
       "  (917, 0.00),\n",
       "  (919, 0.00),\n",
       "  (920, 0.00),\n",
       "  (921, 0.00),\n",
       "  (922, 0.00),\n",
       "  (924, 0.00),\n",
       "  (925, 0.00),\n",
       "  (926, 0.00),\n",
       "  (927, 0.00),\n",
       "  (928, 0.00),\n",
       "  (929, 0.00),\n",
       "  (930, 0.00),\n",
       "  (931, 0.00),\n",
       "  (933, 0.00),\n",
       "  (935, 0.00),\n",
       "  (936, 0.00),\n",
       "  (940, 0.00),\n",
       "  (941, 0.00),\n",
       "  (942, 0.00),\n",
       "  (943, 0.00),\n",
       "  (945, 0.00),\n",
       "  (946, 0.00),\n",
       "  (947, 0.00),\n",
       "  (948, 0.00),\n",
       "  (949, 0.00),\n",
       "  (950, 0.00),\n",
       "  (951, 0.00),\n",
       "  (952, 0.00),\n",
       "  (954, 0.00),\n",
       "  (956, 0.00),\n",
       "  (957, 0.00),\n",
       "  (958, 0.00),\n",
       "  (959, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (962, 0.00),\n",
       "  (964, 0.00),\n",
       "  (965, 0.00),\n",
       "  (966, 0.00),\n",
       "  (967, 0.00),\n",
       "  (968, 0.00),\n",
       "  (969, 0.00),\n",
       "  (970, 0.00),\n",
       "  (972, 0.00),\n",
       "  (974, 0.00),\n",
       "  (975, 0.00),\n",
       "  (976, 0.00),\n",
       "  (977, 0.00),\n",
       "  (978, 0.00),\n",
       "  (979, 0.00),\n",
       "  (980, 0.00),\n",
       "  (983, 0.00),\n",
       "  (985, 0.00),\n",
       "  (986, 0.00),\n",
       "  (988, 0.00),\n",
       "  (989, 0.00),\n",
       "  (990, 0.00),\n",
       "  (991, 0.00),\n",
       "  (993, 0.00),\n",
       "  (994, 0.00),\n",
       "  (995, 0.00),\n",
       "  (996, 0.00),\n",
       "  (998, 0.00),\n",
       "  (999, 0.00)])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 5\n",
      "at batch no 10\n",
      "at batch no 15\n",
      "at batch no 20\n",
      "at batch no 25\n",
      "at batch no 30\n",
      "at batch no 35\n",
      "at batch no 40\n",
      "at batch no 45\n",
      "at batch no 50\n",
      "at batch no 55\n",
      "at batch no 60\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(177,\n",
       " [(669, 108.40),\n",
       "  (971, 103.40),\n",
       "  (556, 84.10),\n",
       "  (520, 71.90),\n",
       "  (721, 68.40),\n",
       "  (588, 60.70),\n",
       "  (431, 44.90),\n",
       "  (828, 32.40),\n",
       "  (61, 31.20),\n",
       "  (750, 19.60),\n",
       "  (411, 17.80),\n",
       "  (414, 17.60),\n",
       "  (794, 16.40),\n",
       "  (39, 12.70),\n",
       "  (904, 12.00),\n",
       "  (599, 11.80),\n",
       "  (709, 8.90),\n",
       "  (490, 6.30),\n",
       "  (489, 6.00),\n",
       "  (770, 5.80),\n",
       "  (84, 5.60),\n",
       "  (801, 5.10),\n",
       "  (570, 4.70),\n",
       "  (955, 4.30),\n",
       "  (581, 4.00),\n",
       "  (790, 4.00),\n",
       "  (48, 3.90),\n",
       "  (604, 3.90),\n",
       "  (837, 3.30),\n",
       "  (60, 2.90),\n",
       "  (419, 2.50),\n",
       "  (879, 2.40),\n",
       "  (907, 2.30),\n",
       "  (591, 2.20),\n",
       "  (806, 2.20),\n",
       "  (56, 2.10),\n",
       "  (108, 2.00),\n",
       "  (572, 2.00),\n",
       "  (614, 2.00),\n",
       "  (788, 2.00),\n",
       "  (987, 2.00),\n",
       "  (621, 1.90),\n",
       "  (872, 1.90),\n",
       "  (55, 1.70),\n",
       "  (62, 1.70),\n",
       "  (893, 1.70),\n",
       "  (953, 1.70),\n",
       "  (68, 1.60),\n",
       "  (516, 1.60),\n",
       "  (118, 1.50),\n",
       "  (754, 1.50),\n",
       "  (892, 1.50),\n",
       "  (973, 1.50),\n",
       "  (124, 1.40),\n",
       "  (401, 1.40),\n",
       "  (464, 1.40),\n",
       "  (496, 1.40),\n",
       "  (865, 1.40),\n",
       "  (151, 1.30),\n",
       "  (575, 1.30),\n",
       "  (646, 1.30),\n",
       "  (762, 1.30),\n",
       "  (864, 1.30),\n",
       "  (67, 1.20),\n",
       "  (96, 1.20),\n",
       "  (360, 1.20),\n",
       "  (406, 1.20),\n",
       "  (408, 1.20),\n",
       "  (518, 1.20),\n",
       "  (580, 1.20),\n",
       "  (619, 1.20),\n",
       "  (791, 1.20),\n",
       "  (815, 1.20),\n",
       "  (857, 1.20),\n",
       "  (300, 1.10),\n",
       "  (306, 1.10),\n",
       "  (453, 1.10),\n",
       "  (468, 1.10),\n",
       "  (526, 1.10),\n",
       "  (641, 1.10),\n",
       "  (738, 1.10),\n",
       "  (746, 1.10),\n",
       "  (868, 1.10),\n",
       "  (7, 1.00),\n",
       "  (25, 1.00),\n",
       "  (33, 1.00),\n",
       "  (57, 1.00),\n",
       "  (91, 1.00),\n",
       "  (105, 1.00),\n",
       "  (115, 1.00),\n",
       "  (120, 1.00),\n",
       "  (123, 1.00),\n",
       "  (128, 1.00),\n",
       "  (163, 1.00),\n",
       "  (186, 1.00),\n",
       "  (242, 1.00),\n",
       "  (247, 1.00),\n",
       "  (254, 1.00),\n",
       "  (260, 1.00),\n",
       "  (274, 1.00),\n",
       "  (281, 1.00),\n",
       "  (290, 1.00),\n",
       "  (292, 1.00),\n",
       "  (314, 1.00),\n",
       "  (316, 1.00),\n",
       "  (334, 1.00),\n",
       "  (342, 1.00),\n",
       "  (376, 1.00),\n",
       "  (392, 1.00),\n",
       "  (429, 1.00),\n",
       "  (445, 1.00),\n",
       "  (474, 1.00),\n",
       "  (482, 1.00),\n",
       "  (488, 1.00),\n",
       "  (498, 1.00),\n",
       "  (506, 1.00),\n",
       "  (507, 1.00),\n",
       "  (528, 1.00),\n",
       "  (533, 1.00),\n",
       "  (566, 1.00),\n",
       "  (612, 1.00),\n",
       "  (637, 1.00),\n",
       "  (661, 1.00),\n",
       "  (694, 1.00),\n",
       "  (753, 1.00),\n",
       "  (783, 1.00),\n",
       "  (787, 1.00),\n",
       "  (805, 1.00),\n",
       "  (816, 1.00),\n",
       "  (826, 1.00),\n",
       "  (858, 1.00),\n",
       "  (923, 1.00),\n",
       "  (934, 1.00),\n",
       "  (937, 1.00),\n",
       "  (944, 1.00),\n",
       "  (992, 1.00),\n",
       "  (997, 1.00),\n",
       "  (1, 0.90),\n",
       "  (15, 0.90),\n",
       "  (219, 0.90),\n",
       "  (301, 0.90),\n",
       "  (307, 0.90),\n",
       "  (393, 0.90),\n",
       "  (398, 0.90),\n",
       "  (410, 0.90),\n",
       "  (440, 0.90),\n",
       "  (443, 0.90),\n",
       "  (485, 0.90),\n",
       "  (523, 0.90),\n",
       "  (538, 0.90),\n",
       "  (625, 0.90),\n",
       "  (645, 0.90),\n",
       "  (651, 0.90),\n",
       "  (656, 0.90),\n",
       "  (711, 0.90),\n",
       "  (716, 0.90),\n",
       "  (734, 0.90),\n",
       "  (781, 0.90),\n",
       "  (803, 0.90),\n",
       "  (847, 0.90),\n",
       "  (898, 0.90),\n",
       "  (982, 0.90),\n",
       "  (90, 0.80),\n",
       "  (109, 0.80),\n",
       "  (387, 0.80),\n",
       "  (412, 0.80),\n",
       "  (457, 0.80),\n",
       "  (476, 0.80),\n",
       "  (492, 0.80),\n",
       "  (697, 0.80),\n",
       "  (705, 0.80),\n",
       "  (796, 0.80),\n",
       "  (819, 0.80),\n",
       "  (889, 0.80),\n",
       "  (8, 0.70),\n",
       "  (45, 0.70),\n",
       "  (46, 0.70),\n",
       "  (52, 0.70),\n",
       "  (83, 0.70),\n",
       "  (87, 0.70),\n",
       "  (235, 0.70),\n",
       "  (304, 0.70),\n",
       "  (355, 0.70),\n",
       "  (369, 0.70),\n",
       "  (417, 0.70),\n",
       "  (444, 0.70),\n",
       "  (532, 0.70),\n",
       "  (539, 0.70),\n",
       "  (552, 0.70),\n",
       "  (576, 0.70),\n",
       "  (728, 0.70),\n",
       "  (735, 0.70),\n",
       "  (748, 0.70),\n",
       "  (829, 0.70),\n",
       "  (850, 0.70),\n",
       "  (871, 0.70),\n",
       "  (939, 0.70),\n",
       "  (107, 0.60),\n",
       "  (189, 0.60),\n",
       "  (205, 0.60),\n",
       "  (293, 0.60),\n",
       "  (407, 0.60),\n",
       "  (483, 0.60),\n",
       "  (530, 0.60),\n",
       "  (582, 0.60),\n",
       "  (620, 0.60),\n",
       "  (676, 0.60),\n",
       "  (691, 0.60),\n",
       "  (981, 0.60),\n",
       "  (176, 0.50),\n",
       "  (218, 0.50),\n",
       "  (396, 0.50),\n",
       "  (433, 0.50),\n",
       "  (459, 0.50),\n",
       "  (508, 0.50),\n",
       "  (509, 0.50),\n",
       "  (515, 0.50),\n",
       "  (768, 0.50),\n",
       "  (779, 0.50),\n",
       "  (897, 0.50),\n",
       "  (963, 0.50),\n",
       "  (37, 0.40),\n",
       "  (40, 0.40),\n",
       "  (86, 0.40),\n",
       "  (102, 0.40),\n",
       "  (231, 0.40),\n",
       "  (294, 0.40),\n",
       "  (399, 0.40),\n",
       "  (430, 0.40),\n",
       "  (472, 0.40),\n",
       "  (643, 0.40),\n",
       "  (741, 0.40),\n",
       "  (814, 0.40),\n",
       "  (822, 0.40),\n",
       "  (42, 0.30),\n",
       "  (63, 0.30),\n",
       "  (82, 0.30),\n",
       "  (116, 0.30),\n",
       "  (157, 0.30),\n",
       "  (230, 0.30),\n",
       "  (291, 0.30),\n",
       "  (327, 0.30),\n",
       "  (336, 0.30),\n",
       "  (397, 0.30),\n",
       "  (447, 0.30),\n",
       "  (517, 0.30),\n",
       "  (527, 0.30),\n",
       "  (545, 0.30),\n",
       "  (547, 0.30),\n",
       "  (562, 0.30),\n",
       "  (564, 0.30),\n",
       "  (629, 0.30),\n",
       "  (680, 0.30),\n",
       "  (684, 0.30),\n",
       "  (725, 0.30),\n",
       "  (752, 0.30),\n",
       "  (786, 0.30),\n",
       "  (854, 0.30),\n",
       "  (0, 0.20),\n",
       "  (47, 0.20),\n",
       "  (94, 0.20),\n",
       "  (138, 0.20),\n",
       "  (164, 0.20),\n",
       "  (249, 0.20),\n",
       "  (318, 0.20),\n",
       "  (375, 0.20),\n",
       "  (409, 0.20),\n",
       "  (441, 0.20),\n",
       "  (555, 0.20),\n",
       "  (602, 0.20),\n",
       "  (605, 0.20),\n",
       "  (609, 0.20),\n",
       "  (632, 0.20),\n",
       "  (654, 0.20),\n",
       "  (671, 0.20),\n",
       "  (698, 0.20),\n",
       "  (706, 0.20),\n",
       "  (722, 0.20),\n",
       "  (743, 0.20),\n",
       "  (751, 0.20),\n",
       "  (808, 0.20),\n",
       "  (836, 0.20),\n",
       "  (906, 0.20),\n",
       "  (17, 0.10),\n",
       "  (18, 0.10),\n",
       "  (28, 0.10),\n",
       "  (66, 0.10),\n",
       "  (76, 0.10),\n",
       "  (92, 0.10),\n",
       "  (159, 0.10),\n",
       "  (171, 0.10),\n",
       "  (213, 0.10),\n",
       "  (289, 0.10),\n",
       "  (302, 0.10),\n",
       "  (308, 0.10),\n",
       "  (313, 0.10),\n",
       "  (319, 0.10),\n",
       "  (321, 0.10),\n",
       "  (347, 0.10),\n",
       "  (353, 0.10),\n",
       "  (363, 0.10),\n",
       "  (383, 0.10),\n",
       "  (425, 0.10),\n",
       "  (481, 0.10),\n",
       "  (495, 0.10),\n",
       "  (505, 0.10),\n",
       "  (522, 0.10),\n",
       "  (529, 0.10),\n",
       "  (531, 0.10),\n",
       "  (595, 0.10),\n",
       "  (607, 0.10),\n",
       "  (611, 0.10),\n",
       "  (627, 0.10),\n",
       "  (633, 0.10),\n",
       "  (655, 0.10),\n",
       "  (703, 0.10),\n",
       "  (712, 0.10),\n",
       "  (719, 0.10),\n",
       "  (736, 0.10),\n",
       "  (737, 0.10),\n",
       "  (792, 0.10),\n",
       "  (809, 0.10),\n",
       "  (817, 0.10),\n",
       "  (820, 0.10),\n",
       "  (824, 0.10),\n",
       "  (842, 0.10),\n",
       "  (843, 0.10),\n",
       "  (867, 0.10),\n",
       "  (886, 0.10),\n",
       "  (910, 0.10),\n",
       "  (915, 0.10),\n",
       "  (918, 0.10),\n",
       "  (946, 0.10),\n",
       "  (984, 0.10),\n",
       "  (2, 0.00),\n",
       "  (3, 0.00),\n",
       "  (4, 0.00),\n",
       "  (5, 0.00),\n",
       "  (6, 0.00),\n",
       "  (9, 0.00),\n",
       "  (10, 0.00),\n",
       "  (11, 0.00),\n",
       "  (12, 0.00),\n",
       "  (13, 0.00),\n",
       "  (14, 0.00),\n",
       "  (16, 0.00),\n",
       "  (19, 0.00),\n",
       "  (20, 0.00),\n",
       "  (21, 0.00),\n",
       "  (22, 0.00),\n",
       "  (23, 0.00),\n",
       "  (24, 0.00),\n",
       "  (26, 0.00),\n",
       "  (27, 0.00),\n",
       "  (29, 0.00),\n",
       "  (30, 0.00),\n",
       "  (31, 0.00),\n",
       "  (32, 0.00),\n",
       "  (34, 0.00),\n",
       "  (35, 0.00),\n",
       "  (36, 0.00),\n",
       "  (38, 0.00),\n",
       "  (41, 0.00),\n",
       "  (43, 0.00),\n",
       "  (44, 0.00),\n",
       "  (49, 0.00),\n",
       "  (50, 0.00),\n",
       "  (51, 0.00),\n",
       "  (53, 0.00),\n",
       "  (54, 0.00),\n",
       "  (58, 0.00),\n",
       "  (59, 0.00),\n",
       "  (64, 0.00),\n",
       "  (65, 0.00),\n",
       "  (69, 0.00),\n",
       "  (70, 0.00),\n",
       "  (71, 0.00),\n",
       "  (72, 0.00),\n",
       "  (73, 0.00),\n",
       "  (74, 0.00),\n",
       "  (75, 0.00),\n",
       "  (77, 0.00),\n",
       "  (78, 0.00),\n",
       "  (79, 0.00),\n",
       "  (80, 0.00),\n",
       "  (81, 0.00),\n",
       "  (85, 0.00),\n",
       "  (88, 0.00),\n",
       "  (89, 0.00),\n",
       "  (93, 0.00),\n",
       "  (95, 0.00),\n",
       "  (97, 0.00),\n",
       "  (98, 0.00),\n",
       "  (99, 0.00),\n",
       "  (100, 0.00),\n",
       "  (101, 0.00),\n",
       "  (103, 0.00),\n",
       "  (104, 0.00),\n",
       "  (106, 0.00),\n",
       "  (110, 0.00),\n",
       "  (111, 0.00),\n",
       "  (112, 0.00),\n",
       "  (113, 0.00),\n",
       "  (114, 0.00),\n",
       "  (117, 0.00),\n",
       "  (119, 0.00),\n",
       "  (121, 0.00),\n",
       "  (122, 0.00),\n",
       "  (125, 0.00),\n",
       "  (126, 0.00),\n",
       "  (127, 0.00),\n",
       "  (129, 0.00),\n",
       "  (130, 0.00),\n",
       "  (131, 0.00),\n",
       "  (132, 0.00),\n",
       "  (133, 0.00),\n",
       "  (134, 0.00),\n",
       "  (135, 0.00),\n",
       "  (136, 0.00),\n",
       "  (137, 0.00),\n",
       "  (139, 0.00),\n",
       "  (140, 0.00),\n",
       "  (141, 0.00),\n",
       "  (142, 0.00),\n",
       "  (143, 0.00),\n",
       "  (144, 0.00),\n",
       "  (145, 0.00),\n",
       "  (146, 0.00),\n",
       "  (147, 0.00),\n",
       "  (148, 0.00),\n",
       "  (149, 0.00),\n",
       "  (150, 0.00),\n",
       "  (152, 0.00),\n",
       "  (153, 0.00),\n",
       "  (154, 0.00),\n",
       "  (155, 0.00),\n",
       "  (156, 0.00),\n",
       "  (158, 0.00),\n",
       "  (160, 0.00),\n",
       "  (161, 0.00),\n",
       "  (162, 0.00),\n",
       "  (165, 0.00),\n",
       "  (166, 0.00),\n",
       "  (167, 0.00),\n",
       "  (168, 0.00),\n",
       "  (169, 0.00),\n",
       "  (170, 0.00),\n",
       "  (172, 0.00),\n",
       "  (173, 0.00),\n",
       "  (174, 0.00),\n",
       "  (175, 0.00),\n",
       "  (177, 0.00),\n",
       "  (178, 0.00),\n",
       "  (179, 0.00),\n",
       "  (180, 0.00),\n",
       "  (181, 0.00),\n",
       "  (182, 0.00),\n",
       "  (183, 0.00),\n",
       "  (184, 0.00),\n",
       "  (185, 0.00),\n",
       "  (187, 0.00),\n",
       "  (188, 0.00),\n",
       "  (190, 0.00),\n",
       "  (191, 0.00),\n",
       "  (192, 0.00),\n",
       "  (193, 0.00),\n",
       "  (194, 0.00),\n",
       "  (195, 0.00),\n",
       "  (196, 0.00),\n",
       "  (197, 0.00),\n",
       "  (198, 0.00),\n",
       "  (199, 0.00),\n",
       "  (200, 0.00),\n",
       "  (201, 0.00),\n",
       "  (202, 0.00),\n",
       "  (203, 0.00),\n",
       "  (204, 0.00),\n",
       "  (206, 0.00),\n",
       "  (207, 0.00),\n",
       "  (208, 0.00),\n",
       "  (209, 0.00),\n",
       "  (210, 0.00),\n",
       "  (211, 0.00),\n",
       "  (212, 0.00),\n",
       "  (214, 0.00),\n",
       "  (215, 0.00),\n",
       "  (216, 0.00),\n",
       "  (217, 0.00),\n",
       "  (220, 0.00),\n",
       "  (221, 0.00),\n",
       "  (222, 0.00),\n",
       "  (223, 0.00),\n",
       "  (224, 0.00),\n",
       "  (225, 0.00),\n",
       "  (226, 0.00),\n",
       "  (227, 0.00),\n",
       "  (228, 0.00),\n",
       "  (229, 0.00),\n",
       "  (232, 0.00),\n",
       "  (233, 0.00),\n",
       "  (234, 0.00),\n",
       "  (236, 0.00),\n",
       "  (237, 0.00),\n",
       "  (238, 0.00),\n",
       "  (239, 0.00),\n",
       "  (240, 0.00),\n",
       "  (241, 0.00),\n",
       "  (243, 0.00),\n",
       "  (244, 0.00),\n",
       "  (245, 0.00),\n",
       "  (246, 0.00),\n",
       "  (248, 0.00),\n",
       "  (250, 0.00),\n",
       "  (251, 0.00),\n",
       "  (252, 0.00),\n",
       "  (253, 0.00),\n",
       "  (255, 0.00),\n",
       "  (256, 0.00),\n",
       "  (257, 0.00),\n",
       "  (258, 0.00),\n",
       "  (259, 0.00),\n",
       "  (261, 0.00),\n",
       "  (262, 0.00),\n",
       "  (263, 0.00),\n",
       "  (264, 0.00),\n",
       "  (265, 0.00),\n",
       "  (266, 0.00),\n",
       "  (267, 0.00),\n",
       "  (268, 0.00),\n",
       "  (269, 0.00),\n",
       "  (270, 0.00),\n",
       "  (271, 0.00),\n",
       "  (272, 0.00),\n",
       "  (273, 0.00),\n",
       "  (275, 0.00),\n",
       "  (276, 0.00),\n",
       "  (277, 0.00),\n",
       "  (278, 0.00),\n",
       "  (279, 0.00),\n",
       "  (280, 0.00),\n",
       "  (282, 0.00),\n",
       "  (283, 0.00),\n",
       "  (284, 0.00),\n",
       "  (285, 0.00),\n",
       "  (286, 0.00),\n",
       "  (287, 0.00),\n",
       "  (288, 0.00),\n",
       "  (295, 0.00),\n",
       "  (296, 0.00),\n",
       "  (297, 0.00),\n",
       "  (298, 0.00),\n",
       "  (299, 0.00),\n",
       "  (303, 0.00),\n",
       "  (305, 0.00),\n",
       "  (309, 0.00),\n",
       "  (310, 0.00),\n",
       "  (311, 0.00),\n",
       "  (312, 0.00),\n",
       "  (315, 0.00),\n",
       "  (317, 0.00),\n",
       "  (320, 0.00),\n",
       "  (322, 0.00),\n",
       "  (323, 0.00),\n",
       "  (324, 0.00),\n",
       "  (325, 0.00),\n",
       "  (326, 0.00),\n",
       "  (328, 0.00),\n",
       "  (329, 0.00),\n",
       "  (330, 0.00),\n",
       "  (331, 0.00),\n",
       "  (332, 0.00),\n",
       "  (333, 0.00),\n",
       "  (335, 0.00),\n",
       "  (337, 0.00),\n",
       "  (338, 0.00),\n",
       "  (339, 0.00),\n",
       "  (340, 0.00),\n",
       "  (341, 0.00),\n",
       "  (343, 0.00),\n",
       "  (344, 0.00),\n",
       "  (345, 0.00),\n",
       "  (346, 0.00),\n",
       "  (348, 0.00),\n",
       "  (349, 0.00),\n",
       "  (350, 0.00),\n",
       "  (351, 0.00),\n",
       "  (352, 0.00),\n",
       "  (354, 0.00),\n",
       "  (356, 0.00),\n",
       "  (357, 0.00),\n",
       "  (358, 0.00),\n",
       "  (359, 0.00),\n",
       "  (361, 0.00),\n",
       "  (362, 0.00),\n",
       "  (364, 0.00),\n",
       "  (365, 0.00),\n",
       "  (366, 0.00),\n",
       "  (367, 0.00),\n",
       "  (368, 0.00),\n",
       "  (370, 0.00),\n",
       "  (371, 0.00),\n",
       "  (372, 0.00),\n",
       "  (373, 0.00),\n",
       "  (374, 0.00),\n",
       "  (377, 0.00),\n",
       "  (378, 0.00),\n",
       "  (379, 0.00),\n",
       "  (380, 0.00),\n",
       "  (381, 0.00),\n",
       "  (382, 0.00),\n",
       "  (384, 0.00),\n",
       "  (385, 0.00),\n",
       "  (386, 0.00),\n",
       "  (388, 0.00),\n",
       "  (389, 0.00),\n",
       "  (390, 0.00),\n",
       "  (391, 0.00),\n",
       "  (394, 0.00),\n",
       "  (395, 0.00),\n",
       "  (400, 0.00),\n",
       "  (402, 0.00),\n",
       "  (403, 0.00),\n",
       "  (404, 0.00),\n",
       "  (405, 0.00),\n",
       "  (413, 0.00),\n",
       "  (415, 0.00),\n",
       "  (416, 0.00),\n",
       "  (418, 0.00),\n",
       "  (420, 0.00),\n",
       "  (421, 0.00),\n",
       "  (422, 0.00),\n",
       "  (423, 0.00),\n",
       "  (424, 0.00),\n",
       "  (426, 0.00),\n",
       "  (427, 0.00),\n",
       "  (428, 0.00),\n",
       "  (432, 0.00),\n",
       "  (434, 0.00),\n",
       "  (435, 0.00),\n",
       "  (436, 0.00),\n",
       "  (437, 0.00),\n",
       "  (438, 0.00),\n",
       "  (439, 0.00),\n",
       "  (442, 0.00),\n",
       "  (446, 0.00),\n",
       "  (448, 0.00),\n",
       "  (449, 0.00),\n",
       "  (450, 0.00),\n",
       "  (451, 0.00),\n",
       "  (452, 0.00),\n",
       "  (454, 0.00),\n",
       "  (455, 0.00),\n",
       "  (456, 0.00),\n",
       "  (458, 0.00),\n",
       "  (460, 0.00),\n",
       "  (461, 0.00),\n",
       "  (462, 0.00),\n",
       "  (463, 0.00),\n",
       "  (465, 0.00),\n",
       "  (466, 0.00),\n",
       "  (467, 0.00),\n",
       "  (469, 0.00),\n",
       "  (470, 0.00),\n",
       "  (471, 0.00),\n",
       "  (473, 0.00),\n",
       "  (475, 0.00),\n",
       "  (477, 0.00),\n",
       "  (478, 0.00),\n",
       "  (479, 0.00),\n",
       "  (480, 0.00),\n",
       "  (484, 0.00),\n",
       "  (486, 0.00),\n",
       "  (487, 0.00),\n",
       "  (491, 0.00),\n",
       "  (493, 0.00),\n",
       "  (494, 0.00),\n",
       "  (497, 0.00),\n",
       "  (499, 0.00),\n",
       "  (500, 0.00),\n",
       "  (501, 0.00),\n",
       "  (502, 0.00),\n",
       "  (503, 0.00),\n",
       "  (504, 0.00),\n",
       "  (510, 0.00),\n",
       "  (511, 0.00),\n",
       "  (512, 0.00),\n",
       "  (513, 0.00),\n",
       "  (514, 0.00),\n",
       "  (519, 0.00),\n",
       "  (521, 0.00),\n",
       "  (524, 0.00),\n",
       "  (525, 0.00),\n",
       "  (534, 0.00),\n",
       "  (535, 0.00),\n",
       "  (536, 0.00),\n",
       "  (537, 0.00),\n",
       "  (540, 0.00),\n",
       "  (541, 0.00),\n",
       "  (542, 0.00),\n",
       "  (543, 0.00),\n",
       "  (544, 0.00),\n",
       "  (546, 0.00),\n",
       "  (548, 0.00),\n",
       "  (549, 0.00),\n",
       "  (550, 0.00),\n",
       "  (551, 0.00),\n",
       "  (553, 0.00),\n",
       "  (554, 0.00),\n",
       "  (557, 0.00),\n",
       "  (558, 0.00),\n",
       "  (559, 0.00),\n",
       "  (560, 0.00),\n",
       "  (561, 0.00),\n",
       "  (563, 0.00),\n",
       "  (565, 0.00),\n",
       "  (567, 0.00),\n",
       "  (568, 0.00),\n",
       "  (569, 0.00),\n",
       "  (571, 0.00),\n",
       "  (573, 0.00),\n",
       "  (574, 0.00),\n",
       "  (577, 0.00),\n",
       "  (578, 0.00),\n",
       "  (579, 0.00),\n",
       "  (583, 0.00),\n",
       "  (584, 0.00),\n",
       "  (585, 0.00),\n",
       "  (586, 0.00),\n",
       "  (587, 0.00),\n",
       "  (589, 0.00),\n",
       "  (590, 0.00),\n",
       "  (592, 0.00),\n",
       "  (593, 0.00),\n",
       "  (594, 0.00),\n",
       "  (596, 0.00),\n",
       "  (597, 0.00),\n",
       "  (598, 0.00),\n",
       "  (600, 0.00),\n",
       "  (601, 0.00),\n",
       "  (603, 0.00),\n",
       "  (606, 0.00),\n",
       "  (608, 0.00),\n",
       "  (610, 0.00),\n",
       "  (613, 0.00),\n",
       "  (615, 0.00),\n",
       "  (616, 0.00),\n",
       "  (617, 0.00),\n",
       "  (618, 0.00),\n",
       "  (622, 0.00),\n",
       "  (623, 0.00),\n",
       "  (624, 0.00),\n",
       "  (626, 0.00),\n",
       "  (628, 0.00),\n",
       "  (630, 0.00),\n",
       "  (631, 0.00),\n",
       "  (634, 0.00),\n",
       "  (635, 0.00),\n",
       "  (636, 0.00),\n",
       "  (638, 0.00),\n",
       "  (639, 0.00),\n",
       "  (640, 0.00),\n",
       "  (642, 0.00),\n",
       "  (644, 0.00),\n",
       "  (647, 0.00),\n",
       "  (648, 0.00),\n",
       "  (649, 0.00),\n",
       "  (650, 0.00),\n",
       "  (652, 0.00),\n",
       "  (653, 0.00),\n",
       "  (657, 0.00),\n",
       "  (658, 0.00),\n",
       "  (659, 0.00),\n",
       "  (660, 0.00),\n",
       "  (662, 0.00),\n",
       "  (663, 0.00),\n",
       "  (664, 0.00),\n",
       "  (665, 0.00),\n",
       "  (666, 0.00),\n",
       "  (667, 0.00),\n",
       "  (668, 0.00),\n",
       "  (670, 0.00),\n",
       "  (672, 0.00),\n",
       "  (673, 0.00),\n",
       "  (674, 0.00),\n",
       "  (675, 0.00),\n",
       "  (677, 0.00),\n",
       "  (678, 0.00),\n",
       "  (679, 0.00),\n",
       "  (681, 0.00),\n",
       "  (682, 0.00),\n",
       "  (683, 0.00),\n",
       "  (685, 0.00),\n",
       "  (686, 0.00),\n",
       "  (687, 0.00),\n",
       "  (688, 0.00),\n",
       "  (689, 0.00),\n",
       "  (690, 0.00),\n",
       "  (692, 0.00),\n",
       "  (693, 0.00),\n",
       "  (695, 0.00),\n",
       "  (696, 0.00),\n",
       "  (699, 0.00),\n",
       "  (700, 0.00),\n",
       "  (701, 0.00),\n",
       "  (702, 0.00),\n",
       "  (704, 0.00),\n",
       "  (707, 0.00),\n",
       "  (708, 0.00),\n",
       "  (710, 0.00),\n",
       "  (713, 0.00),\n",
       "  (714, 0.00),\n",
       "  (715, 0.00),\n",
       "  (717, 0.00),\n",
       "  (718, 0.00),\n",
       "  (720, 0.00),\n",
       "  (723, 0.00),\n",
       "  (724, 0.00),\n",
       "  (726, 0.00),\n",
       "  (727, 0.00),\n",
       "  (729, 0.00),\n",
       "  (730, 0.00),\n",
       "  (731, 0.00),\n",
       "  (732, 0.00),\n",
       "  (733, 0.00),\n",
       "  (739, 0.00),\n",
       "  (740, 0.00),\n",
       "  (742, 0.00),\n",
       "  (744, 0.00),\n",
       "  (745, 0.00),\n",
       "  (747, 0.00),\n",
       "  (749, 0.00),\n",
       "  (755, 0.00),\n",
       "  (756, 0.00),\n",
       "  (757, 0.00),\n",
       "  (758, 0.00),\n",
       "  (759, 0.00),\n",
       "  (760, 0.00),\n",
       "  (761, 0.00),\n",
       "  (763, 0.00),\n",
       "  (764, 0.00),\n",
       "  (765, 0.00),\n",
       "  (766, 0.00),\n",
       "  (767, 0.00),\n",
       "  (769, 0.00),\n",
       "  (771, 0.00),\n",
       "  (772, 0.00),\n",
       "  (773, 0.00),\n",
       "  (774, 0.00),\n",
       "  (775, 0.00),\n",
       "  (776, 0.00),\n",
       "  (777, 0.00),\n",
       "  (778, 0.00),\n",
       "  (780, 0.00),\n",
       "  (782, 0.00),\n",
       "  (784, 0.00),\n",
       "  (785, 0.00),\n",
       "  (789, 0.00),\n",
       "  (793, 0.00),\n",
       "  (795, 0.00),\n",
       "  (797, 0.00),\n",
       "  (798, 0.00),\n",
       "  (799, 0.00),\n",
       "  (800, 0.00),\n",
       "  (802, 0.00),\n",
       "  (804, 0.00),\n",
       "  (807, 0.00),\n",
       "  (810, 0.00),\n",
       "  (811, 0.00),\n",
       "  (812, 0.00),\n",
       "  (813, 0.00),\n",
       "  (818, 0.00),\n",
       "  (821, 0.00),\n",
       "  (823, 0.00),\n",
       "  (825, 0.00),\n",
       "  (827, 0.00),\n",
       "  (830, 0.00),\n",
       "  (831, 0.00),\n",
       "  (832, 0.00),\n",
       "  (833, 0.00),\n",
       "  (834, 0.00),\n",
       "  (835, 0.00),\n",
       "  (838, 0.00),\n",
       "  (839, 0.00),\n",
       "  (840, 0.00),\n",
       "  (841, 0.00),\n",
       "  (844, 0.00),\n",
       "  (845, 0.00),\n",
       "  (846, 0.00),\n",
       "  (848, 0.00),\n",
       "  (849, 0.00),\n",
       "  (851, 0.00),\n",
       "  (852, 0.00),\n",
       "  (853, 0.00),\n",
       "  (855, 0.00),\n",
       "  (856, 0.00),\n",
       "  (859, 0.00),\n",
       "  (860, 0.00),\n",
       "  (861, 0.00),\n",
       "  (862, 0.00),\n",
       "  (863, 0.00),\n",
       "  (866, 0.00),\n",
       "  (869, 0.00),\n",
       "  (870, 0.00),\n",
       "  (873, 0.00),\n",
       "  (874, 0.00),\n",
       "  (875, 0.00),\n",
       "  (876, 0.00),\n",
       "  (877, 0.00),\n",
       "  (878, 0.00),\n",
       "  (880, 0.00),\n",
       "  (881, 0.00),\n",
       "  (882, 0.00),\n",
       "  (883, 0.00),\n",
       "  (884, 0.00),\n",
       "  (885, 0.00),\n",
       "  (887, 0.00),\n",
       "  (888, 0.00),\n",
       "  (890, 0.00),\n",
       "  (891, 0.00),\n",
       "  (894, 0.00),\n",
       "  (895, 0.00),\n",
       "  (896, 0.00),\n",
       "  (899, 0.00),\n",
       "  (900, 0.00),\n",
       "  (901, 0.00),\n",
       "  (902, 0.00),\n",
       "  (903, 0.00),\n",
       "  (905, 0.00),\n",
       "  (908, 0.00),\n",
       "  (909, 0.00),\n",
       "  (911, 0.00),\n",
       "  (912, 0.00),\n",
       "  (913, 0.00),\n",
       "  (914, 0.00),\n",
       "  (916, 0.00),\n",
       "  (917, 0.00),\n",
       "  (919, 0.00),\n",
       "  (920, 0.00),\n",
       "  (921, 0.00),\n",
       "  (922, 0.00),\n",
       "  (924, 0.00),\n",
       "  (925, 0.00),\n",
       "  (926, 0.00),\n",
       "  (927, 0.00),\n",
       "  (928, 0.00),\n",
       "  (929, 0.00),\n",
       "  (930, 0.00),\n",
       "  (931, 0.00),\n",
       "  (932, 0.00),\n",
       "  (933, 0.00),\n",
       "  (935, 0.00),\n",
       "  (936, 0.00),\n",
       "  (938, 0.00),\n",
       "  (940, 0.00),\n",
       "  (941, 0.00),\n",
       "  (942, 0.00),\n",
       "  (943, 0.00),\n",
       "  (945, 0.00),\n",
       "  (947, 0.00),\n",
       "  (948, 0.00),\n",
       "  (949, 0.00),\n",
       "  (950, 0.00),\n",
       "  (951, 0.00),\n",
       "  (952, 0.00),\n",
       "  (954, 0.00),\n",
       "  (956, 0.00),\n",
       "  (957, 0.00),\n",
       "  (958, 0.00),\n",
       "  (959, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (962, 0.00),\n",
       "  (964, 0.00),\n",
       "  (965, 0.00),\n",
       "  (966, 0.00),\n",
       "  (967, 0.00),\n",
       "  (968, 0.00),\n",
       "  (969, 0.00),\n",
       "  (970, 0.00),\n",
       "  (972, 0.00),\n",
       "  (974, 0.00),\n",
       "  (975, 0.00),\n",
       "  (976, 0.00),\n",
       "  (977, 0.00),\n",
       "  (978, 0.00),\n",
       "  (979, 0.00),\n",
       "  (980, 0.00),\n",
       "  (983, 0.00),\n",
       "  (985, 0.00),\n",
       "  (986, 0.00),\n",
       "  (988, 0.00),\n",
       "  (989, 0.00),\n",
       "  (990, 0.00),\n",
       "  (991, 0.00),\n",
       "  (993, 0.00),\n",
       "  (994, 0.00),\n",
       "  (995, 0.00),\n",
       "  (996, 0.00),\n",
       "  (998, 0.00),\n",
       "  (999, 0.00)])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcc31fe3c50>]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_hist = sorted(hist, key=lambda x: x[0], reverse = False)\n",
    "values = [elem[1] for elem in sorted_hist]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.5756)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 5\n",
      "at batch no 10\n",
      "at batch no 15\n",
      "at batch no 20\n",
      "at batch no 25\n",
      "at batch no 30\n",
      "at batch no 35\n",
      "at batch no 40\n",
      "at batch no 45\n",
      "at batch no 50\n",
      "at batch no 55\n",
      "at batch no 60\n",
      "at batch no 65\n",
      "at batch no 70\n",
      "at batch no 75\n",
      "at batch no 80\n",
      "at batch no 85\n",
      "at batch no 90\n",
      "at batch no 95\n",
      "at batch no 100\n",
      "at batch no 105\n",
      "at batch no 110\n",
      "at batch no 115\n",
      "at batch no 120\n",
      "at batch no 125\n",
      "at batch no 130\n",
      "at batch no 135\n",
      "at batch no 140\n",
      "at batch no 145\n",
      "at batch no 150\n",
      "at batch no 155\n",
      "at batch no 160\n",
      "at batch no 165\n",
      "at batch no 170\n",
      "at batch no 175\n",
      "at batch no 180\n",
      "at batch no 185\n",
      "at batch no 190\n",
      "at batch no 195\n",
      "at batch no 200\n",
      "at batch no 205\n",
      "at batch no 210\n",
      "at batch no 215\n",
      "at batch no 220\n",
      "at batch no 225\n",
      "at batch no 230\n",
      "at batch no 235\n",
      "at batch no 240\n",
      "at batch no 245\n",
      "at batch no 250\n",
      "at batch no 255\n",
      "at batch no 260\n",
      "at batch no 265\n",
      "at batch no 270\n",
      "at batch no 275\n",
      "at batch no 280\n",
      "at batch no 285\n",
      "at batch no 290\n",
      "at batch no 295\n",
      "at batch no 300\n",
      "at batch no 305\n",
      "at batch no 310\n",
      "at batch no 315\n",
      "at batch no 320\n",
      "at batch no 325\n",
      "at batch no 330\n",
      "at batch no 335\n",
      "at batch no 340\n",
      "at batch no 345\n",
      "at batch no 350\n",
      "at batch no 355\n",
      "at batch no 360\n",
      "at batch no 365\n",
      "at batch no 370\n",
      "at batch no 375\n",
      "at batch no 380\n",
      "at batch no 385\n",
      "at batch no 390\n",
      "at batch no 395\n",
      "at batch no 400\n",
      "at batch no 405\n",
      "at batch no 410\n",
      "at batch no 415\n",
      "at batch no 420\n",
      "at batch no 425\n",
      "at batch no 430\n",
      "at batch no 435\n",
      "at batch no 440\n",
      "at batch no 445\n",
      "at batch no 450\n",
      "at batch no 455\n",
      "at batch no 460\n",
      "at batch no 465\n",
      "at batch no 470\n",
      "at batch no 475\n",
      "at batch no 480\n",
      "at batch no 485\n",
      "at batch no 490\n",
      "at batch no 495\n",
      "at batch no 500\n",
      "at batch no 505\n",
      "at batch no 510\n",
      "at batch no 515\n",
      "at batch no 520\n",
      "at batch no 525\n",
      "at batch no 530\n",
      "at batch no 535\n",
      "at batch no 540\n",
      "at batch no 545\n",
      "at batch no 550\n",
      "at batch no 555\n",
      "at batch no 560\n",
      "at batch no 565\n",
      "at batch no 570\n",
      "at batch no 575\n",
      "at batch no 580\n",
      "at batch no 585\n",
      "at batch no 590\n",
      "at batch no 595\n",
      "at batch no 600\n",
      "at batch no 605\n",
      "at batch no 610\n",
      "at batch no 615\n",
      "at batch no 620\n",
      "at batch no 625\n",
      "at batch no 630\n",
      "at batch no 635\n",
      "at batch no 640\n",
      "at batch no 645\n",
      "at batch no 650\n",
      "at batch no 655\n",
      "at batch no 660\n",
      "at batch no 665\n",
      "at batch no 670\n",
      "at batch no 675\n",
      "at batch no 680\n",
      "at batch no 685\n",
      "at batch no 690\n",
      "at batch no 695\n",
      "at batch no 700\n",
      "at batch no 705\n",
      "at batch no 710\n",
      "at batch no 715\n",
      "at batch no 720\n",
      "at batch no 725\n",
      "at batch no 730\n",
      "at batch no 735\n",
      "at batch no 740\n",
      "at batch no 745\n",
      "at batch no 750\n",
      "at batch no 755\n",
      "at batch no 760\n",
      "at batch no 765\n",
      "at batch no 770\n",
      "at batch no 775\n",
      "at batch no 780\n",
      "at batch no 785\n",
      "at batch no 790\n",
      "at batch no 795\n",
      "at batch no 800\n",
      "at batch no 805\n",
      "at batch no 810\n",
      "at batch no 815\n",
      "at batch no 820\n",
      "at batch no 825\n",
      "at batch no 830\n",
      "at batch no 835\n",
      "at batch no 840\n",
      "at batch no 845\n",
      "at batch no 850\n",
      "at batch no 855\n",
      "at batch no 860\n",
      "at batch no 865\n",
      "at batch no 870\n",
      "at batch no 875\n",
      "at batch no 880\n",
      "at batch no 885\n",
      "at batch no 890\n",
      "at batch no 895\n",
      "at batch no 900\n",
      "at batch no 905\n",
      "at batch no 910\n",
      "at batch no 915\n",
      "at batch no 920\n",
      "at batch no 925\n",
      "at batch no 930\n",
      "at batch no 935\n",
      "at batch no 940\n",
      "at batch no 945\n",
      "at batch no 950\n",
      "at batch no 955\n",
      "at batch no 960\n",
      "at batch no 965\n",
      "at batch no 970\n",
      "at batch no 975\n",
      "at batch no 980\n",
      "at batch no 985\n",
      "at batch no 990\n",
      "at batch no 995\n",
      "at batch no 1000\n",
      "at batch no 1005\n",
      "at batch no 1010\n",
      "at batch no 1015\n",
      "at batch no 1020\n",
      "at batch no 1025\n",
      "at batch no 1030\n",
      "at batch no 1035\n",
      "at batch no 1040\n",
      "at batch no 1045\n",
      "at batch no 1050\n",
      "at batch no 1055\n",
      "at batch no 1060\n",
      "at batch no 1065\n",
      "at batch no 1070\n",
      "at batch no 1075\n",
      "at batch no 1080\n",
      "at batch no 1085\n",
      "at batch no 1090\n",
      "at batch no 1095\n",
      "at batch no 1100\n",
      "at batch no 1105\n",
      "at batch no 1110\n",
      "at batch no 1115\n",
      "at batch no 1120\n",
      "at batch no 1125\n",
      "at batch no 1130\n",
      "at batch no 1135\n",
      "at batch no 1140\n",
      "at batch no 1145\n",
      "at batch no 1150\n",
      "at batch no 1155\n",
      "at batch no 1160\n",
      "at batch no 1165\n",
      "at batch no 1170\n",
      "at batch no 1175\n",
      "at batch no 1180\n",
      "at batch no 1185\n",
      "at batch no 1190\n",
      "at batch no 1195\n",
      "at batch no 1200\n",
      "at batch no 1205\n",
      "at batch no 1210\n",
      "at batch no 1215\n",
      "at batch no 1220\n",
      "at batch no 1225\n",
      "at batch no 1230\n",
      "at batch no 1235\n",
      "at batch no 1240\n",
      "at batch no 1245\n",
      "at batch no 1250\n",
      "at batch no 1255\n",
      "at batch no 1260\n",
      "at batch no 1265\n",
      "at batch no 1270\n",
      "at batch no 1275\n",
      "at batch no 1280\n",
      "at batch no 1285\n",
      "at batch no 1290\n",
      "at batch no 1295\n",
      "at batch no 1300\n",
      "at batch no 1305\n",
      "at batch no 1310\n",
      "at batch no 1315\n",
      "at batch no 1320\n",
      "at batch no 1325\n",
      "at batch no 1330\n",
      "at batch no 1335\n",
      "at batch no 1340\n",
      "at batch no 1345\n",
      "at batch no 1350\n",
      "at batch no 1355\n",
      "at batch no 1360\n",
      "at batch no 1365\n",
      "at batch no 1370\n",
      "at batch no 1375\n",
      "at batch no 1380\n",
      "at batch no 1385\n",
      "at batch no 1390\n",
      "at batch no 1395\n",
      "at batch no 1400\n",
      "at batch no 1405\n",
      "at batch no 1410\n",
      "at batch no 1415\n",
      "at batch no 1420\n",
      "at batch no 1425\n",
      "at batch no 1430\n",
      "at batch no 1435\n",
      "at batch no 1440\n",
      "at batch no 1445\n",
      "at batch no 1450\n",
      "at batch no 1455\n",
      "at batch no 1460\n",
      "at batch no 1465\n",
      "at batch no 1470\n",
      "at batch no 1475\n",
      "at batch no 1480\n",
      "at batch no 1485\n",
      "at batch no 1490\n",
      "at batch no 1495\n",
      "at batch no 1500\n",
      "at batch no 1505\n",
      "at batch no 1510\n",
      "at batch no 1515\n",
      "at batch no 1520\n",
      "at batch no 1525\n",
      "at batch no 1530\n",
      "at batch no 1535\n",
      "at batch no 1540\n",
      "at batch no 1545\n",
      "at batch no 1550\n",
      "at batch no 1555\n",
      "at batch no 1560\n",
      "at batch no 1565\n",
      "at batch no 1570\n",
      "at batch no 1575\n",
      "at batch no 1580\n",
      "at batch no 1585\n",
      "at batch no 1590\n",
      "at batch no 1595\n",
      "at batch no 1600\n",
      "at batch no 1605\n",
      "at batch no 1610\n",
      "at batch no 1615\n",
      "at batch no 1620\n",
      "at batch no 1625\n",
      "at batch no 1630\n",
      "at batch no 1635\n",
      "at batch no 1640\n",
      "at batch no 1645\n",
      "at batch no 1650\n",
      "at batch no 1655\n",
      "at batch no 1660\n",
      "at batch no 1665\n",
      "at batch no 1670\n",
      "at batch no 1675\n",
      "at batch no 1680\n",
      "at batch no 1685\n",
      "at batch no 1690\n",
      "at batch no 1695\n",
      "at batch no 1700\n",
      "at batch no 1705\n",
      "at batch no 1710\n",
      "at batch no 1715\n",
      "at batch no 1720\n",
      "at batch no 1725\n",
      "at batch no 1730\n",
      "at batch no 1735\n",
      "at batch no 1740\n",
      "at batch no 1745\n",
      "at batch no 1750\n",
      "at batch no 1755\n",
      "at batch no 1760\n",
      "at batch no 1765\n",
      "at batch no 1770\n",
      "at batch no 1775\n",
      "at batch no 1780\n",
      "at batch no 1785\n",
      "at batch no 1790\n",
      "at batch no 1795\n",
      "at batch no 1800\n",
      "at batch no 1805\n",
      "at batch no 1810\n",
      "at batch no 1815\n",
      "at batch no 1820\n",
      "at batch no 1825\n",
      "at batch no 1830\n",
      "at batch no 1835\n",
      "at batch no 1840\n",
      "at batch no 1845\n",
      "at batch no 1850\n",
      "at batch no 1855\n",
      "at batch no 1860\n",
      "at batch no 1865\n",
      "at batch no 1870\n",
      "at batch no 1875\n",
      "at batch no 1880\n",
      "at batch no 1885\n",
      "at batch no 1890\n",
      "at batch no 1895\n",
      "at batch no 1900\n",
      "at batch no 1905\n",
      "at batch no 1910\n",
      "at batch no 1915\n",
      "at batch no 1920\n",
      "at batch no 1925\n",
      "at batch no 1930\n",
      "at batch no 1935\n",
      "at batch no 1940\n",
      "at batch no 1945\n",
      "at batch no 1950\n",
      "at batch no 1955\n",
      "at batch no 1960\n",
      "at batch no 1965\n",
      "at batch no 1970\n",
      "at batch no 1975\n",
      "at batch no 1980\n",
      "at batch no 1985\n",
      "at batch no 1990\n",
      "at batch no 1995\n",
      "at batch no 2000\n",
      "at batch no 2005\n",
      "at batch no 2010\n",
      "at batch no 2015\n",
      "at batch no 2020\n",
      "at batch no 2025\n",
      "at batch no 2030\n",
      "at batch no 2035\n",
      "at batch no 2040\n",
      "at batch no 2045\n",
      "at batch no 2050\n",
      "at batch no 2055\n",
      "at batch no 2060\n",
      "at batch no 2065\n",
      "at batch no 2070\n",
      "at batch no 2075\n",
      "at batch no 2080\n",
      "at batch no 2085\n",
      "at batch no 2090\n",
      "at batch no 2095\n",
      "at batch no 2100\n",
      "at batch no 2105\n",
      "at batch no 2110\n",
      "at batch no 2115\n",
      "at batch no 2120\n",
      "at batch no 2125\n",
      "at batch no 2130\n",
      "at batch no 2135\n",
      "at batch no 2140\n",
      "at batch no 2145\n",
      "at batch no 2150\n",
      "at batch no 2155\n",
      "at batch no 2160\n",
      "at batch no 2165\n",
      "at batch no 2170\n",
      "at batch no 2175\n",
      "at batch no 2180\n",
      "at batch no 2185\n",
      "at batch no 2190\n",
      "at batch no 2195\n",
      "at batch no 2200\n",
      "at batch no 2205\n",
      "at batch no 2210\n",
      "at batch no 2215\n",
      "at batch no 2220\n",
      "at batch no 2225\n",
      "at batch no 2230\n",
      "at batch no 2235\n",
      "at batch no 2240\n",
      "at batch no 2245\n",
      "at batch no 2250\n",
      "at batch no 2255\n",
      "at batch no 2260\n",
      "at batch no 2265\n",
      "at batch no 2270\n",
      "at batch no 2275\n",
      "at batch no 2280\n",
      "at batch no 2285\n",
      "at batch no 2290\n",
      "at batch no 2295\n",
      "at batch no 2300\n",
      "at batch no 2305\n",
      "at batch no 2310\n",
      "at batch no 2315\n",
      "at batch no 2320\n",
      "at batch no 2325\n",
      "at batch no 2330\n",
      "at batch no 2335\n",
      "at batch no 2340\n",
      "at batch no 2345\n",
      "at batch no 2350\n",
      "at batch no 2355\n",
      "at batch no 2360\n",
      "at batch no 2365\n",
      "at batch no 2370\n",
      "at batch no 2375\n",
      "at batch no 2380\n",
      "at batch no 2385\n",
      "at batch no 2390\n",
      "at batch no 2395\n",
      "at batch no 2400\n",
      "at batch no 2405\n",
      "at batch no 2410\n",
      "at batch no 2415\n",
      "at batch no 2420\n",
      "at batch no 2425\n",
      "at batch no 2430\n",
      "at batch no 2435\n",
      "at batch no 2440\n",
      "at batch no 2445\n",
      "at batch no 2450\n",
      "at batch no 2455\n",
      "at batch no 2460\n",
      "at batch no 2465\n",
      "at batch no 2470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 2475\n",
      "at batch no 2480\n",
      "at batch no 2485\n",
      "at batch no 2490\n",
      "at batch no 2495\n",
      "at batch no 2500\n",
      "at batch no 2505\n",
      "at batch no 2510\n",
      "at batch no 2515\n",
      "at batch no 2520\n",
      "at batch no 2525\n",
      "at batch no 2530\n",
      "at batch no 2535\n",
      "at batch no 2540\n",
      "at batch no 2545\n",
      "at batch no 2550\n",
      "at batch no 2555\n",
      "at batch no 2560\n",
      "at batch no 2565\n",
      "at batch no 2570\n",
      "at batch no 2575\n",
      "at batch no 2580\n",
      "at batch no 2585\n",
      "at batch no 2590\n",
      "at batch no 2595\n",
      "at batch no 2600\n",
      "at batch no 2605\n",
      "at batch no 2610\n",
      "at batch no 2615\n",
      "at batch no 2620\n",
      "at batch no 2625\n",
      "at batch no 2630\n",
      "at batch no 2635\n",
      "at batch no 2640\n",
      "at batch no 2645\n",
      "at batch no 2650\n",
      "at batch no 2655\n",
      "at batch no 2660\n",
      "at batch no 2665\n",
      "at batch no 2670\n",
      "at batch no 2675\n",
      "at batch no 2680\n",
      "at batch no 2685\n",
      "at batch no 2690\n",
      "at batch no 2695\n",
      "at batch no 2700\n",
      "at batch no 2705\n",
      "at batch no 2710\n",
      "at batch no 2715\n",
      "at batch no 2720\n",
      "at batch no 2725\n",
      "at batch no 2730\n",
      "at batch no 2735\n",
      "at batch no 2740\n",
      "at batch no 2745\n",
      "at batch no 2750\n",
      "at batch no 2755\n",
      "at batch no 2760\n",
      "at batch no 2765\n",
      "at batch no 2770\n",
      "at batch no 2775\n",
      "at batch no 2780\n",
      "at batch no 2785\n",
      "at batch no 2790\n",
      "at batch no 2795\n",
      "at batch no 2800\n",
      "at batch no 2805\n",
      "at batch no 2810\n",
      "at batch no 2815\n",
      "at batch no 2820\n",
      "at batch no 2825\n",
      "at batch no 2830\n",
      "at batch no 2835\n",
      "at batch no 2840\n",
      "at batch no 2845\n",
      "at batch no 2850\n",
      "at batch no 2855\n",
      "at batch no 2860\n",
      "at batch no 2865\n",
      "at batch no 2870\n",
      "at batch no 2875\n",
      "at batch no 2880\n",
      "at batch no 2885\n",
      "at batch no 2890\n",
      "at batch no 2895\n",
      "at batch no 2900\n",
      "at batch no 2905\n",
      "at batch no 2910\n",
      "at batch no 2915\n",
      "at batch no 2920\n",
      "at batch no 2925\n",
      "at batch no 2930\n",
      "at batch no 2935\n",
      "at batch no 2940\n",
      "at batch no 2945\n",
      "at batch no 2950\n",
      "at batch no 2955\n",
      "at batch no 2960\n",
      "at batch no 2965\n",
      "at batch no 2970\n",
      "at batch no 2975\n",
      "at batch no 2980\n",
      "at batch no 2985\n",
      "at batch no 2990\n",
      "at batch no 2995\n",
      "at batch no 3000\n",
      "at batch no 3005\n",
      "at batch no 3010\n",
      "at batch no 3015\n",
      "at batch no 3020\n",
      "at batch no 3025\n",
      "at batch no 3030\n",
      "at batch no 3035\n",
      "at batch no 3040\n",
      "at batch no 3045\n",
      "at batch no 3050\n",
      "at batch no 3055\n",
      "at batch no 3060\n",
      "at batch no 3065\n",
      "at batch no 3070\n",
      "at batch no 3075\n",
      "at batch no 3080\n",
      "at batch no 3085\n",
      "at batch no 3090\n",
      "at batch no 3095\n",
      "at batch no 3100\n",
      "at batch no 3105\n",
      "at batch no 3110\n",
      "at batch no 3115\n",
      "at batch no 3120\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(320,\n",
       " [(721, 14501.80),\n",
       "  (669, 3372.80),\n",
       "  (520, 3202.30),\n",
       "  (750, 2116.30),\n",
       "  (556, 2073.50),\n",
       "  (414, 1936.70),\n",
       "  (588, 1714.80),\n",
       "  (431, 1556.00),\n",
       "  (411, 1513.90),\n",
       "  (61, 1296.50),\n",
       "  (828, 1069.60),\n",
       "  (39, 556.00),\n",
       "  (904, 484.50),\n",
       "  (709, 433.30),\n",
       "  (794, 431.50),\n",
       "  (971, 369.80),\n",
       "  (489, 295.10),\n",
       "  (614, 281.80),\n",
       "  (790, 258.20),\n",
       "  (599, 249.00),\n",
       "  (893, 236.70),\n",
       "  (581, 230.00),\n",
       "  (955, 229.90),\n",
       "  (490, 229.70),\n",
       "  (770, 198.40),\n",
       "  (509, 150.40),\n",
       "  (84, 147.10),\n",
       "  (401, 126.80),\n",
       "  (419, 119.20),\n",
       "  (48, 113.50),\n",
       "  (60, 107.90),\n",
       "  (56, 106.10),\n",
       "  (741, 103.50),\n",
       "  (516, 102.80),\n",
       "  (67, 101.10),\n",
       "  (748, 98.20),\n",
       "  (646, 81.90),\n",
       "  (824, 81.70),\n",
       "  (837, 81.60),\n",
       "  (762, 76.80),\n",
       "  (703, 75.10),\n",
       "  (46, 74.00),\n",
       "  (580, 73.00),\n",
       "  (412, 71.00),\n",
       "  (582, 69.70),\n",
       "  (151, 69.00),\n",
       "  (907, 68.20),\n",
       "  (621, 64.10),\n",
       "  (737, 64.10),\n",
       "  (464, 64.00),\n",
       "  (791, 62.90),\n",
       "  (973, 62.80),\n",
       "  (55, 62.60),\n",
       "  (406, 60.70),\n",
       "  (591, 58.20),\n",
       "  (636, 57.40),\n",
       "  (319, 56.80),\n",
       "  (455, 56.70),\n",
       "  (805, 56.40),\n",
       "  (800, 56.00),\n",
       "  (443, 54.30),\n",
       "  (879, 53.60),\n",
       "  (468, 52.20),\n",
       "  (754, 51.50),\n",
       "  (706, 50.70),\n",
       "  (987, 48.40),\n",
       "  (575, 48.30),\n",
       "  (651, 47.50),\n",
       "  (555, 46.50),\n",
       "  (171, 44.70),\n",
       "  (992, 44.50),\n",
       "  (570, 43.90),\n",
       "  (340, 43.00),\n",
       "  (533, 42.90),\n",
       "  (40, 42.60),\n",
       "  (788, 42.40),\n",
       "  (815, 41.80),\n",
       "  (138, 41.50),\n",
       "  (407, 41.40),\n",
       "  (410, 41.30),\n",
       "  (806, 40.60),\n",
       "  (485, 40.10),\n",
       "  (97, 40.00),\n",
       "  (645, 39.90),\n",
       "  (698, 39.70),\n",
       "  (476, 39.30),\n",
       "  (293, 39.00),\n",
       "  (572, 38.80),\n",
       "  (116, 37.70),\n",
       "  (304, 37.30),\n",
       "  (300, 36.50),\n",
       "  (692, 35.50),\n",
       "  (779, 35.30),\n",
       "  (155, 35.20),\n",
       "  (801, 35.20),\n",
       "  (878, 34.50),\n",
       "  (868, 34.00),\n",
       "  (496, 33.90),\n",
       "  (734, 33.90),\n",
       "  (654, 33.70),\n",
       "  (746, 33.60),\n",
       "  (796, 33.20),\n",
       "  (436, 33.00),\n",
       "  (661, 32.90),\n",
       "  (323, 32.80),\n",
       "  (497, 32.40),\n",
       "  (604, 32.20),\n",
       "  (953, 32.20),\n",
       "  (981, 32.20),\n",
       "  (711, 31.80),\n",
       "  (562, 31.30),\n",
       "  (0, 30.80),\n",
       "  (94, 30.70),\n",
       "  (829, 30.70),\n",
       "  (611, 30.60),\n",
       "  (676, 30.00),\n",
       "  (781, 30.00),\n",
       "  (819, 29.90),\n",
       "  (858, 28.90),\n",
       "  (705, 28.80),\n",
       "  (768, 28.70),\n",
       "  (547, 28.40),\n",
       "  (916, 28.40),\n",
       "  (62, 28.30),\n",
       "  (254, 28.30),\n",
       "  (612, 28.00),\n",
       "  (655, 28.00),\n",
       "  (109, 27.40),\n",
       "  (162, 27.40),\n",
       "  (643, 27.40),\n",
       "  (886, 27.20),\n",
       "  (440, 27.10),\n",
       "  (850, 26.60),\n",
       "  (363, 26.50),\n",
       "  (738, 25.80),\n",
       "  (915, 25.80),\n",
       "  (195, 25.50),\n",
       "  (396, 25.00),\n",
       "  (619, 25.00),\n",
       "  (752, 24.70),\n",
       "  (870, 24.60),\n",
       "  (96, 24.30),\n",
       "  (506, 24.30),\n",
       "  (118, 23.90),\n",
       "  (635, 23.90),\n",
       "  (518, 23.80),\n",
       "  (946, 23.70),\n",
       "  (576, 23.60),\n",
       "  (917, 23.50),\n",
       "  (237, 23.40),\n",
       "  (128, 23.30),\n",
       "  (353, 23.20),\n",
       "  (854, 23.20),\n",
       "  (843, 22.90),\n",
       "  (63, 22.60),\n",
       "  (565, 22.50),\n",
       "  (944, 22.10),\n",
       "  (863, 21.90),\n",
       "  (522, 21.70),\n",
       "  (627, 21.70),\n",
       "  (90, 21.60),\n",
       "  (110, 21.60),\n",
       "  (722, 21.50),\n",
       "  (25, 21.30),\n",
       "  (82, 21.20),\n",
       "  (238, 21.20),\n",
       "  (444, 21.20),\n",
       "  (65, 21.00),\n",
       "  (31, 20.80),\n",
       "  (47, 20.80),\n",
       "  (880, 20.80),\n",
       "  (482, 20.70),\n",
       "  (108, 20.60),\n",
       "  (641, 20.60),\n",
       "  (290, 20.30),\n",
       "  (564, 20.20),\n",
       "  (253, 20.00),\n",
       "  (866, 19.90),\n",
       "  (765, 19.70),\n",
       "  (640, 19.60),\n",
       "  (526, 19.50),\n",
       "  (135, 19.30),\n",
       "  (288, 19.10),\n",
       "  (474, 19.10),\n",
       "  (808, 19.10),\n",
       "  (892, 19.00),\n",
       "  (560, 18.80),\n",
       "  (826, 18.80),\n",
       "  (515, 18.70),\n",
       "  (308, 18.50),\n",
       "  (716, 18.40),\n",
       "  (8, 18.30),\n",
       "  (772, 18.30),\n",
       "  (867, 18.30),\n",
       "  (327, 18.20),\n",
       "  (898, 18.20),\n",
       "  (538, 18.10),\n",
       "  (398, 18.00),\n",
       "  (656, 18.00),\n",
       "  (251, 17.90),\n",
       "  (874, 17.60),\n",
       "  (918, 17.60),\n",
       "  (441, 17.50),\n",
       "  (459, 17.50),\n",
       "  (178, 17.40),\n",
       "  (241, 17.40),\n",
       "  (430, 17.40),\n",
       "  (687, 17.40),\n",
       "  (679, 17.30),\n",
       "  (523, 17.00),\n",
       "  (671, 17.00),\n",
       "  (603, 16.80),\n",
       "  (735, 16.80),\n",
       "  (545, 16.70),\n",
       "  (849, 16.60),\n",
       "  (897, 16.60),\n",
       "  (136, 16.50),\n",
       "  (561, 16.50),\n",
       "  (872, 16.50),\n",
       "  (670, 16.40),\n",
       "  (7, 16.30),\n",
       "  (817, 16.30),\n",
       "  (857, 16.30),\n",
       "  (159, 16.20),\n",
       "  (292, 16.20),\n",
       "  (472, 16.10),\n",
       "  (541, 16.10),\n",
       "  (832, 16.10),\n",
       "  (453, 16.00),\n",
       "  (566, 16.00),\n",
       "  (609, 15.90),\n",
       "  (129, 15.80),\n",
       "  (730, 15.80),\n",
       "  (847, 15.80),\n",
       "  (124, 15.70),\n",
       "  (579, 15.70),\n",
       "  (620, 15.60),\n",
       "  (625, 15.60),\n",
       "  (423, 15.50),\n",
       "  (457, 15.40),\n",
       "  (586, 15.40),\n",
       "  (725, 15.40),\n",
       "  (352, 15.30),\n",
       "  (668, 15.30),\n",
       "  (852, 15.30),\n",
       "  (937, 15.30),\n",
       "  (173, 15.20),\n",
       "  (642, 15.00),\n",
       "  (968, 15.00),\n",
       "  (822, 14.90),\n",
       "  (956, 14.90),\n",
       "  (130, 14.80),\n",
       "  (539, 14.80),\n",
       "  (417, 14.70),\n",
       "  (76, 14.60),\n",
       "  (133, 14.60),\n",
       "  (282, 14.60),\n",
       "  (532, 14.40),\n",
       "  (757, 14.40),\n",
       "  (984, 14.30),\n",
       "  (563, 14.20),\n",
       "  (949, 14.20),\n",
       "  (123, 14.10),\n",
       "  (275, 14.10),\n",
       "  (530, 14.10),\n",
       "  (88, 14.00),\n",
       "  (783, 13.90),\n",
       "  (885, 13.80),\n",
       "  (685, 13.70),\n",
       "  (328, 13.60),\n",
       "  (595, 13.60),\n",
       "  (601, 13.50),\n",
       "  (102, 13.40),\n",
       "  (199, 13.40),\n",
       "  (425, 13.40),\n",
       "  (447, 13.40),\n",
       "  (9, 13.30),\n",
       "  (694, 13.30),\n",
       "  (594, 13.20),\n",
       "  (665, 13.20),\n",
       "  (724, 13.20),\n",
       "  (912, 13.10),\n",
       "  (231, 12.90),\n",
       "  (37, 12.80),\n",
       "  (717, 12.80),\n",
       "  (727, 12.80),\n",
       "  (864, 12.80),\n",
       "  (911, 12.80),\n",
       "  (85, 12.70),\n",
       "  (354, 12.70),\n",
       "  (985, 12.70),\n",
       "  (552, 12.60),\n",
       "  (723, 12.50),\n",
       "  (445, 12.20),\n",
       "  (936, 12.20),\n",
       "  (950, 12.20),\n",
       "  (192, 12.10),\n",
       "  (751, 12.10),\n",
       "  (144, 12.00),\n",
       "  (387, 12.00),\n",
       "  (887, 12.00),\n",
       "  (982, 12.00),\n",
       "  (57, 11.90),\n",
       "  (98, 11.80),\n",
       "  (274, 11.60),\n",
       "  (632, 11.50),\n",
       "  (181, 11.40),\n",
       "  (820, 11.40),\n",
       "  (45, 11.30),\n",
       "  (215, 11.30),\n",
       "  (658, 11.30),\n",
       "  (693, 11.30),\n",
       "  (888, 11.30),\n",
       "  (263, 11.20),\n",
       "  (208, 11.10),\n",
       "  (652, 11.10),\n",
       "  (954, 11.10),\n",
       "  (554, 11.00),\n",
       "  (821, 11.00),\n",
       "  (938, 11.00),\n",
       "  (559, 10.90),\n",
       "  (602, 10.90),\n",
       "  (91, 10.70),\n",
       "  (311, 10.70),\n",
       "  (330, 10.70),\n",
       "  (393, 10.70),\n",
       "  (571, 10.70),\n",
       "  (624, 10.70),\n",
       "  (637, 10.70),\n",
       "  (23, 10.60),\n",
       "  (927, 10.60),\n",
       "  (119, 10.50),\n",
       "  (134, 10.50),\n",
       "  (301, 10.50),\n",
       "  (409, 10.50),\n",
       "  (439, 10.50),\n",
       "  (492, 10.50),\n",
       "  (667, 10.50),\n",
       "  (855, 10.50),\n",
       "  (211, 10.20),\n",
       "  (424, 10.20),\n",
       "  (392, 10.10),\n",
       "  (639, 10.10),\n",
       "  (273, 10.00),\n",
       "  (278, 10.00),\n",
       "  (334, 10.00),\n",
       "  (778, 10.00),\n",
       "  (216, 9.90),\n",
       "  (573, 9.90),\n",
       "  (839, 9.90),\n",
       "  (230, 9.80),\n",
       "  (306, 9.80),\n",
       "  (99, 9.60),\n",
       "  (107, 9.50),\n",
       "  (429, 9.50),\n",
       "  (6, 9.40),\n",
       "  (161, 9.40),\n",
       "  (448, 9.40),\n",
       "  (77, 9.30),\n",
       "  (157, 9.30),\n",
       "  (30, 9.10),\n",
       "  (239, 9.10),\n",
       "  (303, 9.10),\n",
       "  (498, 9.10),\n",
       "  (121, 9.00),\n",
       "  (249, 9.00),\n",
       "  (467, 9.00),\n",
       "  (616, 9.00),\n",
       "  (388, 8.90),\n",
       "  (458, 8.90),\n",
       "  (695, 8.90),\n",
       "  (766, 8.90),\n",
       "  (905, 8.90),\n",
       "  (243, 8.80),\n",
       "  (487, 8.80),\n",
       "  (830, 8.80),\n",
       "  (83, 8.70),\n",
       "  (361, 8.70),\n",
       "  (217, 8.60),\n",
       "  (219, 8.60),\n",
       "  (250, 8.60),\n",
       "  (704, 8.60),\n",
       "  (823, 8.60),\n",
       "  (182, 8.50),\n",
       "  (206, 8.50),\n",
       "  (86, 8.40),\n",
       "  (298, 8.40),\n",
       "  (466, 8.40),\n",
       "  (27, 8.30),\n",
       "  (148, 8.30),\n",
       "  (242, 8.20),\n",
       "  (537, 8.20),\n",
       "  (638, 8.20),\n",
       "  (141, 8.10),\n",
       "  (189, 8.10),\n",
       "  (531, 8.10),\n",
       "  (549, 8.10),\n",
       "  (42, 8.00),\n",
       "  (343, 8.00),\n",
       "  (786, 8.00),\n",
       "  (321, 7.90),\n",
       "  (584, 7.90),\n",
       "  (758, 7.90),\n",
       "  (963, 7.90),\n",
       "  (260, 7.70),\n",
       "  (397, 7.70),\n",
       "  (774, 7.70),\n",
       "  (811, 7.60),\n",
       "  (997, 7.60),\n",
       "  (104, 7.50),\n",
       "  (201, 7.50),\n",
       "  (775, 7.50),\n",
       "  (777, 7.50),\n",
       "  (840, 7.50),\n",
       "  (71, 7.40),\n",
       "  (481, 7.40),\n",
       "  (836, 7.40),\n",
       "  (986, 7.40),\n",
       "  (853, 7.30),\n",
       "  (28, 7.20),\n",
       "  (247, 7.20),\n",
       "  (264, 7.20),\n",
       "  (375, 7.20),\n",
       "  (483, 7.20),\n",
       "  (569, 7.20),\n",
       "  (605, 7.20),\n",
       "  (684, 7.20),\n",
       "  (732, 7.20),\n",
       "  (814, 7.20),\n",
       "  (865, 7.20),\n",
       "  (873, 7.20),\n",
       "  (164, 7.10),\n",
       "  (825, 7.10),\n",
       "  (875, 7.10),\n",
       "  (763, 7.00),\n",
       "  (127, 6.90),\n",
       "  (180, 6.90),\n",
       "  (355, 6.90),\n",
       "  (408, 6.90),\n",
       "  (659, 6.90),\n",
       "  (365, 6.80),\n",
       "  (462, 6.80),\n",
       "  (922, 6.80),\n",
       "  (15, 6.70),\n",
       "  (131, 6.70),\n",
       "  (205, 6.70),\n",
       "  (214, 6.70),\n",
       "  (235, 6.70),\n",
       "  (470, 6.60),\n",
       "  (529, 6.60),\n",
       "  (699, 6.60),\n",
       "  (198, 6.50),\n",
       "  (316, 6.50),\n",
       "  (382, 6.50),\n",
       "  (433, 6.50),\n",
       "  (454, 6.50),\n",
       "  (471, 6.50),\n",
       "  (784, 6.50),\n",
       "  (225, 6.40),\n",
       "  (307, 6.40),\n",
       "  (383, 6.40),\n",
       "  (764, 6.40),\n",
       "  (792, 6.40),\n",
       "  (890, 6.40),\n",
       "  (337, 6.30),\n",
       "  (759, 6.30),\n",
       "  (889, 6.30),\n",
       "  (11, 6.20),\n",
       "  (221, 6.20),\n",
       "  (733, 6.20),\n",
       "  (50, 6.10),\n",
       "  (186, 6.10),\n",
       "  (358, 6.10),\n",
       "  (477, 6.10),\n",
       "  (663, 6.10),\n",
       "  (932, 6.10),\n",
       "  (74, 6.00),\n",
       "  (310, 6.00),\n",
       "  (881, 6.00),\n",
       "  (70, 5.90),\n",
       "  (776, 5.90),\n",
       "  (900, 5.90),\n",
       "  (193, 5.80),\n",
       "  (248, 5.80),\n",
       "  (491, 5.80),\n",
       "  (508, 5.80),\n",
       "  (527, 5.80),\n",
       "  (557, 5.80),\n",
       "  (618, 5.80),\n",
       "  (939, 5.80),\n",
       "  (79, 5.70),\n",
       "  (115, 5.70),\n",
       "  (156, 5.70),\n",
       "  (449, 5.70),\n",
       "  (451, 5.70),\n",
       "  (924, 5.70),\n",
       "  (100, 5.60),\n",
       "  (172, 5.60),\n",
       "  (209, 5.60),\n",
       "  (281, 5.60),\n",
       "  (313, 5.60),\n",
       "  (756, 5.60),\n",
       "  (966, 5.60),\n",
       "  (218, 5.50),\n",
       "  (234, 5.40),\n",
       "  (544, 5.40),\n",
       "  (701, 5.40),\n",
       "  (236, 5.30),\n",
       "  (257, 5.30),\n",
       "  (259, 5.30),\n",
       "  (318, 5.30),\n",
       "  (919, 5.30),\n",
       "  (391, 5.20),\n",
       "  (402, 5.20),\n",
       "  (626, 5.20),\n",
       "  (882, 5.20),\n",
       "  (339, 5.10),\n",
       "  (418, 5.10),\n",
       "  (519, 5.10),\n",
       "  (622, 5.10),\n",
       "  (910, 5.10),\n",
       "  (921, 5.10),\n",
       "  (80, 5.00),\n",
       "  (92, 5.00),\n",
       "  (196, 5.00),\n",
       "  (280, 5.00),\n",
       "  (283, 5.00),\n",
       "  (317, 5.00),\n",
       "  (479, 5.00),\n",
       "  (653, 5.00),\n",
       "  (660, 5.00),\n",
       "  (920, 5.00),\n",
       "  (24, 4.90),\n",
       "  (142, 4.90),\n",
       "  (295, 4.90),\n",
       "  (359, 4.90),\n",
       "  (628, 4.90),\n",
       "  (902, 4.90),\n",
       "  (267, 4.80),\n",
       "  (276, 4.80),\n",
       "  (351, 4.80),\n",
       "  (650, 4.80),\n",
       "  (688, 4.80),\n",
       "  (816, 4.80),\n",
       "  (931, 4.80),\n",
       "  (957, 4.80),\n",
       "  (41, 4.70),\n",
       "  (325, 4.70),\n",
       "  (488, 4.70),\n",
       "  (606, 4.70),\n",
       "  (93, 4.60),\n",
       "  (348, 4.60),\n",
       "  (390, 4.60),\n",
       "  (592, 4.60),\n",
       "  (629, 4.60),\n",
       "  (630, 4.60),\n",
       "  (782, 4.60),\n",
       "  (951, 4.60),\n",
       "  (58, 4.50),\n",
       "  (113, 4.50),\n",
       "  (320, 4.50),\n",
       "  (456, 4.50),\n",
       "  (463, 4.50),\n",
       "  (546, 4.50),\n",
       "  (574, 4.50),\n",
       "  (991, 4.50),\n",
       "  (585, 4.40),\n",
       "  (707, 4.40),\n",
       "  (18, 4.30),\n",
       "  (72, 4.30),\n",
       "  (224, 4.30),\n",
       "  (347, 4.30),\n",
       "  (428, 4.30),\n",
       "  (543, 4.30),\n",
       "  (377, 4.20),\n",
       "  (502, 4.20),\n",
       "  (512, 4.20),\n",
       "  (760, 4.20),\n",
       "  (994, 4.20),\n",
       "  (14, 4.10),\n",
       "  (210, 4.10),\n",
       "  (255, 4.10),\n",
       "  (271, 4.10),\n",
       "  (360, 4.10),\n",
       "  (577, 4.10),\n",
       "  (934, 4.10),\n",
       "  (948, 4.10),\n",
       "  (314, 4.00),\n",
       "  (514, 4.00),\n",
       "  (802, 4.00),\n",
       "  (803, 4.00),\n",
       "  (899, 4.00),\n",
       "  (990, 4.00),\n",
       "  (1, 3.90),\n",
       "  (132, 3.90),\n",
       "  (153, 3.90),\n",
       "  (191, 3.90),\n",
       "  (223, 3.90),\n",
       "  (331, 3.90),\n",
       "  (336, 3.90),\n",
       "  (517, 3.90),\n",
       "  (664, 3.90),\n",
       "  (49, 3.80),\n",
       "  (165, 3.80),\n",
       "  (270, 3.80),\n",
       "  (385, 3.80),\n",
       "  (389, 3.80),\n",
       "  (507, 3.80),\n",
       "  (511, 3.80),\n",
       "  (593, 3.80),\n",
       "  (797, 3.80),\n",
       "  (983, 3.80),\n",
       "  (125, 3.70),\n",
       "  (170, 3.70),\n",
       "  (395, 3.70),\n",
       "  (528, 3.70),\n",
       "  (631, 3.70),\n",
       "  (710, 3.70),\n",
       "  (44, 3.60),\n",
       "  (376, 3.60),\n",
       "  (484, 3.60),\n",
       "  (597, 3.60),\n",
       "  (672, 3.60),\n",
       "  (675, 3.60),\n",
       "  (185, 3.50),\n",
       "  (269, 3.50),\n",
       "  (495, 3.50),\n",
       "  (607, 3.50),\n",
       "  (891, 3.50),\n",
       "  (302, 3.40),\n",
       "  (838, 3.40),\n",
       "  (36, 3.30),\n",
       "  (68, 3.30),\n",
       "  (117, 3.30),\n",
       "  (420, 3.30),\n",
       "  (513, 3.30),\n",
       "  (608, 3.30),\n",
       "  (272, 3.20),\n",
       "  (345, 3.20),\n",
       "  (691, 3.20),\n",
       "  (12, 3.10),\n",
       "  (265, 3.10),\n",
       "  (613, 3.10),\n",
       "  (835, 3.10),\n",
       "  (859, 3.10),\n",
       "  (894, 3.10),\n",
       "  (941, 3.10),\n",
       "  (952, 3.10),\n",
       "  (120, 3.00),\n",
       "  (158, 3.00),\n",
       "  (232, 3.00),\n",
       "  (284, 3.00),\n",
       "  (322, 3.00),\n",
       "  (446, 3.00),\n",
       "  (486, 3.00),\n",
       "  (510, 3.00),\n",
       "  (696, 3.00),\n",
       "  (761, 3.00),\n",
       "  (831, 3.00),\n",
       "  (884, 3.00),\n",
       "  (958, 3.00),\n",
       "  (988, 3.00),\n",
       "  (989, 3.00),\n",
       "  (203, 2.90),\n",
       "  (245, 2.90),\n",
       "  (258, 2.90),\n",
       "  (315, 2.90),\n",
       "  (678, 2.90),\n",
       "  (690, 2.90),\n",
       "  (697, 2.90),\n",
       "  (700, 2.90),\n",
       "  (812, 2.90),\n",
       "  (877, 2.90),\n",
       "  (163, 2.80),\n",
       "  (188, 2.80),\n",
       "  (344, 2.80),\n",
       "  (404, 2.80),\n",
       "  (542, 2.80),\n",
       "  (787, 2.80),\n",
       "  (75, 2.70),\n",
       "  (294, 2.70),\n",
       "  (297, 2.70),\n",
       "  (356, 2.70),\n",
       "  (368, 2.70),\n",
       "  (583, 2.70),\n",
       "  (587, 2.70),\n",
       "  (736, 2.70),\n",
       "  (906, 2.70),\n",
       "  (38, 2.60),\n",
       "  (207, 2.60),\n",
       "  (644, 2.60),\n",
       "  (767, 2.60),\n",
       "  (851, 2.60),\n",
       "  (35, 2.50),\n",
       "  (122, 2.50),\n",
       "  (154, 2.50),\n",
       "  (176, 2.50),\n",
       "  (177, 2.50),\n",
       "  (226, 2.50),\n",
       "  (309, 2.50),\n",
       "  (332, 2.50),\n",
       "  (342, 2.50),\n",
       "  (362, 2.50),\n",
       "  (421, 2.50),\n",
       "  (432, 2.50),\n",
       "  (558, 2.50),\n",
       "  (17, 2.40),\n",
       "  (52, 2.40),\n",
       "  (53, 2.40),\n",
       "  (101, 2.40),\n",
       "  (174, 2.40),\n",
       "  (220, 2.40),\n",
       "  (244, 2.40),\n",
       "  (399, 2.40),\n",
       "  (551, 2.40),\n",
       "  (666, 2.40),\n",
       "  (674, 2.40),\n",
       "  (860, 2.40),\n",
       "  (286, 2.30),\n",
       "  (291, 2.30),\n",
       "  (503, 2.30),\n",
       "  (535, 2.30),\n",
       "  (714, 2.30),\n",
       "  (753, 2.30),\n",
       "  (862, 2.30),\n",
       "  (871, 2.30),\n",
       "  (959, 2.30),\n",
       "  (43, 2.20),\n",
       "  (228, 2.20),\n",
       "  (256, 2.20),\n",
       "  (475, 2.20),\n",
       "  (749, 2.20),\n",
       "  (789, 2.20),\n",
       "  (833, 2.20),\n",
       "  (845, 2.20),\n",
       "  (848, 2.20),\n",
       "  (929, 2.20),\n",
       "  (194, 2.10),\n",
       "  (268, 2.10),\n",
       "  (371, 2.10),\n",
       "  (426, 2.10),\n",
       "  (452, 2.10),\n",
       "  (589, 2.10),\n",
       "  (680, 2.10),\n",
       "  (683, 2.10),\n",
       "  (807, 2.10),\n",
       "  (883, 2.10),\n",
       "  (979, 2.10),\n",
       "  (995, 2.10),\n",
       "  (10, 2.00),\n",
       "  (350, 2.00),\n",
       "  (747, 2.00),\n",
       "  (842, 2.00),\n",
       "  (998, 2.00),\n",
       "  (16, 1.90),\n",
       "  (89, 1.90),\n",
       "  (229, 1.90),\n",
       "  (369, 1.90),\n",
       "  (370, 1.90),\n",
       "  (374, 1.90),\n",
       "  (400, 1.90),\n",
       "  (743, 1.90),\n",
       "  (999, 1.90),\n",
       "  (114, 1.80),\n",
       "  (143, 1.80),\n",
       "  (183, 1.80),\n",
       "  (346, 1.80),\n",
       "  (720, 1.80),\n",
       "  (947, 1.80),\n",
       "  (137, 1.70),\n",
       "  (179, 1.70),\n",
       "  (222, 1.70),\n",
       "  (338, 1.70),\n",
       "  (364, 1.70),\n",
       "  (381, 1.70),\n",
       "  (505, 1.70),\n",
       "  (202, 1.60),\n",
       "  (213, 1.60),\n",
       "  (240, 1.60),\n",
       "  (341, 1.60),\n",
       "  (367, 1.60),\n",
       "  (438, 1.60),\n",
       "  (465, 1.60),\n",
       "  (521, 1.60),\n",
       "  (536, 1.60),\n",
       "  (728, 1.60),\n",
       "  (769, 1.60),\n",
       "  (876, 1.60),\n",
       "  (925, 1.60),\n",
       "  (66, 1.50),\n",
       "  (145, 1.50),\n",
       "  (146, 1.50),\n",
       "  (166, 1.50),\n",
       "  (405, 1.50),\n",
       "  (540, 1.50),\n",
       "  (567, 1.50),\n",
       "  (702, 1.50),\n",
       "  (962, 1.50),\n",
       "  (51, 1.40),\n",
       "  (54, 1.40),\n",
       "  (105, 1.40),\n",
       "  (204, 1.40),\n",
       "  (261, 1.40),\n",
       "  (312, 1.40),\n",
       "  (329, 1.40),\n",
       "  (380, 1.40),\n",
       "  (437, 1.40),\n",
       "  (795, 1.40),\n",
       "  (809, 1.40),\n",
       "  (846, 1.40),\n",
       "  (861, 1.40),\n",
       "  (32, 1.30),\n",
       "  (59, 1.30),\n",
       "  (227, 1.30),\n",
       "  (550, 1.30),\n",
       "  (615, 1.30),\n",
       "  (755, 1.30),\n",
       "  (175, 1.20),\n",
       "  (252, 1.20),\n",
       "  (378, 1.20),\n",
       "  (427, 1.20),\n",
       "  (647, 1.20),\n",
       "  (804, 1.20),\n",
       "  (813, 1.20),\n",
       "  (945, 1.20),\n",
       "  (5, 1.10),\n",
       "  (78, 1.10),\n",
       "  (81, 1.10),\n",
       "  (87, 1.10),\n",
       "  (126, 1.10),\n",
       "  (140, 1.10),\n",
       "  (434, 1.10),\n",
       "  (596, 1.10),\n",
       "  (718, 1.10),\n",
       "  (731, 1.10),\n",
       "  (972, 1.10),\n",
       "  (2, 1.00),\n",
       "  (64, 1.00),\n",
       "  (69, 1.00),\n",
       "  (95, 1.00),\n",
       "  (139, 1.00),\n",
       "  (169, 1.00),\n",
       "  (262, 1.00),\n",
       "  (372, 1.00),\n",
       "  (379, 1.00),\n",
       "  (633, 1.00),\n",
       "  (657, 1.00),\n",
       "  (712, 1.00),\n",
       "  (827, 1.00),\n",
       "  (841, 1.00),\n",
       "  (869, 1.00),\n",
       "  (19, 0.90),\n",
       "  (197, 0.90),\n",
       "  (394, 0.90),\n",
       "  (689, 0.90),\n",
       "  (933, 0.90),\n",
       "  (975, 0.90),\n",
       "  (152, 0.80),\n",
       "  (277, 0.80),\n",
       "  (386, 0.80),\n",
       "  (478, 0.80),\n",
       "  (548, 0.80),\n",
       "  (856, 0.80),\n",
       "  (943, 0.80),\n",
       "  (969, 0.80),\n",
       "  (22, 0.70),\n",
       "  (285, 0.70),\n",
       "  (534, 0.70),\n",
       "  (719, 0.70),\n",
       "  (73, 0.60),\n",
       "  (111, 0.60),\n",
       "  (147, 0.60),\n",
       "  (305, 0.60),\n",
       "  (333, 0.60),\n",
       "  (469, 0.60),\n",
       "  (493, 0.60),\n",
       "  (494, 0.60),\n",
       "  (590, 0.60),\n",
       "  (610, 0.60),\n",
       "  (708, 0.60),\n",
       "  (740, 0.60),\n",
       "  (930, 0.60),\n",
       "  (964, 0.60),\n",
       "  (21, 0.50),\n",
       "  (34, 0.50),\n",
       "  (200, 0.50),\n",
       "  (212, 0.50),\n",
       "  (289, 0.50),\n",
       "  (357, 0.50),\n",
       "  (435, 0.50),\n",
       "  (793, 0.50),\n",
       "  (909, 0.50),\n",
       "  (33, 0.40),\n",
       "  (160, 0.40),\n",
       "  (266, 0.40),\n",
       "  (279, 0.40),\n",
       "  (442, 0.40),\n",
       "  (500, 0.40),\n",
       "  (745, 0.40),\n",
       "  (799, 0.40),\n",
       "  (926, 0.40),\n",
       "  (13, 0.30),\n",
       "  (26, 0.30),\n",
       "  (246, 0.30),\n",
       "  (296, 0.30),\n",
       "  (326, 0.30),\n",
       "  (413, 0.30),\n",
       "  (416, 0.30),\n",
       "  (461, 0.30),\n",
       "  (798, 0.30),\n",
       "  (834, 0.30),\n",
       "  (896, 0.30),\n",
       "  (149, 0.20),\n",
       "  (287, 0.20),\n",
       "  (299, 0.20),\n",
       "  (373, 0.20),\n",
       "  (422, 0.20),\n",
       "  (480, 0.20),\n",
       "  (578, 0.20),\n",
       "  (623, 0.20),\n",
       "  (673, 0.20),\n",
       "  (681, 0.20),\n",
       "  (844, 0.20),\n",
       "  (903, 0.20),\n",
       "  (996, 0.20),\n",
       "  (20, 0.10),\n",
       "  (29, 0.10),\n",
       "  (167, 0.10),\n",
       "  (184, 0.10),\n",
       "  (187, 0.10),\n",
       "  (190, 0.10),\n",
       "  (384, 0.10),\n",
       "  (415, 0.10),\n",
       "  (686, 0.10),\n",
       "  (923, 0.10),\n",
       "  (942, 0.10),\n",
       "  (3, 0.00),\n",
       "  (4, 0.00),\n",
       "  (103, 0.00),\n",
       "  (106, 0.00),\n",
       "  (112, 0.00),\n",
       "  (150, 0.00),\n",
       "  (168, 0.00),\n",
       "  (233, 0.00),\n",
       "  (324, 0.00),\n",
       "  (335, 0.00),\n",
       "  (349, 0.00),\n",
       "  (366, 0.00),\n",
       "  (403, 0.00),\n",
       "  (450, 0.00),\n",
       "  (460, 0.00),\n",
       "  (473, 0.00),\n",
       "  (499, 0.00),\n",
       "  (501, 0.00),\n",
       "  (504, 0.00),\n",
       "  (524, 0.00),\n",
       "  (525, 0.00),\n",
       "  (553, 0.00),\n",
       "  (568, 0.00),\n",
       "  (598, 0.00),\n",
       "  (600, 0.00),\n",
       "  (617, 0.00),\n",
       "  (634, 0.00),\n",
       "  (648, 0.00),\n",
       "  (649, 0.00),\n",
       "  (662, 0.00),\n",
       "  (677, 0.00),\n",
       "  (682, 0.00),\n",
       "  (713, 0.00),\n",
       "  (715, 0.00),\n",
       "  (726, 0.00),\n",
       "  (729, 0.00),\n",
       "  (739, 0.00),\n",
       "  (742, 0.00),\n",
       "  (744, 0.00),\n",
       "  (771, 0.00),\n",
       "  (773, 0.00),\n",
       "  (780, 0.00),\n",
       "  (785, 0.00),\n",
       "  (810, 0.00),\n",
       "  (818, 0.00),\n",
       "  (895, 0.00),\n",
       "  (901, 0.00),\n",
       "  (908, 0.00),\n",
       "  (913, 0.00),\n",
       "  (914, 0.00),\n",
       "  (928, 0.00),\n",
       "  (935, 0.00),\n",
       "  (940, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (965, 0.00),\n",
       "  (967, 0.00),\n",
       "  (970, 0.00),\n",
       "  (974, 0.00),\n",
       "  (976, 0.00),\n",
       "  (977, 0.00),\n",
       "  (978, 0.00),\n",
       "  (980, 0.00),\n",
       "  (993, 0.00)])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 5\n",
      "at batch no 10\n",
      "at batch no 15\n",
      "at batch no 20\n",
      "at batch no 25\n",
      "at batch no 30\n",
      "at batch no 35\n",
      "at batch no 40\n",
      "at batch no 45\n",
      "at batch no 50\n",
      "at batch no 55\n",
      "at batch no 60\n",
      "at batch no 65\n",
      "at batch no 70\n",
      "at batch no 75\n",
      "at batch no 80\n",
      "at batch no 85\n",
      "at batch no 90\n",
      "at batch no 95\n",
      "at batch no 100\n",
      "at batch no 105\n",
      "at batch no 110\n",
      "at batch no 115\n",
      "at batch no 120\n",
      "at batch no 125\n",
      "at batch no 130\n",
      "at batch no 135\n",
      "at batch no 140\n",
      "at batch no 145\n",
      "at batch no 150\n",
      "at batch no 155\n",
      "at batch no 160\n",
      "at batch no 165\n",
      "at batch no 170\n",
      "at batch no 175\n",
      "at batch no 180\n",
      "at batch no 185\n",
      "at batch no 190\n",
      "at batch no 195\n",
      "at batch no 200\n",
      "at batch no 205\n",
      "at batch no 210\n",
      "at batch no 215\n",
      "at batch no 220\n",
      "at batch no 225\n",
      "at batch no 230\n",
      "at batch no 235\n",
      "at batch no 240\n",
      "at batch no 245\n",
      "at batch no 250\n",
      "at batch no 255\n",
      "at batch no 260\n",
      "at batch no 265\n",
      "at batch no 270\n",
      "at batch no 275\n",
      "at batch no 280\n",
      "at batch no 285\n",
      "at batch no 290\n",
      "at batch no 295\n",
      "at batch no 300\n",
      "at batch no 305\n",
      "at batch no 310\n",
      "at batch no 315\n",
      "at batch no 320\n",
      "at batch no 325\n",
      "at batch no 330\n",
      "at batch no 335\n",
      "at batch no 340\n",
      "at batch no 345\n",
      "at batch no 350\n",
      "at batch no 355\n",
      "at batch no 360\n",
      "at batch no 365\n",
      "at batch no 370\n",
      "at batch no 375\n",
      "at batch no 380\n",
      "at batch no 385\n",
      "at batch no 390\n",
      "at batch no 395\n",
      "at batch no 400\n",
      "at batch no 405\n",
      "at batch no 410\n",
      "at batch no 415\n",
      "at batch no 420\n",
      "at batch no 425\n",
      "at batch no 430\n",
      "at batch no 435\n",
      "at batch no 440\n",
      "at batch no 445\n",
      "at batch no 450\n",
      "at batch no 455\n",
      "at batch no 460\n",
      "at batch no 465\n",
      "at batch no 470\n",
      "at batch no 475\n",
      "at batch no 480\n",
      "at batch no 485\n",
      "at batch no 490\n",
      "at batch no 495\n",
      "at batch no 500\n",
      "at batch no 505\n",
      "at batch no 510\n",
      "at batch no 515\n",
      "at batch no 520\n",
      "at batch no 525\n",
      "at batch no 530\n",
      "at batch no 535\n",
      "at batch no 540\n",
      "at batch no 545\n",
      "at batch no 550\n",
      "at batch no 555\n",
      "at batch no 560\n",
      "at batch no 565\n",
      "at batch no 570\n",
      "at batch no 575\n",
      "at batch no 580\n",
      "at batch no 585\n",
      "at batch no 590\n",
      "at batch no 595\n",
      "at batch no 600\n",
      "at batch no 605\n",
      "at batch no 610\n",
      "at batch no 615\n",
      "at batch no 620\n",
      "at batch no 625\n",
      "at batch no 630\n",
      "at batch no 635\n",
      "at batch no 640\n",
      "at batch no 645\n",
      "at batch no 650\n",
      "at batch no 655\n",
      "at batch no 660\n",
      "at batch no 665\n",
      "at batch no 670\n",
      "at batch no 675\n",
      "at batch no 680\n",
      "at batch no 685\n",
      "at batch no 690\n",
      "at batch no 695\n",
      "at batch no 700\n",
      "at batch no 705\n",
      "at batch no 710\n",
      "at batch no 715\n",
      "at batch no 720\n",
      "at batch no 725\n",
      "at batch no 730\n",
      "at batch no 735\n",
      "at batch no 740\n",
      "at batch no 745\n",
      "at batch no 750\n",
      "at batch no 755\n",
      "at batch no 760\n",
      "at batch no 765\n",
      "at batch no 770\n",
      "at batch no 775\n",
      "at batch no 780\n",
      "at batch no 785\n",
      "at batch no 790\n",
      "at batch no 795\n",
      "at batch no 800\n",
      "at batch no 805\n",
      "at batch no 810\n",
      "at batch no 815\n",
      "at batch no 820\n",
      "at batch no 825\n",
      "at batch no 830\n",
      "at batch no 835\n",
      "at batch no 840\n",
      "at batch no 845\n",
      "at batch no 850\n",
      "at batch no 855\n",
      "at batch no 860\n",
      "at batch no 865\n",
      "at batch no 870\n",
      "at batch no 875\n",
      "at batch no 880\n",
      "at batch no 885\n",
      "at batch no 890\n",
      "at batch no 895\n",
      "at batch no 900\n",
      "at batch no 905\n",
      "at batch no 910\n",
      "at batch no 915\n",
      "at batch no 920\n",
      "at batch no 925\n",
      "at batch no 930\n",
      "at batch no 935\n",
      "at batch no 940\n",
      "at batch no 945\n",
      "at batch no 950\n",
      "at batch no 955\n",
      "at batch no 960\n",
      "at batch no 965\n",
      "at batch no 970\n",
      "at batch no 975\n",
      "at batch no 980\n",
      "at batch no 985\n",
      "at batch no 990\n",
      "at batch no 995\n",
      "at batch no 1000\n",
      "at batch no 1005\n",
      "at batch no 1010\n",
      "at batch no 1015\n",
      "at batch no 1020\n",
      "at batch no 1025\n",
      "at batch no 1030\n",
      "at batch no 1035\n",
      "at batch no 1040\n",
      "at batch no 1045\n",
      "at batch no 1050\n",
      "at batch no 1055\n",
      "at batch no 1060\n",
      "at batch no 1065\n",
      "at batch no 1070\n",
      "at batch no 1075\n",
      "at batch no 1080\n",
      "at batch no 1085\n",
      "at batch no 1090\n",
      "at batch no 1095\n",
      "at batch no 1100\n",
      "at batch no 1105\n",
      "at batch no 1110\n",
      "at batch no 1115\n",
      "at batch no 1120\n",
      "at batch no 1125\n",
      "at batch no 1130\n",
      "at batch no 1135\n",
      "at batch no 1140\n",
      "at batch no 1145\n",
      "at batch no 1150\n",
      "at batch no 1155\n",
      "at batch no 1160\n",
      "at batch no 1165\n",
      "at batch no 1170\n",
      "at batch no 1175\n",
      "at batch no 1180\n",
      "at batch no 1185\n",
      "at batch no 1190\n",
      "at batch no 1195\n",
      "at batch no 1200\n",
      "at batch no 1205\n",
      "at batch no 1210\n",
      "at batch no 1215\n",
      "at batch no 1220\n",
      "at batch no 1225\n",
      "at batch no 1230\n",
      "at batch no 1235\n",
      "at batch no 1240\n",
      "at batch no 1245\n",
      "at batch no 1250\n",
      "at batch no 1255\n",
      "at batch no 1260\n",
      "at batch no 1265\n",
      "at batch no 1270\n",
      "at batch no 1275\n",
      "at batch no 1280\n",
      "at batch no 1285\n",
      "at batch no 1290\n",
      "at batch no 1295\n",
      "at batch no 1300\n",
      "at batch no 1305\n",
      "at batch no 1310\n",
      "at batch no 1315\n",
      "at batch no 1320\n",
      "at batch no 1325\n",
      "at batch no 1330\n",
      "at batch no 1335\n",
      "at batch no 1340\n",
      "at batch no 1345\n",
      "at batch no 1350\n",
      "at batch no 1355\n",
      "at batch no 1360\n",
      "at batch no 1365\n",
      "at batch no 1370\n",
      "at batch no 1375\n",
      "at batch no 1380\n",
      "at batch no 1385\n",
      "at batch no 1390\n",
      "at batch no 1395\n",
      "at batch no 1400\n",
      "at batch no 1405\n",
      "at batch no 1410\n",
      "at batch no 1415\n",
      "at batch no 1420\n",
      "at batch no 1425\n",
      "at batch no 1430\n",
      "at batch no 1435\n",
      "at batch no 1440\n",
      "at batch no 1445\n",
      "at batch no 1450\n",
      "at batch no 1455\n",
      "at batch no 1460\n",
      "at batch no 1465\n",
      "at batch no 1470\n",
      "at batch no 1475\n",
      "at batch no 1480\n",
      "at batch no 1485\n",
      "at batch no 1490\n",
      "at batch no 1495\n",
      "at batch no 1500\n",
      "at batch no 1505\n",
      "at batch no 1510\n",
      "at batch no 1515\n",
      "at batch no 1520\n",
      "at batch no 1525\n",
      "at batch no 1530\n",
      "at batch no 1535\n",
      "at batch no 1540\n",
      "at batch no 1545\n",
      "at batch no 1550\n",
      "at batch no 1555\n",
      "at batch no 1560\n",
      "at batch no 1565\n",
      "at batch no 1570\n",
      "at batch no 1575\n",
      "at batch no 1580\n",
      "at batch no 1585\n",
      "at batch no 1590\n",
      "at batch no 1595\n",
      "at batch no 1600\n",
      "at batch no 1605\n",
      "at batch no 1610\n",
      "at batch no 1615\n",
      "at batch no 1620\n",
      "at batch no 1625\n",
      "at batch no 1630\n",
      "at batch no 1635\n",
      "at batch no 1640\n",
      "at batch no 1645\n",
      "at batch no 1650\n",
      "at batch no 1655\n",
      "at batch no 1660\n",
      "at batch no 1665\n",
      "at batch no 1670\n",
      "at batch no 1675\n",
      "at batch no 1680\n",
      "at batch no 1685\n",
      "at batch no 1690\n",
      "at batch no 1695\n",
      "at batch no 1700\n",
      "at batch no 1705\n",
      "at batch no 1710\n",
      "at batch no 1715\n",
      "at batch no 1720\n",
      "at batch no 1725\n",
      "at batch no 1730\n",
      "at batch no 1735\n",
      "at batch no 1740\n",
      "at batch no 1745\n",
      "at batch no 1750\n",
      "at batch no 1755\n",
      "at batch no 1760\n",
      "at batch no 1765\n",
      "at batch no 1770\n",
      "at batch no 1775\n",
      "at batch no 1780\n",
      "at batch no 1785\n",
      "at batch no 1790\n",
      "at batch no 1795\n",
      "at batch no 1800\n",
      "at batch no 1805\n",
      "at batch no 1810\n",
      "at batch no 1815\n",
      "at batch no 1820\n",
      "at batch no 1825\n",
      "at batch no 1830\n",
      "at batch no 1835\n",
      "at batch no 1840\n",
      "at batch no 1845\n",
      "at batch no 1850\n",
      "at batch no 1855\n",
      "at batch no 1860\n",
      "at batch no 1865\n",
      "at batch no 1870\n",
      "at batch no 1875\n",
      "at batch no 1880\n",
      "at batch no 1885\n",
      "at batch no 1890\n",
      "at batch no 1895\n",
      "at batch no 1900\n",
      "at batch no 1905\n",
      "at batch no 1910\n",
      "at batch no 1915\n",
      "at batch no 1920\n",
      "at batch no 1925\n",
      "at batch no 1930\n",
      "at batch no 1935\n",
      "at batch no 1940\n",
      "at batch no 1945\n",
      "at batch no 1950\n",
      "at batch no 1955\n",
      "at batch no 1960\n",
      "at batch no 1965\n",
      "at batch no 1970\n",
      "at batch no 1975\n",
      "at batch no 1980\n",
      "at batch no 1985\n",
      "at batch no 1990\n",
      "at batch no 1995\n",
      "at batch no 2000\n",
      "at batch no 2005\n",
      "at batch no 2010\n",
      "at batch no 2015\n",
      "at batch no 2020\n",
      "at batch no 2025\n",
      "at batch no 2030\n",
      "at batch no 2035\n",
      "at batch no 2040\n",
      "at batch no 2045\n",
      "at batch no 2050\n",
      "at batch no 2055\n",
      "at batch no 2060\n",
      "at batch no 2065\n",
      "at batch no 2070\n",
      "at batch no 2075\n",
      "at batch no 2080\n",
      "at batch no 2085\n",
      "at batch no 2090\n",
      "at batch no 2095\n",
      "at batch no 2100\n",
      "at batch no 2105\n",
      "at batch no 2110\n",
      "at batch no 2115\n",
      "at batch no 2120\n",
      "at batch no 2125\n",
      "at batch no 2130\n",
      "at batch no 2135\n",
      "at batch no 2140\n",
      "at batch no 2145\n",
      "at batch no 2150\n",
      "at batch no 2155\n",
      "at batch no 2160\n",
      "at batch no 2165\n",
      "at batch no 2170\n",
      "at batch no 2175\n",
      "at batch no 2180\n",
      "at batch no 2185\n",
      "at batch no 2190\n",
      "at batch no 2195\n",
      "at batch no 2200\n",
      "at batch no 2205\n",
      "at batch no 2210\n",
      "at batch no 2215\n",
      "at batch no 2220\n",
      "at batch no 2225\n",
      "at batch no 2230\n",
      "at batch no 2235\n",
      "at batch no 2240\n",
      "at batch no 2245\n",
      "at batch no 2250\n",
      "at batch no 2255\n",
      "at batch no 2260\n",
      "at batch no 2265\n",
      "at batch no 2270\n",
      "at batch no 2275\n",
      "at batch no 2280\n",
      "at batch no 2285\n",
      "at batch no 2290\n",
      "at batch no 2295\n",
      "at batch no 2300\n",
      "at batch no 2305\n",
      "at batch no 2310\n",
      "at batch no 2315\n",
      "at batch no 2320\n",
      "at batch no 2325\n",
      "at batch no 2330\n",
      "at batch no 2335\n",
      "at batch no 2340\n",
      "at batch no 2345\n",
      "at batch no 2350\n",
      "at batch no 2355\n",
      "at batch no 2360\n",
      "at batch no 2365\n",
      "at batch no 2370\n",
      "at batch no 2375\n",
      "at batch no 2380\n",
      "at batch no 2385\n",
      "at batch no 2390\n",
      "at batch no 2395\n",
      "at batch no 2400\n",
      "at batch no 2405\n",
      "at batch no 2410\n",
      "at batch no 2415\n",
      "at batch no 2420\n",
      "at batch no 2425\n",
      "at batch no 2430\n",
      "at batch no 2435\n",
      "at batch no 2440\n",
      "at batch no 2445\n",
      "at batch no 2450\n",
      "at batch no 2455\n",
      "at batch no 2460\n",
      "at batch no 2465\n",
      "at batch no 2470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 2475\n",
      "at batch no 2480\n",
      "at batch no 2485\n",
      "at batch no 2490\n",
      "at batch no 2495\n",
      "at batch no 2500\n",
      "at batch no 2505\n",
      "at batch no 2510\n",
      "at batch no 2515\n",
      "at batch no 2520\n",
      "at batch no 2525\n",
      "at batch no 2530\n",
      "at batch no 2535\n",
      "at batch no 2540\n",
      "at batch no 2545\n",
      "at batch no 2550\n",
      "at batch no 2555\n",
      "at batch no 2560\n",
      "at batch no 2565\n",
      "at batch no 2570\n",
      "at batch no 2575\n",
      "at batch no 2580\n",
      "at batch no 2585\n",
      "at batch no 2590\n",
      "at batch no 2595\n",
      "at batch no 2600\n",
      "at batch no 2605\n",
      "at batch no 2610\n",
      "at batch no 2615\n",
      "at batch no 2620\n",
      "at batch no 2625\n",
      "at batch no 2630\n",
      "at batch no 2635\n",
      "at batch no 2640\n",
      "at batch no 2645\n",
      "at batch no 2650\n",
      "at batch no 2655\n",
      "at batch no 2660\n",
      "at batch no 2665\n",
      "at batch no 2670\n",
      "at batch no 2675\n",
      "at batch no 2680\n",
      "at batch no 2685\n",
      "at batch no 2690\n",
      "at batch no 2695\n",
      "at batch no 2700\n",
      "at batch no 2705\n",
      "at batch no 2710\n",
      "at batch no 2715\n",
      "at batch no 2720\n",
      "at batch no 2725\n",
      "at batch no 2730\n",
      "at batch no 2735\n",
      "at batch no 2740\n",
      "at batch no 2745\n",
      "at batch no 2750\n",
      "at batch no 2755\n",
      "at batch no 2760\n",
      "at batch no 2765\n",
      "at batch no 2770\n",
      "at batch no 2775\n",
      "at batch no 2780\n",
      "at batch no 2785\n",
      "at batch no 2790\n",
      "at batch no 2795\n",
      "at batch no 2800\n",
      "at batch no 2805\n",
      "at batch no 2810\n",
      "at batch no 2815\n",
      "at batch no 2820\n",
      "at batch no 2825\n",
      "at batch no 2830\n",
      "at batch no 2835\n",
      "at batch no 2840\n",
      "at batch no 2845\n",
      "at batch no 2850\n",
      "at batch no 2855\n",
      "at batch no 2860\n",
      "at batch no 2865\n",
      "at batch no 2870\n",
      "at batch no 2875\n",
      "at batch no 2880\n",
      "at batch no 2885\n",
      "at batch no 2890\n",
      "at batch no 2895\n",
      "at batch no 2900\n",
      "at batch no 2905\n",
      "at batch no 2910\n",
      "at batch no 2915\n",
      "at batch no 2920\n",
      "at batch no 2925\n",
      "at batch no 2930\n",
      "at batch no 2935\n",
      "at batch no 2940\n",
      "at batch no 2945\n",
      "at batch no 2950\n",
      "at batch no 2955\n",
      "at batch no 2960\n",
      "at batch no 2965\n",
      "at batch no 2970\n",
      "at batch no 2975\n",
      "at batch no 2980\n",
      "at batch no 2985\n",
      "at batch no 2990\n",
      "at batch no 2995\n",
      "at batch no 3000\n",
      "at batch no 3005\n",
      "at batch no 3010\n",
      "at batch no 3015\n",
      "at batch no 3020\n",
      "at batch no 3025\n",
      "at batch no 3030\n",
      "at batch no 3035\n",
      "at batch no 3040\n",
      "at batch no 3045\n",
      "at batch no 3050\n",
      "at batch no 3055\n",
      "at batch no 3060\n",
      "at batch no 3065\n",
      "at batch no 3070\n",
      "at batch no 3075\n",
      "at batch no 3080\n",
      "at batch no 3085\n",
      "at batch no 3090\n",
      "at batch no 3095\n",
      "at batch no 3100\n",
      "at batch no 3105\n",
      "at batch no 3110\n",
      "at batch no 3115\n",
      "at batch no 3120\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(357,\n",
       " [(721, 10334.30),\n",
       "  (669, 3492.10),\n",
       "  (971, 3086.20),\n",
       "  (556, 2534.20),\n",
       "  (414, 2156.00),\n",
       "  (520, 2136.60),\n",
       "  (588, 2111.90),\n",
       "  (750, 2070.40),\n",
       "  (431, 1500.50),\n",
       "  (828, 1262.30),\n",
       "  (61, 1207.60),\n",
       "  (411, 994.40),\n",
       "  (904, 655.90),\n",
       "  (39, 618.20),\n",
       "  (794, 582.00),\n",
       "  (709, 373.80),\n",
       "  (489, 363.20),\n",
       "  (614, 348.80),\n",
       "  (599, 327.60),\n",
       "  (490, 258.50),\n",
       "  (955, 252.50),\n",
       "  (790, 240.20),\n",
       "  (581, 238.20),\n",
       "  (84, 218.80),\n",
       "  (893, 218.60),\n",
       "  (770, 209.30),\n",
       "  (48, 143.20),\n",
       "  (509, 141.90),\n",
       "  (604, 139.40),\n",
       "  (401, 122.10),\n",
       "  (907, 113.60),\n",
       "  (837, 111.10),\n",
       "  (419, 110.80),\n",
       "  (824, 109.50),\n",
       "  (46, 95.70),\n",
       "  (67, 95.00),\n",
       "  (748, 94.90),\n",
       "  (60, 93.80),\n",
       "  (741, 93.10),\n",
       "  (56, 86.30),\n",
       "  (572, 83.30),\n",
       "  (412, 82.00),\n",
       "  (973, 79.90),\n",
       "  (580, 78.20),\n",
       "  (762, 78.10),\n",
       "  (805, 78.00),\n",
       "  (591, 76.30),\n",
       "  (651, 75.80),\n",
       "  (319, 72.00),\n",
       "  (646, 71.90),\n",
       "  (151, 70.90),\n",
       "  (621, 69.30),\n",
       "  (516, 69.00),\n",
       "  (582, 68.40),\n",
       "  (55, 67.70),\n",
       "  (737, 66.90),\n",
       "  (703, 65.80),\n",
       "  (464, 63.20),\n",
       "  (406, 62.40),\n",
       "  (879, 62.20),\n",
       "  (443, 60.60),\n",
       "  (570, 59.20),\n",
       "  (987, 58.80),\n",
       "  (791, 58.50),\n",
       "  (455, 58.40),\n",
       "  (800, 56.30),\n",
       "  (815, 55.40),\n",
       "  (468, 55.20),\n",
       "  (754, 53.40),\n",
       "  (706, 53.10),\n",
       "  (485, 52.10),\n",
       "  (788, 51.20),\n",
       "  (711, 51.10),\n",
       "  (575, 50.60),\n",
       "  (97, 47.80),\n",
       "  (340, 47.40),\n",
       "  (40, 46.30),\n",
       "  (555, 45.60),\n",
       "  (801, 44.80),\n",
       "  (94, 44.70),\n",
       "  (992, 44.70),\n",
       "  (138, 44.10),\n",
       "  (636, 43.60),\n",
       "  (533, 42.70),\n",
       "  (410, 41.80),\n",
       "  (116, 41.70),\n",
       "  (698, 41.50),\n",
       "  (407, 40.80),\n",
       "  (171, 40.40),\n",
       "  (806, 40.10),\n",
       "  (300, 39.80),\n",
       "  (293, 39.60),\n",
       "  (476, 39.60),\n",
       "  (796, 39.30),\n",
       "  (441, 38.60),\n",
       "  (734, 38.50),\n",
       "  (645, 38.40),\n",
       "  (692, 38.10),\n",
       "  (304, 38.00),\n",
       "  (155, 37.50),\n",
       "  (953, 37.50),\n",
       "  (0, 37.10),\n",
       "  (562, 36.60),\n",
       "  (746, 36.60),\n",
       "  (878, 36.60),\n",
       "  (661, 35.50),\n",
       "  (497, 35.40),\n",
       "  (779, 35.40),\n",
       "  (323, 34.10),\n",
       "  (981, 33.60),\n",
       "  (868, 33.40),\n",
       "  (643, 32.80),\n",
       "  (518, 32.50),\n",
       "  (436, 32.40),\n",
       "  (768, 32.40),\n",
       "  (611, 32.30),\n",
       "  (62, 32.10),\n",
       "  (654, 31.90),\n",
       "  (109, 31.70),\n",
       "  (916, 31.60),\n",
       "  (858, 30.90),\n",
       "  (676, 30.30),\n",
       "  (619, 29.60),\n",
       "  (819, 29.40),\n",
       "  (829, 29.40),\n",
       "  (195, 29.20),\n",
       "  (547, 28.90),\n",
       "  (496, 28.70),\n",
       "  (515, 28.40),\n",
       "  (752, 28.40),\n",
       "  (162, 28.20),\n",
       "  (254, 27.90),\n",
       "  (612, 27.90),\n",
       "  (781, 27.80),\n",
       "  (363, 27.70),\n",
       "  (850, 27.70),\n",
       "  (635, 27.60),\n",
       "  (655, 27.60),\n",
       "  (96, 27.30),\n",
       "  (118, 27.20),\n",
       "  (738, 26.60),\n",
       "  (917, 26.60),\n",
       "  (870, 26.30),\n",
       "  (396, 26.10),\n",
       "  (440, 25.90),\n",
       "  (353, 25.80),\n",
       "  (705, 25.80),\n",
       "  (886, 25.80),\n",
       "  (506, 25.70),\n",
       "  (854, 25.20),\n",
       "  (90, 25.10),\n",
       "  (308, 24.80),\n",
       "  (237, 24.60),\n",
       "  (641, 24.50),\n",
       "  (946, 24.30),\n",
       "  (238, 23.60),\n",
       "  (482, 23.60),\n",
       "  (915, 23.60),\n",
       "  (108, 23.50),\n",
       "  (722, 23.50),\n",
       "  (576, 23.40),\n",
       "  (110, 23.30),\n",
       "  (290, 23.30),\n",
       "  (128, 23.20),\n",
       "  (47, 23.10),\n",
       "  (565, 23.10),\n",
       "  (808, 23.10),\n",
       "  (522, 23.00),\n",
       "  (25, 22.90),\n",
       "  (863, 22.60),\n",
       "  (63, 22.50),\n",
       "  (843, 22.50),\n",
       "  (944, 22.50),\n",
       "  (898, 22.40),\n",
       "  (545, 22.10),\n",
       "  (253, 21.80),\n",
       "  (135, 21.60),\n",
       "  (444, 21.60),\n",
       "  (288, 21.50),\n",
       "  (457, 21.10),\n",
       "  (627, 20.80),\n",
       "  (866, 20.80),\n",
       "  (526, 20.70),\n",
       "  (640, 20.70),\n",
       "  (560, 20.60),\n",
       "  (564, 20.60),\n",
       "  (82, 20.50),\n",
       "  (897, 20.20),\n",
       "  (867, 20.00),\n",
       "  (31, 19.90),\n",
       "  (459, 19.90),\n",
       "  (826, 19.90),\n",
       "  (892, 19.80),\n",
       "  (679, 19.70),\n",
       "  (880, 19.70),\n",
       "  (523, 19.60),\n",
       "  (65, 19.50),\n",
       "  (327, 19.40),\n",
       "  (474, 19.40),\n",
       "  (765, 19.40),\n",
       "  (7, 19.20),\n",
       "  (251, 19.20),\n",
       "  (772, 19.00),\n",
       "  (691, 18.90),\n",
       "  (725, 18.90),\n",
       "  (124, 18.70),\n",
       "  (735, 18.70),\n",
       "  (398, 18.40),\n",
       "  (538, 18.40),\n",
       "  (603, 18.30),\n",
       "  (423, 18.20),\n",
       "  (687, 18.20),\n",
       "  (241, 18.10),\n",
       "  (8, 18.00),\n",
       "  (178, 18.00),\n",
       "  (671, 18.00),\n",
       "  (852, 17.90),\n",
       "  (918, 17.90),\n",
       "  (430, 17.80),\n",
       "  (656, 17.80),\n",
       "  (817, 17.70),\n",
       "  (159, 17.60),\n",
       "  (716, 17.60),\n",
       "  (88, 17.50),\n",
       "  (532, 17.50),\n",
       "  (292, 17.30),\n",
       "  (173, 17.20),\n",
       "  (282, 17.00),\n",
       "  (566, 17.00),\n",
       "  (857, 17.00),\n",
       "  (76, 16.90),\n",
       "  (549, 16.90),\n",
       "  (620, 16.90),\n",
       "  (670, 16.90),\n",
       "  (874, 16.90),\n",
       "  (453, 16.80),\n",
       "  (136, 16.70),\n",
       "  (563, 16.70),\n",
       "  (275, 16.60),\n",
       "  (561, 16.50),\n",
       "  (937, 16.40),\n",
       "  (352, 16.30),\n",
       "  (595, 16.30),\n",
       "  (757, 16.30),\n",
       "  (783, 16.30),\n",
       "  (822, 16.30),\n",
       "  (133, 16.00),\n",
       "  (530, 16.00),\n",
       "  (579, 16.00),\n",
       "  (632, 16.00),\n",
       "  (849, 16.00),\n",
       "  (129, 15.90),\n",
       "  (872, 15.90),\n",
       "  (231, 15.80),\n",
       "  (625, 15.80),\n",
       "  (730, 15.50),\n",
       "  (417, 15.30),\n",
       "  (723, 15.30),\n",
       "  (123, 15.20),\n",
       "  (847, 15.20),\n",
       "  (609, 15.10),\n",
       "  (832, 15.10),\n",
       "  (968, 15.10),\n",
       "  (102, 15.00),\n",
       "  (274, 15.00),\n",
       "  (472, 15.00),\n",
       "  (586, 15.00),\n",
       "  (949, 14.80),\n",
       "  (425, 14.70),\n",
       "  (354, 14.60),\n",
       "  (668, 14.60),\n",
       "  (938, 14.60),\n",
       "  (984, 14.60),\n",
       "  (601, 14.50),\n",
       "  (950, 14.50),\n",
       "  (539, 14.40),\n",
       "  (985, 14.40),\n",
       "  (447, 14.30),\n",
       "  (541, 14.30),\n",
       "  (199, 14.20),\n",
       "  (685, 14.20),\n",
       "  (409, 14.10),\n",
       "  (594, 14.00),\n",
       "  (642, 14.00),\n",
       "  (751, 14.00),\n",
       "  (130, 13.90),\n",
       "  (552, 13.90),\n",
       "  (85, 13.80),\n",
       "  (208, 13.80),\n",
       "  (330, 13.80),\n",
       "  (982, 13.80),\n",
       "  (328, 13.70),\n",
       "  (717, 13.70),\n",
       "  (665, 13.50),\n",
       "  (864, 13.40),\n",
       "  (263, 13.30),\n",
       "  (936, 13.20),\n",
       "  (724, 13.10),\n",
       "  (727, 13.10),\n",
       "  (956, 13.10),\n",
       "  (37, 13.00),\n",
       "  (424, 13.00),\n",
       "  (45, 12.90),\n",
       "  (77, 12.80),\n",
       "  (98, 12.80),\n",
       "  (465, 12.80),\n",
       "  (912, 12.80),\n",
       "  (9, 12.50),\n",
       "  (192, 12.50),\n",
       "  (855, 12.50),\n",
       "  (559, 12.40),\n",
       "  (885, 12.30),\n",
       "  (387, 12.20),\n",
       "  (778, 12.20),\n",
       "  (144, 12.10),\n",
       "  (311, 12.10),\n",
       "  (445, 12.10),\n",
       "  (181, 12.00),\n",
       "  (616, 12.00),\n",
       "  (887, 12.00),\n",
       "  (211, 11.90),\n",
       "  (694, 11.90),\n",
       "  (602, 11.80),\n",
       "  (693, 11.80),\n",
       "  (811, 11.80),\n",
       "  (927, 11.80),\n",
       "  (91, 11.60),\n",
       "  (273, 11.60),\n",
       "  (301, 11.60),\n",
       "  (732, 11.60),\n",
       "  (821, 11.60),\n",
       "  (119, 11.50),\n",
       "  (242, 11.50),\n",
       "  (439, 11.50),\n",
       "  (911, 11.50),\n",
       "  (954, 11.50),\n",
       "  (492, 11.40),\n",
       "  (393, 11.30),\n",
       "  (571, 11.30),\n",
       "  (23, 11.20),\n",
       "  (57, 11.20),\n",
       "  (392, 11.10),\n",
       "  (215, 10.90),\n",
       "  (637, 10.90),\n",
       "  (820, 10.90),\n",
       "  (905, 10.90),\n",
       "  (527, 10.80),\n",
       "  (639, 10.80),\n",
       "  (888, 10.80),\n",
       "  (667, 10.70),\n",
       "  (189, 10.60),\n",
       "  (448, 10.60),\n",
       "  (554, 10.60),\n",
       "  (986, 10.60),\n",
       "  (107, 10.50),\n",
       "  (388, 10.50),\n",
       "  (30, 10.40),\n",
       "  (624, 10.40),\n",
       "  (836, 10.40),\n",
       "  (487, 10.30),\n",
       "  (774, 10.30),\n",
       "  (99, 10.20),\n",
       "  (303, 10.20),\n",
       "  (216, 10.10),\n",
       "  (230, 10.10),\n",
       "  (250, 10.10),\n",
       "  (658, 10.00),\n",
       "  (6, 9.90),\n",
       "  (157, 9.90),\n",
       "  (249, 9.90),\n",
       "  (839, 9.90),\n",
       "  (217, 9.80),\n",
       "  (573, 9.80),\n",
       "  (278, 9.70),\n",
       "  (306, 9.70),\n",
       "  (316, 9.70),\n",
       "  (429, 9.70),\n",
       "  (865, 9.70),\n",
       "  (201, 9.60),\n",
       "  (695, 9.60),\n",
       "  (86, 9.40),\n",
       "  (134, 9.40),\n",
       "  (161, 9.40),\n",
       "  (182, 9.40),\n",
       "  (243, 9.40),\n",
       "  (786, 9.40),\n",
       "  (498, 9.30),\n",
       "  (766, 9.30),\n",
       "  (121, 9.20),\n",
       "  (260, 9.20),\n",
       "  (334, 9.20),\n",
       "  (343, 9.20),\n",
       "  (361, 9.20),\n",
       "  (458, 9.20),\n",
       "  (531, 9.20),\n",
       "  (605, 9.20),\n",
       "  (141, 9.10),\n",
       "  (466, 9.10),\n",
       "  (618, 9.10),\n",
       "  (638, 9.10),\n",
       "  (823, 9.10),\n",
       "  (219, 9.00),\n",
       "  (321, 9.00),\n",
       "  (825, 9.00),\n",
       "  (966, 9.00),\n",
       "  (28, 8.90),\n",
       "  (239, 8.90),\n",
       "  (899, 8.90),\n",
       "  (27, 8.80),\n",
       "  (782, 8.80),\n",
       "  (814, 8.80),\n",
       "  (830, 8.80),\n",
       "  (42, 8.70),\n",
       "  (704, 8.70),\n",
       "  (375, 8.60),\n",
       "  (875, 8.60),\n",
       "  (50, 8.50),\n",
       "  (83, 8.50),\n",
       "  (104, 8.50),\n",
       "  (365, 8.50),\n",
       "  (680, 8.50),\n",
       "  (206, 8.40),\n",
       "  (470, 8.40),\n",
       "  (652, 8.40),\n",
       "  (467, 8.30),\n",
       "  (759, 8.30),\n",
       "  (127, 8.20),\n",
       "  (214, 8.20),\n",
       "  (763, 8.20),\n",
       "  (11, 8.10),\n",
       "  (71, 8.10),\n",
       "  (225, 8.10),\n",
       "  (481, 8.10),\n",
       "  (537, 8.10),\n",
       "  (584, 8.10),\n",
       "  (963, 8.10),\n",
       "  (148, 7.90),\n",
       "  (397, 7.90),\n",
       "  (758, 7.90),\n",
       "  (920, 7.90),\n",
       "  (355, 7.80),\n",
       "  (74, 7.70),\n",
       "  (24, 7.60),\n",
       "  (180, 7.60),\n",
       "  (298, 7.60),\n",
       "  (383, 7.60),\n",
       "  (408, 7.60),\n",
       "  (853, 7.60),\n",
       "  (873, 7.60),\n",
       "  (882, 7.60),\n",
       "  (922, 7.60),\n",
       "  (15, 7.40),\n",
       "  (186, 7.40),\n",
       "  (193, 7.40),\n",
       "  (247, 7.40),\n",
       "  (264, 7.40),\n",
       "  (310, 7.40),\n",
       "  (483, 7.40),\n",
       "  (659, 7.40),\n",
       "  (775, 7.40),\n",
       "  (784, 7.40),\n",
       "  (840, 7.40),\n",
       "  (198, 7.30),\n",
       "  (438, 7.30),\n",
       "  (684, 7.30),\n",
       "  (701, 7.30),\n",
       "  (760, 7.30),\n",
       "  (777, 7.30),\n",
       "  (997, 7.30),\n",
       "  (164, 7.20),\n",
       "  (235, 7.20),\n",
       "  (248, 7.20),\n",
       "  (382, 7.20),\n",
       "  (606, 7.20),\n",
       "  (626, 7.20),\n",
       "  (707, 7.20),\n",
       "  (234, 7.10),\n",
       "  (281, 7.10),\n",
       "  (359, 7.10),\n",
       "  (1, 6.90),\n",
       "  (41, 6.90),\n",
       "  (100, 6.90),\n",
       "  (710, 6.90),\n",
       "  (881, 6.90),\n",
       "  (890, 6.90),\n",
       "  (92, 6.80),\n",
       "  (337, 6.80),\n",
       "  (851, 6.80),\n",
       "  (115, 6.70),\n",
       "  (221, 6.70),\n",
       "  (313, 6.70),\n",
       "  (477, 6.70),\n",
       "  (776, 6.70),\n",
       "  (70, 6.60),\n",
       "  (205, 6.60),\n",
       "  (660, 6.60),\n",
       "  (156, 6.50),\n",
       "  (508, 6.50),\n",
       "  (664, 6.50),\n",
       "  (951, 6.50),\n",
       "  (131, 6.40),\n",
       "  (307, 6.40),\n",
       "  (433, 6.40),\n",
       "  (733, 6.40),\n",
       "  (764, 6.40),\n",
       "  (889, 6.40),\n",
       "  (902, 6.40),\n",
       "  (932, 6.40),\n",
       "  (218, 6.30),\n",
       "  (236, 6.30),\n",
       "  (259, 6.30),\n",
       "  (462, 6.30),\n",
       "  (699, 6.30),\n",
       "  (339, 6.20),\n",
       "  (358, 6.20),\n",
       "  (557, 6.20),\n",
       "  (650, 6.20),\n",
       "  (792, 6.20),\n",
       "  (900, 6.20),\n",
       "  (939, 6.20),\n",
       "  (72, 6.10),\n",
       "  (257, 6.10),\n",
       "  (454, 6.10),\n",
       "  (471, 6.10),\n",
       "  (712, 6.10),\n",
       "  (79, 6.00),\n",
       "  (196, 6.00),\n",
       "  (348, 6.00),\n",
       "  (449, 6.00),\n",
       "  (479, 6.00),\n",
       "  (491, 6.00),\n",
       "  (688, 6.00),\n",
       "  (700, 6.00),\n",
       "  (957, 6.00),\n",
       "  (172, 5.90),\n",
       "  (391, 5.90),\n",
       "  (283, 5.80),\n",
       "  (451, 5.80),\n",
       "  (663, 5.80),\n",
       "  (948, 5.80),\n",
       "  (80, 5.70),\n",
       "  (142, 5.70),\n",
       "  (317, 5.70),\n",
       "  (318, 5.70),\n",
       "  (418, 5.70),\n",
       "  (495, 5.70),\n",
       "  (544, 5.70),\n",
       "  (271, 5.60),\n",
       "  (390, 5.60),\n",
       "  (512, 5.60),\n",
       "  (592, 5.60),\n",
       "  (756, 5.60),\n",
       "  (255, 5.50),\n",
       "  (280, 5.50),\n",
       "  (529, 5.50),\n",
       "  (569, 5.50),\n",
       "  (93, 5.40),\n",
       "  (267, 5.40),\n",
       "  (360, 5.40),\n",
       "  (224, 5.30),\n",
       "  (320, 5.30),\n",
       "  (574, 5.30),\n",
       "  (633, 5.30),\n",
       "  (921, 5.30),\n",
       "  (210, 5.20),\n",
       "  (276, 5.20),\n",
       "  (488, 5.20),\n",
       "  (514, 5.20),\n",
       "  (622, 5.20),\n",
       "  (910, 5.20),\n",
       "  (14, 5.10),\n",
       "  (113, 5.10),\n",
       "  (270, 5.10),\n",
       "  (302, 5.10),\n",
       "  (402, 5.10),\n",
       "  (653, 5.10),\n",
       "  (18, 5.00),\n",
       "  (314, 5.00),\n",
       "  (519, 5.00),\n",
       "  (816, 5.00),\n",
       "  (153, 4.90),\n",
       "  (209, 4.90),\n",
       "  (325, 4.90),\n",
       "  (117, 4.80),\n",
       "  (295, 4.80),\n",
       "  (528, 4.80),\n",
       "  (585, 4.80),\n",
       "  (628, 4.80),\n",
       "  (38, 4.70),\n",
       "  (158, 4.70),\n",
       "  (507, 4.70),\n",
       "  (894, 4.70),\n",
       "  (919, 4.70),\n",
       "  (990, 4.70),\n",
       "  (58, 4.60),\n",
       "  (132, 4.60),\n",
       "  (351, 4.60),\n",
       "  (463, 4.60),\n",
       "  (629, 4.60),\n",
       "  (906, 4.60),\n",
       "  (998, 4.60),\n",
       "  (125, 4.50),\n",
       "  (517, 4.50),\n",
       "  (630, 4.50),\n",
       "  (696, 4.50),\n",
       "  (802, 4.50),\n",
       "  (991, 4.50),\n",
       "  (203, 4.40),\n",
       "  (269, 4.40),\n",
       "  (309, 4.40),\n",
       "  (44, 4.30),\n",
       "  (331, 4.30),\n",
       "  (428, 4.30),\n",
       "  (577, 4.30),\n",
       "  (931, 4.30),\n",
       "  (983, 4.30),\n",
       "  (284, 4.20),\n",
       "  (389, 4.20),\n",
       "  (597, 4.20),\n",
       "  (631, 4.20),\n",
       "  (75, 4.10),\n",
       "  (170, 4.10),\n",
       "  (376, 4.10),\n",
       "  (543, 4.10),\n",
       "  (607, 4.10),\n",
       "  (924, 4.10),\n",
       "  (185, 4.00),\n",
       "  (245, 4.00),\n",
       "  (347, 4.00),\n",
       "  (456, 4.00),\n",
       "  (511, 4.00),\n",
       "  (513, 4.00),\n",
       "  (608, 4.00),\n",
       "  (761, 4.00),\n",
       "  (884, 4.00),\n",
       "  (934, 4.00),\n",
       "  (272, 3.90),\n",
       "  (286, 3.90),\n",
       "  (315, 3.90),\n",
       "  (502, 3.90),\n",
       "  (672, 3.90),\n",
       "  (678, 3.90),\n",
       "  (803, 3.90),\n",
       "  (859, 3.90),\n",
       "  (891, 3.90),\n",
       "  (165, 3.80),\n",
       "  (223, 3.80),\n",
       "  (395, 3.80),\n",
       "  (958, 3.80),\n",
       "  (994, 3.80),\n",
       "  (36, 3.70),\n",
       "  (176, 3.70),\n",
       "  (232, 3.70),\n",
       "  (404, 3.70),\n",
       "  (613, 3.70),\n",
       "  (675, 3.70),\n",
       "  (747, 3.70),\n",
       "  (838, 3.70),\n",
       "  (12, 3.60),\n",
       "  (265, 3.60),\n",
       "  (377, 3.60),\n",
       "  (420, 3.60),\n",
       "  (446, 3.60),\n",
       "  (486, 3.60),\n",
       "  (542, 3.60),\n",
       "  (546, 3.60),\n",
       "  (736, 3.60),\n",
       "  (871, 3.60),\n",
       "  (191, 3.50),\n",
       "  (207, 3.50),\n",
       "  (294, 3.50),\n",
       "  (332, 3.50),\n",
       "  (336, 3.50),\n",
       "  (399, 3.50),\n",
       "  (535, 3.50),\n",
       "  (952, 3.50),\n",
       "  (49, 3.40),\n",
       "  (120, 3.40),\n",
       "  (258, 3.40),\n",
       "  (322, 3.40),\n",
       "  (344, 3.40),\n",
       "  (385, 3.40),\n",
       "  (714, 3.40),\n",
       "  (797, 3.40),\n",
       "  (877, 3.40),\n",
       "  (988, 3.40),\n",
       "  (995, 3.40),\n",
       "  (154, 3.30),\n",
       "  (342, 3.30),\n",
       "  (510, 3.30),\n",
       "  (587, 3.30),\n",
       "  (941, 3.30),\n",
       "  (17, 3.20),\n",
       "  (68, 3.20),\n",
       "  (226, 3.20),\n",
       "  (356, 3.20),\n",
       "  (593, 3.20),\n",
       "  (989, 3.20),\n",
       "  (163, 3.10),\n",
       "  (256, 3.10),\n",
       "  (437, 3.10),\n",
       "  (674, 3.10),\n",
       "  (697, 3.10),\n",
       "  (767, 3.10),\n",
       "  (787, 3.10),\n",
       "  (812, 3.10),\n",
       "  (831, 3.10),\n",
       "  (833, 3.10),\n",
       "  (835, 3.10),\n",
       "  (860, 3.10),\n",
       "  (177, 3.00),\n",
       "  (188, 3.00),\n",
       "  (345, 3.00),\n",
       "  (484, 3.00),\n",
       "  (848, 3.00),\n",
       "  (52, 2.90),\n",
       "  (101, 2.80),\n",
       "  (166, 2.80),\n",
       "  (291, 2.80),\n",
       "  (297, 2.80),\n",
       "  (749, 2.80),\n",
       "  (845, 2.80),\n",
       "  (105, 2.70),\n",
       "  (174, 2.70),\n",
       "  (194, 2.70),\n",
       "  (220, 2.70),\n",
       "  (362, 2.70),\n",
       "  (368, 2.70),\n",
       "  (551, 2.70),\n",
       "  (959, 2.70),\n",
       "  (89, 2.60),\n",
       "  (503, 2.60),\n",
       "  (558, 2.60),\n",
       "  (647, 2.60),\n",
       "  (666, 2.60),\n",
       "  (683, 2.60),\n",
       "  (947, 2.60),\n",
       "  (16, 2.50),\n",
       "  (35, 2.50),\n",
       "  (43, 2.50),\n",
       "  (51, 2.50),\n",
       "  (53, 2.50),\n",
       "  (183, 2.50),\n",
       "  (202, 2.50),\n",
       "  (807, 2.50),\n",
       "  (861, 2.50),\n",
       "  (862, 2.50),\n",
       "  (122, 2.40),\n",
       "  (228, 2.40),\n",
       "  (364, 2.40),\n",
       "  (370, 2.40),\n",
       "  (400, 2.40),\n",
       "  (426, 2.40),\n",
       "  (475, 2.40),\n",
       "  (644, 2.40),\n",
       "  (728, 2.40),\n",
       "  (753, 2.40),\n",
       "  (213, 2.30),\n",
       "  (367, 2.30),\n",
       "  (452, 2.30),\n",
       "  (583, 2.30),\n",
       "  (720, 2.30),\n",
       "  (962, 2.30),\n",
       "  (227, 2.20),\n",
       "  (244, 2.20),\n",
       "  (329, 2.20),\n",
       "  (371, 2.20),\n",
       "  (842, 2.20),\n",
       "  (999, 2.20),\n",
       "  (22, 2.10),\n",
       "  (145, 2.10),\n",
       "  (346, 2.10),\n",
       "  (350, 2.10),\n",
       "  (432, 2.10),\n",
       "  (589, 2.10),\n",
       "  (769, 2.10),\n",
       "  (795, 2.10),\n",
       "  (10, 2.00),\n",
       "  (222, 2.00),\n",
       "  (369, 2.00),\n",
       "  (381, 2.00),\n",
       "  (505, 2.00),\n",
       "  (534, 2.00),\n",
       "  (690, 2.00),\n",
       "  (846, 2.00),\n",
       "  (197, 1.90),\n",
       "  (261, 1.90),\n",
       "  (268, 1.90),\n",
       "  (386, 1.90),\n",
       "  (421, 1.90),\n",
       "  (540, 1.90),\n",
       "  (615, 1.90),\n",
       "  (925, 1.90),\n",
       "  (54, 1.80),\n",
       "  (59, 1.80),\n",
       "  (146, 1.80),\n",
       "  (179, 1.80),\n",
       "  (312, 1.80),\n",
       "  (341, 1.80),\n",
       "  (536, 1.80),\n",
       "  (743, 1.80),\n",
       "  (876, 1.80),\n",
       "  (883, 1.80),\n",
       "  (929, 1.80),\n",
       "  (979, 1.80),\n",
       "  (32, 1.70),\n",
       "  (114, 1.70),\n",
       "  (240, 1.70),\n",
       "  (374, 1.70),\n",
       "  (380, 1.70),\n",
       "  (478, 1.70),\n",
       "  (550, 1.70),\n",
       "  (673, 1.70),\n",
       "  (789, 1.70),\n",
       "  (809, 1.70),\n",
       "  (81, 1.60),\n",
       "  (87, 1.60),\n",
       "  (305, 1.60),\n",
       "  (338, 1.60),\n",
       "  (521, 1.60),\n",
       "  (702, 1.60),\n",
       "  (804, 1.60),\n",
       "  (972, 1.60),\n",
       "  (143, 1.50),\n",
       "  (175, 1.50),\n",
       "  (657, 1.50),\n",
       "  (718, 1.50),\n",
       "  (64, 1.40),\n",
       "  (66, 1.40),\n",
       "  (126, 1.40),\n",
       "  (140, 1.40),\n",
       "  (160, 1.40),\n",
       "  (229, 1.40),\n",
       "  (262, 1.40),\n",
       "  (405, 1.40),\n",
       "  (434, 1.40),\n",
       "  (596, 1.40),\n",
       "  (813, 1.40),\n",
       "  (5, 1.30),\n",
       "  (78, 1.30),\n",
       "  (139, 1.30),\n",
       "  (212, 1.30),\n",
       "  (333, 1.30),\n",
       "  (427, 1.30),\n",
       "  (719, 1.30),\n",
       "  (827, 1.30),\n",
       "  (252, 1.20),\n",
       "  (493, 1.20),\n",
       "  (548, 1.20),\n",
       "  (567, 1.20),\n",
       "  (731, 1.20),\n",
       "  (740, 1.20),\n",
       "  (755, 1.20),\n",
       "  (841, 1.20),\n",
       "  (896, 1.20),\n",
       "  (69, 1.10),\n",
       "  (152, 1.10),\n",
       "  (204, 1.10),\n",
       "  (299, 1.10),\n",
       "  (378, 1.10),\n",
       "  (708, 1.10),\n",
       "  (742, 1.10),\n",
       "  (856, 1.10),\n",
       "  (923, 1.10),\n",
       "  (945, 1.10),\n",
       "  (95, 1.00),\n",
       "  (169, 1.00),\n",
       "  (279, 1.00),\n",
       "  (289, 1.00),\n",
       "  (413, 1.00),\n",
       "  (745, 1.00),\n",
       "  (869, 1.00),\n",
       "  (969, 1.00),\n",
       "  (2, 0.90),\n",
       "  (19, 0.90),\n",
       "  (33, 0.90),\n",
       "  (137, 0.90),\n",
       "  (147, 0.90),\n",
       "  (372, 0.90),\n",
       "  (394, 0.90),\n",
       "  (909, 0.90),\n",
       "  (933, 0.90),\n",
       "  (21, 0.80),\n",
       "  (73, 0.80),\n",
       "  (689, 0.80),\n",
       "  (20, 0.70),\n",
       "  (187, 0.70),\n",
       "  (379, 0.70),\n",
       "  (480, 0.70),\n",
       "  (975, 0.70),\n",
       "  (26, 0.60),\n",
       "  (34, 0.60),\n",
       "  (200, 0.60),\n",
       "  (435, 0.60),\n",
       "  (442, 0.60),\n",
       "  (494, 0.60),\n",
       "  (500, 0.60),\n",
       "  (590, 0.60),\n",
       "  (610, 0.60),\n",
       "  (681, 0.60),\n",
       "  (926, 0.60),\n",
       "  (930, 0.60),\n",
       "  (13, 0.50),\n",
       "  (246, 0.50),\n",
       "  (277, 0.50),\n",
       "  (285, 0.50),\n",
       "  (416, 0.50),\n",
       "  (461, 0.50),\n",
       "  (834, 0.50),\n",
       "  (943, 0.50),\n",
       "  (964, 0.50),\n",
       "  (168, 0.40),\n",
       "  (357, 0.40),\n",
       "  (422, 0.40),\n",
       "  (469, 0.40),\n",
       "  (578, 0.40),\n",
       "  (623, 0.40),\n",
       "  (677, 0.40),\n",
       "  (773, 0.40),\n",
       "  (785, 0.40),\n",
       "  (799, 0.40),\n",
       "  (818, 0.40),\n",
       "  (844, 0.40),\n",
       "  (111, 0.30),\n",
       "  (167, 0.30),\n",
       "  (287, 0.30),\n",
       "  (553, 0.30),\n",
       "  (686, 0.30),\n",
       "  (793, 0.30),\n",
       "  (996, 0.30),\n",
       "  (149, 0.20),\n",
       "  (184, 0.20),\n",
       "  (190, 0.20),\n",
       "  (266, 0.20),\n",
       "  (324, 0.20),\n",
       "  (373, 0.20),\n",
       "  (473, 0.20),\n",
       "  (600, 0.20),\n",
       "  (682, 0.20),\n",
       "  (744, 0.20),\n",
       "  (798, 0.20),\n",
       "  (903, 0.20),\n",
       "  (942, 0.20),\n",
       "  (976, 0.20),\n",
       "  (977, 0.20),\n",
       "  (106, 0.10),\n",
       "  (326, 0.10),\n",
       "  (335, 0.10),\n",
       "  (366, 0.10),\n",
       "  (384, 0.10),\n",
       "  (415, 0.10),\n",
       "  (450, 0.10),\n",
       "  (568, 0.10),\n",
       "  (617, 0.10),\n",
       "  (662, 0.10),\n",
       "  (715, 0.10),\n",
       "  (729, 0.10),\n",
       "  (739, 0.10),\n",
       "  (913, 0.10),\n",
       "  (914, 0.10),\n",
       "  (928, 0.10),\n",
       "  (978, 0.10),\n",
       "  (993, 0.10),\n",
       "  (3, 0.00),\n",
       "  (4, 0.00),\n",
       "  (29, 0.00),\n",
       "  (103, 0.00),\n",
       "  (112, 0.00),\n",
       "  (150, 0.00),\n",
       "  (233, 0.00),\n",
       "  (296, 0.00),\n",
       "  (349, 0.00),\n",
       "  (403, 0.00),\n",
       "  (460, 0.00),\n",
       "  (499, 0.00),\n",
       "  (501, 0.00),\n",
       "  (504, 0.00),\n",
       "  (524, 0.00),\n",
       "  (525, 0.00),\n",
       "  (598, 0.00),\n",
       "  (634, 0.00),\n",
       "  (648, 0.00),\n",
       "  (649, 0.00),\n",
       "  (713, 0.00),\n",
       "  (726, 0.00),\n",
       "  (771, 0.00),\n",
       "  (780, 0.00),\n",
       "  (810, 0.00),\n",
       "  (895, 0.00),\n",
       "  (901, 0.00),\n",
       "  (908, 0.00),\n",
       "  (935, 0.00),\n",
       "  (940, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (965, 0.00),\n",
       "  (967, 0.00),\n",
       "  (970, 0.00),\n",
       "  (974, 0.00),\n",
       "  (980, 0.00)])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 5\n",
      "at batch no 10\n",
      "at batch no 15\n",
      "at batch no 20\n",
      "at batch no 25\n",
      "at batch no 30\n",
      "at batch no 35\n",
      "at batch no 40\n",
      "at batch no 45\n",
      "at batch no 50\n",
      "at batch no 55\n",
      "at batch no 60\n",
      "at batch no 65\n",
      "at batch no 70\n",
      "at batch no 75\n",
      "at batch no 80\n",
      "at batch no 85\n",
      "at batch no 90\n",
      "at batch no 95\n",
      "at batch no 100\n",
      "at batch no 105\n",
      "at batch no 110\n",
      "at batch no 115\n",
      "at batch no 120\n",
      "at batch no 125\n",
      "at batch no 130\n",
      "at batch no 135\n",
      "at batch no 140\n",
      "at batch no 145\n",
      "at batch no 150\n",
      "at batch no 155\n",
      "at batch no 160\n",
      "at batch no 165\n",
      "at batch no 170\n",
      "at batch no 175\n",
      "at batch no 180\n",
      "at batch no 185\n",
      "at batch no 190\n",
      "at batch no 195\n",
      "at batch no 200\n",
      "at batch no 205\n",
      "at batch no 210\n",
      "at batch no 215\n",
      "at batch no 220\n",
      "at batch no 225\n",
      "at batch no 230\n",
      "at batch no 235\n",
      "at batch no 240\n",
      "at batch no 245\n",
      "at batch no 250\n",
      "at batch no 255\n",
      "at batch no 260\n",
      "at batch no 265\n",
      "at batch no 270\n",
      "at batch no 275\n",
      "at batch no 280\n",
      "at batch no 285\n",
      "at batch no 290\n",
      "at batch no 295\n",
      "at batch no 300\n",
      "at batch no 305\n",
      "at batch no 310\n",
      "at batch no 315\n",
      "at batch no 320\n",
      "at batch no 325\n",
      "at batch no 330\n",
      "at batch no 335\n",
      "at batch no 340\n",
      "at batch no 345\n",
      "at batch no 350\n",
      "at batch no 355\n",
      "at batch no 360\n",
      "at batch no 365\n",
      "at batch no 370\n",
      "at batch no 375\n",
      "at batch no 380\n",
      "at batch no 385\n",
      "at batch no 390\n",
      "at batch no 395\n",
      "at batch no 400\n",
      "at batch no 405\n",
      "at batch no 410\n",
      "at batch no 415\n",
      "at batch no 420\n",
      "at batch no 425\n",
      "at batch no 430\n",
      "at batch no 435\n",
      "at batch no 440\n",
      "at batch no 445\n",
      "at batch no 450\n",
      "at batch no 455\n",
      "at batch no 460\n",
      "at batch no 465\n",
      "at batch no 470\n",
      "at batch no 475\n",
      "at batch no 480\n",
      "at batch no 485\n",
      "at batch no 490\n",
      "at batch no 495\n",
      "at batch no 500\n",
      "at batch no 505\n",
      "at batch no 510\n",
      "at batch no 515\n",
      "at batch no 520\n",
      "at batch no 525\n",
      "at batch no 530\n",
      "at batch no 535\n",
      "at batch no 540\n",
      "at batch no 545\n",
      "at batch no 550\n",
      "at batch no 555\n",
      "at batch no 560\n",
      "at batch no 565\n",
      "at batch no 570\n",
      "at batch no 575\n",
      "at batch no 580\n",
      "at batch no 585\n",
      "at batch no 590\n",
      "at batch no 595\n",
      "at batch no 600\n",
      "at batch no 605\n",
      "at batch no 610\n",
      "at batch no 615\n",
      "at batch no 620\n",
      "at batch no 625\n",
      "at batch no 630\n",
      "at batch no 635\n",
      "at batch no 640\n",
      "at batch no 645\n",
      "at batch no 650\n",
      "at batch no 655\n",
      "at batch no 660\n",
      "at batch no 665\n",
      "at batch no 670\n",
      "at batch no 675\n",
      "at batch no 680\n",
      "at batch no 685\n",
      "at batch no 690\n",
      "at batch no 695\n",
      "at batch no 700\n",
      "at batch no 705\n",
      "at batch no 710\n",
      "at batch no 715\n",
      "at batch no 720\n",
      "at batch no 725\n",
      "at batch no 730\n",
      "at batch no 735\n",
      "at batch no 740\n",
      "at batch no 745\n",
      "at batch no 750\n",
      "at batch no 755\n",
      "at batch no 760\n",
      "at batch no 765\n",
      "at batch no 770\n",
      "at batch no 775\n",
      "at batch no 780\n",
      "at batch no 785\n",
      "at batch no 790\n",
      "at batch no 795\n",
      "at batch no 800\n",
      "at batch no 805\n",
      "at batch no 810\n",
      "at batch no 815\n",
      "at batch no 820\n",
      "at batch no 825\n",
      "at batch no 830\n",
      "at batch no 835\n",
      "at batch no 840\n",
      "at batch no 845\n",
      "at batch no 850\n",
      "at batch no 855\n",
      "at batch no 860\n",
      "at batch no 865\n",
      "at batch no 870\n",
      "at batch no 875\n",
      "at batch no 880\n",
      "at batch no 885\n",
      "at batch no 890\n",
      "at batch no 895\n",
      "at batch no 900\n",
      "at batch no 905\n",
      "at batch no 910\n",
      "at batch no 915\n",
      "at batch no 920\n",
      "at batch no 925\n",
      "at batch no 930\n",
      "at batch no 935\n",
      "at batch no 940\n",
      "at batch no 945\n",
      "at batch no 950\n",
      "at batch no 955\n",
      "at batch no 960\n",
      "at batch no 965\n",
      "at batch no 970\n",
      "at batch no 975\n",
      "at batch no 980\n",
      "at batch no 985\n",
      "at batch no 990\n",
      "at batch no 995\n",
      "at batch no 1000\n",
      "at batch no 1005\n",
      "at batch no 1010\n",
      "at batch no 1015\n",
      "at batch no 1020\n",
      "at batch no 1025\n",
      "at batch no 1030\n",
      "at batch no 1035\n",
      "at batch no 1040\n",
      "at batch no 1045\n",
      "at batch no 1050\n",
      "at batch no 1055\n",
      "at batch no 1060\n",
      "at batch no 1065\n",
      "at batch no 1070\n",
      "at batch no 1075\n",
      "at batch no 1080\n",
      "at batch no 1085\n",
      "at batch no 1090\n",
      "at batch no 1095\n",
      "at batch no 1100\n",
      "at batch no 1105\n",
      "at batch no 1110\n",
      "at batch no 1115\n",
      "at batch no 1120\n",
      "at batch no 1125\n",
      "at batch no 1130\n",
      "at batch no 1135\n",
      "at batch no 1140\n",
      "at batch no 1145\n",
      "at batch no 1150\n",
      "at batch no 1155\n",
      "at batch no 1160\n",
      "at batch no 1165\n",
      "at batch no 1170\n",
      "at batch no 1175\n",
      "at batch no 1180\n",
      "at batch no 1185\n",
      "at batch no 1190\n",
      "at batch no 1195\n",
      "at batch no 1200\n",
      "at batch no 1205\n",
      "at batch no 1210\n",
      "at batch no 1215\n",
      "at batch no 1220\n",
      "at batch no 1225\n",
      "at batch no 1230\n",
      "at batch no 1235\n",
      "at batch no 1240\n",
      "at batch no 1245\n",
      "at batch no 1250\n",
      "at batch no 1255\n",
      "at batch no 1260\n",
      "at batch no 1265\n",
      "at batch no 1270\n",
      "at batch no 1275\n",
      "at batch no 1280\n",
      "at batch no 1285\n",
      "at batch no 1290\n",
      "at batch no 1295\n",
      "at batch no 1300\n",
      "at batch no 1305\n",
      "at batch no 1310\n",
      "at batch no 1315\n",
      "at batch no 1320\n",
      "at batch no 1325\n",
      "at batch no 1330\n",
      "at batch no 1335\n",
      "at batch no 1340\n",
      "at batch no 1345\n",
      "at batch no 1350\n",
      "at batch no 1355\n",
      "at batch no 1360\n",
      "at batch no 1365\n",
      "at batch no 1370\n",
      "at batch no 1375\n",
      "at batch no 1380\n",
      "at batch no 1385\n",
      "at batch no 1390\n",
      "at batch no 1395\n",
      "at batch no 1400\n",
      "at batch no 1405\n",
      "at batch no 1410\n",
      "at batch no 1415\n",
      "at batch no 1420\n",
      "at batch no 1425\n",
      "at batch no 1430\n",
      "at batch no 1435\n",
      "at batch no 1440\n",
      "at batch no 1445\n",
      "at batch no 1450\n",
      "at batch no 1455\n",
      "at batch no 1460\n",
      "at batch no 1465\n",
      "at batch no 1470\n",
      "at batch no 1475\n",
      "at batch no 1480\n",
      "at batch no 1485\n",
      "at batch no 1490\n",
      "at batch no 1495\n",
      "at batch no 1500\n",
      "at batch no 1505\n",
      "at batch no 1510\n",
      "at batch no 1515\n",
      "at batch no 1520\n",
      "at batch no 1525\n",
      "at batch no 1530\n",
      "at batch no 1535\n",
      "at batch no 1540\n",
      "at batch no 1545\n",
      "at batch no 1550\n",
      "at batch no 1555\n",
      "at batch no 1560\n",
      "at batch no 1565\n",
      "at batch no 1570\n",
      "at batch no 1575\n",
      "at batch no 1580\n",
      "at batch no 1585\n",
      "at batch no 1590\n",
      "at batch no 1595\n",
      "at batch no 1600\n",
      "at batch no 1605\n",
      "at batch no 1610\n",
      "at batch no 1615\n",
      "at batch no 1620\n",
      "at batch no 1625\n",
      "at batch no 1630\n",
      "at batch no 1635\n",
      "at batch no 1640\n",
      "at batch no 1645\n",
      "at batch no 1650\n",
      "at batch no 1655\n",
      "at batch no 1660\n",
      "at batch no 1665\n",
      "at batch no 1670\n",
      "at batch no 1675\n",
      "at batch no 1680\n",
      "at batch no 1685\n",
      "at batch no 1690\n",
      "at batch no 1695\n",
      "at batch no 1700\n",
      "at batch no 1705\n",
      "at batch no 1710\n",
      "at batch no 1715\n",
      "at batch no 1720\n",
      "at batch no 1725\n",
      "at batch no 1730\n",
      "at batch no 1735\n",
      "at batch no 1740\n",
      "at batch no 1745\n",
      "at batch no 1750\n",
      "at batch no 1755\n",
      "at batch no 1760\n",
      "at batch no 1765\n",
      "at batch no 1770\n",
      "at batch no 1775\n",
      "at batch no 1780\n",
      "at batch no 1785\n",
      "at batch no 1790\n",
      "at batch no 1795\n",
      "at batch no 1800\n",
      "at batch no 1805\n",
      "at batch no 1810\n",
      "at batch no 1815\n",
      "at batch no 1820\n",
      "at batch no 1825\n",
      "at batch no 1830\n",
      "at batch no 1835\n",
      "at batch no 1840\n",
      "at batch no 1845\n",
      "at batch no 1850\n",
      "at batch no 1855\n",
      "at batch no 1860\n",
      "at batch no 1865\n",
      "at batch no 1870\n",
      "at batch no 1875\n",
      "at batch no 1880\n",
      "at batch no 1885\n",
      "at batch no 1890\n",
      "at batch no 1895\n",
      "at batch no 1900\n",
      "at batch no 1905\n",
      "at batch no 1910\n",
      "at batch no 1915\n",
      "at batch no 1920\n",
      "at batch no 1925\n",
      "at batch no 1930\n",
      "at batch no 1935\n",
      "at batch no 1940\n",
      "at batch no 1945\n",
      "at batch no 1950\n",
      "at batch no 1955\n",
      "at batch no 1960\n",
      "at batch no 1965\n",
      "at batch no 1970\n",
      "at batch no 1975\n",
      "at batch no 1980\n",
      "at batch no 1985\n",
      "at batch no 1990\n",
      "at batch no 1995\n",
      "at batch no 2000\n",
      "at batch no 2005\n",
      "at batch no 2010\n",
      "at batch no 2015\n",
      "at batch no 2020\n",
      "at batch no 2025\n",
      "at batch no 2030\n",
      "at batch no 2035\n",
      "at batch no 2040\n",
      "at batch no 2045\n",
      "at batch no 2050\n",
      "at batch no 2055\n",
      "at batch no 2060\n",
      "at batch no 2065\n",
      "at batch no 2070\n",
      "at batch no 2075\n",
      "at batch no 2080\n",
      "at batch no 2085\n",
      "at batch no 2090\n",
      "at batch no 2095\n",
      "at batch no 2100\n",
      "at batch no 2105\n",
      "at batch no 2110\n",
      "at batch no 2115\n",
      "at batch no 2120\n",
      "at batch no 2125\n",
      "at batch no 2130\n",
      "at batch no 2135\n",
      "at batch no 2140\n",
      "at batch no 2145\n",
      "at batch no 2150\n",
      "at batch no 2155\n",
      "at batch no 2160\n",
      "at batch no 2165\n",
      "at batch no 2170\n",
      "at batch no 2175\n",
      "at batch no 2180\n",
      "at batch no 2185\n",
      "at batch no 2190\n",
      "at batch no 2195\n",
      "at batch no 2200\n",
      "at batch no 2205\n",
      "at batch no 2210\n",
      "at batch no 2215\n",
      "at batch no 2220\n",
      "at batch no 2225\n",
      "at batch no 2230\n",
      "at batch no 2235\n",
      "at batch no 2240\n",
      "at batch no 2245\n",
      "at batch no 2250\n",
      "at batch no 2255\n",
      "at batch no 2260\n",
      "at batch no 2265\n",
      "at batch no 2270\n",
      "at batch no 2275\n",
      "at batch no 2280\n",
      "at batch no 2285\n",
      "at batch no 2290\n",
      "at batch no 2295\n",
      "at batch no 2300\n",
      "at batch no 2305\n",
      "at batch no 2310\n",
      "at batch no 2315\n",
      "at batch no 2320\n",
      "at batch no 2325\n",
      "at batch no 2330\n",
      "at batch no 2335\n",
      "at batch no 2340\n",
      "at batch no 2345\n",
      "at batch no 2350\n",
      "at batch no 2355\n",
      "at batch no 2360\n",
      "at batch no 2365\n",
      "at batch no 2370\n",
      "at batch no 2375\n",
      "at batch no 2380\n",
      "at batch no 2385\n",
      "at batch no 2390\n",
      "at batch no 2395\n",
      "at batch no 2400\n",
      "at batch no 2405\n",
      "at batch no 2410\n",
      "at batch no 2415\n",
      "at batch no 2420\n",
      "at batch no 2425\n",
      "at batch no 2430\n",
      "at batch no 2435\n",
      "at batch no 2440\n",
      "at batch no 2445\n",
      "at batch no 2450\n",
      "at batch no 2455\n",
      "at batch no 2460\n",
      "at batch no 2465\n",
      "at batch no 2470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 2475\n",
      "at batch no 2480\n",
      "at batch no 2485\n",
      "at batch no 2490\n",
      "at batch no 2495\n",
      "at batch no 2500\n",
      "at batch no 2505\n",
      "at batch no 2510\n",
      "at batch no 2515\n",
      "at batch no 2520\n",
      "at batch no 2525\n",
      "at batch no 2530\n",
      "at batch no 2535\n",
      "at batch no 2540\n",
      "at batch no 2545\n",
      "at batch no 2550\n",
      "at batch no 2555\n",
      "at batch no 2560\n",
      "at batch no 2565\n",
      "at batch no 2570\n",
      "at batch no 2575\n",
      "at batch no 2580\n",
      "at batch no 2585\n",
      "at batch no 2590\n",
      "at batch no 2595\n",
      "at batch no 2600\n",
      "at batch no 2605\n",
      "at batch no 2610\n",
      "at batch no 2615\n",
      "at batch no 2620\n",
      "at batch no 2625\n",
      "at batch no 2630\n",
      "at batch no 2635\n",
      "at batch no 2640\n",
      "at batch no 2645\n",
      "at batch no 2650\n",
      "at batch no 2655\n",
      "at batch no 2660\n",
      "at batch no 2665\n",
      "at batch no 2670\n",
      "at batch no 2675\n",
      "at batch no 2680\n",
      "at batch no 2685\n",
      "at batch no 2690\n",
      "at batch no 2695\n",
      "at batch no 2700\n",
      "at batch no 2705\n",
      "at batch no 2710\n",
      "at batch no 2715\n",
      "at batch no 2720\n",
      "at batch no 2725\n",
      "at batch no 2730\n",
      "at batch no 2735\n",
      "at batch no 2740\n",
      "at batch no 2745\n",
      "at batch no 2750\n",
      "at batch no 2755\n",
      "at batch no 2760\n",
      "at batch no 2765\n",
      "at batch no 2770\n",
      "at batch no 2775\n",
      "at batch no 2780\n",
      "at batch no 2785\n",
      "at batch no 2790\n",
      "at batch no 2795\n",
      "at batch no 2800\n",
      "at batch no 2805\n",
      "at batch no 2810\n",
      "at batch no 2815\n",
      "at batch no 2820\n",
      "at batch no 2825\n",
      "at batch no 2830\n",
      "at batch no 2835\n",
      "at batch no 2840\n",
      "at batch no 2845\n",
      "at batch no 2850\n",
      "at batch no 2855\n",
      "at batch no 2860\n",
      "at batch no 2865\n",
      "at batch no 2870\n",
      "at batch no 2875\n",
      "at batch no 2880\n",
      "at batch no 2885\n",
      "at batch no 2890\n",
      "at batch no 2895\n",
      "at batch no 2900\n",
      "at batch no 2905\n",
      "at batch no 2910\n",
      "at batch no 2915\n",
      "at batch no 2920\n",
      "at batch no 2925\n",
      "at batch no 2930\n",
      "at batch no 2935\n",
      "at batch no 2940\n",
      "at batch no 2945\n",
      "at batch no 2950\n",
      "at batch no 2955\n",
      "at batch no 2960\n",
      "at batch no 2965\n",
      "at batch no 2970\n",
      "at batch no 2975\n",
      "at batch no 2980\n",
      "at batch no 2985\n",
      "at batch no 2990\n",
      "at batch no 2995\n",
      "at batch no 3000\n",
      "at batch no 3005\n",
      "at batch no 3010\n",
      "at batch no 3015\n",
      "at batch no 3020\n",
      "at batch no 3025\n",
      "at batch no 3030\n",
      "at batch no 3035\n",
      "at batch no 3040\n",
      "at batch no 3045\n",
      "at batch no 3050\n",
      "at batch no 3055\n",
      "at batch no 3060\n",
      "at batch no 3065\n",
      "at batch no 3070\n",
      "at batch no 3075\n",
      "at batch no 3080\n",
      "at batch no 3085\n",
      "at batch no 3090\n",
      "at batch no 3095\n",
      "at batch no 3100\n",
      "at batch no 3105\n",
      "at batch no 3110\n",
      "at batch no 3115\n",
      "at batch no 3120\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(323,\n",
       " [(721, 10615.70),\n",
       "  (520, 5087.20),\n",
       "  (669, 3467.10),\n",
       "  (971, 2482.60),\n",
       "  (411, 2048.30),\n",
       "  (414, 1855.90),\n",
       "  (750, 1826.30),\n",
       "  (556, 1802.90),\n",
       "  (431, 1549.50),\n",
       "  (61, 1295.20),\n",
       "  (588, 1134.90),\n",
       "  (828, 1087.60),\n",
       "  (39, 547.10),\n",
       "  (904, 507.60),\n",
       "  (709, 430.10),\n",
       "  (794, 388.20),\n",
       "  (599, 331.70),\n",
       "  (489, 314.10),\n",
       "  (790, 286.00),\n",
       "  (614, 260.70),\n",
       "  (955, 238.90),\n",
       "  (490, 230.90),\n",
       "  (581, 230.00),\n",
       "  (893, 216.60),\n",
       "  (770, 186.50),\n",
       "  (84, 167.90),\n",
       "  (419, 164.90),\n",
       "  (509, 147.10),\n",
       "  (516, 146.30),\n",
       "  (837, 132.50),\n",
       "  (56, 122.30),\n",
       "  (48, 121.80),\n",
       "  (401, 119.20),\n",
       "  (60, 107.50),\n",
       "  (801, 104.90),\n",
       "  (67, 103.30),\n",
       "  (570, 103.30),\n",
       "  (741, 96.20),\n",
       "  (604, 86.50),\n",
       "  (748, 86.40),\n",
       "  (907, 82.20),\n",
       "  (824, 82.10),\n",
       "  (46, 76.30),\n",
       "  (973, 74.50),\n",
       "  (646, 74.40),\n",
       "  (762, 74.00),\n",
       "  (703, 73.00),\n",
       "  (580, 72.40),\n",
       "  (582, 68.70),\n",
       "  (151, 68.00),\n",
       "  (443, 67.30),\n",
       "  (737, 66.60),\n",
       "  (464, 64.60),\n",
       "  (412, 63.80),\n",
       "  (621, 63.60),\n",
       "  (455, 63.30),\n",
       "  (55, 62.70),\n",
       "  (791, 61.80),\n",
       "  (651, 61.20),\n",
       "  (406, 61.10),\n",
       "  (591, 59.60),\n",
       "  (805, 59.00),\n",
       "  (319, 57.40),\n",
       "  (754, 56.20),\n",
       "  (800, 54.30),\n",
       "  (636, 52.50),\n",
       "  (468, 52.20),\n",
       "  (879, 51.00),\n",
       "  (572, 49.80),\n",
       "  (788, 49.20),\n",
       "  (987, 49.20),\n",
       "  (706, 48.00),\n",
       "  (575, 47.60),\n",
       "  (555, 46.40),\n",
       "  (485, 45.70),\n",
       "  (992, 44.50),\n",
       "  (171, 44.30),\n",
       "  (410, 43.70),\n",
       "  (815, 43.40),\n",
       "  (533, 42.50),\n",
       "  (340, 42.20),\n",
       "  (407, 41.00),\n",
       "  (97, 40.80),\n",
       "  (40, 40.10),\n",
       "  (476, 40.00),\n",
       "  (138, 39.40),\n",
       "  (496, 39.40),\n",
       "  (116, 39.10),\n",
       "  (711, 39.10),\n",
       "  (698, 38.40),\n",
       "  (293, 38.20),\n",
       "  (645, 37.50),\n",
       "  (806, 37.50),\n",
       "  (300, 36.90),\n",
       "  (796, 35.90),\n",
       "  (304, 35.60),\n",
       "  (779, 35.40),\n",
       "  (878, 35.10),\n",
       "  (746, 35.00),\n",
       "  (734, 34.70),\n",
       "  (155, 33.60),\n",
       "  (661, 33.20),\n",
       "  (868, 33.10),\n",
       "  (323, 33.00),\n",
       "  (692, 32.80),\n",
       "  (0, 32.60),\n",
       "  (953, 32.50),\n",
       "  (643, 32.40),\n",
       "  (981, 31.70),\n",
       "  (436, 31.30),\n",
       "  (611, 30.80),\n",
       "  (497, 30.60),\n",
       "  (562, 30.60),\n",
       "  (94, 30.40),\n",
       "  (654, 29.30),\n",
       "  (676, 29.30),\n",
       "  (547, 29.00),\n",
       "  (781, 29.00),\n",
       "  (829, 29.00),\n",
       "  (858, 28.80),\n",
       "  (916, 28.70),\n",
       "  (109, 28.40),\n",
       "  (819, 28.30),\n",
       "  (62, 27.90),\n",
       "  (738, 26.90),\n",
       "  (612, 26.70),\n",
       "  (850, 26.70),\n",
       "  (363, 26.50),\n",
       "  (254, 26.30),\n",
       "  (518, 26.20),\n",
       "  (768, 26.10),\n",
       "  (396, 25.90),\n",
       "  (705, 25.90),\n",
       "  (886, 25.90),\n",
       "  (440, 25.80),\n",
       "  (635, 25.60),\n",
       "  (118, 25.40),\n",
       "  (655, 25.40),\n",
       "  (195, 25.30),\n",
       "  (162, 25.20),\n",
       "  (870, 24.90),\n",
       "  (506, 24.70),\n",
       "  (96, 24.40),\n",
       "  (752, 24.20),\n",
       "  (854, 24.20),\n",
       "  (917, 24.20),\n",
       "  (237, 24.10),\n",
       "  (441, 24.00),\n",
       "  (946, 23.60),\n",
       "  (619, 23.30),\n",
       "  (915, 22.80),\n",
       "  (353, 22.70),\n",
       "  (576, 22.60),\n",
       "  (108, 22.40),\n",
       "  (110, 22.10),\n",
       "  (290, 22.10),\n",
       "  (565, 22.10),\n",
       "  (65, 22.00),\n",
       "  (25, 21.90),\n",
       "  (90, 21.90),\n",
       "  (128, 21.90),\n",
       "  (522, 21.60),\n",
       "  (863, 21.40),\n",
       "  (843, 21.30),\n",
       "  (944, 21.30),\n",
       "  (482, 21.20),\n",
       "  (459, 21.00),\n",
       "  (63, 20.90),\n",
       "  (444, 20.70),\n",
       "  (640, 20.70),\n",
       "  (238, 20.60),\n",
       "  (722, 20.50),\n",
       "  (253, 19.90),\n",
       "  (641, 19.90),\n",
       "  (880, 19.80),\n",
       "  (898, 19.80),\n",
       "  (627, 19.60),\n",
       "  (82, 19.50),\n",
       "  (47, 19.30),\n",
       "  (474, 19.30),\n",
       "  (526, 19.30),\n",
       "  (308, 19.20),\n",
       "  (31, 18.90),\n",
       "  (327, 18.80),\n",
       "  (826, 18.80),\n",
       "  (892, 18.80),\n",
       "  (679, 18.70),\n",
       "  (288, 18.60),\n",
       "  (808, 18.50),\n",
       "  (866, 18.50),\n",
       "  (765, 18.40),\n",
       "  (135, 18.30),\n",
       "  (515, 18.30),\n",
       "  (523, 18.30),\n",
       "  (772, 18.30),\n",
       "  (398, 18.20),\n",
       "  (430, 17.90),\n",
       "  (867, 17.90),\n",
       "  (560, 17.80),\n",
       "  (897, 17.80),\n",
       "  (687, 17.70),\n",
       "  (918, 17.70),\n",
       "  (8, 17.60),\n",
       "  (124, 17.60),\n",
       "  (178, 17.60),\n",
       "  (564, 17.60),\n",
       "  (716, 17.60),\n",
       "  (7, 17.30),\n",
       "  (241, 17.30),\n",
       "  (735, 17.30),\n",
       "  (656, 17.10),\n",
       "  (874, 17.10),\n",
       "  (457, 16.90),\n",
       "  (538, 16.90),\n",
       "  (603, 16.90),\n",
       "  (671, 16.90),\n",
       "  (817, 16.80),\n",
       "  (545, 16.70),\n",
       "  (561, 16.70),\n",
       "  (136, 16.60),\n",
       "  (159, 16.60),\n",
       "  (847, 16.60),\n",
       "  (251, 16.30),\n",
       "  (453, 16.30),\n",
       "  (282, 16.20),\n",
       "  (292, 16.20),\n",
       "  (670, 16.20),\n",
       "  (937, 16.20),\n",
       "  (566, 16.10),\n",
       "  (857, 16.10),\n",
       "  (725, 16.00),\n",
       "  (352, 15.90),\n",
       "  (730, 15.70),\n",
       "  (129, 15.50),\n",
       "  (632, 15.50),\n",
       "  (609, 15.30),\n",
       "  (275, 15.20),\n",
       "  (685, 15.20),\n",
       "  (849, 15.20),\n",
       "  (88, 15.10),\n",
       "  (541, 15.10),\n",
       "  (579, 15.10),\n",
       "  (832, 15.00),\n",
       "  (173, 14.90),\n",
       "  (472, 14.90),\n",
       "  (539, 14.90),\n",
       "  (586, 14.90),\n",
       "  (822, 14.90),\n",
       "  (872, 14.90),\n",
       "  (123, 14.80),\n",
       "  (76, 14.60),\n",
       "  (625, 14.60),\n",
       "  (423, 14.50),\n",
       "  (642, 14.40),\n",
       "  (133, 14.30),\n",
       "  (620, 14.30),\n",
       "  (757, 14.30),\n",
       "  (130, 14.20),\n",
       "  (354, 14.20),\n",
       "  (984, 14.20),\n",
       "  (836, 14.10),\n",
       "  (530, 14.00),\n",
       "  (417, 13.90),\n",
       "  (691, 13.90),\n",
       "  (328, 13.80),\n",
       "  (85, 13.70),\n",
       "  (199, 13.70),\n",
       "  (783, 13.70),\n",
       "  (852, 13.70),\n",
       "  (949, 13.70),\n",
       "  (102, 13.60),\n",
       "  (231, 13.60),\n",
       "  (563, 13.60),\n",
       "  (595, 13.60),\n",
       "  (665, 13.60),\n",
       "  (956, 13.40),\n",
       "  (447, 13.30),\n",
       "  (594, 13.30),\n",
       "  (885, 13.20),\n",
       "  (532, 13.10),\n",
       "  (601, 13.10),\n",
       "  (864, 13.00),\n",
       "  (37, 12.90),\n",
       "  (912, 12.90),\n",
       "  (968, 12.80),\n",
       "  (425, 12.60),\n",
       "  (950, 12.60),\n",
       "  (552, 12.50),\n",
       "  (724, 12.50),\n",
       "  (274, 12.40),\n",
       "  (98, 12.20),\n",
       "  (985, 12.20),\n",
       "  (9, 12.10),\n",
       "  (694, 12.10),\n",
       "  (723, 12.10),\n",
       "  (936, 12.10),\n",
       "  (982, 12.10),\n",
       "  (717, 11.90),\n",
       "  (727, 11.90),\n",
       "  (192, 11.80),\n",
       "  (263, 11.80),\n",
       "  (445, 11.80),\n",
       "  (668, 11.80),\n",
       "  (693, 11.80),\n",
       "  (751, 11.80),\n",
       "  (938, 11.80),\n",
       "  (887, 11.70),\n",
       "  (911, 11.70),\n",
       "  (45, 11.50),\n",
       "  (119, 11.50),\n",
       "  (144, 11.50),\n",
       "  (181, 11.40),\n",
       "  (330, 11.40),\n",
       "  (393, 11.30),\n",
       "  (820, 11.30),\n",
       "  (208, 11.20),\n",
       "  (409, 11.20),\n",
       "  (57, 11.10),\n",
       "  (424, 11.10),\n",
       "  (602, 11.10),\n",
       "  (927, 11.10),\n",
       "  (387, 10.90),\n",
       "  (652, 10.90),\n",
       "  (658, 10.90),\n",
       "  (888, 10.90),\n",
       "  (215, 10.70),\n",
       "  (439, 10.70),\n",
       "  (571, 10.70),\n",
       "  (211, 10.60),\n",
       "  (334, 10.60),\n",
       "  (559, 10.60),\n",
       "  (637, 10.60),\n",
       "  (855, 10.60),\n",
       "  (954, 10.60),\n",
       "  (23, 10.50),\n",
       "  (311, 10.50),\n",
       "  (492, 10.50),\n",
       "  (554, 10.50),\n",
       "  (91, 10.40),\n",
       "  (134, 10.40),\n",
       "  (216, 10.40),\n",
       "  (273, 10.40),\n",
       "  (392, 10.40),\n",
       "  (624, 10.30),\n",
       "  (30, 10.20),\n",
       "  (821, 10.10),\n",
       "  (667, 10.00),\n",
       "  (6, 9.80),\n",
       "  (99, 9.80),\n",
       "  (301, 9.80),\n",
       "  (77, 9.70),\n",
       "  (278, 9.70),\n",
       "  (778, 9.70),\n",
       "  (306, 9.60),\n",
       "  (448, 9.60),\n",
       "  (573, 9.60),\n",
       "  (107, 9.50),\n",
       "  (243, 9.40),\n",
       "  (86, 9.20),\n",
       "  (121, 9.20),\n",
       "  (839, 9.20),\n",
       "  (189, 9.00),\n",
       "  (230, 9.00),\n",
       "  (239, 9.00),\n",
       "  (487, 9.00),\n",
       "  (498, 9.00),\n",
       "  (616, 9.00),\n",
       "  (830, 9.00),\n",
       "  (217, 8.90),\n",
       "  (388, 8.90),\n",
       "  (429, 8.90),\n",
       "  (823, 8.90),\n",
       "  (467, 8.80),\n",
       "  (161, 8.70),\n",
       "  (303, 8.70),\n",
       "  (639, 8.70),\n",
       "  (249, 8.60),\n",
       "  (250, 8.60),\n",
       "  (466, 8.60),\n",
       "  (786, 8.60),\n",
       "  (157, 8.50),\n",
       "  (219, 8.50),\n",
       "  (458, 8.50),\n",
       "  (529, 8.50),\n",
       "  (531, 8.50),\n",
       "  (242, 8.40),\n",
       "  (321, 8.40),\n",
       "  (695, 8.40),\n",
       "  (766, 8.40),\n",
       "  (986, 8.40),\n",
       "  (182, 8.30),\n",
       "  (201, 8.30),\n",
       "  (298, 8.30),\n",
       "  (758, 8.30),\n",
       "  (361, 8.20),\n",
       "  (206, 8.10),\n",
       "  (537, 8.10),\n",
       "  (704, 8.10),\n",
       "  (905, 8.10),\n",
       "  (141, 8.00),\n",
       "  (260, 8.00),\n",
       "  (408, 8.00),\n",
       "  (483, 8.00),\n",
       "  (584, 8.00),\n",
       "  (638, 8.00),\n",
       "  (865, 8.00),\n",
       "  (27, 7.90),\n",
       "  (42, 7.90),\n",
       "  (50, 7.90),\n",
       "  (83, 7.90),\n",
       "  (104, 7.90),\n",
       "  (343, 7.90),\n",
       "  (549, 7.90),\n",
       "  (605, 7.90),\n",
       "  (774, 7.80),\n",
       "  (963, 7.80),\n",
       "  (397, 7.70),\n",
       "  (148, 7.60),\n",
       "  (732, 7.60),\n",
       "  (365, 7.50),\n",
       "  (825, 7.50),\n",
       "  (875, 7.50),\n",
       "  (71, 7.40),\n",
       "  (840, 7.40),\n",
       "  (375, 7.30),\n",
       "  (470, 7.20),\n",
       "  (28, 7.10),\n",
       "  (684, 7.10),\n",
       "  (811, 7.10),\n",
       "  (853, 7.10),\n",
       "  (15, 7.00),\n",
       "  (164, 7.00),\n",
       "  (186, 7.00),\n",
       "  (763, 7.00),\n",
       "  (873, 7.00),\n",
       "  (214, 6.90),\n",
       "  (221, 6.90),\n",
       "  (316, 6.90),\n",
       "  (481, 6.90),\n",
       "  (775, 6.90),\n",
       "  (784, 6.90),\n",
       "  (383, 6.80),\n",
       "  (235, 6.70),\n",
       "  (264, 6.70),\n",
       "  (477, 6.70),\n",
       "  (777, 6.70),\n",
       "  (922, 6.70),\n",
       "  (932, 6.70),\n",
       "  (997, 6.70),\n",
       "  (127, 6.60),\n",
       "  (225, 6.60),\n",
       "  (355, 6.60),\n",
       "  (100, 6.50),\n",
       "  (198, 6.50),\n",
       "  (248, 6.50),\n",
       "  (310, 6.50),\n",
       "  (462, 6.50),\n",
       "  (814, 6.50),\n",
       "  (193, 6.40),\n",
       "  (205, 6.40),\n",
       "  (247, 6.40),\n",
       "  (382, 6.40),\n",
       "  (792, 6.40),\n",
       "  (889, 6.40),\n",
       "  (358, 6.30),\n",
       "  (433, 6.30),\n",
       "  (569, 6.30),\n",
       "  (131, 6.20),\n",
       "  (172, 6.20),\n",
       "  (307, 6.20),\n",
       "  (659, 6.20),\n",
       "  (699, 6.20),\n",
       "  (11, 6.10),\n",
       "  (115, 6.10),\n",
       "  (281, 6.10),\n",
       "  (454, 6.10),\n",
       "  (471, 6.10),\n",
       "  (491, 6.10),\n",
       "  (508, 6.10),\n",
       "  (966, 6.10),\n",
       "  (180, 6.00),\n",
       "  (760, 6.00),\n",
       "  (890, 6.00),\n",
       "  (24, 5.90),\n",
       "  (79, 5.90),\n",
       "  (218, 5.90),\n",
       "  (527, 5.90),\n",
       "  (899, 5.90),\n",
       "  (920, 5.90),\n",
       "  (196, 5.80),\n",
       "  (317, 5.80),\n",
       "  (688, 5.80),\n",
       "  (939, 5.80),\n",
       "  (234, 5.70),\n",
       "  (759, 5.70),\n",
       "  (209, 5.60),\n",
       "  (390, 5.60),\n",
       "  (451, 5.60),\n",
       "  (479, 5.60),\n",
       "  (618, 5.60),\n",
       "  (776, 5.60),\n",
       "  (41, 5.50),\n",
       "  (70, 5.50),\n",
       "  (156, 5.50),\n",
       "  (236, 5.50),\n",
       "  (337, 5.50),\n",
       "  (900, 5.50),\n",
       "  (921, 5.50),\n",
       "  (951, 5.50),\n",
       "  (318, 5.40),\n",
       "  (782, 5.40),\n",
       "  (957, 5.40),\n",
       "  (80, 5.30),\n",
       "  (276, 5.30),\n",
       "  (359, 5.30),\n",
       "  (701, 5.30),\n",
       "  (764, 5.30),\n",
       "  (910, 5.30),\n",
       "  (1, 5.20),\n",
       "  (72, 5.20),\n",
       "  (259, 5.20),\n",
       "  (313, 5.20),\n",
       "  (391, 5.20),\n",
       "  (544, 5.20),\n",
       "  (626, 5.20),\n",
       "  (257, 5.10),\n",
       "  (267, 5.10),\n",
       "  (402, 5.10),\n",
       "  (449, 5.10),\n",
       "  (663, 5.10),\n",
       "  (339, 5.00),\n",
       "  (816, 5.00),\n",
       "  (881, 5.00),\n",
       "  (924, 5.00),\n",
       "  (280, 4.90),\n",
       "  (348, 4.90),\n",
       "  (418, 4.90),\n",
       "  (488, 4.90),\n",
       "  (519, 4.90),\n",
       "  (622, 4.90),\n",
       "  (629, 4.90),\n",
       "  (74, 4.80),\n",
       "  (92, 4.80),\n",
       "  (142, 4.80),\n",
       "  (320, 4.80),\n",
       "  (463, 4.80),\n",
       "  (574, 4.80),\n",
       "  (585, 4.80),\n",
       "  (650, 4.80),\n",
       "  (271, 4.70),\n",
       "  (295, 4.70),\n",
       "  (325, 4.70),\n",
       "  (660, 4.70),\n",
       "  (733, 4.70),\n",
       "  (756, 4.70),\n",
       "  (882, 4.70),\n",
       "  (919, 4.70),\n",
       "  (153, 4.60),\n",
       "  (283, 4.60),\n",
       "  (628, 4.60),\n",
       "  (653, 4.60),\n",
       "  (983, 4.60),\n",
       "  (125, 4.50),\n",
       "  (546, 4.50),\n",
       "  (557, 4.50),\n",
       "  (931, 4.50),\n",
       "  (991, 4.50),\n",
       "  (113, 4.40),\n",
       "  (351, 4.40),\n",
       "  (495, 4.40),\n",
       "  (512, 4.40),\n",
       "  (902, 4.40),\n",
       "  (210, 4.30),\n",
       "  (255, 4.30),\n",
       "  (302, 4.30),\n",
       "  (360, 4.30),\n",
       "  (389, 4.30),\n",
       "  (438, 4.30),\n",
       "  (456, 4.30),\n",
       "  (608, 4.30),\n",
       "  (707, 4.30),\n",
       "  (934, 4.30),\n",
       "  (990, 4.30),\n",
       "  (93, 4.20),\n",
       "  (502, 4.20),\n",
       "  (507, 4.20),\n",
       "  (592, 4.20),\n",
       "  (606, 4.20),\n",
       "  (630, 4.20),\n",
       "  (664, 4.20),\n",
       "  (994, 4.20),\n",
       "  (58, 4.10),\n",
       "  (514, 4.10),\n",
       "  (680, 4.10),\n",
       "  (18, 4.00),\n",
       "  (191, 4.00),\n",
       "  (528, 4.00),\n",
       "  (543, 4.00),\n",
       "  (597, 4.00),\n",
       "  (948, 4.00),\n",
       "  (14, 3.90),\n",
       "  (44, 3.90),\n",
       "  (132, 3.90),\n",
       "  (154, 3.90),\n",
       "  (347, 3.90),\n",
       "  (511, 3.90),\n",
       "  (577, 3.90),\n",
       "  (170, 3.80),\n",
       "  (223, 3.80),\n",
       "  (270, 3.80),\n",
       "  (377, 3.80),\n",
       "  (385, 3.80),\n",
       "  (428, 3.80),\n",
       "  (607, 3.80),\n",
       "  (891, 3.80),\n",
       "  (38, 3.70),\n",
       "  (158, 3.70),\n",
       "  (224, 3.70),\n",
       "  (265, 3.70),\n",
       "  (314, 3.70),\n",
       "  (395, 3.70),\n",
       "  (631, 3.70),\n",
       "  (696, 3.70),\n",
       "  (165, 3.60),\n",
       "  (203, 3.60),\n",
       "  (269, 3.60),\n",
       "  (336, 3.60),\n",
       "  (376, 3.60),\n",
       "  (802, 3.60),\n",
       "  (894, 3.60),\n",
       "  (331, 3.50),\n",
       "  (672, 3.50),\n",
       "  (761, 3.50),\n",
       "  (49, 3.40),\n",
       "  (284, 3.40),\n",
       "  (345, 3.40),\n",
       "  (517, 3.40),\n",
       "  (675, 3.40),\n",
       "  (803, 3.30),\n",
       "  (838, 3.30),\n",
       "  (884, 3.30),\n",
       "  (952, 3.30),\n",
       "  (36, 3.20),\n",
       "  (68, 3.20),\n",
       "  (185, 3.20),\n",
       "  (258, 3.20),\n",
       "  (513, 3.20),\n",
       "  (12, 3.10),\n",
       "  (75, 3.10),\n",
       "  (120, 3.10),\n",
       "  (176, 3.10),\n",
       "  (294, 3.10),\n",
       "  (510, 3.10),\n",
       "  (678, 3.10),\n",
       "  (859, 3.10),\n",
       "  (877, 3.10),\n",
       "  (988, 3.10),\n",
       "  (117, 3.00),\n",
       "  (332, 3.00),\n",
       "  (593, 3.00),\n",
       "  (697, 3.00),\n",
       "  (797, 3.00),\n",
       "  (812, 3.00),\n",
       "  (35, 2.90),\n",
       "  (297, 2.90),\n",
       "  (322, 2.90),\n",
       "  (613, 2.90),\n",
       "  (714, 2.90),\n",
       "  (831, 2.90),\n",
       "  (835, 2.90),\n",
       "  (848, 2.90),\n",
       "  (989, 2.90),\n",
       "  (53, 2.80),\n",
       "  (163, 2.80),\n",
       "  (207, 2.80),\n",
       "  (232, 2.80),\n",
       "  (309, 2.80),\n",
       "  (344, 2.80),\n",
       "  (420, 2.80),\n",
       "  (446, 2.80),\n",
       "  (674, 2.80),\n",
       "  (787, 2.80),\n",
       "  (941, 2.80),\n",
       "  (17, 2.70),\n",
       "  (220, 2.70),\n",
       "  (272, 2.70),\n",
       "  (399, 2.70),\n",
       "  (503, 2.70),\n",
       "  (535, 2.70),\n",
       "  (710, 2.70),\n",
       "  (736, 2.70),\n",
       "  (851, 2.70),\n",
       "  (958, 2.70),\n",
       "  (188, 2.60),\n",
       "  (368, 2.60),\n",
       "  (475, 2.60),\n",
       "  (542, 2.60),\n",
       "  (700, 2.60),\n",
       "  (998, 2.60),\n",
       "  (52, 2.50),\n",
       "  (122, 2.50),\n",
       "  (245, 2.50),\n",
       "  (286, 2.50),\n",
       "  (291, 2.50),\n",
       "  (342, 2.50),\n",
       "  (404, 2.50),\n",
       "  (587, 2.50),\n",
       "  (644, 2.50),\n",
       "  (666, 2.50),\n",
       "  (862, 2.50),\n",
       "  (906, 2.50),\n",
       "  (166, 2.40),\n",
       "  (486, 2.40),\n",
       "  (551, 2.40),\n",
       "  (633, 2.40),\n",
       "  (753, 2.40),\n",
       "  (842, 2.40),\n",
       "  (959, 2.40),\n",
       "  (995, 2.40),\n",
       "  (174, 2.30),\n",
       "  (177, 2.30),\n",
       "  (256, 2.30),\n",
       "  (356, 2.30),\n",
       "  (362, 2.30),\n",
       "  (371, 2.30),\n",
       "  (558, 2.30),\n",
       "  (807, 2.30),\n",
       "  (860, 2.30),\n",
       "  (101, 2.20),\n",
       "  (226, 2.20),\n",
       "  (228, 2.20),\n",
       "  (244, 2.20),\n",
       "  (315, 2.20),\n",
       "  (484, 2.20),\n",
       "  (583, 2.20),\n",
       "  (683, 2.20),\n",
       "  (767, 2.20),\n",
       "  (871, 2.20),\n",
       "  (10, 2.10),\n",
       "  (51, 2.10),\n",
       "  (89, 2.10),\n",
       "  (421, 2.10),\n",
       "  (432, 2.10),\n",
       "  (452, 2.10),\n",
       "  (690, 2.10),\n",
       "  (833, 2.10),\n",
       "  (929, 2.10),\n",
       "  (43, 2.00),\n",
       "  (194, 2.00),\n",
       "  (202, 2.00),\n",
       "  (213, 2.00),\n",
       "  (350, 2.00),\n",
       "  (369, 2.00),\n",
       "  (789, 2.00),\n",
       "  (861, 2.00),\n",
       "  (999, 2.00),\n",
       "  (143, 1.90),\n",
       "  (146, 1.90),\n",
       "  (183, 1.90),\n",
       "  (329, 1.90),\n",
       "  (370, 1.90),\n",
       "  (381, 1.90),\n",
       "  (465, 1.90),\n",
       "  (505, 1.90),\n",
       "  (845, 1.90),\n",
       "  (876, 1.90),\n",
       "  (979, 1.90),\n",
       "  (179, 1.80),\n",
       "  (222, 1.80),\n",
       "  (240, 1.80),\n",
       "  (364, 1.80),\n",
       "  (367, 1.80),\n",
       "  (426, 1.80),\n",
       "  (589, 1.80),\n",
       "  (728, 1.80),\n",
       "  (749, 1.80),\n",
       "  (16, 1.70),\n",
       "  (105, 1.70),\n",
       "  (114, 1.70),\n",
       "  (145, 1.70),\n",
       "  (268, 1.70),\n",
       "  (400, 1.70),\n",
       "  (521, 1.70),\n",
       "  (536, 1.70),\n",
       "  (712, 1.70),\n",
       "  (720, 1.70),\n",
       "  (66, 1.60),\n",
       "  (227, 1.60),\n",
       "  (341, 1.60),\n",
       "  (346, 1.60),\n",
       "  (374, 1.60),\n",
       "  (567, 1.60),\n",
       "  (747, 1.60),\n",
       "  (795, 1.60),\n",
       "  (883, 1.60),\n",
       "  (945, 1.60),\n",
       "  (962, 1.60),\n",
       "  (54, 1.50),\n",
       "  (405, 1.50),\n",
       "  (534, 1.50),\n",
       "  (596, 1.50),\n",
       "  (647, 1.50),\n",
       "  (743, 1.50),\n",
       "  (769, 1.50),\n",
       "  (846, 1.50),\n",
       "  (380, 1.40),\n",
       "  (804, 1.40),\n",
       "  (813, 1.40),\n",
       "  (925, 1.40),\n",
       "  (972, 1.40),\n",
       "  (32, 1.30),\n",
       "  (540, 1.30),\n",
       "  (550, 1.30),\n",
       "  (615, 1.30),\n",
       "  (702, 1.30),\n",
       "  (718, 1.30),\n",
       "  (809, 1.30),\n",
       "  (969, 1.30),\n",
       "  (2, 1.20),\n",
       "  (5, 1.20),\n",
       "  (81, 1.20),\n",
       "  (137, 1.20),\n",
       "  (175, 1.20),\n",
       "  (261, 1.20),\n",
       "  (262, 1.20),\n",
       "  (305, 1.20),\n",
       "  (312, 1.20),\n",
       "  (338, 1.20),\n",
       "  (437, 1.20),\n",
       "  (947, 1.20),\n",
       "  (87, 1.10),\n",
       "  (139, 1.10),\n",
       "  (147, 1.10),\n",
       "  (197, 1.10),\n",
       "  (204, 1.10),\n",
       "  (252, 1.10),\n",
       "  (427, 1.10),\n",
       "  (740, 1.10),\n",
       "  (59, 1.00),\n",
       "  (64, 1.00),\n",
       "  (69, 1.00),\n",
       "  (78, 1.00),\n",
       "  (95, 1.00),\n",
       "  (126, 1.00),\n",
       "  (140, 1.00),\n",
       "  (152, 1.00),\n",
       "  (169, 1.00),\n",
       "  (333, 1.00),\n",
       "  (434, 1.00),\n",
       "  (827, 1.00),\n",
       "  (841, 1.00),\n",
       "  (869, 1.00),\n",
       "  (19, 0.90),\n",
       "  (22, 0.90),\n",
       "  (160, 0.90),\n",
       "  (187, 0.90),\n",
       "  (212, 0.90),\n",
       "  (229, 0.90),\n",
       "  (372, 0.90),\n",
       "  (378, 0.90),\n",
       "  (386, 0.90),\n",
       "  (394, 0.90),\n",
       "  (493, 0.90),\n",
       "  (548, 0.90),\n",
       "  (590, 0.90),\n",
       "  (657, 0.90),\n",
       "  (755, 0.90),\n",
       "  (933, 0.90),\n",
       "  (13, 0.80),\n",
       "  (73, 0.80),\n",
       "  (279, 0.80),\n",
       "  (469, 0.80),\n",
       "  (478, 0.80),\n",
       "  (719, 0.80),\n",
       "  (856, 0.80),\n",
       "  (184, 0.70),\n",
       "  (289, 0.70),\n",
       "  (379, 0.70),\n",
       "  (610, 0.70),\n",
       "  (689, 0.70),\n",
       "  (708, 0.70),\n",
       "  (975, 0.70),\n",
       "  (285, 0.60),\n",
       "  (373, 0.60),\n",
       "  (442, 0.60),\n",
       "  (500, 0.60),\n",
       "  (731, 0.60),\n",
       "  (930, 0.60),\n",
       "  (943, 0.60),\n",
       "  (26, 0.50),\n",
       "  (33, 0.50),\n",
       "  (357, 0.50),\n",
       "  (413, 0.50),\n",
       "  (480, 0.50),\n",
       "  (494, 0.50),\n",
       "  (623, 0.50),\n",
       "  (673, 0.50),\n",
       "  (798, 0.50),\n",
       "  (964, 0.50),\n",
       "  (111, 0.40),\n",
       "  (200, 0.40),\n",
       "  (277, 0.40),\n",
       "  (299, 0.40),\n",
       "  (422, 0.40),\n",
       "  (677, 0.40),\n",
       "  (799, 0.40),\n",
       "  (896, 0.40),\n",
       "  (909, 0.40),\n",
       "  (996, 0.40),\n",
       "  (21, 0.30),\n",
       "  (34, 0.30),\n",
       "  (246, 0.30),\n",
       "  (416, 0.30),\n",
       "  (435, 0.30),\n",
       "  (461, 0.30),\n",
       "  (578, 0.30),\n",
       "  (681, 0.30),\n",
       "  (686, 0.30),\n",
       "  (923, 0.30),\n",
       "  (168, 0.20),\n",
       "  (190, 0.20),\n",
       "  (266, 0.20),\n",
       "  (326, 0.20),\n",
       "  (662, 0.20),\n",
       "  (793, 0.20),\n",
       "  (834, 0.20),\n",
       "  (844, 0.20),\n",
       "  (903, 0.20),\n",
       "  (926, 0.20),\n",
       "  (942, 0.20),\n",
       "  (20, 0.10),\n",
       "  (29, 0.10),\n",
       "  (106, 0.10),\n",
       "  (149, 0.10),\n",
       "  (167, 0.10),\n",
       "  (296, 0.10),\n",
       "  (324, 0.10),\n",
       "  (384, 0.10),\n",
       "  (600, 0.10),\n",
       "  (744, 0.10),\n",
       "  (773, 0.10),\n",
       "  (785, 0.10),\n",
       "  (3, 0.00),\n",
       "  (4, 0.00),\n",
       "  (103, 0.00),\n",
       "  (112, 0.00),\n",
       "  (150, 0.00),\n",
       "  (233, 0.00),\n",
       "  (287, 0.00),\n",
       "  (335, 0.00),\n",
       "  (349, 0.00),\n",
       "  (366, 0.00),\n",
       "  (403, 0.00),\n",
       "  (415, 0.00),\n",
       "  (450, 0.00),\n",
       "  (460, 0.00),\n",
       "  (473, 0.00),\n",
       "  (499, 0.00),\n",
       "  (501, 0.00),\n",
       "  (504, 0.00),\n",
       "  (524, 0.00),\n",
       "  (525, 0.00),\n",
       "  (553, 0.00),\n",
       "  (568, 0.00),\n",
       "  (598, 0.00),\n",
       "  (617, 0.00),\n",
       "  (634, 0.00),\n",
       "  (648, 0.00),\n",
       "  (649, 0.00),\n",
       "  (682, 0.00),\n",
       "  (713, 0.00),\n",
       "  (715, 0.00),\n",
       "  (726, 0.00),\n",
       "  (729, 0.00),\n",
       "  (739, 0.00),\n",
       "  (742, 0.00),\n",
       "  (745, 0.00),\n",
       "  (771, 0.00),\n",
       "  (780, 0.00),\n",
       "  (810, 0.00),\n",
       "  (818, 0.00),\n",
       "  (895, 0.00),\n",
       "  (901, 0.00),\n",
       "  (908, 0.00),\n",
       "  (913, 0.00),\n",
       "  (914, 0.00),\n",
       "  (928, 0.00),\n",
       "  (935, 0.00),\n",
       "  (940, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (965, 0.00),\n",
       "  (967, 0.00),\n",
       "  (970, 0.00),\n",
       "  (974, 0.00),\n",
       "  (976, 0.00),\n",
       "  (977, 0.00),\n",
       "  (978, 0.00),\n",
       "  (980, 0.00),\n",
       "  (993, 0.00)])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcc307cc4a8>]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# on test\n",
    "sorted_hist = sorted(hist, key=lambda x: x[0], reverse = False)\n",
    "values = [elem[1] for elem in sorted_hist]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.6691)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "entropy(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fCvFG0VMKts"
   },
   "outputs": [],
   "source": [
    "def make_triplet_samples(z, margin, r2, r3):\n",
    "  positive_sample = z + random_vector_volume(z.shape, 0, margin).cuda() \n",
    "  negative_sample = z + random_vector_volume(z.shape, r2, r3).cuda()\n",
    "  return positive_sample, negative_sample\n",
    "\n",
    "def random_vector_surface(shape, r = 1.):\n",
    "  mat = torch.randn(size=shape).cuda()\n",
    "  norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "  return (mat/norm) * r\n",
    "\n",
    "def random_vector_volume(shape, inner_r, outer_r):\n",
    "  fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "  fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "  fraction.unsqueeze_(-1)\n",
    "  return random_vector_surface(shape, 1) * fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PizmBkGqMKtu"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_frequent(x):\n",
    "  return Counter(x).most_common(1)[0]\n",
    "\n",
    "def preds_around(center, radius, n_preds, model, dummy_img):\n",
    "  z_s = random_vector_volume([n_preds, 10], radius, radius + 0.01) + center[None]\n",
    "  noises = model.forward_z(z_s)\n",
    "  perturbed_imgs = noises + dummy_img \n",
    "  return torch.argmax(arch(perturbed_imgs), 1)\n",
    "  \n",
    "def most_freq_pred_around(center, radius, n_preds, model, dummy_img):\n",
    "  preds = preds_around(center, radius, n_preds, model, dummy_img)\n",
    "  most_freq = most_frequent(preds.tolist())\n",
    "  return (class_index_to_label(most_freq[0]), most_freq[1]/n_preds)\n",
    "\n",
    "def investigate_neighborhood(z, step, model, dummy_img):\n",
    "  with torch.no_grad():\n",
    "    result = []\n",
    "    for radius in np.arange(0.1, 6., step):\n",
    "#       print(\"creating {} more preds\".format(int(10 + 5 * (radius ** 2))))\n",
    "      most_freq_pred = most_freq_pred_around(z, radius, int(10 + 5 * (radius ** 2)), model, dummy_img)\n",
    "      result.append((radius, most_freq_pred))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-btRW4qMKtw",
    "outputId": "90e81f6a-1b9e-45a4-ae82-bda370319bd9"
   },
   "outputs": [],
   "source": [
    "#experiment 1\n",
    "\n",
    "z = torch.tensor([0.5] * 10).cuda()\n",
    "# z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "# z_s = z[None]\n",
    "\n",
    "model = learn.model.eval()\n",
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "  \n",
    "for i in range(6):\n",
    "  z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "  print(\"investigation for: \", z)\n",
    "  for elem in investigate_neighborhood(z, 0.5, model, x_img):\n",
    "    print(elem)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 1-1: modified investigate_z\n",
    "z_investigate_path = '/root/Derakhshani/adversarial/textual_notes/investigate_z_{}.txt'.format(env.save_filename)\n",
    "if Path(z_investigate_path).exists(): raise FileExistsError(\"file already exists\")\n",
    "file = open(str(z_investigate_path), 'w')\n",
    "        \n",
    "for i, (z, noise) in enumerate(zip(pruned_z_s, pruned_noises)):\n",
    "  hist = compute_prediction_histogram(learn, noise)\n",
    "  indexed_hist = [(i, val) for i, val in enumerate(hist)]\n",
    "  sorted_hist = sorted(indexed_hist, key=lambda x: x[1], reverse=True)\n",
    "  labeled_hist = [(class_index_to_label(i), count) for i, count in sorted_hist]\n",
    "  print(\"result {}:\".format(i))\n",
    "  print(big_vector_to_str(z))\n",
    "  print(labeled_hist[:6])\n",
    "  print(\"\\n\\n\")\n",
    "  \n",
    "  file.write(\"result {}:\\n\".format(i))\n",
    "  file.write(big_vector_to_str(z) + \"\\n\")\n",
    "  file.write(str(labeled_hist[:6]))\n",
    "  file.write(\"\\n\\n\\n\")\n",
    "  file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bp6YOnipMKtz"
   },
   "outputs": [],
   "source": [
    "#experiment 2\n",
    "import itertools\n",
    "z_s = [torch.tensor(t).cuda() for t in itertools.product( *([[-0.33, 0.33]] * 10) )]\n",
    "model = learn.model.eval()\n",
    "noises = []\n",
    "with torch.no_grad():\n",
    "  for z in z_s:\n",
    "    noises.append(model.forward_single_z(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "55lErWDyMKt1",
    "outputId": "93d8fb71-3fd5-44a5-d3ec-8013e13f17ba"
   },
   "outputs": [],
   "source": [
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "\n",
    "preds = []\n",
    "for noise in noises:\n",
    "  perturbed_img = x_img + noise\n",
    "  preds.append(torch.argmax(arch(perturbed_img[None]), 1)[0].item())\n",
    "\n",
    "from collections import Counter\n",
    "result = [(class_index_to_label(index), count) for index, count in Counter(preds).most_common(5)]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4WSg-wBFMKt5"
   },
   "outputs": [],
   "source": [
    "#experiment 3\n",
    "import itertools\n",
    "dimension_values = [[-0.9, 0.9]] * z_dim\n",
    "for i in range(z_dim):\n",
    "  if i % 100 != 0:\n",
    "    dimension_values[i] = [0.]\n",
    "# dimension_values[0] = [0.]\n",
    "# dimension_values[3] = [0.]\n",
    "# dimension_values[6] = [0.]\n",
    "# dimension_values[9] = [0.]\n",
    "pruned_z_s = [torch.tensor(t).cuda() for t in itertools.product(*dimension_values)]\n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 3: for the targeted-attack case\n",
    "pruned_z_s = []\n",
    "for i in range(z_dim):\n",
    "  new_z = torch.zeros(z_dim).cuda()\n",
    "  new_z[i] = 1.\n",
    "  pruned_z_s.append(new_z)\n",
    "  \n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 3-1: noises for \n",
    "pruned_z_s = []\n",
    "# for i in range(z_dim):\n",
    "#   new_z = torch.empty(z_dim).uniform_(0,1).cuda().detach()\n",
    "#   pruned_z_s.append(new_z)\n",
    "\n",
    "for i in range(z_dim):\n",
    "  new_z = torch.zeros(z_dim).cuda().detach()\n",
    "  new_z[i] = 1.\n",
    "  pruned_z_s.append(new_z)\n",
    "  \n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise in pruned_noises[0:200]:\n",
    "  img = noise_to_image(noise)\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider web\n",
    "z_values = [\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33],\n",
    "  [-0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33,  0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33,  0.33,  0.33],\n",
    "  [-0.33,  0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_values = [\n",
    "  # window screen\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuAVZzmKMKt9",
    "outputId": "7e6643e0-ce55-438b-e0ae-79bde3ee4cef"
   },
   "outputs": [],
   "source": [
    "#vgg-16_12 most repeated labels:\n",
    "l = [(611, 215.0),\n",
    "  (474, 194.1),\n",
    "  (398, 120.3),\n",
    "  (721, 79.6),\n",
    "  (741, 73.5),\n",
    "  (510, 62.5)]\n",
    "\n",
    "[(class_index_to_label(index), count) for index, count in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAgk-YyWc3rG"
   },
   "outputs": [],
   "source": [
    "# learn.recorder.plot_losses()\n",
    "# learn.recorder.plot_lr()\n",
    "# learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTHG4Bt7VDYp"
   },
   "outputs": [],
   "source": [
    "fooling_rates = []\n",
    "model = learn.model.eval()\n",
    "learn.metrics = [validation_single_perturbation]\n",
    "for i in range(10):\n",
    "  global_perturbations = model(torch.rand(1, 3, 224, 244).cuda())[0]\n",
    "  nag_util.global_perturbations = global_perturbations\n",
    "  fooling_rates.append(learn.validate()[1].cpu().item())\n",
    "  print(\"%d : %f\"%(i, fooling_rates[-1]))\n",
    "\n",
    "mean = np.mean(fooling_rates)\n",
    "stddev = np.std(fooling_rates)\n",
    "print(mean, stddev); print(fooling_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "OFCjzI7UaY3C",
    "outputId": "740185b4-dd54-46f4-b0af-79ee452568e1"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[200][0]\n",
    "x = normalize(x_img.data.cuda())\n",
    "z = torch.tensor([-0.33,  0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33], dtype=torch.float32).cuda()\n",
    "# z = torch.empty(z_dim).uniform_(-1,1).cuda()\n",
    "p = model.forward_single_z(z).detach()\n",
    "\n",
    "p_x = x + p\n",
    "# print(\"img range, noise range\")\n",
    "# print_range(x); print_range(p)\n",
    "adv_label = class_index_to_label(arch(p_x[None]).argmax(1).item())\n",
    "print_big_vector(arch(p_x[None])[0])\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0., 1.])\n",
    "p_img = Image(p)\n",
    "x_img.show()\n",
    "p_img.show()\n",
    "p_x_img.show()\n",
    "\n",
    "\n",
    "# print_range(p)\n",
    "# print_range(denormalize(x))\n",
    "# print_range(p_x)\n",
    "\n",
    "benign_label = class_index_to_label(arch(x[None]).argmax(1).item())\n",
    "\n",
    "print_big_vector(arch(x[None])[0])\n",
    "print(benign_label, adv_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzwsI2P1ZANz"
   },
   "outputs": [],
   "source": [
    "z1 = torch.tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p1 = model.forward_single_z(z1)\n",
    "\n",
    "z2 = torch.tensor([1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p2 = model.forward_single_z(z2)\n",
    "\n",
    "z3 = torch.tensor([1, 1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p3 = model.forward_single_z(z3)\n",
    "\n",
    "l2_distance(p1, p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eroI82OKSnAL"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[4][0]\n",
    "x = x_img.data[None].cuda()\n",
    "p = model(x)[0].squeeze().detach() \n",
    "x = x.squeeze()\n",
    "x = normalize(x)\n",
    "\n",
    "p_x = x + p\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0.,1.])\n",
    "p_img = Image(p)\n",
    "# x_img.show()\n",
    "p_img.show()\n",
    "# p_x_img.show()\n",
    "\n",
    "print_range(p)\n",
    "print_range(x)\n",
    "print_range(p_x)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NAG-tripletLossExperiment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
