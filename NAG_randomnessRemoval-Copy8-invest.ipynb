{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ahmad-PH/nag-notebooks/blob/master/NAG_tripletLossExperiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqeZpz16do4y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up-to-date.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os; import subprocess\n",
    "\n",
    "def detect_env():\n",
    "    return 'colab' if 'content' in os.listdir('/') else 'IBM'\n",
    "  \n",
    "def run_shell_command(cmd):\n",
    "  p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "  print(str(p.communicate()[0], 'utf-8'))\n",
    "  \n",
    "if detect_env() == 'colab': root_folder = '/content'\n",
    "elif detect_env() == 'IBM' : root_folder = '/root/Derakhshani/adversarial'\n",
    "python_files_path = root_folder + '/nag-public'\n",
    "if os.path.isdir(python_files_path):\n",
    "  initial_dir = os.getcwd()\n",
    "  os.chdir(python_files_path)\n",
    "  run_shell_command('git pull')\n",
    "  os.chdir(initial_dir)\n",
    "else:\n",
    "  os.chdir('/root/Derakhshani/adversarial')\n",
    "  run_shell_command('git clone https://github.com/ahmad-PH/nag-public.git')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.imports import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import sys; import os; import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(python_files_path + '/NAG-11May-beforeDenoiser')\n",
    "\n",
    "from nag_util import *\n",
    "import nag_util\n",
    "from environment import *\n",
    "from visualization import *\n",
    "\n",
    "env = create_env()\n",
    "env.setup(cuda_index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_arch = \"targeted\"\n",
    "# gen_arch = \"non-targeted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralRelu(nn.Module):\n",
    "  def __init__(self, leak=None, sub=None, maxv=None):\n",
    "    super().__init__()\n",
    "    self.leak,self.sub,self.maxv = leak,sub,maxv\n",
    "\n",
    "  def forward(self, x): \n",
    "    x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n",
    "    if self.sub is not None: x.sub_(self.sub)\n",
    "    if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "    return x\n",
    "  \n",
    "class deconv_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k_size = (4,4), s = (2,2), pad = (1,1), b = True, activation = True):\n",
    "        super(deconv_layer, self).__init__()\n",
    "\n",
    "        self.CT2d = nn.ConvTranspose2d(in_channels = in_ch,\n",
    "                                  out_channels = out_ch,\n",
    "                                  kernel_size = k_size,\n",
    "                                  stride = s, \n",
    "                                  padding = pad,\n",
    "                                  bias = b)\n",
    "        self.BN2d = nn.BatchNorm2d(out_ch)\n",
    "        \n",
    "        self.activation = activation\n",
    "        if self.activation:\n",
    "            self.relu = GeneralRelu(0, 0.2, 5)\n",
    "        \n",
    "        self.weight_init()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.activation:\n",
    "            return self.relu(self.BN2d(self.CT2d(input)))\n",
    "        else:\n",
    "            return self.BN2d(self.CT2d(input))\n",
    "\n",
    "    def weight_init(self):\n",
    "        self.CT2d.weight.data.normal_(mean = 0, std = 0.02)\n",
    "        self.CT2d.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deconv_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k_size = (4,4), s = (2,2), pad = (1,1), b = True, activation = True):\n",
    "        super(deconv_layer, self).__init__()\n",
    "\n",
    "        self.CT2d = nn.ConvTranspose2d(in_channels = in_ch,\n",
    "                                  out_channels = out_ch,\n",
    "                                  kernel_size = k_size,\n",
    "                                  stride = s, \n",
    "                                  padding = pad,\n",
    "                                  bias = b)\n",
    "        self.BN2d = nn.BatchNorm2d(out_ch)\n",
    "        self.activation = activation\n",
    "\n",
    "        self.weight_init()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.activation:\n",
    "            return F.relu(self.BN2d(self.CT2d(input)), inplace=True)\n",
    "        else:\n",
    "            return self.BN2d(self.CT2d(input))\n",
    "\n",
    "    def weight_init(self):\n",
    "        self.CT2d.weight.data.normal_(mean = 0, std = 0.02)\n",
    "        self.CT2d.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gen_arch == \"targeted\":\n",
    "  class Gen(nn.Module):\n",
    "    def __init__(self, z_dim, active_labels = (0, 1000), gf_dim=64, y_dim = None,\n",
    "                 df_dim = 64, image_shape = [3,128,128]):\n",
    "      super(Gen, self).__init__()\n",
    "      \n",
    "      self.bs = None\n",
    "      self.z_dim = z_dim\n",
    "      self.gf_dim = gf_dim\n",
    "      self.y_dim = y_dim\n",
    "      self.df_dim = df_dim\n",
    "      self.image_shape = image_shape\n",
    "      self.active_labels = active_labels\n",
    "      \n",
    "      self.n_unit_coeffs = [10, 7, 4, 2, 1, 1, 1]\n",
    "      self.n_units = [coeff * self.gf_dim for coeff in self.n_unit_coeffs]\n",
    "      \n",
    "      self.z_ = nn.Linear(self.z_dim, self.n_units[0] * 4 * 4, bias=True)\n",
    "      self.z_.bias.data.fill_(0)\n",
    "      self.BN_ = nn.BatchNorm2d(self.n_units[0])\n",
    "\n",
    "      self.half = max(self.gf_dim // 2, 1) \n",
    "      self.quarter = max(self.gf_dim // 4, 1)\n",
    "      self.eighth = max(self.gf_dim // 8, 1)\n",
    "      # sixteenth = max(self.gf_dim // 16, 1)\n",
    "\n",
    "      self.CT2d_1 = deconv_layer(self.n_units[0], self.n_units[1], k_size = (5,5), pad = (2,2))\n",
    "      self.CT2d_2 = deconv_layer(self.n_units[1], self.n_units[2])    \n",
    "      self.CT2d_3 = deconv_layer(self.n_units[2], self.n_units[3])\n",
    "      self.CT2d_4 = deconv_layer(self.n_units[3], self.n_units[4])\n",
    "      self.CT2d_5 = deconv_layer(self.n_units[4], self.n_units[5])\n",
    "      self.CT2d_6 = deconv_layer(self.n_units[5], self.n_units[6])\n",
    "      self.CT2d_7 = deconv_layer(self.n_units[6], 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "    def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "      h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "      h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "      output = deconv_layer(h_input)\n",
    "      assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "              \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "              \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "      return output\n",
    "\n",
    "    def forward_z(self, z):\n",
    "      self.bs = z.shape[0]\n",
    "\n",
    "      h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "      assert h0.shape[2:] == (4, 4), \"Unexpected shape, it shoud be (4,4)\"\n",
    "\n",
    "      h1 = self.CT2d_1(h0)\n",
    "      h2 = self.CT2d_2(h1)\n",
    "      h3 = self.CT2d_3(h2)\n",
    "      h4 = self.CT2d_4(h3)\n",
    "      h5 = self.CT2d_5(h4)\n",
    "      h6 = self.CT2d_6(h5)\n",
    "      h7 = self.CT2d_7(h6)\n",
    "      \n",
    "      \n",
    "#       h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "#       h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "#       h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "#       h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "#       h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "#       h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "#       h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "      ksi = 10.0\n",
    "      output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "      # this coeff scales the output to be appropriate for images that are \n",
    "      # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "      # interval)\n",
    "      return output_coeff * torch.tanh(h7)\n",
    "\n",
    "  #   # blind-selection\n",
    "    def forward(self, inputs):\n",
    "      self.bs = inputs.shape[0]\n",
    "\n",
    "      benign_preds_onehot = arch(inputs)\n",
    "#       benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "      worst_preds = torch.argmin(benign_preds_onehot, dim = 1)\n",
    "#       second_best_preds = torch.topk(benign_preds_onehot, 2, dim=1)[1][:, 1]\n",
    "      \n",
    "      z = torch.zeros([self.bs, 1000]).cuda()\n",
    "      for i in range(self.bs):\n",
    "        random_label = worst_preds[i].item()\n",
    "        z[i][random_label] = 1.\n",
    "\n",
    "      z_out = self.forward_z(z)\n",
    "\n",
    "      return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "\n",
    "  #   #second-best selection: made validation so much worse\n",
    "  #   def forward(self, inputs):\n",
    "  #     self.bs = inputs.shape[0]\n",
    "\n",
    "  #     benign_preds_onehot = arch(inputs)\n",
    "  #     target_preds = torch.topk(benign_preds_onehot, 2, dim = 1).indices[:, 1:]\n",
    "\n",
    "  #     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "  #     for i in range(self.bs):\n",
    "  #       z[i][target_preds[i]] = 1.\n",
    "\n",
    "  #     z_out = self.forward_z(z)\n",
    "\n",
    "  #     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "\n",
    "  #    def forward(self, inputs):\n",
    "  #     self.bs = inputs.shape[0]\n",
    "\n",
    "  #     benign_preds_onehot = arch(inputs)\n",
    "  #     benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "\n",
    "  #     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "  #     random_label = self.randint(0,1000, exclude = benign_preds.tolist())\n",
    "  #     for i in range(self.bs):\n",
    "  #       z[i][random_label] = 1.\n",
    "\n",
    "  #     z_out = self.forward_z(z)\n",
    "\n",
    "  #     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "\n",
    "    @staticmethod\n",
    "    def randint(low, high, exclude):\n",
    "      if exclude >= low and exclude < high:\n",
    "        temp = np.random.randint(low, high - 1)\n",
    "        if temp >= exclude:\n",
    "          temp = temp + 1\n",
    "        return temp\n",
    "      else:\n",
    "        return np.random.randint(low, high)\n",
    "\n",
    "    def forward_single_z(self, z):\n",
    "      return self.forward_z(z[None]).squeeze()\n",
    "\n",
    "    def generate_single_noise(self):\n",
    "      z = torch.empty(self.z_dim).uniform_(-1,1).cuda()\n",
    "      return self.forward_single_z(z)         \n",
    "\n",
    "    def make_triplet_samples(self, z, margin, r2, r3):\n",
    "      positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "      negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "      return positive_sample, negative_sample\n",
    "\n",
    "    def random_vector_surface(self, shape, r = 1.):\n",
    "      mat = torch.randn(size=shape).cuda()\n",
    "      norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "      return (mat/norm) * r\n",
    "\n",
    "\n",
    "    def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "      fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "      fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "      fraction.unsqueeze_(-1)\n",
    "      return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "    def make_z(self, in_shape):\n",
    "      return torch.empty(in_shape).cuda().uniform_(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = Gen(z_dim = 1000).cuda()\n",
    "# t = torch.empty(1000).uniform_().cuda()\n",
    "# g.forward_single_z(t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gen_arch == \"non-targeted\":\n",
    "  class Gen(nn.Module):\n",
    "    def __init__(self, z_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "      super(Gen, self).__init__()\n",
    "\n",
    "      self.bs = None\n",
    "      self.z_dim = z_dim\n",
    "      self.gf_dim = gf_dim\n",
    "      self.y_dim = y_dim\n",
    "      self.df_dim = df_dim\n",
    "      self.image_shape = image_shape\n",
    "\n",
    "      self.z_ = nn.Linear(self.z_dim, self.gf_dim * 7 * 4 * 4, bias=True)\n",
    "      self.z_.bias.data.fill_(0)\n",
    "      self.BN_ = nn.BatchNorm2d(self.gf_dim * 7)\n",
    "\n",
    "      self.half = max(self.gf_dim // 2, 1) \n",
    "      self.quarter = max(self.gf_dim // 4, 1)\n",
    "      self.eighth = max(self.gf_dim // 8, 1)\n",
    "      # sixteenth = max(self.gf_dim // 16, 1)\n",
    "\n",
    "      self.CT2d_1 = deconv_layer(self.gf_dim * 8, self.gf_dim * 4, k_size = (5,5), pad = (2,2))\n",
    "      self.CT2d_2 = deconv_layer(self.gf_dim * 5, self.gf_dim * 2)    \n",
    "      self.CT2d_3 = deconv_layer(self.gf_dim * 2 + self.half, self.gf_dim * 1)\n",
    "      self.CT2d_4 = deconv_layer(self.gf_dim * 1 + self.quarter, self.gf_dim * 1)\n",
    "      self.CT2d_5 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "      self.CT2d_6 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "      self.CT2d_7 = deconv_layer(self.gf_dim * 1 + self.eighth, 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "    def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "      h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "      h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "      output = deconv_layer(h_input)\n",
    "      assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "              \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "              \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "      return output\n",
    "\n",
    "    def forward_z(self, z):\n",
    "      self.bs = z.shape[0]\n",
    "\n",
    "      h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "      assert h0.shape[2:] == (4, 4), \"Non-expected shape, it shoud be (4,4)\"\n",
    "\n",
    "      h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "      h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "      h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "      h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "      h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "      h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "      h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "      ksi = 10.0\n",
    "      output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "      # this coeff scales the output to be appropriate for images that are \n",
    "      # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "      # interval)\n",
    "      return output_coeff * torch.tanh(h7)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "      self.bs = inputs.shape[0]\n",
    "      z = inputs.new_empty([self.bs, self.z_dim]).uniform_(-1,1).cuda()\n",
    "      p, n = self.make_triplet_samples(z, 0.1, 0.1, 2.)\n",
    "\n",
    "      z_out = self.forward_z(z)\n",
    "#       p_out = self.forward_z(p)\n",
    "#       n_out = self.forward_z(n)\n",
    "\n",
    "#       return z_out, p_out, n_out, inputs, z\n",
    "      return z_out, None, None, inputs, z\n",
    "\n",
    "    def forward_single_z(self, z):\n",
    "      return self.forward_z(z[None]).squeeze()\n",
    "\n",
    "    def generate_single_noise(self):\n",
    "      z = torch.empty(self.z_dim).uniform_(-1,1).cuda()\n",
    "      return self.forward_single_z(z)\n",
    "\n",
    "\n",
    "    def make_triplet_samples(self, z, margin, r2, r3):\n",
    "      positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "      negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "      return positive_sample, negative_sample\n",
    "\n",
    "    def random_vector_surface(self, shape, r = 1.):\n",
    "      mat = torch.randn(size=shape).cuda()\n",
    "      norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "      return (mat/norm) * r\n",
    "\n",
    "\n",
    "    def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "      fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "      fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "      fraction.unsqueeze_(-1)\n",
    "      return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "    def make_z(self, in_shape):\n",
    "      return torch.empty(in_shape).cuda().uniform_(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def js_distance(x1, x2):\n",
    "  m = 0.5 * (x1 + x2)\n",
    "  return 0.5 * (F.kl_div(x1, m) + F.kl_div(x2, m))\n",
    "\n",
    "def kl_distance(x1, x2):\n",
    "  inp = torch.log(x1)\n",
    "  target = x2\n",
    "  return F.kl_div(inp, target, reduction='batchmean')\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  x1 = tensorify(x1)\n",
    "  x2 = tensorify(x2)\n",
    "  x1 = x1 / torch.sum(x1)\n",
    "  x2 = x2 / torch.sum(x2)\n",
    "  return kl_distance(x1[None], x2[None])\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  if not isinstance(x1, torch.Tensor): x1 = torch.tensor(x1)\n",
    "  if not isinstance(x2, torch.Tensor): x2 = torch.tensor(x2)\n",
    "  x1 = x1 * 100. / torch.sum(x1)\n",
    "  x2 = x2 * 100. / torch.sum(x2)\n",
    "  return torch.norm(x1 - x2, 2)\n",
    "\n",
    "def distance_from_uniform(x):\n",
    "  return distrib_distance(x, [1.] * len(x))\n",
    "\n",
    "def wasserstein_distance(x1, x2):\n",
    "  return torch.mean(x1 - x2)\n",
    "\n",
    "def l1_distance(x1, x2):\n",
    "  return F.l1_loss(x1, x2)\n",
    "\n",
    "def l2_distance(x1, x2):\n",
    "  return F.mse_loss(x1 * 10, x2 * 10)\n",
    "\n",
    "def mse_loss(x1, x2):\n",
    "  return F.mse_loss(x1, x2)\n",
    "\n",
    "def cos_distance(x1, x2, dim = 1):\n",
    "  return -1 * torch.mean(F.cosine_similarity(x1, x2, dim=dim))\n",
    "\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, distance_func, margin):\n",
    "  # max distance when using l1_distance is 2\n",
    "  # max distacne when using l2-distance is sqrt(2)\n",
    "  ap_dist = distance_func(anchor, positive)\n",
    "  an_dist = distance_func(anchor, negative)\n",
    "\n",
    "  triplet_loss.call_count += 1\n",
    "  if triplet_loss.call_count % 200 == 0 : #and anchor.shape[1] == 1000:\n",
    "#     print(\"a: \", end=\"\"); print_big_vector(anchor[0])\n",
    "#     print(\"p: \", end=\"\"); print_big_vector(positive[0])\n",
    "#     print(\"n: \", end=\"\"); print_big_vector(negative[0])\n",
    "    print(\"func:{}, ap_dist: {}, an_dist: {}\".format(distance_func.__name__, ap_dist, an_dist))\n",
    "    \n",
    "  return torch.mean(F.relu(ap_dist - an_dist + margin))\n",
    "\n",
    "triplet_loss.call_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def diversity_loss(embeddings, z_s):\n",
    "#   size = z_s.shape[0]\n",
    "#   result = 0\n",
    "#   for i in range(size):\n",
    "#     for j in range(i+1, size):\n",
    "# #       a = F.cosine_similarity(embeddings[i], embeddings[j], dim = 0)\n",
    "# #       b = torch.norm(z_s[i] - z_s[j], 2, dim = 0)\n",
    "# #       print('embeddings: ')\n",
    "# #       print_big_vector(embeddings[i])\n",
    "# #       print_big_vector(embeddings[j])\n",
    "# #       print(f'a: {a}, b:{b}, multiple: {a*b}')\n",
    "#       result += F.cosine_similarity(embeddings[i], embeddings[j], dim = 0) * \\\n",
    "#                 torch.norm(z_s[i] - z_s[j], 2, dim = 0)\n",
    "#   n_pairs = (size * (size - 1)) / 2\n",
    "#   mean = result / n_pairs \n",
    "# #   print(f'result {result}, n_pairs {n_pairs}, mean {mean}')\n",
    "#   return mean\n",
    "  \n",
    "\n",
    "# # normalized with shuffling\n",
    "# def diversity_loss(embeddings, deranged_embeddings, z_s, deranged_z_s):\n",
    "#     cos_similarity = F.cosine_similarity(embeddings, deranged_embeddings)\n",
    "#     z_distance = torch.norm(z_s - deranged_z_s, 2, dim = 1)\n",
    "#     return torch.mean(cos_similarity * z_distance)\n",
    "  \n",
    "# # normalized with shuffling\n",
    "# def diversity_loss(embeddings, z_s):\n",
    "#     deranged_embeddings, deranged_z_s = derange(embeddings, z_s)\n",
    "#     cos_similarity = F.cosine_similarity(embeddings, deranged_embeddings)\n",
    "#     z_distance = torch.norm(z_s - deranged_z_s, dim = 1)\n",
    "#     max_possible_z_distance = 6.3246\n",
    "#     return torch.mean(cos_similarity * (z_distance/max_possible_z_distance))\n",
    "\n",
    "\n",
    "def diversity_loss(input, target):\n",
    "#   return -1 * torch.mean(torch.pow(f_x_a-f_x_s,2))\n",
    "  if input.shape[0] != batch_size:\n",
    "    print(\"input shape: \", input.shape)\n",
    "    print(\"target shape: \", target.shape, \"\\n\\n\")\n",
    "  return torch.mean(F.cosine_similarity(\n",
    "    input.view([batch_size, -1]),\n",
    "    target.view([batch_size, -1]), \n",
    "  ))\n",
    "\n",
    "\n",
    "if gen_arch == 'non-targeted':\n",
    "  def fool_loss(input, target):\n",
    "    true_class = torch.argmax(target, dim=1).view(-1,1).long().cuda()\n",
    "    target_probabilities = input.gather(1, true_class)\n",
    "    epsilon = 1e-10\n",
    "    result =  torch.mean(-1 * torch.log(1 - target_probabilities + epsilon))\n",
    "\n",
    "    fool_loss.call_count += 1\n",
    "    if fool_loss.call_count % 200 == 0:\n",
    "      print(\"target probs {}, loss: {}: \".format(target_probabilities, result))\n",
    "\n",
    "    return result\n",
    "\n",
    "  fool_loss.call_count = 0\n",
    "\n",
    "if gen_arch == 'targeted':\n",
    "  def fool_loss(model_output, target_labels):\n",
    "    target_labels = target_labels.view(-1, 1).long().cuda()\n",
    "    target_probabilities = model_output.gather(1, target_labels)\n",
    "    epsilon = 1e-10\n",
    "    # highest possible fool_loss is - log(1e-10) == 23\n",
    "    result = torch.mean(-1 * torch.log(target_probabilities + epsilon))\n",
    "\n",
    "    fool_loss.call_count += 1\n",
    "    if fool_loss.call_count % 200 == 0:\n",
    "      print(\"target probs {}, loss: {}: \".format(target_probabilities, result))\n",
    "\n",
    "    return result\n",
    "\n",
    "  fool_loss.call_count = 0\n",
    "\n",
    "\n",
    "def targeted_validation(gen_output, target):\n",
    "  perturbations, _, _, clean_images, _, z = gen_output\n",
    "  perturbed_images = clean_images + perturbations\n",
    "  target_labels = torch.argmax(z, 1)\n",
    "  adversary_preds = torch.argmax(arch(perturbed_images), 1)\n",
    "#   print('adv preds: ', adversary_preds.shape, adversary_preds)\n",
    "#   print('target_labels: ', target_labels.shape, target_labels)\n",
    "#   print('eq: ', (adversary_preds == target_labels))\n",
    "  return (adversary_preds == target_labels).float().mean()\n",
    "  \n",
    "\n",
    "# # targeted \n",
    "# def validation(gen_output, target):\n",
    "#   perturbations, _, _, clean_images, _, _ = gen_output\n",
    "#   return validation_(perturbations, clean_images)\n",
    "\n",
    "# # non-targeted\n",
    "# def validation(gen_output, target):\n",
    "#   perturbations, _, _, clean_images, _ = gen_output\n",
    "#   return validation_(perturbations, clean_images)\n",
    "\n",
    "# # general\n",
    "def validation(gen_output, target):\n",
    "  perturbations = gen_output[0]\n",
    "  clean_images = gen_output[3]\n",
    "  return validation_(perturbations, clean_images)\n",
    "\n",
    "unfooled_histogram = np.array([0.] * 1000)\n",
    "fooled_histogram = np.array([0.] * 1000)\n",
    "valid_cnt = 0\n",
    "\n",
    "def print_hist(unfooled, fooled):\n",
    "  indexed = [(i, u) for i, u in enumerate(unfooled)]\n",
    "  summarized = list(filter(lambda x: x[1] > 0.0, indexed))\n",
    "  total = fooled + unfooled\n",
    "\n",
    "  percent_total = [(i, 100. * u / (total[i] + 1e-10), total[i]) for i, u in enumerate(unfooled)]\n",
    "  sorted_percent_total = sorted(percent_total, key =lambda x: x[1], reverse = True)\n",
    "\n",
    "  print('\\npercent_total: ')\n",
    "  print(list(filter(lambda x: x[1] > 0.0, sorted_percent_total)))\n",
    "  print('\\n')\n",
    "  \n",
    "  return sorted_percent_total\n",
    "\n",
    "def validation_(perturbations, clean_images):\n",
    "  perturbed_images = clean_images + perturbations\n",
    "  benign_preds = torch.argmax(arch(clean_images), 1)\n",
    "  adversary_preds = torch.argmax(arch(perturbed_images), 1)\n",
    "\n",
    "  is_unfooled = (benign_preds == adversary_preds)\n",
    "  for i , unfooled in enumerate(is_unfooled):\n",
    "    if unfooled == 1:\n",
    "      unfooled_histogram[benign_preds[i]] += 1\n",
    "    else:\n",
    "      fooled_histogram[benign_preds[i]] += 1\n",
    "  \n",
    "#   global valid_cnt\n",
    "#   valid_cnt += 1\n",
    "#   if valid_cnt % 10 == 0:\n",
    "#     print_hist(unfooled_histogram, fooled_histogram)\n",
    "    \n",
    "  return (benign_preds != adversary_preds).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gen_arch == 'targeted':\n",
    "  class FeatureLoss(nn.Module):\n",
    "      def __name__(self):\n",
    "        return \"feature_loss\"\n",
    "\n",
    "      def __init__(self, dis, layers, layer_weights):\n",
    "          super().__init__()\n",
    "\n",
    "          # define generator here \n",
    "          self.dis = dis\n",
    "          self.diversity_layers = layers\n",
    "          self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "          self.weights = layer_weights\n",
    "          self.metric_names = [\"fool_loss\"] #+ [f\"div_loss_{i}\" for i in range(len(layers))] #maybe Gram\n",
    "  #         self.triplet_hooks = hook_outputs([arch.m.features[4]], detach=False)\n",
    "\n",
    "      def make_features(self, x, clone=False):\n",
    "          y = self.dis(x)\n",
    "          return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "\n",
    "      def forward(self, inp, target):\n",
    "        sigma_B, _, _, X_B, B_Y, z = inp\n",
    "\n",
    "        X_A = X_B + sigma_B\n",
    "  #       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "        A_Y, A_feat = self.make_features(X_A)\n",
    "  #       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "        chosen_labels = z.argmax(dim=1)\n",
    "        fooling_loss =  fool_loss(A_Y, chosen_labels)\n",
    "\n",
    "  #       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "  #       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "  #       self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "  #       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "        self.losses = [fooling_loss]\n",
    "        self.metrics = dict(zip(self.metric_names, [fooling_loss]))\n",
    "\n",
    "        return sum(self.losses)\n",
    "\n",
    "      def add_perturbation_shuffled(self, inp, perturbation):\n",
    "  #       j = torch.randperm(inp.shape[0])\n",
    "          j = derangement(inp.shape[0])\n",
    "          return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derange(*args):\n",
    "  if len(args) == 0: raise ValueError('shuffle function needs atleast one argument')\n",
    "  deranged_indexes = derangement(args[0].shape[0])\n",
    "  if not all([args[0].shape[0] == arg.shape[0] for arg in args]): \n",
    "    raise ValueError('inputs to shuffle must all have the same 0th dimension')\n",
    "  return [arg[deranged_indexes] for arg in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gen_arch == 'non-targeted':\n",
    "  class FeatureLoss(nn.Module):\n",
    "      def __name__(self):\n",
    "        return \"feature_loss\"\n",
    "\n",
    "      def __init__(self, dis, layers, layer_weights):\n",
    "          super().__init__()\n",
    "\n",
    "          self.dis = dis\n",
    "          self.diversity_layers = layers\n",
    "          self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "          self.weights = layer_weights\n",
    "\n",
    "  #         self.metric_names = [\"fool_loss\"] + [f\"div_loss_{i}\" for i in range(len(layers))] + ['triplet_loss']# Maybe Gram\n",
    "  #         self.metric_names = [\"div_loss\"] + ['triplet_loss']# Maybe Gram\n",
    "          self.metric_names = [\"fool_loss\"] + ['div_loss']# Maybe Gram\n",
    "          self.triplet_weight = 4.\n",
    "          self.div_weight = 1.\n",
    "          self.fooling_weight = 1.\n",
    "\n",
    "      def make_features(self, x, clone=False):\n",
    "          y = self.dis(x)\n",
    "          return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "\n",
    "      # contrastive loss\n",
    "      def forward(self, inp, target):\n",
    "          sigma_B, sigma_pos, sigma_neg, X_B, z_B = inp\n",
    "\n",
    "          deranged_perturbations, deranged_z_s = derange(sigma_B, z_B)\n",
    "\n",
    "          X_A = X_B + sigma_B\n",
    "          X_S = X_B + deranged_perturbations\n",
    "#           X_A_pos = X_B + sigma_pos\n",
    "#           X_A_neg = X_B + sigma_neg\n",
    "\n",
    "          B_Y, _ = self.make_features(X_B)\n",
    "          A_Y, A_feat = self.make_features(X_A)\n",
    "          _, S_feat = self.make_features(X_S)\n",
    "#           pos_softmax, _ = self.make_features(X_A_pos)\n",
    "#           neg_softmax, _ = self.make_features(X_A_neg)\n",
    "\n",
    "          raw_fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "          weighted_fooling_loss = self.fooling_weight * raw_fooling_loss\n",
    "\n",
    "#           raw_diversity_loss = diversity_loss(A_feat[0], S_feat[0], z_B, deranged_z_s)\n",
    "          raw_diversity_loss = diversity_loss(A_feat[0], S_feat[0])\n",
    "          weighted_diversity_loss = raw_diversity_loss * self.div_weight\n",
    "\n",
    "#           raw_triplet_loss = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "#           weighted_triplet_loss = raw_triplet_loss * self.triplet_weight\n",
    "\n",
    "  #         self.losses = weighted_diversity_losses + [weighted_triplet_loss]\n",
    "  #         raw_losses = raw_diversity_losses + [raw_triplet_loss]\n",
    "\n",
    "          self.losses = [weighted_fooling_loss] + [weighted_diversity_loss] #+ [weighted_triplet_loss]\n",
    "          raw_losses = [raw_fooling_loss] + [raw_diversity_loss] #+ [raw_triplet_loss]\n",
    "\n",
    "  #         self.losses = [fooling_loss] + [weighted_triplet_loss]\n",
    "  #         self.metrics = dict(zip(self.metric_names, [fooling_loss] + [raw_triplet_loss]))\n",
    "\n",
    "          if len(self.metric_names) != len(raw_losses):\n",
    "            raise Exception(\"length of metric names unequals length of losses\")\n",
    "\n",
    "          self.metrics = dict(zip(self.metric_names, raw_losses))\n",
    "          return sum(self.losses)\n",
    "\n",
    "\n",
    "\n",
    "  # #     triplet loss\n",
    "  #     def forward(self, inp, target):\n",
    "  #         sigma_B, sigma_pos, sigma_neg, X_B, _ = inp\n",
    "\n",
    "  #         X_A = self.add_perturbation(X_B, sigma_B) \n",
    "  #         X_A_pos = self.add_perturbation(X_B, sigma_pos)\n",
    "  #         X_A_neg = self.add_perturbation(X_B, sigma_neg) \n",
    "  #         X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "  # #         B_Y, _ = self.make_features(X_B)\n",
    "  #         A_Y, A_feat = self.make_features(X_A)\n",
    "  # #         _, S_feat = self.make_features(X_S)\n",
    "  #         pos_softmax, _ = self.make_features(X_A_pos)\n",
    "  #         neg_softmax, _ = self.make_features(X_A_neg)\n",
    "\n",
    "  # #         raw_fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "  # #         weighted_fooling_loss = self.fooling_weight * raw_fooling_loss\n",
    "\n",
    "  #         raw_diversity_losses = [diversity_loss(a_f, s_f, sigma_B, ) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "  #         weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "  #         raw_triplet_loss = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "  #         weighted_triplet_loss = raw_triplet_loss * self.triplet_weight\n",
    "\n",
    "  #         self.losses = weighted_diversity_losses + [weighted_triplet_loss]\n",
    "  #         raw_losses = raw_diversity_losses + [raw_triplet_loss]\n",
    "\n",
    "  # #         self.losses = [weighted_fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss]\n",
    "  # #         raw_losses = [raw_fooling_loss] + raw_diversity_losses + [raw_triplet_loss]\n",
    "\n",
    "  # #         self.losses = [fooling_loss] + [weighted_triplet_loss]\n",
    "  # #         self.metrics = dict(zip(self.metric_names, [fooling_loss] + [raw_triplet_loss]))\n",
    "\n",
    "  #         if len(self.metric_names) != len(raw_losses):\n",
    "  #           raise Exception(\"length of metric names unequals length of losses\")\n",
    "\n",
    "  #         self.metrics = dict(zip(self.metric_names, raw_losses))\n",
    "  #         return sum(self.losses)\n",
    "\n",
    "\n",
    "  #     #use two types of triplet losses\n",
    "  #     def forward(self, inp, target):\n",
    "  #       sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "  #       X_A = self.add_perturbation(X_B, sigma_B) \n",
    "  #       X_A_pos = self.add_perturbation(X_B, sigma_pos)\n",
    "  #       X_A_neg = self.add_perturbation(X_B, sigma_neg) \n",
    "\n",
    "  #       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "  #       B_Y, _ = self.make_features(X_B)\n",
    "  #       A_Y, A_feat = self.make_features(X_A)\n",
    "  #       _, S_feat = self.make_features(X_S)\n",
    "  #       pos_softmax, _ = self.make_features(X_A_pos)\n",
    "  #       neg_softmax, _ = self.make_features(X_A_neg)\n",
    "\n",
    "  #       fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "  #       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "  #       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "  #       raw_triplet_loss_sm = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "  #       weighted_triplet_loss_sm = raw_triplet_loss_sm * self.triplet_weight_sm\n",
    "\n",
    "  #       raw_triplet_loss_noise = triplet_loss(sigma_B, sigma_pos, sigma_neg, l2_distance, 5.)\n",
    "  #       weighted_triplet_loss_noise = raw_triplet_loss_noise * self.triplet_weight_noise\n",
    "\n",
    "  #       self.losses = [fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss_sm, weighted_triplet_loss_noise] \n",
    "  #       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses + [weighted_triplet_loss_sm, weighted_triplet_loss_noise]))\n",
    "\n",
    "  #       return sum(self.losses)\n",
    "\n",
    "  #     # just fooling and diversity\n",
    "  #     def forward(self, inp, target):\n",
    "  #       sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "  #       X_A = self.add_perturbation(X_B, sigma_B) \n",
    "\n",
    "  #       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "  #       B_Y, _ = self.make_features(X_B)\n",
    "  #       A_Y, A_feat = self.make_features(X_A)\n",
    "  #       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "  #       fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "  #       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "  #       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "  #       self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "  #       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "  #       return sum(self.losses)\n",
    "\n",
    "\n",
    "      def add_perturbation_shuffled(self, inp, perturbation):\n",
    "        j = derangement(inp.shape[0])\n",
    "        return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def produce_summary(root_folder, n_files):\n",
    "  def writeline(file, values, fmt_string):\n",
    "    file.write(', '.join(fmt_string.format(v) for v in values) + '\\n')\n",
    "  \n",
    "  last_rows = []\n",
    "  for i in range(n_files):\n",
    "    prefix = '/root/Derakhshani/adversarial/textual_notes/CSVs'\n",
    "    df = pd.read_csv(\"{}/{}/{}.csv\".format(prefix, root_folder, i))\n",
    "    last_rows.append(df.iloc[-1][1:-1].values.tolist())\n",
    "  \n",
    "  last_rows = np.array(last_rows)\n",
    "  \n",
    "  labels = list(df.columns[1:-1])\n",
    "  means = np.mean(last_rows, axis=0).tolist()\n",
    "  outfile = open('{}/{}/summary.txt'.format(prefix, root_folder), 'w+')\n",
    "  outfile.write('means: \\n')\n",
    "  writeline(outfile, labels, '{: >20}')\n",
    "  writeline(outfile, means, '{: >20.3}')\n",
    "  outfile.write('\\n')\n",
    "      \n",
    "  operations = []\n",
    "  for column in df.columns[1:-1]:\n",
    "    if column in ['train_loss', 'valid_loss', 'fool_loss', 'triplet_loss'] or column[:8] == 'div_loss':\n",
    "      operations.append('min')\n",
    "    elif column in ['validation', 'targeted_validation', 'div_metric', 'entropy']:\n",
    "      operations.append('max')\n",
    "    else:\n",
    "      raise ValueError('column {} is not recognized'.format(column))\n",
    "    \n",
    "  results = []\n",
    "  indexes = []\n",
    "  \n",
    "  for i in range(len(operations)):\n",
    "    values = last_rows[:, i]\n",
    "    if operations[i] == 'max': operation = np.max\n",
    "    elif operations[i] == 'min': operation = np.min\n",
    "    result = operation(values)\n",
    "    results.append(result)\n",
    "    indexes.append(values.tolist().index(result))\n",
    "  \n",
    "  outfile.write('bests: \\n')\n",
    "  writeline(outfile, labels, '{: >20}')\n",
    "  writeline(outfile, operations, '{: >20}')\n",
    "  writeline(outfile, results, '{: >20.3}')\n",
    "  writeline(outfile, indexes, '{: >20}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils import dir_util \n",
    "\n",
    "def investigate_initial_settings(n_settings, n_epochs, lr, wd, results_dir):\n",
    "  os.mkdir(env.get_csv_dir() + results_dir)\n",
    "  os.mkdir(env.get_models_dir() + results_dir)\n",
    "  \n",
    "  for setting_ind in range(n_settings):\n",
    "    print(f\"investigation no: {setting_ind}\")\n",
    "    learn = None; gen = None; gc.collect()\n",
    "    gen = Gen(z_dim = z_dim)\n",
    "    init_cnn(gen, True)\n",
    "    \n",
    "    tmp_csv_filename =  env.temp_csv_path + '/' + results_dir + '/' + str(setting_ind)\n",
    "    csv_logger = partial(ImmediateCSVLogger, filename=tmp_csv_filename)\n",
    "\n",
    "    if gen_arch == 'non-targeted':\n",
    "      metrics = [validation]\n",
    "    elif gen_arch == 'targeted':\n",
    "      metrics = [validation, targeted_validation]\n",
    "      \n",
    "    learn = Learner(data, gen, loss_func = feat_loss, metrics=metrics, \n",
    "                    model_dir = env.get_learner_models_dir(), \n",
    "                    callback_fns=[DiversityMetric, LossMetrics, csv_logger])\n",
    "    \n",
    "    saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=model.__name__ + \"-best\")\n",
    "    saver_every_epoch = SaveModelCallback(learn, every='epoch', name=model.__name__)\n",
    "\n",
    "    learn.fit(n_epochs, lr=lr, wd = wd, callbacks=[saver_best, saver_every_epoch])\n",
    "    \n",
    "    shutil.copyfile(tmp_csv_filename + \".csv\", env.get_csv_dir() + results_dir + '/' + str(setting_ind) + '.csv')\n",
    "    \n",
    "    model_dest = env.get_models_dir() + results_dir + '/' + str(setting_ind)\n",
    "    os.mkdir(model_dest)\n",
    "    dir_util.copy_tree(env.data_path/env.get_learner_models_dir(), model_dest)\n",
    "    shutil.rmtree(env.data_path/env.get_learner_models_dir())  \n",
    "    \n",
    "  produce_summary(results_dir, n_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_perturbations(learn, n_perturbations):\n",
    "  initial_training_mode = learn.model.training\n",
    "  gen = learn.model.eval()\n",
    "  perturbations = [gen.generate_single_noise() for _ in range(n_perturbations)]\n",
    "  learn.model.train(initial_training_mode)  \n",
    "  return perturbations\n",
    "\n",
    "def compute_prediction_histogram(learn, perturbation, verbose=False):\n",
    "  pred_hist = [0] * 1000\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 and verbose: print (\"at batch no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbation[None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      pred_hist[pred] += 1\n",
    "  return pred_hist\n",
    "\n",
    "\n",
    "def compute_mean_prediction_histogram(learn, perturbations, verbose=False):\n",
    "  pred_hist = torch.tensor([0] * 1000).detach_()\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 5 == 0 and verbose: print(f\"at batch no {batch_no}\")\n",
    "    for j, perturbation in enumerate(perturbations):\n",
    "      perturbed_batch = batch + perturbation[None]\n",
    "      preds = arch(perturbed_batch).argmax(1)\n",
    "      for pred in preds:\n",
    "        pred_hist[pred] += 1\n",
    "  pred_hist = pred_hist.float() / len(perturbations)\n",
    "  return pred_hist.tolist()\n",
    "\n",
    "\n",
    "def classes_needed_to_reach(percentage, hist):\n",
    "  hist_sum = np.sum(hist)\n",
    "  indexed_hist = [(i, hist_element) for i,hist_element in  \n",
    "                          enumerate(hist)]\n",
    "  sorted_hist = sorted(indexed_hist, key=lambda x: x[1], reverse = True)\n",
    "  \n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = sorted_hist[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / hist_sum) * 100.\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, sorted_hist\n",
    "\n",
    "def diversity(learn, n_perturbations, percentage = 95, verbose = True):\n",
    "  pred_histogram = compute_mean_prediction_histogram(\n",
    "      learn, generate_perturbations(learn, n_perturbations), verbose\n",
    "  )\n",
    "  print(\"finished creating the prediction histogram\")\n",
    "\n",
    "  return classes_needed_to_reach(95, pred_histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiversityMetric(LearnerCallback):\n",
    "  _order = -20 # Needs to run before the recorder\n",
    "  \n",
    "  def __init__(self, learn):\n",
    "    super().__init__(learn)\n",
    "    self.average_over = 4\n",
    "    self.n_perturbations = 10\n",
    "    self.percentage = 95\n",
    "  \n",
    "  def on_train_begin(self, **kwargs):\n",
    "    self.learn.recorder.add_metric_names(['div_metric', 'entropy'])\n",
    "    \n",
    "  def on_epoch_begin(self, **kwargs):\n",
    "    global learn\n",
    "    self.perturbations_list = [generate_perturbations(self.learn, self.n_perturbations) \\\n",
    "                          for _ in range(self.average_over)]\n",
    "    self.pred_hist_list = [torch.tensor([0] * 1000).detach_() for _ in range(self.average_over)]\n",
    "    \n",
    "  def on_batch_end(self, last_output, train, **kwargs):\n",
    "    if not train:\n",
    "      images = last_output[3]; assert(images.shape[1:] == (3,224, 224))\n",
    "      for perturbations, pred_hist in zip(self.perturbations_list, self.pred_hist_list):\n",
    "        for j, perturbation in enumerate(perturbations):\n",
    "          perturbed_batch = images + perturbation[None]\n",
    "          preds = arch(perturbed_batch).argmax(1)\n",
    "          for pred in preds:\n",
    "            pred_hist[pred] += 1\n",
    "  \n",
    "  def on_epoch_end(self, last_metrics, **kwargs):\n",
    "    for i in range(len(self.pred_hist_list)):\n",
    "      self.pred_hist_list[i] = (self.pred_hist_list[i].float() / self.n_perturbations).tolist()\n",
    "    \n",
    "    div_metric_list = [classes_needed_to_reach(self.percentage, pred_hist)[0] \\\n",
    "                          for pred_hist in self.pred_hist_list]\n",
    "    entropy_list = [entropy(pred_hist) for pred_hist in self.pred_hist_list]\n",
    "    return add_metrics(last_metrics, [np.mean(div_metric_list), np.mean(entropy_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetedDiversityMetric(DiversityMetric):\n",
    "    def __init__(self, n_perturbations, percentage):\n",
    "      super().__init__(n_perturbations, percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoolingWeightScheduler(LearnerCallback):\n",
    "  def __init__(self, learn: Learner):\n",
    "    super().__init__(learn)\n",
    "    self.weights_history = []\n",
    "    self.fooling_loss_history = []\n",
    "  \n",
    "  def get_metric_value(self, metric_name):\n",
    "    for value, name in zip(self.learn.recorder.metrics[-1],self.learn.recorder.names[3:-1]):\n",
    "      if name == metric_name:\n",
    "        return value\n",
    "    raise ValueError('Could not find {} metric.'.format(metric_name))\n",
    "  \n",
    "  def on_epoch_end(self, last_metrics, **kwargs):\n",
    "    # history keeping\n",
    "    self.weights_history.append((kwargs['epoch'], self.learn.loss_func.fooling_weight))\n",
    "    \n",
    "    # the actual functionality\n",
    "    fooling_loss = self.get_metric_value('fool_loss')\n",
    "    self.fooling_loss_history.append(fooling_loss)\n",
    "    \n",
    "    if len(self.weights_history) < 2:\n",
    "      return\n",
    "    \n",
    "    if self.fooling_loss_history[-1] > self.fooling_loss_history[-2]:\n",
    "      self.learn.loss_func.fooling_weight += 0.3    \n",
    "      print('fooling weight increased to {} at the end of epoch {}'.format(\n",
    "        self.learn.loss_func.fooling_weight, kwargs['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tltucTv2ep9-"
   },
   "outputs": [],
   "source": [
    "# mode = 'sanity_check'\n",
    "mode = 'normal'\n",
    "# mode = 'div_metric_calc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.resnet50\n",
    "# model = models.resnet152\n",
    "# model = models.vgg16_bn\n",
    "model = torchvision.models.googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SO1h55obXzOv",
    "outputId": "54414cc5-84d5-4f45-ecab-87374a58dd33"
   },
   "outputs": [],
   "source": [
    "if mode == \"normal\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "elif mode == \"sanity_check\":\n",
    "  env.load_dataset('dataset_sanity_check_small', 'dataset_sanity_check_small')  \n",
    "  env.set_data_path('dataset_sanity_check_small')\n",
    "elif mode == \"div_metric_calc\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "  env.load_test_dataset(str(env.data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "koaQZmjMom7w"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "gpu_flag = True\n",
    "nag_util.batch_size = batch_size; nag_util.gpu_flag = gpu_flag;\n",
    "tfms = get_transforms(do_flip=False, max_rotate=0)\n",
    "data = (ImageList.from_folder(env.data_path)\n",
    "        .split_by_folder(valid=('test' if mode == 'div_metric_calc' else 'valid'))\n",
    "        .label_from_folder()\n",
    "        .transform(tfms, size=224)\n",
    "        .databunch(bs=batch_size, num_workers=1)\n",
    "        .normalize(imagenet_stats))\n",
    "\n",
    "# data.show_batch(rows=2, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDBkRV8yovwV"
   },
   "outputs": [],
   "source": [
    "if gen_arch == \"non-targeted\":\n",
    "  z_dim = 10\n",
    "elif gen_arch == \"targeted\":\n",
    "  z_dim = 1000\n",
    "  \n",
    "arch = SoftmaxWrapper(model(pretrained=True).cuda().eval())\n",
    "nag_util.arch = arch\n",
    "requires_grad(arch, False)\n",
    "\n",
    "# vgg:\n",
    "# layers = []\n",
    "# blocks = [i-1 for i,o in enumerate(children(arch.features)) if isinstance(o, nn.MaxPool2d)]\n",
    "# layers = [arch.features[i] for i in blocks]\n",
    "# layer_weights = [1] * len(layers)\n",
    "\n",
    "layers = [\n",
    "    arch.softmax\n",
    "]\n",
    "\n",
    "layer_weights = [1.] * len(layers)\n",
    "\n",
    "# inception:\n",
    "# layers = [\n",
    "#     arch.Conv2d_1a_3x3,\n",
    "#     arch.Mixed_6e,\n",
    "#     arch.Mixed_7a,\n",
    "#     arch.fc    \n",
    "# ]\n",
    "# layer_weights = [1.0/4.0] * len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qd9gXUy_ovww"
   },
   "outputs": [],
   "source": [
    "feat_loss = FeatureLoss(arch, layers, layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRAnneal(LearnerCallback):\n",
    "  _order = -20 # Needs to run before the recorder\n",
    "  \n",
    "  def __init__(self, learn, final_value):\n",
    "    super().__init__(learn)\n",
    "    self.final_value = final_value\n",
    "  \n",
    "  def on_train_begin(self, **kwargs):\n",
    "    self.initial_value = self.opt.lr\n",
    "    self.learn.recorder.add_metric_names(['lr'])\n",
    "  \n",
    "  def on_epoch_end(self, epoch, n_epochs, last_metrics, **kwargs):\n",
    "    self.opt.lr = annealing_linear(self.initial_value, self.final_value, float(epoch) / n_epochs)\n",
    "    return add_metrics(last_metrics, self.opt.lr)\n",
    "  \n",
    "# class LRMonitor(LearnerCallBack):\n",
    "#   def __init__(self, learn):\n",
    "#     super().__init__(learn)\n",
    "#     self.name = 'lr'\n",
    "    \n",
    "#   def on_epoch_end(self, last_metrics, **kwargs):\n",
    "#     return add_metrics(last_metrics, self.opt.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; import time\n",
    "\n",
    "class FileControl(LearnerCallback):\n",
    "    def __init__(self, learn, root_folder, gen):\n",
    "      super().__init__(learn)\n",
    "      self.root_folder = root_folder\n",
    "      self.gen = gen\n",
    "      \n",
    "    def on_epoch_end(self, epoch, **kwargs):\n",
    "      with open(self.root_folder + '/ctrl.txt', 'r') as control_file:\n",
    "        control_data = json.loads(control_file.read())\n",
    "      \n",
    "      if str(epoch) in control_data:\n",
    "        action = control_data[str(epoch)]\n",
    "        self.epoch = epoch\n",
    "        return self.perform_action(action)\n",
    "        \n",
    "    def perform_action(self, action):\n",
    "      if action == 'stop':\n",
    "        return {'stop_training': True}\n",
    "      elif action == 'ask':\n",
    "        print('prompted to ask for action at epoch {}:'.format(self.epoch))\n",
    "        new_action = input()\n",
    "        self.perform_action(new_action)\n",
    "      elif action == 'ask_file':\n",
    "        return self.ask_from_file()\n",
    "      elif action == 'double_labels':\n",
    "        self.gen.n_active_labels = min(self.gen.n_active_labels * 2, 1000)\n",
    "        print('increased n_active_labels to {} at end of epoch {}'.format(self.gen.n_active_labels ,self.epoch))\n",
    "      elif action == 'continue':\n",
    "        return \n",
    "      else:\n",
    "        print('invalid action: \\\"{}\\\". please enter a valid action:'.format(action))\n",
    "        return self.perform_action(input())\n",
    "    \n",
    "    def ask_from_file(self):\n",
    "      wait_file = open(self.root_folder + '/wait.txt', 'w')\n",
    "      while True:\n",
    "        if not os.path.isfile(self.root_folder + '/answer.txt'):\n",
    "          open(self.root_folder + '/answer.txt', 'x')\n",
    "        answers_file = open(self.root_folder + '/answer.txt', 'r')\n",
    "        action = answers_file.read().strip()\n",
    "        if action in ['stop', 'double_labels', 'continue']:\n",
    "          print('action read: \\\"{}\\\"'.format(action))\n",
    "          wait_file.close()\n",
    "          os.remove(self.root_folder + '/wait.txt')\n",
    "          return self.perform_action(action)\n",
    "        else:\n",
    "          wait_file.truncate(0)\n",
    "          wait_file.write('invalid action \\\"{}\\\"\\n'.format(action))\n",
    "          wait_file.flush()\n",
    "          time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.save_filename = 'resnet50_65' #resnet50_64\n",
    "# env.save_filename = 'resnet50_17'\n",
    "env.save_filename = 'googlenet_31x'\n",
    "\n",
    "if Path(env.get_csv_path() + '.csv').exists(): raise FileExistsError(\"csv_path already exists\")\n",
    "if Path(env.get_models_path()).exists(): raise FileExistsError(\"models_path already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9J20CBLS8S9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models_directory returned is:  models/626\n"
     ]
    }
   ],
   "source": [
    "learn = None; gen = None; gc.collect()\n",
    "csv_logger = partial(ImmediateCSVLogger, filename= env.temp_csv_path + '/' + env.save_filename)\n",
    "gen = Gen(z_dim=z_dim)\n",
    "init_cnn(gen, True)\n",
    "\n",
    "if gen_arch == 'non-targeted':\n",
    "  metrics = [validation]\n",
    "elif gen_arch == 'targeted':\n",
    "  metrics = [validation, targeted_validation]\n",
    "    \n",
    "learn = Learner(data, gen, loss_func = feat_loss, model_dir = env.get_learner_models_dir(),\n",
    "                metrics=metrics, callback_fns=[DiversityMetric, LossMetrics, csv_logger])\n",
    "\n",
    "# learn = Learner(data, Gen(z_dim=10), loss_func = feat_loss, metrics=[validation], callback_fns=LossMetrics, opt_func = optim.SGD)\n",
    "# learn = Learner(data, Gen(z_dim=z_dim), loss_func = feat_loss, metrics=[validation], callback_fns=[LossMetrics, DiversityWeightsScheduler])\n",
    "\n",
    "# load_starting_point(learn, model.__name__, z_dim)\n",
    "# random_seed(42, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wOZYzOHDEdB"
   },
   "outputs": [],
   "source": [
    "# !cp \"/content/gdrive/My Drive/DL/models/vgg16_12-last.pth\"  \"/content/\"\n",
    "# learn.load('/content/vgg16_12-last')\n",
    "\n",
    "# load_filename = 'resnet50-11_39'\n",
    "# load_filename = 'resnet50_startpoint_0'\n",
    "# load_filename = 'googlenet_13_attempt5/googlenet_13_attempt5_29'\n",
    "# load_filename = 'investigate_googlenet_4/1/googlenet_1'\n",
    "# load_filename = 'vgg16_30/vgg16_30_69'\n",
    "# load_filename = 'vgg16_12-last'\n",
    "# load_filename = 'googlenet_25_attempt2/googlenet_25_attempt2_399'\n",
    "# load_filename = 'googlenet_28_labelset1/googlenet_28_labelset1_59'\n",
    "# load_filename = None\n",
    "\n",
    "# learn.load('/root/Derakhshani/adversarial/models/' + load_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find(1e-6, 100)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "investigation no: 0\n",
      "models_directory returned is:  models/626\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>targeted_validation</th>\n",
       "      <th>div_metric</th>\n",
       "      <th>entropy</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10.744584</td>\n",
       "      <td>10.771447</td>\n",
       "      <td>0.544000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>777.000000</td>\n",
       "      <td>9.600423</td>\n",
       "      <td>10.771447</td>\n",
       "      <td>05:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.172745</td>\n",
       "      <td>10.284722</td>\n",
       "      <td>0.582000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>503.750000</td>\n",
       "      <td>8.257477</td>\n",
       "      <td>10.284720</td>\n",
       "      <td>05:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10.024682</td>\n",
       "      <td>10.159061</td>\n",
       "      <td>0.594000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>482.500000</td>\n",
       "      <td>8.370548</td>\n",
       "      <td>10.159064</td>\n",
       "      <td>05:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[2.9663e-04],\n",
      "        [1.0759e-05],\n",
      "        [1.4834e-05],\n",
      "        [6.0310e-06],\n",
      "        [4.4232e-06],\n",
      "        [4.3916e-04],\n",
      "        [1.9627e-06],\n",
      "        [2.4815e-05],\n",
      "        [1.4166e-04],\n",
      "        [4.1525e-07],\n",
      "        [8.9322e-06],\n",
      "        [1.3281e-06],\n",
      "        [1.2958e-06],\n",
      "        [1.1894e-08],\n",
      "        [2.4099e-05],\n",
      "        [7.1813e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.84316635131836: \n",
      "target probs tensor([[1.4678e-04],\n",
      "        [2.6139e-05],\n",
      "        [1.6108e-06],\n",
      "        [1.2389e-05],\n",
      "        [7.2482e-06],\n",
      "        [1.2331e-05],\n",
      "        [2.6998e-05],\n",
      "        [1.6979e-06],\n",
      "        [5.2514e-05],\n",
      "        [9.3467e-05],\n",
      "        [5.0097e-07],\n",
      "        [1.6394e-06],\n",
      "        [1.1539e-05],\n",
      "        [1.2213e-05],\n",
      "        [3.9165e-04],\n",
      "        [3.4390e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.314268112182617: \n",
      "target probs tensor([[2.8408e-05],\n",
      "        [7.3689e-05],\n",
      "        [7.7969e-06],\n",
      "        [8.5815e-06],\n",
      "        [2.4277e-05],\n",
      "        [5.9522e-06],\n",
      "        [1.3762e-04],\n",
      "        [1.3199e-04],\n",
      "        [1.7790e-05],\n",
      "        [1.0545e-04],\n",
      "        [2.1951e-04],\n",
      "        [9.5594e-05],\n",
      "        [1.0264e-04],\n",
      "        [6.6376e-05],\n",
      "        [2.0392e-05],\n",
      "        [3.5615e-06]], device='cuda:0'), loss: 10.238591194152832: \n",
      "Better model found at epoch 0 with validation value: 0.5440000295639038.\n",
      "target probs tensor([[3.5162e-04],\n",
      "        [2.1946e-05],\n",
      "        [1.0850e-05],\n",
      "        [1.5428e-05],\n",
      "        [8.7826e-05],\n",
      "        [2.1942e-05],\n",
      "        [6.6137e-05],\n",
      "        [8.3221e-06],\n",
      "        [8.4341e-06],\n",
      "        [4.6058e-04],\n",
      "        [1.5203e-04],\n",
      "        [1.6237e-05],\n",
      "        [2.2778e-04],\n",
      "        [9.2885e-05],\n",
      "        [1.5504e-04],\n",
      "        [5.6198e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.018513679504395: \n",
      "target probs tensor([[4.4816e-05],\n",
      "        [7.6458e-04],\n",
      "        [3.6297e-04],\n",
      "        [7.8540e-06],\n",
      "        [1.8020e-05],\n",
      "        [2.8325e-05],\n",
      "        [8.5681e-06],\n",
      "        [5.1721e-06],\n",
      "        [1.9426e-05],\n",
      "        [6.9324e-06],\n",
      "        [3.6125e-07],\n",
      "        [1.4936e-05],\n",
      "        [2.2446e-06],\n",
      "        [2.4137e-04],\n",
      "        [1.6085e-04],\n",
      "        [2.0786e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.582712173461914: \n",
      "target probs tensor([[1.0031e-04],\n",
      "        [1.4398e-05],\n",
      "        [5.3708e-06],\n",
      "        [6.4434e-06],\n",
      "        [6.2471e-05],\n",
      "        [7.9005e-06],\n",
      "        [1.8477e-04],\n",
      "        [7.8226e-04],\n",
      "        [5.1583e-08],\n",
      "        [9.1645e-05],\n",
      "        [1.0439e-04],\n",
      "        [4.2402e-04],\n",
      "        [6.4770e-05],\n",
      "        [3.1048e-04],\n",
      "        [6.7175e-07],\n",
      "        [2.7085e-04]], device='cuda:0'), loss: 10.298731803894043: \n",
      "Better model found at epoch 1 with validation value: 0.5820000171661377.\n",
      "target probs tensor([[8.7926e-06],\n",
      "        [1.5637e-05],\n",
      "        [6.9912e-05],\n",
      "        [6.9905e-06],\n",
      "        [3.5152e-04],\n",
      "        [6.1648e-06],\n",
      "        [5.7520e-05],\n",
      "        [3.2712e-05],\n",
      "        [6.0920e-07],\n",
      "        [9.1910e-06],\n",
      "        [3.1518e-06],\n",
      "        [7.2440e-05],\n",
      "        [1.3629e-03],\n",
      "        [6.6766e-06],\n",
      "        [5.4176e-05],\n",
      "        [3.1154e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.688185691833496: \n",
      "target probs tensor([[3.8907e-06],\n",
      "        [1.5950e-05],\n",
      "        [3.6051e-04],\n",
      "        [3.4130e-06],\n",
      "        [1.2756e-04],\n",
      "        [1.8124e-05],\n",
      "        [3.4069e-04],\n",
      "        [4.5850e-06],\n",
      "        [1.1870e-06],\n",
      "        [3.9168e-05],\n",
      "        [7.3612e-05],\n",
      "        [2.0117e-04],\n",
      "        [5.5109e-06],\n",
      "        [6.2701e-05],\n",
      "        [7.6719e-05],\n",
      "        [1.0157e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.403556823730469: \n",
      "target probs tensor([[1.6454e-04],\n",
      "        [1.4457e-03],\n",
      "        [1.7289e-05],\n",
      "        [5.2711e-06],\n",
      "        [1.8062e-06],\n",
      "        [3.2603e-05],\n",
      "        [4.3534e-05],\n",
      "        [1.7255e-05],\n",
      "        [4.0905e-05],\n",
      "        [1.4040e-05],\n",
      "        [1.9686e-04],\n",
      "        [8.6495e-04],\n",
      "        [1.3319e-04],\n",
      "        [2.4446e-05],\n",
      "        [3.3252e-05],\n",
      "        [7.1006e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.806429862976074: \n",
      "Better model found at epoch 2 with validation value: 0.593999981880188.\n",
      "models_directory returned is:  models/626\n",
      "models_directory returned is:  models/626\n",
      "investigation no: 1\n",
      "models_directory returned is:  models/626\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>targeted_validation</th>\n",
       "      <th>div_metric</th>\n",
       "      <th>entropy</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.702040</td>\n",
       "      <td>9.679473</td>\n",
       "      <td>0.777000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>777.000000</td>\n",
       "      <td>9.600423</td>\n",
       "      <td>9.679471</td>\n",
       "      <td>05:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.775044</td>\n",
       "      <td>8.925582</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>256.500000</td>\n",
       "      <td>5.842975</td>\n",
       "      <td>8.925583</td>\n",
       "      <td>05:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.647020</td>\n",
       "      <td>8.817682</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>207.500000</td>\n",
       "      <td>5.046836</td>\n",
       "      <td>8.817680</td>\n",
       "      <td>05:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[9.0007e-06],\n",
      "        [5.8149e-05],\n",
      "        [5.9559e-06],\n",
      "        [5.1881e-05],\n",
      "        [7.3164e-07],\n",
      "        [7.6654e-07],\n",
      "        [2.9732e-05],\n",
      "        [7.2436e-05],\n",
      "        [3.4861e-05],\n",
      "        [1.3871e-04],\n",
      "        [3.1340e-07],\n",
      "        [2.1460e-05],\n",
      "        [3.3098e-06],\n",
      "        [3.6203e-06],\n",
      "        [3.1717e-06],\n",
      "        [1.2766e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.874212265014648: \n",
      "target probs tensor([[5.2652e-05],\n",
      "        [9.4685e-07],\n",
      "        [5.3368e-06],\n",
      "        [5.0750e-04],\n",
      "        [9.9780e-05],\n",
      "        [4.0064e-05],\n",
      "        [4.3779e-05],\n",
      "        [3.5531e-05],\n",
      "        [2.6290e-04],\n",
      "        [6.9217e-06],\n",
      "        [2.0693e-05],\n",
      "        [1.1523e-04],\n",
      "        [6.7357e-07],\n",
      "        [3.6247e-04],\n",
      "        [2.0752e-05],\n",
      "        [8.7961e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.61913776397705: \n",
      "target probs tensor([[1.0597e-04],\n",
      "        [1.0518e-04],\n",
      "        [8.8394e-05],\n",
      "        [2.2588e-04],\n",
      "        [1.2329e-04],\n",
      "        [2.9738e-03],\n",
      "        [2.3634e-04],\n",
      "        [6.4178e-06],\n",
      "        [3.2635e-05],\n",
      "        [1.3997e-04],\n",
      "        [7.2591e-05],\n",
      "        [1.9033e-05],\n",
      "        [2.7042e-03],\n",
      "        [3.6604e-06],\n",
      "        [8.6541e-07],\n",
      "        [1.2197e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.366944313049316: \n",
      "Better model found at epoch 0 with validation value: 0.7770000100135803.\n",
      "target probs tensor([[1.0260e-04],\n",
      "        [7.5058e-05],\n",
      "        [2.0442e-04],\n",
      "        [1.3120e-05],\n",
      "        [2.7799e-06],\n",
      "        [1.4333e-04],\n",
      "        [4.3614e-06],\n",
      "        [4.4553e-04],\n",
      "        [6.4793e-06],\n",
      "        [6.8866e-06],\n",
      "        [3.6735e-05],\n",
      "        [1.0721e-04],\n",
      "        [1.5297e-04],\n",
      "        [8.6120e-05],\n",
      "        [4.0518e-05],\n",
      "        [8.3413e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.059795379638672: \n",
      "target probs tensor([[1.0691e-03],\n",
      "        [1.1903e-04],\n",
      "        [1.5684e-04],\n",
      "        [4.5315e-04],\n",
      "        [3.8998e-04],\n",
      "        [5.1363e-03],\n",
      "        [1.5241e-05],\n",
      "        [2.3781e-05],\n",
      "        [2.0277e-05],\n",
      "        [3.1035e-05],\n",
      "        [6.8711e-04],\n",
      "        [2.2666e-04],\n",
      "        [1.4232e-03],\n",
      "        [8.3290e-05],\n",
      "        [2.3638e-05],\n",
      "        [3.6693e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.660511016845703: \n",
      "target probs tensor([[5.3547e-05],\n",
      "        [1.3488e-03],\n",
      "        [3.4972e-04],\n",
      "        [1.2925e-04],\n",
      "        [3.6196e-04],\n",
      "        [5.3981e-04],\n",
      "        [1.9276e-04],\n",
      "        [2.9815e-04],\n",
      "        [6.5048e-04],\n",
      "        [1.6318e-04],\n",
      "        [1.3683e-04],\n",
      "        [6.9303e-05],\n",
      "        [1.0060e-03],\n",
      "        [1.3913e-03],\n",
      "        [5.3617e-04],\n",
      "        [2.8090e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.074745178222656: \n",
      "Better model found at epoch 1 with validation value: 0.8149999976158142.\n",
      "target probs tensor([[1.9754e-04],\n",
      "        [1.8240e-04],\n",
      "        [1.0775e-03],\n",
      "        [8.6925e-05],\n",
      "        [4.4742e-04],\n",
      "        [1.7167e-04],\n",
      "        [9.9591e-05],\n",
      "        [1.2876e-05],\n",
      "        [4.1289e-04],\n",
      "        [7.3199e-05],\n",
      "        [8.3818e-04],\n",
      "        [1.8629e-04],\n",
      "        [1.4014e-04],\n",
      "        [1.5425e-04],\n",
      "        [2.2529e-04],\n",
      "        [3.0836e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.581144332885742: \n",
      "target probs tensor([[5.7395e-04],\n",
      "        [1.6122e-05],\n",
      "        [8.0496e-04],\n",
      "        [2.1826e-04],\n",
      "        [3.1182e-04],\n",
      "        [1.7746e-04],\n",
      "        [8.2720e-04],\n",
      "        [5.1671e-04],\n",
      "        [2.7483e-04],\n",
      "        [2.8596e-04],\n",
      "        [1.0426e-05],\n",
      "        [4.8170e-04],\n",
      "        [6.3139e-05],\n",
      "        [1.2519e-04],\n",
      "        [9.5488e-04],\n",
      "        [1.1691e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.472527503967285: \n",
      "target probs tensor([[2.2545e-04],\n",
      "        [5.9405e-05],\n",
      "        [8.3513e-05],\n",
      "        [4.9162e-03],\n",
      "        [1.8294e-04],\n",
      "        [3.1237e-03],\n",
      "        [2.0953e-04],\n",
      "        [7.6532e-05],\n",
      "        [9.4507e-05],\n",
      "        [4.2079e-05],\n",
      "        [1.3864e-04],\n",
      "        [2.5061e-04],\n",
      "        [4.6148e-04],\n",
      "        [2.5430e-04],\n",
      "        [9.1280e-04],\n",
      "        [1.2116e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.35321044921875: \n",
      "Better model found at epoch 2 with validation value: 0.8169999718666077.\n",
      "models_directory returned is:  models/626\n",
      "models_directory returned is:  models/626\n",
      "investigation no: 2\n",
      "models_directory returned is:  models/626\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>targeted_validation</th>\n",
       "      <th>div_metric</th>\n",
       "      <th>entropy</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11.059636</td>\n",
       "      <td>11.010725</td>\n",
       "      <td>0.486000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>777.000000</td>\n",
       "      <td>9.600423</td>\n",
       "      <td>11.010727</td>\n",
       "      <td>05:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.931464</td>\n",
       "      <td>9.974799</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>554.500000</td>\n",
       "      <td>8.702093</td>\n",
       "      <td>9.974801</td>\n",
       "      <td>05:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9.591325</td>\n",
       "      <td>9.745042</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>8.080278</td>\n",
       "      <td>9.745042</td>\n",
       "      <td>05:49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[7.1085e-05],\n",
      "        [3.6371e-05],\n",
      "        [9.4754e-06],\n",
      "        [1.3961e-05],\n",
      "        [3.2279e-05],\n",
      "        [3.1589e-05],\n",
      "        [9.2078e-07],\n",
      "        [5.8695e-06],\n",
      "        [2.5798e-05],\n",
      "        [7.1134e-06],\n",
      "        [1.1899e-06],\n",
      "        [4.1699e-06],\n",
      "        [7.6581e-06],\n",
      "        [7.1571e-07],\n",
      "        [1.2763e-07],\n",
      "        [1.8703e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 12.18182373046875: \n",
      "target probs tensor([[1.1536e-04],\n",
      "        [1.8641e-04],\n",
      "        [1.4398e-05],\n",
      "        [1.9753e-05],\n",
      "        [2.3035e-05],\n",
      "        [1.1550e-05],\n",
      "        [1.7509e-06],\n",
      "        [5.0852e-05],\n",
      "        [1.9926e-06],\n",
      "        [1.8465e-05],\n",
      "        [1.9549e-05],\n",
      "        [1.3585e-05],\n",
      "        [1.4921e-05],\n",
      "        [1.0666e-03],\n",
      "        [2.5697e-06],\n",
      "        [8.4611e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.837958335876465: \n",
      "target probs tensor([[1.0022e-04],\n",
      "        [1.7867e-06],\n",
      "        [6.3539e-05],\n",
      "        [3.5604e-05],\n",
      "        [5.2962e-07],\n",
      "        [1.9024e-05],\n",
      "        [2.3890e-05],\n",
      "        [1.4702e-06],\n",
      "        [4.0143e-08],\n",
      "        [2.7110e-05],\n",
      "        [7.2108e-05],\n",
      "        [3.6166e-06],\n",
      "        [1.2166e-05],\n",
      "        [2.2970e-05],\n",
      "        [1.5265e-05],\n",
      "        [2.4566e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.70993423461914: \n",
      "Better model found at epoch 0 with validation value: 0.4860000014305115.\n",
      "target probs tensor([[1.1433e-05],\n",
      "        [6.8927e-05],\n",
      "        [6.7229e-05],\n",
      "        [4.6362e-05],\n",
      "        [1.7414e-05],\n",
      "        [9.6851e-06],\n",
      "        [3.0919e-06],\n",
      "        [1.6951e-08],\n",
      "        [5.3609e-05],\n",
      "        [1.1578e-05],\n",
      "        [7.0158e-05],\n",
      "        [5.3573e-08],\n",
      "        [1.0128e-05],\n",
      "        [1.2818e-05],\n",
      "        [2.3185e-05],\n",
      "        [9.4485e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.4896240234375: \n",
      "target probs tensor([[5.7078e-05],\n",
      "        [8.8501e-05],\n",
      "        [1.5107e-05],\n",
      "        [3.5239e-05],\n",
      "        [4.3087e-05],\n",
      "        [1.1013e-04],\n",
      "        [4.6418e-04],\n",
      "        [2.2909e-05],\n",
      "        [4.8113e-05],\n",
      "        [4.6242e-05],\n",
      "        [2.8248e-05],\n",
      "        [4.8369e-07],\n",
      "        [7.4374e-05],\n",
      "        [2.5758e-05],\n",
      "        [8.7886e-05],\n",
      "        [5.4416e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.278520584106445: \n",
      "target probs tensor([[2.1580e-04],\n",
      "        [6.7483e-05],\n",
      "        [4.6175e-04],\n",
      "        [3.4875e-04],\n",
      "        [3.2297e-05],\n",
      "        [3.4664e-05],\n",
      "        [7.9674e-05],\n",
      "        [1.9784e-04],\n",
      "        [2.4847e-07],\n",
      "        [4.9665e-05],\n",
      "        [4.3064e-06],\n",
      "        [8.7049e-08],\n",
      "        [4.5901e-06],\n",
      "        [5.8242e-05],\n",
      "        [7.3112e-06],\n",
      "        [5.5683e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.603469848632812: \n",
      "target probs tensor([[1.2069e-03],\n",
      "        [3.0064e-04],\n",
      "        [3.3075e-04],\n",
      "        [2.4697e-04],\n",
      "        [1.4371e-04],\n",
      "        [1.3447e-04],\n",
      "        [8.5497e-05],\n",
      "        [4.3956e-04]], device='cuda:0'), loss: 8.251046180725098: \n",
      "Better model found at epoch 1 with validation value: 0.6299999952316284.\n",
      "target probs tensor([[2.0356e-05],\n",
      "        [3.4953e-05],\n",
      "        [2.6935e-04],\n",
      "        [1.3360e-05],\n",
      "        [2.9380e-05],\n",
      "        [4.5712e-03],\n",
      "        [1.5537e-04],\n",
      "        [1.0148e-04],\n",
      "        [3.4234e-06],\n",
      "        [1.1294e-04],\n",
      "        [6.4230e-05],\n",
      "        [1.7847e-04],\n",
      "        [1.4360e-04],\n",
      "        [3.3957e-04],\n",
      "        [1.2190e-05],\n",
      "        [3.9866e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.677264213562012: \n",
      "target probs tensor([[1.4645e-05],\n",
      "        [7.9029e-05],\n",
      "        [5.8710e-04],\n",
      "        [1.3830e-03],\n",
      "        [1.8516e-04],\n",
      "        [1.1405e-04],\n",
      "        [1.7017e-04],\n",
      "        [6.7977e-07],\n",
      "        [1.0339e-03],\n",
      "        [4.8453e-05],\n",
      "        [3.2848e-04],\n",
      "        [3.1095e-05],\n",
      "        [3.8907e-05],\n",
      "        [5.9550e-05],\n",
      "        [2.5627e-05],\n",
      "        [8.1282e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.389714241027832: \n",
      "target probs tensor([[2.6342e-05],\n",
      "        [1.0561e-04],\n",
      "        [1.5219e-05],\n",
      "        [5.6464e-06],\n",
      "        [4.6267e-05],\n",
      "        [8.2563e-06],\n",
      "        [2.8241e-04],\n",
      "        [5.0961e-05],\n",
      "        [1.6224e-05],\n",
      "        [1.4736e-04],\n",
      "        [4.1682e-04],\n",
      "        [2.8610e-05],\n",
      "        [2.5000e-04],\n",
      "        [3.3084e-04],\n",
      "        [2.7635e-04],\n",
      "        [2.8238e-05]], device='cuda:0'), loss: 9.730779647827148: \n",
      "Better model found at epoch 2 with validation value: 0.656000018119812.\n",
      "models_directory returned is:  models/626\n",
      "models_directory returned is:  models/626\n",
      "investigation no: 3\n",
      "models_directory returned is:  models/626\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>targeted_validation</th>\n",
       "      <th>div_metric</th>\n",
       "      <th>entropy</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10.864452</td>\n",
       "      <td>10.903290</td>\n",
       "      <td>0.511000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>777.000000</td>\n",
       "      <td>9.600423</td>\n",
       "      <td>10.903291</td>\n",
       "      <td>06:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.120551</td>\n",
       "      <td>10.256346</td>\n",
       "      <td>0.623000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>528.250000</td>\n",
       "      <td>8.516838</td>\n",
       "      <td>10.256346</td>\n",
       "      <td>05:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9.647437</td>\n",
       "      <td>9.793013</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>429.500000</td>\n",
       "      <td>8.063310</td>\n",
       "      <td>9.793013</td>\n",
       "      <td>05:50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[2.0581e-05],\n",
      "        [1.2814e-04],\n",
      "        [1.1723e-07],\n",
      "        [1.2043e-06],\n",
      "        [4.0682e-05],\n",
      "        [5.6135e-06],\n",
      "        [5.2337e-06],\n",
      "        [2.0599e-04],\n",
      "        [9.0787e-07],\n",
      "        [6.0135e-07],\n",
      "        [7.5869e-06],\n",
      "        [1.2196e-04],\n",
      "        [1.2410e-03],\n",
      "        [1.1057e-04],\n",
      "        [1.5581e-06],\n",
      "        [1.9130e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.329000473022461: \n",
      "target probs tensor([[2.1107e-05],\n",
      "        [2.2466e-07],\n",
      "        [1.3875e-04],\n",
      "        [3.0456e-06],\n",
      "        [3.8570e-07],\n",
      "        [1.3306e-05],\n",
      "        [6.7521e-07],\n",
      "        [2.5423e-05],\n",
      "        [2.8618e-06],\n",
      "        [3.7557e-05],\n",
      "        [1.4151e-05],\n",
      "        [1.7069e-04],\n",
      "        [4.9204e-04],\n",
      "        [3.0516e-07],\n",
      "        [4.8008e-05],\n",
      "        [2.7416e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.519039154052734: \n",
      "target probs tensor([[3.5150e-05],\n",
      "        [7.9206e-07],\n",
      "        [2.8536e-06],\n",
      "        [9.4100e-06],\n",
      "        [4.2048e-05],\n",
      "        [1.1034e-05],\n",
      "        [2.2668e-05],\n",
      "        [3.8230e-04],\n",
      "        [1.6908e-07],\n",
      "        [2.7550e-05],\n",
      "        [8.5214e-05],\n",
      "        [3.5682e-04],\n",
      "        [2.9801e-05],\n",
      "        [1.1198e-04],\n",
      "        [4.0952e-07],\n",
      "        [2.9939e-06]], device='cuda:0'), loss: 11.190348625183105: \n",
      "Better model found at epoch 0 with validation value: 0.5109999775886536.\n",
      "target probs tensor([[6.4059e-04],\n",
      "        [4.0096e-05],\n",
      "        [1.5607e-05],\n",
      "        [1.7793e-05],\n",
      "        [8.8569e-05],\n",
      "        [1.0552e-04],\n",
      "        [1.3528e-04],\n",
      "        [1.2040e-04],\n",
      "        [4.2379e-06],\n",
      "        [4.3122e-06],\n",
      "        [7.9908e-04],\n",
      "        [3.6057e-05],\n",
      "        [3.0787e-05],\n",
      "        [5.9814e-05],\n",
      "        [2.1328e-05],\n",
      "        [1.4888e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.998385429382324: \n",
      "target probs tensor([[1.9168e-05],\n",
      "        [8.6105e-06],\n",
      "        [1.9406e-06],\n",
      "        [2.1878e-05],\n",
      "        [3.4959e-06],\n",
      "        [2.5661e-06],\n",
      "        [4.4826e-06],\n",
      "        [9.7751e-04],\n",
      "        [5.0960e-05],\n",
      "        [1.9536e-05],\n",
      "        [4.1128e-05],\n",
      "        [6.6554e-05],\n",
      "        [9.6695e-05],\n",
      "        [1.6303e-04],\n",
      "        [4.6169e-07],\n",
      "        [2.6771e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.057403564453125: \n",
      "target probs tensor([[7.0656e-06],\n",
      "        [5.9012e-05],\n",
      "        [4.8018e-05],\n",
      "        [5.8064e-05],\n",
      "        [2.1600e-04],\n",
      "        [1.2206e-05],\n",
      "        [1.1302e-06],\n",
      "        [4.4351e-05],\n",
      "        [1.5894e-05],\n",
      "        [1.8233e-06],\n",
      "        [1.0603e-04],\n",
      "        [9.4522e-05],\n",
      "        [1.4626e-05],\n",
      "        [1.1251e-05],\n",
      "        [8.3754e-05],\n",
      "        [5.1043e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.577953338623047: \n",
      "Better model found at epoch 1 with validation value: 0.6230000257492065.\n",
      "target probs tensor([[1.0333e-04],\n",
      "        [4.3922e-05],\n",
      "        [6.1490e-06],\n",
      "        [5.1478e-07],\n",
      "        [5.3671e-06],\n",
      "        [3.9499e-04],\n",
      "        [6.4772e-04],\n",
      "        [8.8064e-04],\n",
      "        [3.4032e-05],\n",
      "        [9.2593e-05],\n",
      "        [1.0534e-05],\n",
      "        [3.4755e-05],\n",
      "        [7.1844e-05],\n",
      "        [1.6889e-04],\n",
      "        [3.9679e-07],\n",
      "        [7.7238e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.379981994628906: \n",
      "target probs tensor([[1.9619e-05],\n",
      "        [3.2688e-04],\n",
      "        [1.9602e-04],\n",
      "        [9.9111e-05],\n",
      "        [5.8871e-04],\n",
      "        [2.4696e-04],\n",
      "        [2.5112e-04],\n",
      "        [4.2983e-04],\n",
      "        [6.9415e-04],\n",
      "        [1.2284e-05],\n",
      "        [2.1549e-04],\n",
      "        [3.7928e-03],\n",
      "        [1.1697e-04],\n",
      "        [2.0637e-04],\n",
      "        [2.2948e-04],\n",
      "        [1.0685e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.792035102844238: \n",
      "target probs tensor([[7.5379e-08],\n",
      "        [1.1005e-04],\n",
      "        [4.6171e-04],\n",
      "        [2.9397e-05],\n",
      "        [7.7009e-05],\n",
      "        [1.4685e-05],\n",
      "        [2.3277e-05],\n",
      "        [3.2240e-05],\n",
      "        [5.3445e-06],\n",
      "        [2.3450e-05],\n",
      "        [1.2447e-05],\n",
      "        [2.0407e-04],\n",
      "        [1.1970e-06],\n",
      "        [6.0146e-04],\n",
      "        [1.3896e-04],\n",
      "        [1.6772e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.547489166259766: \n",
      "Better model found at epoch 2 with validation value: 0.6899999976158142.\n",
      "models_directory returned is:  models/626\n",
      "models_directory returned is:  models/626\n",
      "investigation no: 4\n",
      "models_directory returned is:  models/626\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>targeted_validation</th>\n",
       "      <th>div_metric</th>\n",
       "      <th>entropy</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11.044823</td>\n",
       "      <td>11.145465</td>\n",
       "      <td>0.491000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>777.000000</td>\n",
       "      <td>9.600423</td>\n",
       "      <td>11.145465</td>\n",
       "      <td>05:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.222143</td>\n",
       "      <td>10.289270</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>545.000000</td>\n",
       "      <td>8.601554</td>\n",
       "      <td>10.289270</td>\n",
       "      <td>05:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9.183822</td>\n",
       "      <td>9.250226</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>466.500000</td>\n",
       "      <td>8.311300</td>\n",
       "      <td>9.250225</td>\n",
       "      <td>05:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[6.4776e-05],\n",
      "        [4.1170e-06],\n",
      "        [3.1574e-08],\n",
      "        [1.3787e-06],\n",
      "        [3.9869e-06],\n",
      "        [6.1674e-07],\n",
      "        [1.3398e-06],\n",
      "        [1.7822e-05],\n",
      "        [9.8530e-06],\n",
      "        [3.7797e-05],\n",
      "        [6.7092e-05],\n",
      "        [1.4570e-05],\n",
      "        [2.4859e-06],\n",
      "        [3.7290e-08],\n",
      "        [1.5111e-06],\n",
      "        [3.4514e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 12.508537292480469: \n",
      "target probs tensor([[1.9572e-05],\n",
      "        [1.2579e-06],\n",
      "        [5.1939e-05],\n",
      "        [4.8442e-05],\n",
      "        [2.6145e-05],\n",
      "        [1.0982e-06],\n",
      "        [8.5716e-05],\n",
      "        [1.4268e-05],\n",
      "        [3.2934e-06],\n",
      "        [9.3627e-05],\n",
      "        [5.1608e-06],\n",
      "        [2.8064e-05],\n",
      "        [8.3009e-07],\n",
      "        [2.2514e-05],\n",
      "        [2.9285e-06],\n",
      "        [1.6649e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.52052116394043: \n",
      "target probs tensor([[2.8147e-04],\n",
      "        [6.1297e-04],\n",
      "        [1.1122e-04],\n",
      "        [4.1795e-05],\n",
      "        [1.9512e-04],\n",
      "        [8.3281e-06],\n",
      "        [7.0522e-06],\n",
      "        [4.7451e-06],\n",
      "        [1.9019e-05],\n",
      "        [3.9380e-04],\n",
      "        [3.7578e-06],\n",
      "        [2.6152e-05],\n",
      "        [6.6267e-06],\n",
      "        [2.8254e-05],\n",
      "        [9.5979e-05],\n",
      "        [8.5890e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.26161003112793: \n",
      "Better model found at epoch 0 with validation value: 0.4909999966621399.\n",
      "target probs tensor([[2.6288e-05],\n",
      "        [1.6710e-04],\n",
      "        [5.2199e-06],\n",
      "        [4.5084e-08],\n",
      "        [1.0418e-05],\n",
      "        [3.3633e-06],\n",
      "        [1.7464e-04],\n",
      "        [1.9573e-05],\n",
      "        [1.6084e-05],\n",
      "        [2.1953e-05],\n",
      "        [1.6302e-03],\n",
      "        [1.6031e-05],\n",
      "        [2.4581e-06],\n",
      "        [1.6946e-04],\n",
      "        [3.4772e-05],\n",
      "        [1.2247e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.89298152923584: \n",
      "target probs tensor([[5.2401e-05],\n",
      "        [8.5749e-05],\n",
      "        [1.0802e-04],\n",
      "        [1.6941e-05],\n",
      "        [2.9351e-05],\n",
      "        [1.1907e-03],\n",
      "        [7.0775e-05],\n",
      "        [4.5430e-05],\n",
      "        [7.6792e-05],\n",
      "        [2.1851e-04],\n",
      "        [7.3698e-05],\n",
      "        [4.6216e-05],\n",
      "        [4.3787e-06],\n",
      "        [1.4525e-04],\n",
      "        [1.3754e-05],\n",
      "        [4.2588e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.031496047973633: \n",
      "target probs tensor([[2.7724e-05],\n",
      "        [3.4398e-03],\n",
      "        [7.6874e-05],\n",
      "        [1.0423e-04],\n",
      "        [2.1263e-03],\n",
      "        [6.7468e-05],\n",
      "        [2.6033e-05],\n",
      "        [2.4069e-04],\n",
      "        [7.1898e-06],\n",
      "        [4.6179e-05],\n",
      "        [8.2207e-05],\n",
      "        [9.7282e-06],\n",
      "        [4.7234e-04],\n",
      "        [5.1583e-05],\n",
      "        [4.3037e-05],\n",
      "        [3.3119e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.238883972167969: \n",
      "Better model found at epoch 1 with validation value: 0.5799999833106995.\n",
      "target probs tensor([[8.1788e-05],\n",
      "        [1.1384e-06],\n",
      "        [3.5385e-04],\n",
      "        [3.5362e-04],\n",
      "        [5.0290e-05],\n",
      "        [2.1292e-06],\n",
      "        [8.0081e-06],\n",
      "        [3.1944e-07],\n",
      "        [9.7496e-04],\n",
      "        [2.2125e-03],\n",
      "        [7.0100e-06],\n",
      "        [1.3893e-05],\n",
      "        [1.1683e-05],\n",
      "        [1.2728e-04],\n",
      "        [9.6352e-05],\n",
      "        [2.8425e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.154914855957031: \n",
      "target probs tensor([[4.5433e-06],\n",
      "        [1.8344e-06],\n",
      "        [1.6315e-04],\n",
      "        [4.7139e-04],\n",
      "        [4.9086e-03],\n",
      "        [4.5817e-06],\n",
      "        [4.9919e-03],\n",
      "        [5.9651e-05],\n",
      "        [8.4173e-05],\n",
      "        [5.0312e-05],\n",
      "        [8.5194e-05],\n",
      "        [3.9955e-06],\n",
      "        [2.8680e-05],\n",
      "        [1.1402e-04],\n",
      "        [1.2323e-04],\n",
      "        [1.0195e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.58375072479248: \n",
      "target probs tensor([[5.2695e-05],\n",
      "        [1.1652e-05],\n",
      "        [9.5092e-05],\n",
      "        [7.7108e-04],\n",
      "        [2.1807e-04],\n",
      "        [1.4910e-04],\n",
      "        [2.3074e-05],\n",
      "        [4.7994e-04],\n",
      "        [4.9594e-04],\n",
      "        [9.3928e-04],\n",
      "        [5.6585e-05],\n",
      "        [6.4182e-06],\n",
      "        [1.1200e-03],\n",
      "        [5.1906e-05],\n",
      "        [5.3673e-06],\n",
      "        [1.2496e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.206146240234375: \n",
      "Better model found at epoch 2 with validation value: 0.7749999761581421.\n",
      "models_directory returned is:  models/626\n",
      "models_directory returned is:  models/626\n",
      "investigation no: 5\n",
      "models_directory returned is:  models/626\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>targeted_validation</th>\n",
       "      <th>div_metric</th>\n",
       "      <th>entropy</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11.197498</td>\n",
       "      <td>11.269793</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>777.000000</td>\n",
       "      <td>9.600423</td>\n",
       "      <td>11.269794</td>\n",
       "      <td>05:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.405496</td>\n",
       "      <td>9.375089</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>564.500000</td>\n",
       "      <td>8.728449</td>\n",
       "      <td>9.375089</td>\n",
       "      <td>05:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.794919</td>\n",
       "      <td>8.951882</td>\n",
       "      <td>0.814000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>249.500000</td>\n",
       "      <td>5.882190</td>\n",
       "      <td>8.951885</td>\n",
       "      <td>05:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[8.8584e-07],\n",
      "        [1.0627e-05],\n",
      "        [5.6897e-06],\n",
      "        [2.5103e-06],\n",
      "        [1.8467e-06],\n",
      "        [9.0915e-06],\n",
      "        [7.5400e-07],\n",
      "        [9.7898e-07],\n",
      "        [1.7199e-05],\n",
      "        [3.8766e-05],\n",
      "        [5.4603e-05],\n",
      "        [1.3719e-05],\n",
      "        [3.6114e-05],\n",
      "        [2.3738e-06],\n",
      "        [1.9522e-07],\n",
      "        [1.0514e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 12.208579063415527: \n",
      "target probs tensor([[2.6034e-05],\n",
      "        [8.6670e-06],\n",
      "        [1.3514e-06],\n",
      "        [2.4036e-06],\n",
      "        [2.9314e-05],\n",
      "        [4.4627e-04],\n",
      "        [2.2802e-07],\n",
      "        [1.7716e-04],\n",
      "        [2.4399e-04],\n",
      "        [7.4731e-06],\n",
      "        [1.1093e-05],\n",
      "        [3.7570e-08],\n",
      "        [3.4807e-05],\n",
      "        [5.2957e-05],\n",
      "        [2.2975e-07],\n",
      "        [1.4754e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.76243782043457: \n",
      "target probs tensor([[5.5864e-05],\n",
      "        [3.6020e-05],\n",
      "        [3.7024e-06],\n",
      "        [1.1469e-05],\n",
      "        [1.1577e-04],\n",
      "        [3.9890e-06],\n",
      "        [1.8717e-05],\n",
      "        [4.8211e-06],\n",
      "        [3.1314e-06],\n",
      "        [2.0910e-06],\n",
      "        [9.4181e-07],\n",
      "        [1.3170e-06],\n",
      "        [4.1789e-04],\n",
      "        [2.1298e-05],\n",
      "        [1.3596e-05],\n",
      "        [2.8865e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.368364334106445: \n",
      "target probs tensor([[1.6119e-04],\n",
      "        [3.2943e-04],\n",
      "        [5.6058e-04],\n",
      "        [3.7208e-04],\n",
      "        [2.3421e-04],\n",
      "        [2.3480e-05],\n",
      "        [8.6176e-06],\n",
      "        [7.4962e-05]], device='cuda:0'), loss: 9.03911304473877: \n",
      "Better model found at epoch 0 with validation value: 0.4749999940395355.\n",
      "target probs tensor([[3.1700e-05],\n",
      "        [5.6101e-05],\n",
      "        [3.7405e-05],\n",
      "        [1.1319e-04],\n",
      "        [1.5942e-04],\n",
      "        [2.6081e-05],\n",
      "        [5.4568e-07],\n",
      "        [2.5992e-06],\n",
      "        [9.2765e-06],\n",
      "        [1.5576e-05],\n",
      "        [2.5233e-04],\n",
      "        [1.2856e-05],\n",
      "        [1.1507e-06],\n",
      "        [1.9279e-05],\n",
      "        [3.5739e-05],\n",
      "        [3.3236e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.830883979797363: \n",
      "target probs tensor([[1.2988e-04],\n",
      "        [4.8999e-06],\n",
      "        [1.6790e-05],\n",
      "        [1.6676e-05],\n",
      "        [1.9207e-05],\n",
      "        [1.3292e-04],\n",
      "        [1.8023e-07],\n",
      "        [3.3152e-05],\n",
      "        [7.0639e-04],\n",
      "        [1.7616e-05],\n",
      "        [2.3897e-07],\n",
      "        [3.2637e-04],\n",
      "        [5.7715e-05],\n",
      "        [1.7907e-05],\n",
      "        [3.7832e-05],\n",
      "        [3.5569e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.712067604064941: \n",
      "target probs tensor([[1.8225e-05],\n",
      "        [8.5338e-05],\n",
      "        [2.3746e-05],\n",
      "        [5.7293e-05],\n",
      "        [3.0674e-05],\n",
      "        [3.2660e-05],\n",
      "        [3.6027e-05],\n",
      "        [3.3389e-05],\n",
      "        [3.1929e-04],\n",
      "        [1.3529e-04],\n",
      "        [1.9735e-04],\n",
      "        [1.4612e-04],\n",
      "        [1.6386e-04],\n",
      "        [4.2391e-04],\n",
      "        [5.6783e-04],\n",
      "        [4.9346e-05]], device='cuda:0'), loss: 9.38431167602539: \n",
      "Better model found at epoch 1 with validation value: 0.7699999809265137.\n",
      "target probs tensor([[1.2824e-04],\n",
      "        [3.3140e-04],\n",
      "        [1.5595e-04],\n",
      "        [9.1393e-04],\n",
      "        [1.8220e-05],\n",
      "        [1.9533e-04],\n",
      "        [2.3769e-04],\n",
      "        [1.4164e-03],\n",
      "        [3.6833e-06],\n",
      "        [2.4062e-04],\n",
      "        [6.2790e-05],\n",
      "        [4.1226e-04],\n",
      "        [8.0571e-05],\n",
      "        [1.6797e-04],\n",
      "        [5.9574e-05],\n",
      "        [1.3146e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.743099212646484: \n",
      "target probs tensor([[9.1184e-05],\n",
      "        [1.8490e-04],\n",
      "        [5.5435e-05],\n",
      "        [1.2702e-04],\n",
      "        [1.1053e-04],\n",
      "        [1.7572e-04],\n",
      "        [3.9098e-05],\n",
      "        [6.6037e-04],\n",
      "        [2.5744e-07],\n",
      "        [2.9229e-04],\n",
      "        [9.4016e-04],\n",
      "        [1.1372e-08],\n",
      "        [1.2186e-05],\n",
      "        [1.0051e-05],\n",
      "        [4.7424e-04],\n",
      "        [5.3885e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.047935485839844: \n",
      "target probs tensor([[3.8747e-04],\n",
      "        [1.7815e-04],\n",
      "        [1.7661e-03],\n",
      "        [1.0235e-05],\n",
      "        [2.7948e-04],\n",
      "        [1.0996e-03],\n",
      "        [3.9433e-04],\n",
      "        [3.0462e-04],\n",
      "        [2.3887e-08],\n",
      "        [5.8046e-02],\n",
      "        [1.2690e-03],\n",
      "        [4.2622e-03],\n",
      "        [3.3690e-04],\n",
      "        [3.7492e-04],\n",
      "        [6.4484e-05],\n",
      "        [9.6939e-04]], device='cuda:0'), loss: 8.139991760253906: \n",
      "Better model found at epoch 2 with validation value: 0.8140000104904175.\n",
      "models_directory returned is:  models/626\n",
      "models_directory returned is:  models/626\n",
      "investigation no: 6\n",
      "models_directory returned is:  models/626\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>targeted_validation</th>\n",
       "      <th>div_metric</th>\n",
       "      <th>entropy</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11.172563</td>\n",
       "      <td>11.292088</td>\n",
       "      <td>0.478000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>777.000000</td>\n",
       "      <td>9.600423</td>\n",
       "      <td>11.292089</td>\n",
       "      <td>05:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>11.142214</td>\n",
       "      <td>11.221392</td>\n",
       "      <td>0.491000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>538.500000</td>\n",
       "      <td>8.576469</td>\n",
       "      <td>11.221392</td>\n",
       "      <td>05:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10.923299</td>\n",
       "      <td>11.027184</td>\n",
       "      <td>0.505000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>540.750000</td>\n",
       "      <td>8.568604</td>\n",
       "      <td>11.027183</td>\n",
       "      <td>05:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[2.8108e-05],\n",
      "        [1.9553e-06],\n",
      "        [8.5379e-06],\n",
      "        [5.4669e-05],\n",
      "        [1.3414e-05],\n",
      "        [1.7782e-05],\n",
      "        [4.5818e-05],\n",
      "        [2.8872e-05],\n",
      "        [1.8350e-06],\n",
      "        [1.0302e-05],\n",
      "        [2.1510e-05],\n",
      "        [5.4461e-06],\n",
      "        [2.0749e-05],\n",
      "        [3.1171e-06],\n",
      "        [2.8765e-04],\n",
      "        [1.3274e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.988195419311523: \n",
      "target probs tensor([[3.8041e-05],\n",
      "        [2.4786e-07],\n",
      "        [6.5987e-05],\n",
      "        [1.2027e-04],\n",
      "        [6.2054e-07],\n",
      "        [4.9481e-05],\n",
      "        [1.3103e-05],\n",
      "        [4.8996e-04],\n",
      "        [4.7615e-05],\n",
      "        [8.3824e-05],\n",
      "        [4.5828e-05],\n",
      "        [1.1257e-05],\n",
      "        [8.4153e-05],\n",
      "        [1.0031e-05],\n",
      "        [2.3629e-05],\n",
      "        [2.7389e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.617772102355957: \n",
      "target probs tensor([[1.1072e-05],\n",
      "        [5.1828e-06],\n",
      "        [1.6380e-05],\n",
      "        [1.0168e-04],\n",
      "        [3.3259e-06],\n",
      "        [2.3727e-06],\n",
      "        [3.1253e-04],\n",
      "        [2.0757e-07],\n",
      "        [1.0535e-05],\n",
      "        [1.6883e-04],\n",
      "        [2.5146e-05],\n",
      "        [1.1344e-05],\n",
      "        [6.7817e-08],\n",
      "        [2.7736e-05],\n",
      "        [3.4718e-05],\n",
      "        [4.9090e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.670969009399414: \n",
      "Better model found at epoch 0 with validation value: 0.4779999852180481.\n",
      "target probs tensor([[6.8294e-08],\n",
      "        [2.5962e-05],\n",
      "        [4.9997e-05],\n",
      "        [1.8615e-06],\n",
      "        [1.4488e-05],\n",
      "        [4.2085e-06],\n",
      "        [6.7815e-06],\n",
      "        [9.1312e-07],\n",
      "        [5.7332e-05],\n",
      "        [1.5639e-05],\n",
      "        [5.5621e-05],\n",
      "        [1.9797e-04],\n",
      "        [3.8784e-05],\n",
      "        [5.0287e-06],\n",
      "        [2.8864e-06],\n",
      "        [5.3881e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.473837852478027: \n",
      "target probs tensor([[4.3255e-05],\n",
      "        [3.9400e-05],\n",
      "        [2.1554e-05],\n",
      "        [6.6813e-05],\n",
      "        [5.7900e-06],\n",
      "        [1.8906e-05],\n",
      "        [4.6208e-05],\n",
      "        [2.6613e-06],\n",
      "        [1.0592e-05],\n",
      "        [2.2415e-06],\n",
      "        [6.2221e-06],\n",
      "        [1.0717e-06],\n",
      "        [1.2991e-06],\n",
      "        [4.2670e-06],\n",
      "        [2.2101e-05],\n",
      "        [1.5209e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.514507293701172: \n",
      "target probs tensor([[1.0100e-04],\n",
      "        [3.1242e-06],\n",
      "        [1.9052e-06],\n",
      "        [1.3101e-06],\n",
      "        [2.9206e-05],\n",
      "        [3.7647e-05],\n",
      "        [3.3270e-07],\n",
      "        [3.7890e-05],\n",
      "        [8.7059e-07],\n",
      "        [1.1484e-04],\n",
      "        [2.4708e-06],\n",
      "        [1.2840e-05],\n",
      "        [1.5926e-05],\n",
      "        [9.1817e-05],\n",
      "        [4.4123e-05],\n",
      "        [1.6511e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.431316375732422: \n",
      "Better model found at epoch 1 with validation value: 0.4909999966621399.\n",
      "target probs tensor([[5.4079e-04],\n",
      "        [9.7590e-07],\n",
      "        [1.7203e-05],\n",
      "        [9.8352e-06],\n",
      "        [2.6634e-05],\n",
      "        [4.3169e-06],\n",
      "        [1.2059e-04],\n",
      "        [2.2010e-04],\n",
      "        [1.4068e-05],\n",
      "        [3.7577e-05],\n",
      "        [2.0420e-05],\n",
      "        [2.8333e-05],\n",
      "        [5.5322e-05],\n",
      "        [3.1970e-05],\n",
      "        [5.8433e-06],\n",
      "        [6.6037e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.828606605529785: \n",
      "target probs tensor([[5.3091e-05],\n",
      "        [1.7522e-05],\n",
      "        [1.4281e-06],\n",
      "        [1.3811e-05],\n",
      "        [1.0681e-04],\n",
      "        [3.7444e-05],\n",
      "        [2.3480e-06],\n",
      "        [3.4445e-05],\n",
      "        [4.0884e-05],\n",
      "        [2.6300e-05],\n",
      "        [1.7888e-06],\n",
      "        [6.9970e-06],\n",
      "        [3.0251e-05],\n",
      "        [2.0624e-05],\n",
      "        [9.5550e-06],\n",
      "        [1.0651e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.123612403869629: \n",
      "target probs tensor([[7.7192e-05],\n",
      "        [3.1564e-05],\n",
      "        [3.4262e-04],\n",
      "        [1.3296e-06],\n",
      "        [8.6169e-05],\n",
      "        [2.5750e-05],\n",
      "        [2.5690e-04],\n",
      "        [2.2635e-05],\n",
      "        [1.0702e-04],\n",
      "        [9.1397e-05],\n",
      "        [5.1976e-04],\n",
      "        [2.2828e-06],\n",
      "        [1.5879e-05],\n",
      "        [4.6969e-06],\n",
      "        [3.4141e-05],\n",
      "        [9.1905e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.132814407348633: \n",
      "Better model found at epoch 2 with validation value: 0.5049999952316284.\n",
      "models_directory returned is:  models/626\n",
      "models_directory returned is:  models/626\n",
      "investigation no: 7\n",
      "models_directory returned is:  models/626\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>targeted_validation</th>\n",
       "      <th>div_metric</th>\n",
       "      <th>entropy</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10.712911</td>\n",
       "      <td>10.909875</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>777.000000</td>\n",
       "      <td>9.600423</td>\n",
       "      <td>10.909876</td>\n",
       "      <td>05:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.657717</td>\n",
       "      <td>10.731032</td>\n",
       "      <td>0.519000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>542.250000</td>\n",
       "      <td>8.615392</td>\n",
       "      <td>10.731034</td>\n",
       "      <td>05:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10.496172</td>\n",
       "      <td>10.569756</td>\n",
       "      <td>0.527000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>528.250000</td>\n",
       "      <td>8.604658</td>\n",
       "      <td>10.569757</td>\n",
       "      <td>05:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[1.5543e-05],\n",
      "        [1.7828e-05],\n",
      "        [4.8127e-07],\n",
      "        [2.2150e-07],\n",
      "        [1.6725e-06],\n",
      "        [8.7582e-05],\n",
      "        [5.6213e-06],\n",
      "        [9.3057e-07],\n",
      "        [6.2356e-07],\n",
      "        [1.1143e-05],\n",
      "        [4.3795e-05],\n",
      "        [1.8326e-07],\n",
      "        [1.2448e-06],\n",
      "        [9.0425e-05],\n",
      "        [3.0053e-06],\n",
      "        [7.1165e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 12.45073413848877: \n",
      "target probs tensor([[1.1725e-05],\n",
      "        [1.4453e-05],\n",
      "        [3.3290e-05],\n",
      "        [6.6302e-05],\n",
      "        [3.5467e-07],\n",
      "        [2.4863e-05],\n",
      "        [8.7753e-06],\n",
      "        [1.6363e-05],\n",
      "        [9.8445e-06],\n",
      "        [2.0854e-05],\n",
      "        [6.0971e-05],\n",
      "        [9.7592e-07],\n",
      "        [8.3147e-05],\n",
      "        [3.5689e-07],\n",
      "        [3.1903e-06],\n",
      "        [3.5996e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.614364624023438: \n",
      "target probs tensor([[1.5712e-04],\n",
      "        [4.2944e-05],\n",
      "        [5.1825e-05],\n",
      "        [3.6481e-04],\n",
      "        [1.4064e-03],\n",
      "        [1.2905e-04],\n",
      "        [1.2605e-04],\n",
      "        [4.3187e-05],\n",
      "        [7.8182e-07],\n",
      "        [1.7228e-05],\n",
      "        [8.7872e-05],\n",
      "        [7.9383e-05],\n",
      "        [4.3008e-05],\n",
      "        [8.3180e-06],\n",
      "        [5.8422e-06],\n",
      "        [4.3823e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.924814224243164: \n",
      "Better model found at epoch 0 with validation value: 0.5090000033378601.\n",
      "target probs tensor([[1.0478e-07],\n",
      "        [1.3207e-07],\n",
      "        [9.4820e-06],\n",
      "        [3.5679e-07],\n",
      "        [7.1019e-06],\n",
      "        [1.4051e-04],\n",
      "        [4.3321e-07],\n",
      "        [1.0006e-04],\n",
      "        [1.6208e-05],\n",
      "        [1.2301e-04],\n",
      "        [1.0417e-04],\n",
      "        [1.7109e-04],\n",
      "        [6.8063e-06],\n",
      "        [9.0367e-05],\n",
      "        [1.1050e-05],\n",
      "        [3.7134e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.475491523742676: \n",
      "target probs tensor([[4.7719e-05],\n",
      "        [1.1616e-05],\n",
      "        [2.1874e-05],\n",
      "        [2.0353e-04],\n",
      "        [1.3980e-05],\n",
      "        [9.2306e-04],\n",
      "        [7.8188e-06],\n",
      "        [3.6463e-05],\n",
      "        [2.0678e-04],\n",
      "        [5.9781e-06],\n",
      "        [2.3063e-04],\n",
      "        [1.5517e-05],\n",
      "        [7.2586e-05],\n",
      "        [6.0375e-05],\n",
      "        [8.5347e-07],\n",
      "        [2.0810e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.290382385253906: \n",
      "target probs tensor([[3.5669e-05],\n",
      "        [9.5007e-05],\n",
      "        [1.3927e-05],\n",
      "        [1.1242e-05],\n",
      "        [5.2040e-05],\n",
      "        [1.1607e-06],\n",
      "        [8.5574e-06],\n",
      "        [1.9191e-05],\n",
      "        [3.8131e-06],\n",
      "        [3.5610e-04],\n",
      "        [3.7793e-06],\n",
      "        [1.6444e-05],\n",
      "        [7.2730e-05],\n",
      "        [2.5810e-06],\n",
      "        [3.5640e-04],\n",
      "        [1.2065e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.713560104370117: \n",
      "Better model found at epoch 1 with validation value: 0.5189999938011169.\n",
      "target probs tensor([[4.6565e-05],\n",
      "        [5.2277e-05],\n",
      "        [1.0168e-06],\n",
      "        [2.4367e-04],\n",
      "        [1.6969e-05],\n",
      "        [2.7420e-04],\n",
      "        [1.3433e-07],\n",
      "        [4.8366e-07],\n",
      "        [3.9237e-07],\n",
      "        [1.0918e-04],\n",
      "        [5.1220e-06],\n",
      "        [3.9031e-05],\n",
      "        [8.5684e-06],\n",
      "        [2.0132e-06],\n",
      "        [9.8970e-06],\n",
      "        [2.2116e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.54586410522461: \n",
      "target probs tensor([[3.9385e-05],\n",
      "        [5.3581e-05],\n",
      "        [1.1906e-05],\n",
      "        [9.9340e-06],\n",
      "        [6.1481e-06],\n",
      "        [4.9715e-05],\n",
      "        [1.1963e-08],\n",
      "        [2.2182e-05],\n",
      "        [2.2595e-05],\n",
      "        [6.3721e-06],\n",
      "        [3.5877e-05],\n",
      "        [2.2382e-04],\n",
      "        [1.1832e-04],\n",
      "        [4.0017e-07],\n",
      "        [2.7074e-04],\n",
      "        [3.0029e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.087163925170898: \n",
      "target probs tensor([[9.6080e-06],\n",
      "        [1.4594e-05],\n",
      "        [2.2239e-04],\n",
      "        [1.8430e-05],\n",
      "        [2.2809e-05],\n",
      "        [5.5320e-06],\n",
      "        [7.5206e-05],\n",
      "        [3.1782e-05],\n",
      "        [1.6183e-05],\n",
      "        [2.4583e-05],\n",
      "        [4.0512e-04],\n",
      "        [4.5021e-04],\n",
      "        [6.1194e-04],\n",
      "        [3.5686e-05],\n",
      "        [6.9053e-06],\n",
      "        [3.1938e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.10513973236084: \n",
      "target probs tensor([[5.6907e-04],\n",
      "        [5.3828e-04],\n",
      "        [4.5818e-04],\n",
      "        [1.6501e-04],\n",
      "        [7.5651e-05],\n",
      "        [1.8503e-04],\n",
      "        [5.5811e-05],\n",
      "        [1.5030e-04]], device='cuda:0'), loss: 8.509644508361816: \n",
      "Better model found at epoch 2 with validation value: 0.5270000100135803.\n",
      "models_directory returned is:  models/626\n",
      "models_directory returned is:  models/626\n",
      "investigation no: 8\n",
      "models_directory returned is:  models/626\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>targeted_validation</th>\n",
       "      <th>div_metric</th>\n",
       "      <th>entropy</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11.070563</td>\n",
       "      <td>11.041654</td>\n",
       "      <td>0.498000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>777.000000</td>\n",
       "      <td>9.600423</td>\n",
       "      <td>11.041654</td>\n",
       "      <td>05:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.172196</td>\n",
       "      <td>10.309569</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>535.000000</td>\n",
       "      <td>8.657568</td>\n",
       "      <td>10.309569</td>\n",
       "      <td>06:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9.425042</td>\n",
       "      <td>9.508017</td>\n",
       "      <td>0.713000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>494.750000</td>\n",
       "      <td>8.565810</td>\n",
       "      <td>9.508017</td>\n",
       "      <td>06:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[2.5406e-04],\n",
      "        [2.1311e-05],\n",
      "        [1.0634e-07],\n",
      "        [2.5472e-05],\n",
      "        [7.4030e-06],\n",
      "        [2.0231e-04],\n",
      "        [4.3934e-05],\n",
      "        [7.7704e-07],\n",
      "        [6.6768e-06],\n",
      "        [1.3103e-05],\n",
      "        [8.7255e-06],\n",
      "        [2.1560e-05],\n",
      "        [8.4880e-06],\n",
      "        [4.3418e-06],\n",
      "        [1.2533e-05],\n",
      "        [4.4424e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.454778671264648: \n",
      "target probs tensor([[6.4541e-05],\n",
      "        [3.5900e-07],\n",
      "        [1.4975e-05],\n",
      "        [6.9404e-08],\n",
      "        [2.4964e-05],\n",
      "        [4.9504e-05],\n",
      "        [3.0039e-05],\n",
      "        [2.4081e-05],\n",
      "        [2.0926e-05],\n",
      "        [7.6994e-06],\n",
      "        [4.5017e-06],\n",
      "        [5.7598e-05],\n",
      "        [1.8280e-04],\n",
      "        [2.0962e-04],\n",
      "        [1.0809e-06],\n",
      "        [8.1622e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.299396514892578: \n",
      "target probs tensor([[1.5966e-05],\n",
      "        [3.2675e-05],\n",
      "        [6.1331e-07],\n",
      "        [1.8151e-07],\n",
      "        [1.1017e-04],\n",
      "        [2.6111e-05],\n",
      "        [3.1673e-05],\n",
      "        [1.0369e-04],\n",
      "        [5.9364e-06],\n",
      "        [2.4614e-05],\n",
      "        [4.8831e-05],\n",
      "        [2.4071e-05],\n",
      "        [3.0245e-05],\n",
      "        [1.3749e-04],\n",
      "        [2.8043e-06],\n",
      "        [9.6108e-06]], device='cuda:0'), loss: 11.077731132507324: \n",
      "Better model found at epoch 0 with validation value: 0.49799999594688416.\n",
      "target probs tensor([[1.8622e-04],\n",
      "        [2.1282e-03],\n",
      "        [1.4265e-05],\n",
      "        [2.1837e-05],\n",
      "        [2.6541e-07],\n",
      "        [3.1979e-04],\n",
      "        [1.9486e-06],\n",
      "        [1.1109e-05],\n",
      "        [2.0063e-04],\n",
      "        [2.2877e-06],\n",
      "        [2.7757e-05],\n",
      "        [7.9661e-06],\n",
      "        [1.1048e-05],\n",
      "        [2.9259e-05],\n",
      "        [2.8299e-05],\n",
      "        [1.3494e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.728058815002441: \n",
      "target probs tensor([[9.5045e-08],\n",
      "        [2.3735e-06],\n",
      "        [1.0599e-07],\n",
      "        [1.2388e-05],\n",
      "        [2.0022e-04],\n",
      "        [3.1185e-04],\n",
      "        [9.4582e-05],\n",
      "        [3.8034e-04],\n",
      "        [4.3102e-05],\n",
      "        [3.7332e-05],\n",
      "        [1.0866e-04],\n",
      "        [4.4925e-06],\n",
      "        [1.3331e-08],\n",
      "        [3.6011e-06],\n",
      "        [2.6084e-05],\n",
      "        [5.2702e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.722764015197754: \n",
      "target probs tensor([[3.5438e-05],\n",
      "        [1.2001e-06],\n",
      "        [2.5142e-05],\n",
      "        [7.8420e-06],\n",
      "        [4.5990e-05],\n",
      "        [3.1537e-05],\n",
      "        [7.6350e-05],\n",
      "        [2.0693e-04],\n",
      "        [1.3206e-06],\n",
      "        [1.4392e-04],\n",
      "        [3.5378e-04],\n",
      "        [3.1623e-03],\n",
      "        [6.5932e-05],\n",
      "        [2.5989e-04],\n",
      "        [1.2572e-06],\n",
      "        [1.5639e-04]], device='cuda:0'), loss: 10.053825378417969: \n",
      "Better model found at epoch 1 with validation value: 0.5799999833106995.\n",
      "target probs tensor([[7.9830e-05],\n",
      "        [2.7555e-05],\n",
      "        [2.7320e-04],\n",
      "        [1.3927e-04],\n",
      "        [1.8360e-04],\n",
      "        [5.2565e-05],\n",
      "        [3.2469e-06],\n",
      "        [5.6638e-05],\n",
      "        [5.7437e-04],\n",
      "        [1.2981e-03],\n",
      "        [3.6356e-04],\n",
      "        [1.8947e-04],\n",
      "        [1.3838e-04],\n",
      "        [3.4848e-04],\n",
      "        [4.1269e-04],\n",
      "        [4.7785e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.9425687789917: \n",
      "target probs tensor([[9.7684e-05],\n",
      "        [1.1707e-04],\n",
      "        [2.1688e-04],\n",
      "        [3.1253e-06],\n",
      "        [2.6210e-04],\n",
      "        [1.0838e-05],\n",
      "        [3.5094e-05],\n",
      "        [1.2409e-05],\n",
      "        [3.5678e-04],\n",
      "        [1.7725e-04],\n",
      "        [2.8558e-05],\n",
      "        [1.3597e-05],\n",
      "        [1.3032e-05],\n",
      "        [4.2760e-05],\n",
      "        [5.6566e-04],\n",
      "        [9.2432e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.097347259521484: \n",
      "target probs tensor([[1.2587e-05],\n",
      "        [3.4424e-05],\n",
      "        [1.9159e-04],\n",
      "        [1.5803e-05],\n",
      "        [2.2457e-04],\n",
      "        [6.4083e-04],\n",
      "        [5.2876e-05],\n",
      "        [5.1205e-04],\n",
      "        [2.0209e-04],\n",
      "        [3.5468e-04],\n",
      "        [5.9176e-04],\n",
      "        [1.7627e-04],\n",
      "        [7.8919e-05],\n",
      "        [1.4446e-04],\n",
      "        [1.4302e-04],\n",
      "        [8.0729e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.965448379516602: \n",
      "Better model found at epoch 2 with validation value: 0.7129999995231628.\n",
      "models_directory returned is:  models/626\n",
      "models_directory returned is:  models/626\n",
      "investigation no: 9\n",
      "models_directory returned is:  models/626\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>targeted_validation</th>\n",
       "      <th>div_metric</th>\n",
       "      <th>entropy</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10.727462</td>\n",
       "      <td>10.662751</td>\n",
       "      <td>0.521000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>777.000000</td>\n",
       "      <td>9.600423</td>\n",
       "      <td>10.662751</td>\n",
       "      <td>06:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.991989</td>\n",
       "      <td>10.197903</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>519.500000</td>\n",
       "      <td>8.583364</td>\n",
       "      <td>10.197903</td>\n",
       "      <td>06:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9.956699</td>\n",
       "      <td>10.111820</td>\n",
       "      <td>0.637000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>423.250000</td>\n",
       "      <td>7.885056</td>\n",
       "      <td>10.111820</td>\n",
       "      <td>05:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[7.7324e-06],\n",
      "        [2.2465e-05],\n",
      "        [1.3157e-04],\n",
      "        [2.8438e-07],\n",
      "        [6.8619e-07],\n",
      "        [1.8832e-06],\n",
      "        [6.9300e-05],\n",
      "        [7.2235e-05],\n",
      "        [3.6746e-07],\n",
      "        [7.1842e-06],\n",
      "        [8.0579e-04],\n",
      "        [2.4121e-05],\n",
      "        [5.4562e-06],\n",
      "        [8.3669e-07],\n",
      "        [1.5444e-05],\n",
      "        [4.1472e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.54166316986084: \n",
      "target probs tensor([[1.4719e-05],\n",
      "        [4.2183e-06],\n",
      "        [4.6857e-06],\n",
      "        [3.2619e-05],\n",
      "        [3.1363e-05],\n",
      "        [4.5600e-05],\n",
      "        [4.0454e-05],\n",
      "        [2.3309e-06],\n",
      "        [4.1589e-05],\n",
      "        [1.7546e-07],\n",
      "        [3.0370e-04],\n",
      "        [3.5544e-07],\n",
      "        [5.1765e-05],\n",
      "        [8.8644e-06],\n",
      "        [1.2835e-06],\n",
      "        [3.8113e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.749629974365234: \n",
      "target probs tensor([[1.8174e-04],\n",
      "        [9.4957e-05],\n",
      "        [1.8642e-04],\n",
      "        [7.2280e-05],\n",
      "        [2.4134e-04],\n",
      "        [3.3177e-05],\n",
      "        [4.1377e-06],\n",
      "        [6.1965e-05],\n",
      "        [2.6412e-07],\n",
      "        [5.5310e-05],\n",
      "        [7.8501e-05],\n",
      "        [1.7228e-05],\n",
      "        [4.2653e-04],\n",
      "        [1.1704e-04],\n",
      "        [5.2227e-05],\n",
      "        [2.5818e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.814355850219727: \n",
      "Better model found at epoch 0 with validation value: 0.5210000276565552.\n",
      "target probs tensor([[1.8049e-04],\n",
      "        [5.1883e-06],\n",
      "        [3.7482e-05],\n",
      "        [5.4620e-05],\n",
      "        [6.0304e-05],\n",
      "        [5.7644e-05],\n",
      "        [2.2261e-04],\n",
      "        [2.4839e-07],\n",
      "        [4.8541e-04],\n",
      "        [1.0885e-05],\n",
      "        [2.0403e-04],\n",
      "        [5.2452e-06],\n",
      "        [2.3811e-04],\n",
      "        [3.1884e-05],\n",
      "        [1.8600e-05],\n",
      "        [7.4464e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.168676376342773: \n",
      "target probs tensor([[5.8938e-05],\n",
      "        [1.1529e-04],\n",
      "        [2.7940e-05],\n",
      "        [5.9532e-04],\n",
      "        [7.8192e-05],\n",
      "        [4.4386e-05],\n",
      "        [1.8404e-05],\n",
      "        [1.2433e-03],\n",
      "        [6.8504e-05],\n",
      "        [5.8229e-04],\n",
      "        [3.8093e-05],\n",
      "        [2.7219e-05],\n",
      "        [7.8892e-04],\n",
      "        [9.6218e-05],\n",
      "        [6.7790e-05],\n",
      "        [3.0585e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.243928909301758: \n",
      "target probs tensor([[9.8949e-06],\n",
      "        [5.4833e-04],\n",
      "        [8.6226e-04],\n",
      "        [2.5137e-05],\n",
      "        [1.2853e-04],\n",
      "        [2.3431e-04],\n",
      "        [1.4894e-06],\n",
      "        [1.0149e-05],\n",
      "        [1.2550e-05],\n",
      "        [4.5612e-06],\n",
      "        [2.1496e-04],\n",
      "        [4.1031e-06],\n",
      "        [1.4255e-06],\n",
      "        [2.4985e-04],\n",
      "        [1.0549e-04],\n",
      "        [1.0152e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.359712600708008: \n",
      "Better model found at epoch 1 with validation value: 0.6240000128746033.\n",
      "target probs tensor([[3.1015e-04],\n",
      "        [1.1022e-06],\n",
      "        [2.5631e-05],\n",
      "        [3.2925e-04],\n",
      "        [8.4478e-04],\n",
      "        [3.4278e-06],\n",
      "        [1.1457e-05],\n",
      "        [3.9081e-04],\n",
      "        [3.9630e-05],\n",
      "        [1.3198e-05],\n",
      "        [5.4998e-03],\n",
      "        [3.6373e-04],\n",
      "        [4.3265e-04],\n",
      "        [1.3153e-04],\n",
      "        [2.8582e-05],\n",
      "        [1.3517e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.363637924194336: \n",
      "target probs tensor([[4.2455e-04],\n",
      "        [1.4642e-04],\n",
      "        [7.0476e-05],\n",
      "        [1.0646e-04],\n",
      "        [4.0989e-05],\n",
      "        [3.8799e-06],\n",
      "        [1.6427e-04],\n",
      "        [7.4566e-05],\n",
      "        [1.4012e-05],\n",
      "        [1.8803e-04],\n",
      "        [3.1766e-05],\n",
      "        [3.2897e-06],\n",
      "        [4.5257e-04],\n",
      "        [1.1399e-05],\n",
      "        [5.2088e-05],\n",
      "        [3.1077e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.73993968963623: \n",
      "target probs tensor([[7.6078e-05],\n",
      "        [7.3216e-06],\n",
      "        [1.7720e-06],\n",
      "        [3.3598e-04],\n",
      "        [4.0183e-06],\n",
      "        [2.2086e-05],\n",
      "        [7.6706e-05],\n",
      "        [2.2056e-04],\n",
      "        [4.7320e-05],\n",
      "        [2.9323e-07],\n",
      "        [1.8723e-05],\n",
      "        [8.4236e-06],\n",
      "        [9.4516e-07],\n",
      "        [1.3015e-05],\n",
      "        [2.4305e-04],\n",
      "        [1.1196e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.856365203857422: \n",
      "Better model found at epoch 2 with validation value: 0.6370000243186951.\n",
      "models_directory returned is:  models/626\n",
      "models_directory returned is:  models/626\n"
     ]
    }
   ],
   "source": [
    "results_dir = 'investigate_googlenet_5'\n",
    "investigate_initial_settings(10, 3, lr = 1e-2, wd = 0.0, results_dir = results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree(env.get_models_dir() + results_dir)\n",
    "# shutil.rmtree(env.get_csv_dir() + results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WeightTuner(LearnerCallback):\n",
    "#   def __init__(self, learn: Learner):\n",
    "#     super().__init__(learn)\n",
    "#     self.fooling_weight = learn.loss_func.fooling_weight\n",
    "    \n",
    "#   def on_epoch_begin(self, **kwargs):\n",
    "#     fooling_rate = \n",
    "#     print(\"by how much to increase the fooling_weight? (current value: {})\".format(fooling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicalLRScheduler(LearnerCallback):\n",
    "  def __init__(self, learn, max_lr, min_lr, cycle_len):\n",
    "    super().__init__(learn)\n",
    "    self.max_lr = max_lr\n",
    "    self.min_lr = min_lr\n",
    "    self.cycle_len = cycle_len\n",
    "    \n",
    "  def on_train_begin(self, **kwargs):\n",
    "    self.n_iter_per_epoch = len(self.learn.data.train_dl)\n",
    "    self.cycle_len_iters = self.cycle_len * self.n_iter_per_epoch\n",
    "    self.learn.opt.lr = self.min_lr\n",
    "    \n",
    "    \n",
    "  def on_batch_end(self, iteration, train, **kwargs):\n",
    "    if train:\n",
    "      cycle_index = iteration % self.cycle_len_iters\n",
    "      half_cycle_len = self.cycle_len_iters / 2\n",
    "\n",
    "      if cycle_index < half_cycle_len:\n",
    "        new_lr = float(self.max_lr - self.min_lr) / half_cycle_len * cycle_index + self.min_lr\n",
    "      else:\n",
    "        new_lr = float(self.min_lr - self.max_lr) / half_cycle_len * (cycle_index - half_cycle_len) + self.max_lr\n",
    "\n",
    "#       print('iter: {}, lr: {}'.format(iteration, new_lr))\n",
    "      self.opt.lr = new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the selected settings are : \n",
      "\tmode: normal \n",
      "\tnetw-under-attack: googlenet \n",
      "\tload filename: None \n",
      "      \tsave filename: googlenet_31x\n",
      "\tmetric names: ['fool_loss']\n",
      "\tgen arch: targeted\n",
      "\n",
      "please MAKE SURE that the config is correct.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  load_filename\n",
    "except NameError:\n",
    "  load_filename = None\n",
    "\n",
    "print(\"the selected settings are : \")\n",
    "print('''\\tmode: {} \\n\\tnetw-under-attack: {} \\n\\tload filename: {} \n",
    "      \\tsave filename: {}\\n\\tmetric names: {}\\n\\tgen arch: {}\\n'''.format(\n",
    "      mode, model.__name__, load_filename , env.save_filename, feat_loss.metric_names,\n",
    "      gen_arch\n",
    "))\n",
    "print(\"please MAKE SURE that the config is correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find(1e-6, 100)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='94' class='' max='100', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      94.00% [94/100 7:15:14<27:46]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>targeted_validation</th>\n",
       "      <th>div_metric</th>\n",
       "      <th>entropy</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10.539020</td>\n",
       "      <td>10.549237</td>\n",
       "      <td>0.564000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>777.000000</td>\n",
       "      <td>9.600423</td>\n",
       "      <td>10.549237</td>\n",
       "      <td>05:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.538582</td>\n",
       "      <td>9.600757</td>\n",
       "      <td>0.726000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>499.750000</td>\n",
       "      <td>8.514772</td>\n",
       "      <td>9.600758</td>\n",
       "      <td>05:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9.126520</td>\n",
       "      <td>9.282148</td>\n",
       "      <td>0.764000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>308.500000</td>\n",
       "      <td>6.792299</td>\n",
       "      <td>9.282149</td>\n",
       "      <td>05:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9.051549</td>\n",
       "      <td>9.202923</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>274.500000</td>\n",
       "      <td>6.400857</td>\n",
       "      <td>9.202921</td>\n",
       "      <td>05:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8.970035</td>\n",
       "      <td>9.113808</td>\n",
       "      <td>0.787000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>275.750000</td>\n",
       "      <td>6.411920</td>\n",
       "      <td>9.113809</td>\n",
       "      <td>04:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8.983486</td>\n",
       "      <td>9.064493</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>261.500000</td>\n",
       "      <td>6.349881</td>\n",
       "      <td>9.064494</td>\n",
       "      <td>04:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>8.999749</td>\n",
       "      <td>9.036166</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>6.404052</td>\n",
       "      <td>9.036167</td>\n",
       "      <td>05:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8.837975</td>\n",
       "      <td>9.007480</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>257.750000</td>\n",
       "      <td>6.322439</td>\n",
       "      <td>9.007480</td>\n",
       "      <td>05:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8.839526</td>\n",
       "      <td>8.962658</td>\n",
       "      <td>0.787000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>260.250000</td>\n",
       "      <td>6.377712</td>\n",
       "      <td>8.962659</td>\n",
       "      <td>05:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>8.721956</td>\n",
       "      <td>8.919376</td>\n",
       "      <td>0.796000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>264.750000</td>\n",
       "      <td>6.442516</td>\n",
       "      <td>8.919376</td>\n",
       "      <td>05:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>8.704307</td>\n",
       "      <td>8.904531</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>6.357685</td>\n",
       "      <td>8.904532</td>\n",
       "      <td>05:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>8.732804</td>\n",
       "      <td>8.857956</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>264.500000</td>\n",
       "      <td>6.425831</td>\n",
       "      <td>8.857957</td>\n",
       "      <td>04:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>8.807147</td>\n",
       "      <td>8.846896</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>262.250000</td>\n",
       "      <td>6.331712</td>\n",
       "      <td>8.846895</td>\n",
       "      <td>04:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>8.675593</td>\n",
       "      <td>8.815495</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>259.750000</td>\n",
       "      <td>6.263870</td>\n",
       "      <td>8.815493</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>8.632957</td>\n",
       "      <td>8.792543</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>268.500000</td>\n",
       "      <td>6.348454</td>\n",
       "      <td>8.792542</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>8.656790</td>\n",
       "      <td>8.784573</td>\n",
       "      <td>0.807000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>259.500000</td>\n",
       "      <td>6.300454</td>\n",
       "      <td>8.784573</td>\n",
       "      <td>04:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>8.585941</td>\n",
       "      <td>8.768988</td>\n",
       "      <td>0.814000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>271.000000</td>\n",
       "      <td>6.349324</td>\n",
       "      <td>8.768988</td>\n",
       "      <td>04:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>8.581189</td>\n",
       "      <td>8.748569</td>\n",
       "      <td>0.814000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.250000</td>\n",
       "      <td>6.228798</td>\n",
       "      <td>8.748569</td>\n",
       "      <td>05:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>8.622711</td>\n",
       "      <td>8.730771</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>280.750000</td>\n",
       "      <td>6.438215</td>\n",
       "      <td>8.730769</td>\n",
       "      <td>04:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>8.522577</td>\n",
       "      <td>8.704206</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.250000</td>\n",
       "      <td>6.249487</td>\n",
       "      <td>8.704206</td>\n",
       "      <td>04:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>8.600279</td>\n",
       "      <td>8.698886</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>263.500000</td>\n",
       "      <td>6.256968</td>\n",
       "      <td>8.698886</td>\n",
       "      <td>05:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>8.598883</td>\n",
       "      <td>8.674517</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>299.250000</td>\n",
       "      <td>6.584935</td>\n",
       "      <td>8.674517</td>\n",
       "      <td>05:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>8.510093</td>\n",
       "      <td>8.663691</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>6.549700</td>\n",
       "      <td>8.663691</td>\n",
       "      <td>05:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>8.515132</td>\n",
       "      <td>8.645348</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>301.750000</td>\n",
       "      <td>6.609662</td>\n",
       "      <td>8.645348</td>\n",
       "      <td>05:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>8.489823</td>\n",
       "      <td>8.625213</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>7.034923</td>\n",
       "      <td>8.625214</td>\n",
       "      <td>05:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>8.474109</td>\n",
       "      <td>8.609234</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>369.750000</td>\n",
       "      <td>7.208829</td>\n",
       "      <td>8.609232</td>\n",
       "      <td>05:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>8.497805</td>\n",
       "      <td>8.602651</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>386.500000</td>\n",
       "      <td>7.317960</td>\n",
       "      <td>8.602651</td>\n",
       "      <td>05:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>8.377474</td>\n",
       "      <td>8.595273</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>348.250000</td>\n",
       "      <td>7.026924</td>\n",
       "      <td>8.595274</td>\n",
       "      <td>05:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>8.438902</td>\n",
       "      <td>8.568903</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>416.750000</td>\n",
       "      <td>7.598629</td>\n",
       "      <td>8.568903</td>\n",
       "      <td>05:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>8.354716</td>\n",
       "      <td>8.554714</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>362.500000</td>\n",
       "      <td>7.187309</td>\n",
       "      <td>8.554714</td>\n",
       "      <td>04:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>8.326693</td>\n",
       "      <td>8.531989</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>402.500000</td>\n",
       "      <td>7.455233</td>\n",
       "      <td>8.531988</td>\n",
       "      <td>05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>8.332295</td>\n",
       "      <td>8.526450</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>496.250000</td>\n",
       "      <td>8.006248</td>\n",
       "      <td>8.526450</td>\n",
       "      <td>05:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>8.346409</td>\n",
       "      <td>8.523768</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>471.500000</td>\n",
       "      <td>7.933412</td>\n",
       "      <td>8.523767</td>\n",
       "      <td>05:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>8.255038</td>\n",
       "      <td>8.499586</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>444.000000</td>\n",
       "      <td>7.741803</td>\n",
       "      <td>8.499585</td>\n",
       "      <td>04:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>8.303488</td>\n",
       "      <td>8.500813</td>\n",
       "      <td>0.827000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>426.500000</td>\n",
       "      <td>7.546228</td>\n",
       "      <td>8.500811</td>\n",
       "      <td>04:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>8.232883</td>\n",
       "      <td>8.490255</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>417.500000</td>\n",
       "      <td>7.560564</td>\n",
       "      <td>8.490255</td>\n",
       "      <td>04:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>8.304353</td>\n",
       "      <td>8.480459</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>395.250000</td>\n",
       "      <td>7.305342</td>\n",
       "      <td>8.480460</td>\n",
       "      <td>04:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>8.320178</td>\n",
       "      <td>8.470092</td>\n",
       "      <td>0.813000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>367.500000</td>\n",
       "      <td>7.118146</td>\n",
       "      <td>8.470092</td>\n",
       "      <td>04:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>8.260916</td>\n",
       "      <td>8.439303</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>353.750000</td>\n",
       "      <td>6.990397</td>\n",
       "      <td>8.439303</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>8.280688</td>\n",
       "      <td>8.435474</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>6.827789</td>\n",
       "      <td>8.435473</td>\n",
       "      <td>04:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>8.265963</td>\n",
       "      <td>8.405892</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>378.000000</td>\n",
       "      <td>7.240926</td>\n",
       "      <td>8.405892</td>\n",
       "      <td>04:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>8.211528</td>\n",
       "      <td>8.396999</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>383.750000</td>\n",
       "      <td>7.247482</td>\n",
       "      <td>8.396999</td>\n",
       "      <td>04:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>8.209069</td>\n",
       "      <td>8.379363</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>366.000000</td>\n",
       "      <td>7.090976</td>\n",
       "      <td>8.379363</td>\n",
       "      <td>04:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>8.117586</td>\n",
       "      <td>8.351645</td>\n",
       "      <td>0.827000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>337.250000</td>\n",
       "      <td>6.906893</td>\n",
       "      <td>8.351646</td>\n",
       "      <td>04:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>8.168635</td>\n",
       "      <td>8.332896</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>367.250000</td>\n",
       "      <td>7.137950</td>\n",
       "      <td>8.332894</td>\n",
       "      <td>04:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>8.107117</td>\n",
       "      <td>8.325967</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>360.500000</td>\n",
       "      <td>7.028143</td>\n",
       "      <td>8.325968</td>\n",
       "      <td>04:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>8.086195</td>\n",
       "      <td>8.309758</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>334.500000</td>\n",
       "      <td>6.855018</td>\n",
       "      <td>8.309758</td>\n",
       "      <td>04:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>8.124709</td>\n",
       "      <td>8.293077</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>367.250000</td>\n",
       "      <td>7.183062</td>\n",
       "      <td>8.293077</td>\n",
       "      <td>04:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>8.115945</td>\n",
       "      <td>8.275238</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>400.750000</td>\n",
       "      <td>7.455943</td>\n",
       "      <td>8.275239</td>\n",
       "      <td>04:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>7.973166</td>\n",
       "      <td>8.269414</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>342.500000</td>\n",
       "      <td>6.966085</td>\n",
       "      <td>8.269413</td>\n",
       "      <td>04:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>8.016517</td>\n",
       "      <td>8.256662</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>385.750000</td>\n",
       "      <td>7.312510</td>\n",
       "      <td>8.256662</td>\n",
       "      <td>04:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>8.017156</td>\n",
       "      <td>8.246916</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>391.750000</td>\n",
       "      <td>7.338295</td>\n",
       "      <td>8.246916</td>\n",
       "      <td>04:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>8.015267</td>\n",
       "      <td>8.227650</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>406.750000</td>\n",
       "      <td>7.453839</td>\n",
       "      <td>8.227651</td>\n",
       "      <td>04:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>8.031512</td>\n",
       "      <td>8.214247</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>415.250000</td>\n",
       "      <td>7.466951</td>\n",
       "      <td>8.214247</td>\n",
       "      <td>04:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>7.937110</td>\n",
       "      <td>8.200130</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>460.750000</td>\n",
       "      <td>7.980721</td>\n",
       "      <td>8.200130</td>\n",
       "      <td>04:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>7.981440</td>\n",
       "      <td>8.193174</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>438.750000</td>\n",
       "      <td>7.765077</td>\n",
       "      <td>8.193174</td>\n",
       "      <td>04:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>7.937986</td>\n",
       "      <td>8.181625</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>454.250000</td>\n",
       "      <td>7.935875</td>\n",
       "      <td>8.181625</td>\n",
       "      <td>04:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>7.897354</td>\n",
       "      <td>8.167546</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>435.000000</td>\n",
       "      <td>7.711144</td>\n",
       "      <td>8.167547</td>\n",
       "      <td>04:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>7.849689</td>\n",
       "      <td>8.144475</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>460.000000</td>\n",
       "      <td>7.936472</td>\n",
       "      <td>8.144475</td>\n",
       "      <td>04:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>7.891474</td>\n",
       "      <td>8.142814</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>464.500000</td>\n",
       "      <td>7.975498</td>\n",
       "      <td>8.142815</td>\n",
       "      <td>04:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>7.887388</td>\n",
       "      <td>8.135635</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>8.070023</td>\n",
       "      <td>8.135636</td>\n",
       "      <td>04:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>7.816992</td>\n",
       "      <td>8.113568</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>486.750000</td>\n",
       "      <td>8.175076</td>\n",
       "      <td>8.113566</td>\n",
       "      <td>04:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>7.841470</td>\n",
       "      <td>8.090296</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>511.250000</td>\n",
       "      <td>8.304493</td>\n",
       "      <td>8.090297</td>\n",
       "      <td>04:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>7.850984</td>\n",
       "      <td>8.076687</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>8.047307</td>\n",
       "      <td>8.076687</td>\n",
       "      <td>04:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>7.774748</td>\n",
       "      <td>8.067395</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>468.250000</td>\n",
       "      <td>7.971771</td>\n",
       "      <td>8.067394</td>\n",
       "      <td>04:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>7.729253</td>\n",
       "      <td>8.058369</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>494.750000</td>\n",
       "      <td>8.220470</td>\n",
       "      <td>8.058368</td>\n",
       "      <td>04:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>7.804883</td>\n",
       "      <td>8.037314</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>493.250000</td>\n",
       "      <td>8.221684</td>\n",
       "      <td>8.037313</td>\n",
       "      <td>04:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>7.738674</td>\n",
       "      <td>8.024616</td>\n",
       "      <td>0.827000</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>503.250000</td>\n",
       "      <td>8.306173</td>\n",
       "      <td>8.024616</td>\n",
       "      <td>04:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>7.803976</td>\n",
       "      <td>8.007625</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>510.250000</td>\n",
       "      <td>8.311420</td>\n",
       "      <td>8.007625</td>\n",
       "      <td>04:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>7.737126</td>\n",
       "      <td>8.002109</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>492.500000</td>\n",
       "      <td>8.119585</td>\n",
       "      <td>8.002110</td>\n",
       "      <td>04:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>7.805171</td>\n",
       "      <td>7.986089</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>524.500000</td>\n",
       "      <td>8.413721</td>\n",
       "      <td>7.986090</td>\n",
       "      <td>04:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>7.759450</td>\n",
       "      <td>7.969924</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>494.250000</td>\n",
       "      <td>8.170318</td>\n",
       "      <td>7.969923</td>\n",
       "      <td>04:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>7.643901</td>\n",
       "      <td>7.967997</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>522.000000</td>\n",
       "      <td>8.411936</td>\n",
       "      <td>7.967997</td>\n",
       "      <td>04:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>7.696325</td>\n",
       "      <td>7.954337</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>542.000000</td>\n",
       "      <td>8.567785</td>\n",
       "      <td>7.954335</td>\n",
       "      <td>04:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>7.654782</td>\n",
       "      <td>7.936175</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>514.500000</td>\n",
       "      <td>8.338711</td>\n",
       "      <td>7.936176</td>\n",
       "      <td>04:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>7.693296</td>\n",
       "      <td>7.940475</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>516.750000</td>\n",
       "      <td>8.300385</td>\n",
       "      <td>7.940474</td>\n",
       "      <td>05:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>7.679593</td>\n",
       "      <td>7.913620</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>526.500000</td>\n",
       "      <td>8.368809</td>\n",
       "      <td>7.913620</td>\n",
       "      <td>04:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>7.629689</td>\n",
       "      <td>7.911077</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>510.750000</td>\n",
       "      <td>8.332546</td>\n",
       "      <td>7.911077</td>\n",
       "      <td>04:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>7.587280</td>\n",
       "      <td>7.885829</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>516.500000</td>\n",
       "      <td>8.319250</td>\n",
       "      <td>7.885829</td>\n",
       "      <td>04:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>7.613799</td>\n",
       "      <td>7.868060</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>534.500000</td>\n",
       "      <td>8.529382</td>\n",
       "      <td>7.868059</td>\n",
       "      <td>04:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>7.626361</td>\n",
       "      <td>7.862792</td>\n",
       "      <td>0.828000</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>529.750000</td>\n",
       "      <td>8.481831</td>\n",
       "      <td>7.862791</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>7.543518</td>\n",
       "      <td>7.847295</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>536.000000</td>\n",
       "      <td>8.498869</td>\n",
       "      <td>7.847296</td>\n",
       "      <td>04:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>7.505540</td>\n",
       "      <td>7.815285</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>521.500000</td>\n",
       "      <td>8.402745</td>\n",
       "      <td>7.815285</td>\n",
       "      <td>04:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>7.633868</td>\n",
       "      <td>7.820739</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>517.500000</td>\n",
       "      <td>8.291863</td>\n",
       "      <td>7.820739</td>\n",
       "      <td>04:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>7.558487</td>\n",
       "      <td>7.804825</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>524.750000</td>\n",
       "      <td>8.423656</td>\n",
       "      <td>7.804825</td>\n",
       "      <td>04:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>7.566806</td>\n",
       "      <td>7.782999</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>533.000000</td>\n",
       "      <td>8.517635</td>\n",
       "      <td>7.782998</td>\n",
       "      <td>04:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>7.507692</td>\n",
       "      <td>7.772132</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>519.000000</td>\n",
       "      <td>8.441522</td>\n",
       "      <td>7.772133</td>\n",
       "      <td>04:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>7.477268</td>\n",
       "      <td>7.751863</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>543.750000</td>\n",
       "      <td>8.546373</td>\n",
       "      <td>7.751863</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>7.402545</td>\n",
       "      <td>7.742107</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>547.250000</td>\n",
       "      <td>8.543318</td>\n",
       "      <td>7.742107</td>\n",
       "      <td>04:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>7.426970</td>\n",
       "      <td>7.737175</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>548.250000</td>\n",
       "      <td>8.517465</td>\n",
       "      <td>7.737175</td>\n",
       "      <td>04:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>7.405675</td>\n",
       "      <td>7.733170</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>577.250000</td>\n",
       "      <td>8.774895</td>\n",
       "      <td>7.733169</td>\n",
       "      <td>04:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>7.425478</td>\n",
       "      <td>7.724874</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>553.000000</td>\n",
       "      <td>8.560536</td>\n",
       "      <td>7.724873</td>\n",
       "      <td>04:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>7.509071</td>\n",
       "      <td>7.713431</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>561.500000</td>\n",
       "      <td>8.647264</td>\n",
       "      <td>7.713431</td>\n",
       "      <td>04:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>7.471950</td>\n",
       "      <td>7.684023</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>540.000000</td>\n",
       "      <td>8.504589</td>\n",
       "      <td>7.684024</td>\n",
       "      <td>04:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='512' class='' max='562', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      91.10% [512/562 03:55<00:22 7.4431]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[9.3927e-08],\n",
      "        [1.4204e-08],\n",
      "        [2.0676e-04],\n",
      "        [1.6238e-05],\n",
      "        [2.9843e-06],\n",
      "        [4.7399e-05],\n",
      "        [8.6245e-07],\n",
      "        [7.0573e-04],\n",
      "        [1.0610e-05],\n",
      "        [1.3254e-04],\n",
      "        [5.8507e-06],\n",
      "        [6.8182e-05],\n",
      "        [3.4907e-05],\n",
      "        [8.2731e-05],\n",
      "        [3.7027e-06],\n",
      "        [5.5741e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.35261344909668: \n",
      "target probs tensor([[1.0760e-05],\n",
      "        [7.6270e-05],\n",
      "        [6.7868e-06],\n",
      "        [1.2161e-05],\n",
      "        [1.1132e-05],\n",
      "        [2.9558e-05],\n",
      "        [1.3599e-04],\n",
      "        [1.1360e-04],\n",
      "        [3.8879e-05],\n",
      "        [5.9654e-05],\n",
      "        [2.5850e-05],\n",
      "        [8.0547e-05],\n",
      "        [1.1621e-04],\n",
      "        [4.0961e-04],\n",
      "        [5.9444e-06],\n",
      "        [7.1828e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.2855224609375: \n",
      "target probs tensor([[2.3762e-05],\n",
      "        [4.1191e-05],\n",
      "        [8.7261e-06],\n",
      "        [1.8175e-05],\n",
      "        [1.7390e-05],\n",
      "        [2.0391e-06],\n",
      "        [3.0716e-04],\n",
      "        [7.0206e-05],\n",
      "        [2.5876e-05],\n",
      "        [1.2759e-04],\n",
      "        [1.6665e-04],\n",
      "        [4.1221e-05],\n",
      "        [4.3070e-05],\n",
      "        [1.0272e-04],\n",
      "        [7.7442e-05],\n",
      "        [8.2301e-07]], device='cuda:0'), loss: 10.37884521484375: \n",
      "Better model found at epoch 0 with validation value: 0.5640000104904175.\n",
      "target probs tensor([[1.1782e-05],\n",
      "        [2.5889e-06],\n",
      "        [4.4764e-06],\n",
      "        [2.0469e-04],\n",
      "        [3.3022e-06],\n",
      "        [3.9560e-05],\n",
      "        [1.3332e-04],\n",
      "        [5.0892e-06],\n",
      "        [5.2038e-04],\n",
      "        [2.3755e-05],\n",
      "        [2.1913e-05],\n",
      "        [3.1197e-05],\n",
      "        [5.7800e-04],\n",
      "        [1.5796e-04],\n",
      "        [6.4780e-05],\n",
      "        [4.4649e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.110845565795898: \n",
      "target probs tensor([[4.9502e-04],\n",
      "        [3.8155e-06],\n",
      "        [8.3597e-04],\n",
      "        [3.5940e-05],\n",
      "        [3.2860e-04],\n",
      "        [1.0684e-03],\n",
      "        [1.6537e-04],\n",
      "        [2.0019e-05],\n",
      "        [4.2174e-04],\n",
      "        [2.5352e-06],\n",
      "        [1.4798e-03],\n",
      "        [3.4829e-04],\n",
      "        [1.7180e-05],\n",
      "        [1.4344e-04],\n",
      "        [5.1825e-05],\n",
      "        [7.8166e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.129789352416992: \n",
      "target probs tensor([[3.4052e-04],\n",
      "        [3.9821e-05],\n",
      "        [5.5871e-04],\n",
      "        [1.4248e-05],\n",
      "        [2.9891e-04],\n",
      "        [6.3863e-04],\n",
      "        [1.7879e-04],\n",
      "        [2.8548e-05],\n",
      "        [1.1943e-06],\n",
      "        [9.8662e-04],\n",
      "        [8.7931e-04],\n",
      "        [2.2814e-03],\n",
      "        [1.1552e-04],\n",
      "        [1.3510e-04],\n",
      "        [3.5795e-05],\n",
      "        [1.5006e-03]], device='cuda:0'), loss: 8.732698440551758: \n",
      "Better model found at epoch 1 with validation value: 0.7260000109672546.\n",
      "target probs tensor([[2.8431e-04],\n",
      "        [3.3149e-04],\n",
      "        [7.9331e-04],\n",
      "        [2.9696e-03],\n",
      "        [6.5544e-04],\n",
      "        [7.9077e-05],\n",
      "        [3.3268e-04],\n",
      "        [5.6648e-04],\n",
      "        [1.2795e-04],\n",
      "        [2.8435e-04],\n",
      "        [8.0595e-04],\n",
      "        [3.4121e-04],\n",
      "        [1.1159e-04],\n",
      "        [8.3459e-05],\n",
      "        [1.3700e-04],\n",
      "        [3.6363e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.058635711669922: \n",
      "target probs tensor([[4.4304e-03],\n",
      "        [3.1609e-08],\n",
      "        [2.1256e-04],\n",
      "        [1.4676e-04],\n",
      "        [1.3702e-04],\n",
      "        [2.4136e-04],\n",
      "        [1.6759e-04],\n",
      "        [4.5074e-04],\n",
      "        [4.0529e-04],\n",
      "        [3.6738e-06],\n",
      "        [1.0883e-05],\n",
      "        [1.1046e-04],\n",
      "        [2.9951e-04],\n",
      "        [4.5091e-05],\n",
      "        [2.7046e-04],\n",
      "        [3.0179e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.45002555847168: \n",
      "target probs tensor([[7.7942e-05],\n",
      "        [3.5331e-05],\n",
      "        [2.9715e-06],\n",
      "        [1.3969e-06],\n",
      "        [1.3497e-03],\n",
      "        [3.7319e-05],\n",
      "        [2.4420e-04],\n",
      "        [6.0013e-06],\n",
      "        [4.2689e-04],\n",
      "        [1.5197e-03],\n",
      "        [1.9311e-04],\n",
      "        [1.8861e-04],\n",
      "        [1.7204e-05],\n",
      "        [1.2336e-04],\n",
      "        [1.1746e-04],\n",
      "        [2.8565e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.764068603515625: \n",
      "Better model found at epoch 2 with validation value: 0.7639999985694885.\n",
      "target probs tensor([[2.3563e-05],\n",
      "        [1.2034e-04],\n",
      "        [1.7136e-04],\n",
      "        [7.0492e-04],\n",
      "        [1.4423e-05],\n",
      "        [2.6113e-04],\n",
      "        [1.2095e-04],\n",
      "        [1.9977e-04],\n",
      "        [3.9908e-05],\n",
      "        [5.2367e-04],\n",
      "        [1.2427e-04],\n",
      "        [4.4969e-05],\n",
      "        [7.2542e-05],\n",
      "        [1.6502e-04],\n",
      "        [5.8820e-05],\n",
      "        [7.0764e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.029212951660156: \n",
      "target probs tensor([[1.6938e-04],\n",
      "        [8.7254e-05],\n",
      "        [1.6438e-04],\n",
      "        [1.4551e-04],\n",
      "        [9.5473e-05],\n",
      "        [9.6621e-04],\n",
      "        [3.9809e-04],\n",
      "        [4.2869e-05],\n",
      "        [1.1854e-02],\n",
      "        [1.0504e-04],\n",
      "        [5.9265e-05],\n",
      "        [2.2223e-04],\n",
      "        [4.1357e-04],\n",
      "        [1.5315e-04],\n",
      "        [2.7766e-05],\n",
      "        [8.2280e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.473396301269531: \n",
      "target probs tensor([[1.0549e-04],\n",
      "        [9.0813e-04],\n",
      "        [2.9430e-05],\n",
      "        [7.3942e-05],\n",
      "        [3.2110e-03],\n",
      "        [6.2879e-04],\n",
      "        [1.5301e-04],\n",
      "        [1.8417e-05],\n",
      "        [1.4256e-05],\n",
      "        [1.4212e-04],\n",
      "        [2.8636e-04],\n",
      "        [3.5507e-04],\n",
      "        [5.9755e-04],\n",
      "        [2.1589e-05],\n",
      "        [1.9242e-06],\n",
      "        [3.9212e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.156182289123535: \n",
      "Better model found at epoch 3 with validation value: 0.7749999761581421.\n",
      "target probs tensor([[1.0848e-03],\n",
      "        [1.2094e-04],\n",
      "        [9.5136e-07],\n",
      "        [1.3953e-03],\n",
      "        [6.1568e-06],\n",
      "        [8.1273e-05],\n",
      "        [2.0385e-04],\n",
      "        [4.8509e-04],\n",
      "        [5.8179e-04],\n",
      "        [2.8394e-04],\n",
      "        [3.0603e-05],\n",
      "        [4.4188e-05],\n",
      "        [8.3718e-04],\n",
      "        [3.5232e-04],\n",
      "        [2.5902e-06],\n",
      "        [1.2094e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.174337387084961: \n",
      "target probs tensor([[7.8108e-04],\n",
      "        [1.4140e-04],\n",
      "        [6.7454e-04],\n",
      "        [7.8676e-04],\n",
      "        [1.9568e-04],\n",
      "        [3.6330e-05],\n",
      "        [2.5264e-05],\n",
      "        [7.2368e-05],\n",
      "        [9.8578e-07],\n",
      "        [2.6058e-05],\n",
      "        [2.0213e-05],\n",
      "        [2.0598e-04],\n",
      "        [3.3256e-05],\n",
      "        [1.2278e-05],\n",
      "        [1.4146e-04],\n",
      "        [6.3464e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.573686599731445: \n",
      "target probs tensor([[9.2914e-04],\n",
      "        [1.5251e-04],\n",
      "        [3.9530e-03],\n",
      "        [8.4987e-05],\n",
      "        [6.6397e-05],\n",
      "        [1.3609e-03],\n",
      "        [1.4508e-04],\n",
      "        [2.2939e-05],\n",
      "        [8.2841e-04],\n",
      "        [1.2734e-04],\n",
      "        [6.7024e-05],\n",
      "        [1.1005e-04],\n",
      "        [8.4810e-05],\n",
      "        [3.2513e-04],\n",
      "        [1.3536e-04],\n",
      "        [9.1771e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.551002502441406: \n",
      "Better model found at epoch 4 with validation value: 0.7870000004768372.\n",
      "target probs tensor([[1.4499e-04],\n",
      "        [9.6939e-06],\n",
      "        [5.6112e-03],\n",
      "        [1.6950e-04],\n",
      "        [3.7741e-05],\n",
      "        [1.8801e-05],\n",
      "        [6.5186e-05],\n",
      "        [3.9260e-05],\n",
      "        [4.5185e-04],\n",
      "        [2.8935e-05],\n",
      "        [3.4157e-05],\n",
      "        [3.6360e-05],\n",
      "        [3.7116e-04],\n",
      "        [1.7865e-04],\n",
      "        [1.7596e-04],\n",
      "        [4.5595e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.30797004699707: \n",
      "target probs tensor([[2.8642e-04],\n",
      "        [9.0662e-05],\n",
      "        [5.2615e-05],\n",
      "        [2.1321e-05],\n",
      "        [1.3604e-04],\n",
      "        [5.5420e-07],\n",
      "        [3.0374e-05],\n",
      "        [3.1887e-04],\n",
      "        [5.4016e-05],\n",
      "        [6.9936e-04],\n",
      "        [5.8720e-04],\n",
      "        [9.7669e-05],\n",
      "        [1.7354e-03],\n",
      "        [1.7132e-05],\n",
      "        [1.9709e-04],\n",
      "        [6.0886e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.179250717163086: \n",
      "target probs tensor([[4.6185e-05],\n",
      "        [3.2145e-04],\n",
      "        [8.4271e-04],\n",
      "        [7.5072e-07],\n",
      "        [4.2613e-04],\n",
      "        [7.1703e-05],\n",
      "        [2.4847e-03],\n",
      "        [5.3848e-04],\n",
      "        [2.7181e-05],\n",
      "        [2.0767e-04],\n",
      "        [9.0496e-05],\n",
      "        [9.7076e-05],\n",
      "        [3.7697e-04],\n",
      "        [4.0516e-05],\n",
      "        [3.4305e-04],\n",
      "        [1.9297e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.025482177734375: \n",
      "Better model found at epoch 5 with validation value: 0.7879999876022339.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[1.6789e-04],\n",
      "        [4.1932e-04],\n",
      "        [2.2294e-04],\n",
      "        [1.2662e-04],\n",
      "        [5.1016e-04],\n",
      "        [1.8981e-05],\n",
      "        [2.2049e-04],\n",
      "        [2.4898e-04],\n",
      "        [2.0873e-04],\n",
      "        [2.4609e-05],\n",
      "        [1.3355e-06],\n",
      "        [1.2677e-04],\n",
      "        [1.3336e-04],\n",
      "        [3.9733e-04],\n",
      "        [1.9020e-06],\n",
      "        [5.2540e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.255364418029785: \n",
      "target probs tensor([[2.7405e-05],\n",
      "        [1.7263e-05],\n",
      "        [5.0514e-04],\n",
      "        [9.1626e-05],\n",
      "        [6.6137e-05],\n",
      "        [1.2847e-04],\n",
      "        [9.4670e-05],\n",
      "        [3.0019e-05],\n",
      "        [1.8770e-04],\n",
      "        [2.1299e-05],\n",
      "        [2.1954e-03],\n",
      "        [8.1133e-05],\n",
      "        [2.8916e-04],\n",
      "        [2.9882e-04],\n",
      "        [4.0381e-04],\n",
      "        [2.7926e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.985185623168945: \n",
      "target probs tensor([[1.0601e-04],\n",
      "        [3.9676e-04],\n",
      "        [2.5880e-04],\n",
      "        [1.8317e-04],\n",
      "        [7.3346e-04],\n",
      "        [8.6691e-04],\n",
      "        [5.4224e-04],\n",
      "        [7.7975e-06],\n",
      "        [1.7053e-04],\n",
      "        [1.8246e-04],\n",
      "        [6.8898e-05],\n",
      "        [1.0416e-04],\n",
      "        [1.8179e-05],\n",
      "        [3.7907e-04],\n",
      "        [7.7395e-05],\n",
      "        [1.5833e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.634027481079102: \n",
      "Better model found at epoch 6 with validation value: 0.7919999957084656.\n",
      "target probs tensor([[3.4935e-04],\n",
      "        [1.3100e-04],\n",
      "        [4.3863e-04],\n",
      "        [1.6727e-06],\n",
      "        [2.9319e-04],\n",
      "        [7.2957e-05],\n",
      "        [5.5900e-05],\n",
      "        [1.4115e-04],\n",
      "        [5.0183e-04],\n",
      "        [9.2498e-04],\n",
      "        [4.5094e-05],\n",
      "        [2.7541e-09],\n",
      "        [4.8735e-05],\n",
      "        [5.4860e-04],\n",
      "        [6.5540e-04],\n",
      "        [7.8940e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.401658058166504: \n",
      "target probs tensor([[5.5626e-05],\n",
      "        [2.1741e-05],\n",
      "        [1.0459e-03],\n",
      "        [4.5165e-04],\n",
      "        [8.6669e-04],\n",
      "        [2.0723e-04],\n",
      "        [3.1290e-04],\n",
      "        [4.7735e-04],\n",
      "        [2.1429e-04],\n",
      "        [7.0695e-04],\n",
      "        [1.2719e-03],\n",
      "        [2.8628e-04],\n",
      "        [1.4178e-04],\n",
      "        [4.0027e-04],\n",
      "        [2.0871e-04],\n",
      "        [1.3348e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.328741073608398: \n",
      "target probs tensor([[2.8106e-03],\n",
      "        [2.1399e-04],\n",
      "        [3.0947e-04],\n",
      "        [6.4831e-05],\n",
      "        [7.5815e-04],\n",
      "        [6.1428e-06],\n",
      "        [2.5021e-06],\n",
      "        [1.7427e-07],\n",
      "        [3.4802e-04],\n",
      "        [4.6278e-04],\n",
      "        [1.0303e-03],\n",
      "        [1.5280e-05],\n",
      "        [2.2463e-04],\n",
      "        [2.4773e-04],\n",
      "        [1.6871e-04],\n",
      "        [1.2401e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.086629867553711: \n",
      "target probs tensor([[2.4310e-04],\n",
      "        [4.9061e-04],\n",
      "        [2.7200e-04],\n",
      "        [4.7270e-04],\n",
      "        [2.4905e-05],\n",
      "        [4.7512e-04],\n",
      "        [3.9187e-04],\n",
      "        [5.6776e-04]], device='cuda:0'), loss: 8.172430992126465: \n",
      "target probs tensor([[2.0335e-05],\n",
      "        [1.5767e-04],\n",
      "        [2.2352e-04],\n",
      "        [2.1682e-04],\n",
      "        [3.9420e-05],\n",
      "        [1.2859e-04],\n",
      "        [1.0892e-03],\n",
      "        [1.6834e-04],\n",
      "        [1.4076e-04],\n",
      "        [7.9981e-04],\n",
      "        [1.6347e-05],\n",
      "        [3.6937e-04],\n",
      "        [3.6046e-05],\n",
      "        [1.3229e-05],\n",
      "        [9.4975e-05],\n",
      "        [2.7832e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.053106307983398: \n",
      "target probs tensor([[3.7613e-03],\n",
      "        [4.3456e-05],\n",
      "        [3.4758e-04],\n",
      "        [1.8189e-04],\n",
      "        [9.3309e-04],\n",
      "        [4.7960e-04],\n",
      "        [1.3960e-04],\n",
      "        [2.2685e-04],\n",
      "        [1.3800e-02],\n",
      "        [1.8879e-04],\n",
      "        [2.2040e-04],\n",
      "        [4.4931e-04],\n",
      "        [7.1488e-05],\n",
      "        [7.2413e-05],\n",
      "        [4.1031e-04],\n",
      "        [1.6198e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.186565399169922: \n",
      "target probs tensor([[7.2152e-06],\n",
      "        [2.2287e-04],\n",
      "        [8.7074e-05],\n",
      "        [1.6635e-04],\n",
      "        [3.0605e-05],\n",
      "        [3.7691e-05],\n",
      "        [5.6697e-04],\n",
      "        [4.1728e-05],\n",
      "        [1.9953e-04],\n",
      "        [1.2292e-04],\n",
      "        [4.2065e-04],\n",
      "        [1.4202e-04],\n",
      "        [3.3514e-04],\n",
      "        [3.6721e-04],\n",
      "        [5.3642e-04],\n",
      "        [7.9646e-05]], device='cuda:0'), loss: 8.96713924407959: \n",
      "target probs tensor([[9.1761e-05],\n",
      "        [9.9102e-04],\n",
      "        [9.9035e-05],\n",
      "        [8.8288e-05],\n",
      "        [1.3033e-04],\n",
      "        [2.0809e-04],\n",
      "        [4.7560e-04],\n",
      "        [9.9234e-05],\n",
      "        [6.6584e-06],\n",
      "        [3.9312e-06],\n",
      "        [3.5868e-04],\n",
      "        [1.5239e-04],\n",
      "        [5.6195e-06],\n",
      "        [4.3694e-04],\n",
      "        [3.7480e-04],\n",
      "        [9.6570e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.33813762664795: \n",
      "target probs tensor([[8.5282e-05],\n",
      "        [2.8200e-04],\n",
      "        [1.3850e-04],\n",
      "        [1.2154e-04],\n",
      "        [1.4156e-04],\n",
      "        [1.9592e-04],\n",
      "        [2.1525e-04],\n",
      "        [1.0658e-03],\n",
      "        [2.7568e-04],\n",
      "        [3.0140e-04],\n",
      "        [8.8662e-05],\n",
      "        [1.4371e-03],\n",
      "        [3.6655e-04],\n",
      "        [7.5930e-04],\n",
      "        [4.8219e-07],\n",
      "        [1.4445e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.81843090057373: \n",
      "target probs tensor([[1.2519e-03],\n",
      "        [7.7707e-05],\n",
      "        [7.0494e-04],\n",
      "        [1.8162e-05],\n",
      "        [4.1131e-04],\n",
      "        [1.0029e-03],\n",
      "        [2.9761e-04],\n",
      "        [6.7004e-05],\n",
      "        [4.4762e-07],\n",
      "        [3.7630e-03],\n",
      "        [6.3157e-03],\n",
      "        [3.5405e-03],\n",
      "        [3.8142e-04],\n",
      "        [1.7887e-04],\n",
      "        [5.3333e-05],\n",
      "        [3.5620e-04]], device='cuda:0'), loss: 8.246253967285156: \n",
      "Better model found at epoch 9 with validation value: 0.7960000038146973.\n",
      "target probs tensor([[2.6740e-06],\n",
      "        [3.2476e-06],\n",
      "        [3.3242e-05],\n",
      "        [3.0747e-04],\n",
      "        [2.9241e-04],\n",
      "        [1.1601e-04],\n",
      "        [9.4281e-04],\n",
      "        [4.4880e-04],\n",
      "        [9.0864e-05],\n",
      "        [1.0148e-05],\n",
      "        [5.7046e-05],\n",
      "        [1.9092e-04],\n",
      "        [2.4218e-04],\n",
      "        [6.2339e-04],\n",
      "        [1.5765e-04],\n",
      "        [3.4326e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.20756721496582: \n",
      "target probs tensor([[1.7682e-04],\n",
      "        [5.3915e-06],\n",
      "        [9.5285e-05],\n",
      "        [1.9298e-04],\n",
      "        [6.7744e-04],\n",
      "        [4.3759e-04],\n",
      "        [4.4038e-05],\n",
      "        [5.6898e-04],\n",
      "        [2.0042e-04],\n",
      "        [5.0236e-04],\n",
      "        [9.4936e-05],\n",
      "        [7.2702e-04],\n",
      "        [3.3915e-04],\n",
      "        [6.8888e-04],\n",
      "        [1.8008e-04],\n",
      "        [1.2655e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.392507553100586: \n",
      "target probs tensor([[2.9818e-05],\n",
      "        [3.1882e-04],\n",
      "        [1.5640e-04],\n",
      "        [1.2809e-03],\n",
      "        [2.8361e-04],\n",
      "        [5.1148e-04],\n",
      "        [2.1770e-05],\n",
      "        [1.5276e-04],\n",
      "        [9.2593e-04],\n",
      "        [6.2665e-04],\n",
      "        [6.0981e-04],\n",
      "        [2.1865e-04],\n",
      "        [6.2456e-05],\n",
      "        [1.1782e-03],\n",
      "        [1.6064e-05],\n",
      "        [1.0882e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.352465629577637: \n",
      "Better model found at epoch 10 with validation value: 0.8009999990463257.\n",
      "target probs tensor([[2.1404e-04],\n",
      "        [1.9971e-04],\n",
      "        [1.0609e-04],\n",
      "        [1.2539e-04],\n",
      "        [1.5990e-05],\n",
      "        [2.5484e-05],\n",
      "        [5.3284e-04],\n",
      "        [3.1842e-04],\n",
      "        [2.8946e-04],\n",
      "        [7.3456e-04],\n",
      "        [4.5818e-04],\n",
      "        [2.0287e-05],\n",
      "        [7.4391e-04],\n",
      "        [6.3257e-05],\n",
      "        [2.0730e-04],\n",
      "        [9.9712e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.796106338500977: \n",
      "target probs tensor([[1.3269e-04],\n",
      "        [3.5836e-04],\n",
      "        [2.0684e-04],\n",
      "        [1.8958e-03],\n",
      "        [5.2395e-04],\n",
      "        [1.8413e-04],\n",
      "        [2.6138e-04],\n",
      "        [2.2499e-04],\n",
      "        [1.1789e-05],\n",
      "        [6.5008e-05],\n",
      "        [1.6209e-05],\n",
      "        [2.9346e-05],\n",
      "        [2.1683e-04],\n",
      "        [3.9688e-05],\n",
      "        [7.4010e-05],\n",
      "        [3.5442e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.931181907653809: \n",
      "target probs tensor([[2.4996e-04],\n",
      "        [1.5243e-03],\n",
      "        [2.1469e-05],\n",
      "        [8.5755e-05],\n",
      "        [9.8419e-04],\n",
      "        [4.8108e-05],\n",
      "        [1.0808e-03],\n",
      "        [6.9797e-04],\n",
      "        [9.5429e-04],\n",
      "        [6.6401e-04],\n",
      "        [1.9214e-04],\n",
      "        [5.5425e-05],\n",
      "        [1.6094e-05],\n",
      "        [8.6068e-04],\n",
      "        [9.6187e-05],\n",
      "        [3.8857e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.355194091796875: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 11 with validation value: 0.8029999732971191.\n",
      "target probs tensor([[4.0643e-05],\n",
      "        [2.7245e-04],\n",
      "        [2.9614e-04],\n",
      "        [1.5834e-04],\n",
      "        [4.1325e-04],\n",
      "        [3.0348e-06],\n",
      "        [5.2283e-05],\n",
      "        [6.6885e-06],\n",
      "        [1.1386e-04],\n",
      "        [5.9342e-04],\n",
      "        [4.8012e-05],\n",
      "        [3.8757e-04],\n",
      "        [5.0252e-04],\n",
      "        [2.2963e-05],\n",
      "        [5.2355e-05],\n",
      "        [1.5181e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.293915748596191: \n",
      "target probs tensor([[2.0741e-03],\n",
      "        [3.3850e-04],\n",
      "        [5.2553e-04],\n",
      "        [8.7772e-05],\n",
      "        [9.5159e-06],\n",
      "        [7.1888e-04],\n",
      "        [1.4230e-05],\n",
      "        [7.8707e-04],\n",
      "        [1.0255e-05],\n",
      "        [1.3808e-04],\n",
      "        [4.5035e-04],\n",
      "        [1.6014e-04],\n",
      "        [2.4530e-04],\n",
      "        [5.3940e-05],\n",
      "        [1.2895e-04],\n",
      "        [4.9020e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.731658935546875: \n",
      "target probs tensor([[7.3025e-05],\n",
      "        [1.0688e-03],\n",
      "        [1.0687e-04],\n",
      "        [2.0173e-04],\n",
      "        [2.0209e-04],\n",
      "        [1.9631e-04],\n",
      "        [1.0750e-06],\n",
      "        [7.9897e-05],\n",
      "        [7.2516e-05],\n",
      "        [3.1807e-05],\n",
      "        [5.8260e-04],\n",
      "        [5.3988e-04],\n",
      "        [4.5569e-03],\n",
      "        [2.6811e-05],\n",
      "        [1.2373e-04],\n",
      "        [8.2729e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.963356018066406: \n",
      "target probs tensor([[1.8032e-04],\n",
      "        [1.3424e-05],\n",
      "        [2.1671e-04],\n",
      "        [8.2432e-05],\n",
      "        [1.1387e-04],\n",
      "        [2.2805e-05],\n",
      "        [2.2580e-04],\n",
      "        [1.4763e-04],\n",
      "        [1.7056e-04],\n",
      "        [1.7419e-04],\n",
      "        [2.5034e-04],\n",
      "        [1.0948e-03],\n",
      "        [3.3832e-04],\n",
      "        [1.0229e-03],\n",
      "        [7.5298e-05],\n",
      "        [2.9257e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.725903511047363: \n",
      "target probs tensor([[1.5846e-04],\n",
      "        [5.7831e-04],\n",
      "        [7.2629e-04],\n",
      "        [1.1854e-04],\n",
      "        [8.3919e-04],\n",
      "        [7.9276e-05],\n",
      "        [8.0927e-03],\n",
      "        [4.5253e-04],\n",
      "        [2.7447e-04],\n",
      "        [8.1203e-05],\n",
      "        [7.0820e-04],\n",
      "        [6.7440e-04],\n",
      "        [3.5368e-04],\n",
      "        [4.6893e-05],\n",
      "        [4.2661e-05],\n",
      "        [4.0468e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.092466354370117: \n",
      "target probs tensor([[6.4442e-04],\n",
      "        [2.0343e-04],\n",
      "        [4.7867e-04],\n",
      "        [4.3057e-04],\n",
      "        [6.3054e-04],\n",
      "        [6.1827e-04],\n",
      "        [8.3716e-04],\n",
      "        [4.5247e-05],\n",
      "        [1.1605e-03],\n",
      "        [4.3596e-05],\n",
      "        [8.5772e-05],\n",
      "        [1.9693e-04],\n",
      "        [1.6244e-04],\n",
      "        [2.3354e-04],\n",
      "        [3.3370e-04],\n",
      "        [9.3163e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.259893417358398: \n",
      "target probs tensor([[7.6071e-05],\n",
      "        [2.6644e-04],\n",
      "        [6.2512e-04],\n",
      "        [7.9988e-05],\n",
      "        [1.5444e-06],\n",
      "        [2.2849e-04],\n",
      "        [7.4002e-05],\n",
      "        [1.5745e-04],\n",
      "        [9.3477e-07],\n",
      "        [2.7837e-04],\n",
      "        [1.0453e-05],\n",
      "        [9.5016e-05],\n",
      "        [3.3177e-04],\n",
      "        [3.3318e-05],\n",
      "        [3.2667e-04],\n",
      "        [6.1694e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.731279373168945: \n",
      "target probs tensor([[3.6368e-05],\n",
      "        [1.6192e-04],\n",
      "        [3.8201e-04],\n",
      "        [5.7849e-05],\n",
      "        [8.6379e-04],\n",
      "        [3.0950e-05],\n",
      "        [8.0974e-04],\n",
      "        [2.8456e-04],\n",
      "        [1.2795e-04],\n",
      "        [8.8826e-05],\n",
      "        [8.5694e-05],\n",
      "        [1.2068e-04],\n",
      "        [2.4817e-04],\n",
      "        [4.3037e-04],\n",
      "        [4.7435e-04],\n",
      "        [1.7547e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.645771980285645: \n",
      "target probs tensor([[6.6074e-06],\n",
      "        [5.8637e-04],\n",
      "        [6.4992e-04],\n",
      "        [3.6711e-04],\n",
      "        [5.7643e-04],\n",
      "        [5.8772e-04],\n",
      "        [1.1970e-03],\n",
      "        [1.0425e-04],\n",
      "        [1.5896e-04],\n",
      "        [3.1925e-04],\n",
      "        [1.2568e-04],\n",
      "        [5.3419e-04],\n",
      "        [2.3740e-04],\n",
      "        [1.1210e-04],\n",
      "        [1.0531e-04],\n",
      "        [3.7458e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.46986198425293: \n",
      "Better model found at epoch 14 with validation value: 0.8050000071525574.\n",
      "target probs tensor([[5.0660e-05],\n",
      "        [2.5195e-04],\n",
      "        [1.4664e-04],\n",
      "        [1.7532e-04],\n",
      "        [1.0701e-04],\n",
      "        [1.0817e-04],\n",
      "        [1.0194e-04],\n",
      "        [7.6956e-05],\n",
      "        [3.3430e-04],\n",
      "        [7.8247e-04],\n",
      "        [1.0556e-04],\n",
      "        [4.6643e-07],\n",
      "        [7.5489e-05],\n",
      "        [1.8352e-05],\n",
      "        [1.9162e-05],\n",
      "        [8.6417e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.362198829650879: \n",
      "target probs tensor([[8.3245e-05],\n",
      "        [5.9517e-05],\n",
      "        [5.9579e-04],\n",
      "        [4.9177e-04],\n",
      "        [5.3337e-06],\n",
      "        [1.1831e-03],\n",
      "        [1.1266e-04],\n",
      "        [1.0007e-04],\n",
      "        [1.3784e-07],\n",
      "        [5.7179e-04],\n",
      "        [5.1122e-04],\n",
      "        [8.3457e-04],\n",
      "        [3.5262e-04],\n",
      "        [3.9093e-04],\n",
      "        [1.0688e-04],\n",
      "        [2.3938e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.909795761108398: \n",
      "target probs tensor([[3.6899e-04],\n",
      "        [5.2826e-04],\n",
      "        [2.0117e-04],\n",
      "        [6.8293e-04],\n",
      "        [1.7064e-03],\n",
      "        [1.1352e-03],\n",
      "        [2.3103e-04],\n",
      "        [8.8111e-04],\n",
      "        [1.3054e-04],\n",
      "        [7.5289e-04],\n",
      "        [1.3644e-07],\n",
      "        [1.5685e-04],\n",
      "        [2.0358e-04],\n",
      "        [1.9126e-03],\n",
      "        [5.3242e-04],\n",
      "        [9.4930e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.254634857177734: \n",
      "target probs tensor([[2.8806e-04],\n",
      "        [5.5773e-04],\n",
      "        [4.8927e-04],\n",
      "        [3.3994e-04],\n",
      "        [2.9572e-05],\n",
      "        [7.8815e-04],\n",
      "        [4.2792e-04],\n",
      "        [9.1812e-04]], device='cuda:0'), loss: 7.947196006774902: \n",
      "Better model found at epoch 15 with validation value: 0.8069999814033508.\n",
      "target probs tensor([[4.2163e-04],\n",
      "        [2.2348e-06],\n",
      "        [3.6403e-04],\n",
      "        [1.2582e-04],\n",
      "        [8.9367e-05],\n",
      "        [3.0302e-03],\n",
      "        [6.1210e-05],\n",
      "        [1.3632e-03],\n",
      "        [3.8718e-04],\n",
      "        [2.3077e-05],\n",
      "        [1.5508e-04],\n",
      "        [4.0785e-04],\n",
      "        [7.3491e-04],\n",
      "        [2.8102e-07],\n",
      "        [1.8521e-05],\n",
      "        [1.8593e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.124944686889648: \n",
      "target probs tensor([[1.7585e-04],\n",
      "        [3.7812e-04],\n",
      "        [3.1986e-04],\n",
      "        [6.3297e-04],\n",
      "        [1.9704e-05],\n",
      "        [2.9648e-04],\n",
      "        [1.8576e-06],\n",
      "        [2.0680e-05],\n",
      "        [5.6738e-04],\n",
      "        [1.0918e-04],\n",
      "        [5.4518e-05],\n",
      "        [8.6489e-06],\n",
      "        [1.0527e-04],\n",
      "        [5.0615e-04],\n",
      "        [8.5542e-05],\n",
      "        [1.3081e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.250468254089355: \n",
      "target probs tensor([[9.5673e-06],\n",
      "        [2.0323e-04],\n",
      "        [5.9031e-05],\n",
      "        [1.2395e-04],\n",
      "        [3.0215e-05],\n",
      "        [5.7078e-05],\n",
      "        [5.9875e-04],\n",
      "        [5.3998e-05],\n",
      "        [3.2564e-04],\n",
      "        [1.4029e-04],\n",
      "        [3.6815e-04],\n",
      "        [1.7620e-04],\n",
      "        [2.9145e-04],\n",
      "        [5.5198e-04],\n",
      "        [6.8997e-04],\n",
      "        [1.8437e-04]], device='cuda:0'), loss: 8.824335098266602: \n",
      "Better model found at epoch 16 with validation value: 0.8140000104904175.\n",
      "target probs tensor([[1.3764e-04],\n",
      "        [8.7992e-08],\n",
      "        [6.3935e-04],\n",
      "        [1.7302e-04],\n",
      "        [8.9350e-05],\n",
      "        [3.4387e-04],\n",
      "        [5.4147e-04],\n",
      "        [4.9038e-06],\n",
      "        [4.8117e-06],\n",
      "        [1.6688e-04],\n",
      "        [3.5469e-05],\n",
      "        [1.9045e-04],\n",
      "        [2.6283e-04],\n",
      "        [2.4917e-04],\n",
      "        [3.5639e-04],\n",
      "        [1.0329e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.475762367248535: \n",
      "target probs tensor([[2.4847e-04],\n",
      "        [1.5610e-04],\n",
      "        [5.2690e-04],\n",
      "        [7.3643e-05],\n",
      "        [3.4536e-05],\n",
      "        [1.5957e-04],\n",
      "        [8.6184e-04],\n",
      "        [3.3316e-04],\n",
      "        [8.3506e-04],\n",
      "        [5.9013e-04],\n",
      "        [2.3991e-04],\n",
      "        [3.0991e-04],\n",
      "        [3.7711e-06],\n",
      "        [6.1439e-04],\n",
      "        [4.9100e-04],\n",
      "        [4.1093e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.402912139892578: \n",
      "target probs tensor([[2.1432e-03],\n",
      "        [7.7779e-05],\n",
      "        [2.2168e-03],\n",
      "        [1.9471e-05],\n",
      "        [4.4118e-04],\n",
      "        [9.8406e-04],\n",
      "        [1.6880e-04],\n",
      "        [1.0750e-04],\n",
      "        [1.9058e-06],\n",
      "        [4.5617e-03],\n",
      "        [8.3589e-03],\n",
      "        [2.4372e-03],\n",
      "        [4.4475e-04],\n",
      "        [1.8917e-04],\n",
      "        [4.1254e-05],\n",
      "        [4.6602e-04]], device='cuda:0'), loss: 8.028749465942383: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[1.0878e-04],\n",
      "        [5.6370e-04],\n",
      "        [2.2513e-04],\n",
      "        [6.1167e-04],\n",
      "        [9.9114e-05],\n",
      "        [1.5460e-04],\n",
      "        [3.3802e-04],\n",
      "        [3.2699e-04],\n",
      "        [1.9970e-04],\n",
      "        [2.8192e-04],\n",
      "        [1.4395e-03],\n",
      "        [2.1996e-04],\n",
      "        [1.6767e-04],\n",
      "        [6.6191e-04],\n",
      "        [1.8042e-04],\n",
      "        [6.6402e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.270569801330566: \n",
      "target probs tensor([[2.2530e-04],\n",
      "        [1.4749e-04],\n",
      "        [9.9563e-05],\n",
      "        [7.6979e-05],\n",
      "        [3.9388e-04],\n",
      "        [3.2681e-04],\n",
      "        [1.7357e-05],\n",
      "        [5.1188e-03],\n",
      "        [4.5945e-04],\n",
      "        [9.1228e-04],\n",
      "        [1.8479e-04],\n",
      "        [3.0788e-03],\n",
      "        [1.5827e-04],\n",
      "        [5.6949e-04],\n",
      "        [1.0726e-03],\n",
      "        [1.3708e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.920309543609619: \n",
      "target probs tensor([[1.3695e-03],\n",
      "        [8.0439e-05],\n",
      "        [7.5496e-02],\n",
      "        [3.6837e-04],\n",
      "        [1.0532e-04],\n",
      "        [3.1790e-04],\n",
      "        [1.5362e-05],\n",
      "        [4.5793e-04],\n",
      "        [1.1155e-06],\n",
      "        [1.0452e-04],\n",
      "        [5.6189e-04],\n",
      "        [1.4159e-04],\n",
      "        [4.3528e-04],\n",
      "        [2.4962e-04],\n",
      "        [3.1724e-04],\n",
      "        [1.3885e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.27409553527832: \n",
      "Better model found at epoch 18 with validation value: 0.8169999718666077.\n",
      "target probs tensor([[1.1877e-04],\n",
      "        [1.1523e-05],\n",
      "        [3.3442e-07],\n",
      "        [2.9014e-04],\n",
      "        [3.3966e-06],\n",
      "        [1.0440e-03],\n",
      "        [1.0494e-04],\n",
      "        [2.1068e-04],\n",
      "        [1.5828e-04],\n",
      "        [6.4695e-04],\n",
      "        [1.4800e-05],\n",
      "        [3.9187e-04],\n",
      "        [5.1665e-04],\n",
      "        [1.1007e-04],\n",
      "        [2.0516e-04],\n",
      "        [4.6864e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.27811050415039: \n",
      "target probs tensor([[1.5281e-04],\n",
      "        [3.3835e-05],\n",
      "        [1.9949e-04],\n",
      "        [1.9941e-04],\n",
      "        [1.4281e-04],\n",
      "        [2.4028e-04],\n",
      "        [6.1647e-04],\n",
      "        [8.8508e-05],\n",
      "        [1.2924e-04],\n",
      "        [7.1675e-06],\n",
      "        [3.7252e-04],\n",
      "        [4.9496e-04],\n",
      "        [2.9466e-04],\n",
      "        [6.8934e-04],\n",
      "        [8.9859e-05],\n",
      "        [2.2278e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.717122077941895: \n",
      "target probs tensor([[1.0376e-03],\n",
      "        [3.7417e-04],\n",
      "        [9.8834e-04],\n",
      "        [2.7367e-04],\n",
      "        [3.2078e-05],\n",
      "        [3.0398e-04],\n",
      "        [9.3909e-04],\n",
      "        [1.1444e-03],\n",
      "        [2.4779e-04],\n",
      "        [5.7710e-04],\n",
      "        [2.8430e-05],\n",
      "        [7.6605e-05],\n",
      "        [5.2911e-04],\n",
      "        [3.4081e-04],\n",
      "        [1.1767e-03],\n",
      "        [3.2234e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.8618950843811035: \n",
      "target probs tensor([[2.7837e-03],\n",
      "        [1.7077e-04],\n",
      "        [4.3463e-05],\n",
      "        [1.9951e-04],\n",
      "        [4.6184e-05],\n",
      "        [8.1659e-05],\n",
      "        [3.9102e-04],\n",
      "        [8.4877e-04],\n",
      "        [2.5023e-04],\n",
      "        [1.4195e-03],\n",
      "        [2.7567e-04],\n",
      "        [3.1980e-04],\n",
      "        [1.3816e-04],\n",
      "        [3.3522e-04],\n",
      "        [3.4266e-04],\n",
      "        [4.1142e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.199623107910156: \n",
      "target probs tensor([[4.3702e-05],\n",
      "        [4.4465e-04],\n",
      "        [5.8001e-05],\n",
      "        [1.0839e-04],\n",
      "        [3.1043e-05],\n",
      "        [2.1080e-04],\n",
      "        [1.4835e-04],\n",
      "        [1.7474e-04],\n",
      "        [1.5576e-04],\n",
      "        [8.3154e-04],\n",
      "        [3.3131e-04],\n",
      "        [1.2948e-04],\n",
      "        [1.2019e-04],\n",
      "        [3.0307e-05],\n",
      "        [8.8509e-04],\n",
      "        [6.0519e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.584102630615234: \n",
      "target probs tensor([[5.6613e-05],\n",
      "        [6.9160e-04],\n",
      "        [1.4852e-03],\n",
      "        [4.1008e-06],\n",
      "        [3.9730e-05],\n",
      "        [9.7606e-06],\n",
      "        [2.6406e-05],\n",
      "        [2.3574e-04],\n",
      "        [4.6595e-04],\n",
      "        [5.7003e-06],\n",
      "        [1.1590e-04],\n",
      "        [1.1683e-03],\n",
      "        [2.6920e-04],\n",
      "        [5.7442e-04],\n",
      "        [4.7651e-04],\n",
      "        [4.1288e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.951404571533203: \n",
      "target probs tensor([[2.3973e-04],\n",
      "        [3.3799e-03],\n",
      "        [1.4085e-05],\n",
      "        [2.0128e-04],\n",
      "        [5.5787e-05],\n",
      "        [8.5427e-05],\n",
      "        [4.8118e-05],\n",
      "        [8.6180e-04],\n",
      "        [3.0190e-04],\n",
      "        [1.7910e-04],\n",
      "        [3.9347e-04],\n",
      "        [9.3863e-04],\n",
      "        [3.4823e-04],\n",
      "        [1.2634e-04],\n",
      "        [1.7751e-04],\n",
      "        [1.6109e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.482564926147461: \n",
      "target probs tensor([[8.5644e-05],\n",
      "        [5.7178e-04],\n",
      "        [3.5413e-04],\n",
      "        [4.6114e-03],\n",
      "        [8.3818e-04],\n",
      "        [2.2574e-04],\n",
      "        [7.3871e-05],\n",
      "        [3.5465e-05],\n",
      "        [6.2278e-06],\n",
      "        [6.0437e-04],\n",
      "        [2.8609e-04],\n",
      "        [3.0218e-04],\n",
      "        [2.1889e-04],\n",
      "        [1.1452e-03],\n",
      "        [6.7533e-05],\n",
      "        [1.0635e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.294211387634277: \n",
      "target probs tensor([[8.3974e-04],\n",
      "        [7.4904e-05],\n",
      "        [7.9198e-04],\n",
      "        [8.8725e-06],\n",
      "        [8.2649e-04],\n",
      "        [4.3374e-04],\n",
      "        [2.7543e-04],\n",
      "        [5.1079e-04],\n",
      "        [8.2719e-05],\n",
      "        [3.4226e-04],\n",
      "        [8.0825e-07],\n",
      "        [2.3889e-04],\n",
      "        [7.3115e-05],\n",
      "        [1.5047e-03],\n",
      "        [5.8302e-05],\n",
      "        [7.9585e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.664361000061035: \n",
      "Better model found at epoch 21 with validation value: 0.8180000185966492.\n",
      "target probs tensor([[4.0101e-04],\n",
      "        [2.1925e-04],\n",
      "        [1.2958e-04],\n",
      "        [4.7387e-04],\n",
      "        [4.0575e-04],\n",
      "        [4.0046e-05],\n",
      "        [4.2046e-05],\n",
      "        [5.8207e-04],\n",
      "        [2.4742e-04],\n",
      "        [1.6473e-06],\n",
      "        [6.7445e-04],\n",
      "        [9.7392e-05],\n",
      "        [2.1025e-05],\n",
      "        [7.6960e-05],\n",
      "        [8.1676e-04],\n",
      "        [1.8358e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.901729583740234: \n",
      "target probs tensor([[3.2467e-05],\n",
      "        [3.4082e-05],\n",
      "        [5.5618e-04],\n",
      "        [1.9455e-04],\n",
      "        [3.5153e-05],\n",
      "        [8.4196e-05],\n",
      "        [2.8779e-05],\n",
      "        [8.3601e-04],\n",
      "        [2.2583e-04],\n",
      "        [1.9008e-04],\n",
      "        [1.8845e-04],\n",
      "        [2.6651e-04],\n",
      "        [2.0208e-04],\n",
      "        [3.5070e-04],\n",
      "        [7.2624e-06],\n",
      "        [4.0327e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.982742309570312: \n",
      "target probs tensor([[1.3911e-03],\n",
      "        [5.7991e-02],\n",
      "        [1.1048e-04],\n",
      "        [6.5581e-04],\n",
      "        [1.6522e-04],\n",
      "        [3.9561e-04],\n",
      "        [7.0405e-04],\n",
      "        [5.0230e-04],\n",
      "        [1.8169e-04],\n",
      "        [2.0102e-04],\n",
      "        [1.4100e-04],\n",
      "        [5.9902e-06],\n",
      "        [3.0696e-06],\n",
      "        [5.8190e-04],\n",
      "        [1.6034e-04],\n",
      "        [3.9753e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.393461227416992: \n",
      "target probs tensor([[1.2573e-04],\n",
      "        [5.3061e-04],\n",
      "        [2.1740e-04],\n",
      "        [6.8732e-04],\n",
      "        [9.7263e-04],\n",
      "        [7.2323e-05],\n",
      "        [4.0766e-04],\n",
      "        [5.7135e-04],\n",
      "        [1.9560e-03],\n",
      "        [3.6374e-05],\n",
      "        [1.0990e-07],\n",
      "        [5.3426e-04],\n",
      "        [3.5638e-04],\n",
      "        [4.1311e-04],\n",
      "        [6.1721e-05],\n",
      "        [5.6934e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.699721336364746: \n",
      "target probs tensor([[5.6567e-04],\n",
      "        [3.1910e-04],\n",
      "        [2.2182e-04],\n",
      "        [5.0034e-04],\n",
      "        [9.3354e-04],\n",
      "        [3.2510e-04],\n",
      "        [6.2744e-04],\n",
      "        [3.6409e-04],\n",
      "        [2.5117e-04],\n",
      "        [1.2816e-03],\n",
      "        [4.2013e-04],\n",
      "        [7.1363e-06],\n",
      "        [4.0603e-04],\n",
      "        [2.5848e-04],\n",
      "        [7.7780e-05],\n",
      "        [1.2659e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.038675308227539: \n",
      "target probs tensor([[3.6065e-05],\n",
      "        [5.4358e-05],\n",
      "        [1.6853e-04],\n",
      "        [2.9334e-04],\n",
      "        [4.2280e-04],\n",
      "        [2.4524e-06],\n",
      "        [2.0482e-06],\n",
      "        [1.5250e-04],\n",
      "        [1.0917e-04],\n",
      "        [1.1024e-04],\n",
      "        [7.8755e-05],\n",
      "        [1.3855e-04],\n",
      "        [1.0360e-03],\n",
      "        [4.8223e-04],\n",
      "        [3.3054e-04],\n",
      "        [1.3320e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.216463088989258: \n",
      "target probs tensor([[3.4581e-04],\n",
      "        [9.9912e-04],\n",
      "        [5.3920e-04],\n",
      "        [5.9389e-04],\n",
      "        [4.1553e-05],\n",
      "        [8.1064e-04],\n",
      "        [5.5197e-04],\n",
      "        [1.0678e-03]], device='cuda:0'), loss: 7.67286491394043: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[2.1566e-04],\n",
      "        [9.6138e-04],\n",
      "        [1.0989e-04],\n",
      "        [6.5978e-06],\n",
      "        [6.5956e-05],\n",
      "        [1.0975e-03],\n",
      "        [1.6038e-03],\n",
      "        [8.4764e-05],\n",
      "        [4.1246e-04],\n",
      "        [7.9121e-05],\n",
      "        [7.0512e-03],\n",
      "        [3.8789e-04],\n",
      "        [6.2636e-04],\n",
      "        [5.2058e-04],\n",
      "        [3.4011e-04],\n",
      "        [6.5884e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.06128978729248: \n",
      "target probs tensor([[5.2402e-05],\n",
      "        [1.3967e-04],\n",
      "        [3.6161e-04],\n",
      "        [2.7850e-04],\n",
      "        [3.4132e-05],\n",
      "        [4.3954e-04],\n",
      "        [4.8357e-03],\n",
      "        [2.2824e-04],\n",
      "        [2.9922e-03],\n",
      "        [2.9836e-04],\n",
      "        [7.0444e-04],\n",
      "        [5.9987e-04],\n",
      "        [8.5911e-04],\n",
      "        [1.4905e-02],\n",
      "        [2.9215e-04],\n",
      "        [4.8238e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.638893127441406: \n",
      "target probs tensor([[9.2692e-06],\n",
      "        [1.6760e-04],\n",
      "        [8.9331e-05],\n",
      "        [1.3989e-04],\n",
      "        [2.8702e-05],\n",
      "        [6.5443e-05],\n",
      "        [8.2816e-04],\n",
      "        [6.1578e-05],\n",
      "        [3.4507e-04],\n",
      "        [9.6796e-05],\n",
      "        [4.7406e-04],\n",
      "        [1.8493e-04],\n",
      "        [3.8159e-04],\n",
      "        [6.3942e-04],\n",
      "        [1.0096e-03],\n",
      "        [1.9038e-04]], device='cuda:0'), loss: 8.72000503540039: \n",
      "Better model found at epoch 24 with validation value: 0.8230000138282776.\n",
      "target probs tensor([[2.3068e-04],\n",
      "        [1.4090e-03],\n",
      "        [5.5026e-04],\n",
      "        [4.8169e-06],\n",
      "        [5.2248e-04],\n",
      "        [2.2158e-04],\n",
      "        [5.3798e-04],\n",
      "        [4.6127e-05],\n",
      "        [6.3682e-04],\n",
      "        [6.6350e-05],\n",
      "        [2.0089e-04],\n",
      "        [1.7850e-04],\n",
      "        [3.6960e-04],\n",
      "        [3.1821e-04],\n",
      "        [3.4172e-04],\n",
      "        [3.5477e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.098174095153809: \n",
      "target probs tensor([[7.3744e-03],\n",
      "        [1.8262e-04],\n",
      "        [6.0863e-04],\n",
      "        [5.1318e-04],\n",
      "        [7.5325e-05],\n",
      "        [3.1441e-05],\n",
      "        [2.6201e-04],\n",
      "        [3.0854e-05],\n",
      "        [7.8419e-05],\n",
      "        [7.4703e-05],\n",
      "        [7.2254e-05],\n",
      "        [2.2847e-05],\n",
      "        [4.8129e-04],\n",
      "        [3.2623e-04],\n",
      "        [3.1069e-05],\n",
      "        [2.5264e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.92508316040039: \n",
      "target probs tensor([[2.0429e-03],\n",
      "        [1.1118e-04],\n",
      "        [2.5160e-03],\n",
      "        [2.7320e-05],\n",
      "        [5.0238e-04],\n",
      "        [1.1764e-03],\n",
      "        [2.1265e-04],\n",
      "        [1.0434e-04],\n",
      "        [2.1616e-07],\n",
      "        [4.6058e-03],\n",
      "        [7.8419e-03],\n",
      "        [1.9419e-03],\n",
      "        [5.2293e-04],\n",
      "        [3.2165e-04],\n",
      "        [5.2909e-05],\n",
      "        [1.0203e-03]], device='cuda:0'), loss: 7.9942626953125: \n",
      "target probs tensor([[1.8487e-04],\n",
      "        [1.7796e-05],\n",
      "        [4.8683e-04],\n",
      "        [1.4252e-04],\n",
      "        [3.2844e-04],\n",
      "        [2.4332e-04],\n",
      "        [1.2951e-05],\n",
      "        [7.0530e-04],\n",
      "        [7.6090e-04],\n",
      "        [1.5356e-04],\n",
      "        [2.9774e-05],\n",
      "        [2.1439e-03],\n",
      "        [2.8233e-05],\n",
      "        [8.9212e-05],\n",
      "        [2.3162e-04],\n",
      "        [1.4248e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.776567459106445: \n",
      "target probs tensor([[7.1279e-04],\n",
      "        [8.6100e-05],\n",
      "        [1.5513e-04],\n",
      "        [2.4144e-04],\n",
      "        [4.2404e-04],\n",
      "        [1.4156e-03],\n",
      "        [1.3890e-04],\n",
      "        [2.0960e-05],\n",
      "        [6.6473e-04],\n",
      "        [1.7550e-03],\n",
      "        [8.2486e-07],\n",
      "        [2.9856e-04],\n",
      "        [1.3177e-03],\n",
      "        [8.0461e-04],\n",
      "        [2.9626e-05],\n",
      "        [1.0139e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.553329467773438: \n",
      "target probs tensor([[4.4965e-05],\n",
      "        [3.4182e-04],\n",
      "        [4.2088e-04],\n",
      "        [3.5593e-02],\n",
      "        [1.3890e-04],\n",
      "        [1.2970e-04],\n",
      "        [5.4680e-04],\n",
      "        [1.0303e-03],\n",
      "        [1.5774e-05],\n",
      "        [9.8619e-04],\n",
      "        [8.9026e-05],\n",
      "        [1.1092e-03],\n",
      "        [6.1812e-04],\n",
      "        [1.3202e-04],\n",
      "        [2.9856e-03],\n",
      "        [6.9999e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.945805549621582: \n",
      "target probs tensor([[7.8821e-05],\n",
      "        [8.2991e-05],\n",
      "        [4.2410e-03],\n",
      "        [1.1869e-03],\n",
      "        [6.8745e-05],\n",
      "        [7.3412e-05],\n",
      "        [8.7192e-06],\n",
      "        [4.2729e-04],\n",
      "        [2.5683e-04],\n",
      "        [8.7860e-05],\n",
      "        [1.5852e-04],\n",
      "        [1.5311e-03],\n",
      "        [1.6517e-04],\n",
      "        [2.3537e-04],\n",
      "        [7.3995e-05],\n",
      "        [2.1164e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.589384078979492: \n",
      "target probs tensor([[1.2784e-03],\n",
      "        [8.0150e-04],\n",
      "        [5.3346e-04],\n",
      "        [2.9453e-05],\n",
      "        [2.3804e-02],\n",
      "        [2.3061e-05],\n",
      "        [3.2817e-04],\n",
      "        [1.1010e-04],\n",
      "        [2.0477e-05],\n",
      "        [2.6034e-04],\n",
      "        [4.7080e-06],\n",
      "        [3.3798e-04],\n",
      "        [8.5011e-04],\n",
      "        [1.2324e-05],\n",
      "        [3.0335e-04],\n",
      "        [1.8061e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.463196754455566: \n",
      "target probs tensor([[1.9780e-04],\n",
      "        [1.8382e-03],\n",
      "        [2.4790e-04],\n",
      "        [2.9148e-05],\n",
      "        [1.3695e-05],\n",
      "        [5.5304e-05],\n",
      "        [9.1483e-05],\n",
      "        [3.2235e-04],\n",
      "        [1.7821e-06],\n",
      "        [8.6742e-07],\n",
      "        [8.9275e-05],\n",
      "        [1.4809e-04],\n",
      "        [6.2165e-05],\n",
      "        [4.3016e-04],\n",
      "        [5.0025e-04],\n",
      "        [3.2368e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.395198822021484: \n",
      "target probs tensor([[2.9770e-05],\n",
      "        [4.3487e-04],\n",
      "        [5.2997e-05],\n",
      "        [1.3705e-06],\n",
      "        [1.9439e-04],\n",
      "        [1.1931e-04],\n",
      "        [1.8124e-04],\n",
      "        [4.1187e-04],\n",
      "        [3.2522e-07],\n",
      "        [1.6862e-04],\n",
      "        [3.9660e-04],\n",
      "        [4.9748e-04],\n",
      "        [2.3614e-04],\n",
      "        [5.9473e-04],\n",
      "        [4.8317e-07],\n",
      "        [4.7629e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.8211669921875: \n",
      "target probs tensor([[1.4105e-04],\n",
      "        [2.4805e-04],\n",
      "        [2.2972e-03],\n",
      "        [2.3911e-03],\n",
      "        [3.5207e-04],\n",
      "        [4.8211e-05],\n",
      "        [2.0872e-04],\n",
      "        [1.0149e-03],\n",
      "        [4.9582e-04],\n",
      "        [4.1276e-04],\n",
      "        [1.0184e-03],\n",
      "        [8.4211e-05],\n",
      "        [1.1496e-04],\n",
      "        [8.6825e-05],\n",
      "        [8.7175e-06],\n",
      "        [8.9977e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.206194877624512: \n",
      "target probs tensor([[2.1053e-04],\n",
      "        [1.4792e-04],\n",
      "        [1.4364e-04],\n",
      "        [1.0216e-03],\n",
      "        [5.5690e-04],\n",
      "        [4.6199e-02],\n",
      "        [3.5437e-04],\n",
      "        [3.0074e-05],\n",
      "        [3.5160e-04],\n",
      "        [4.1381e-04],\n",
      "        [1.7762e-03],\n",
      "        [9.6085e-05],\n",
      "        [1.4636e-03],\n",
      "        [1.4402e-03],\n",
      "        [1.8809e-04],\n",
      "        [5.3826e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.50899600982666: \n",
      "target probs tensor([[5.4159e-04],\n",
      "        [1.8987e-04],\n",
      "        [8.7218e-04],\n",
      "        [1.2567e-03],\n",
      "        [3.1242e-04],\n",
      "        [1.3413e-05],\n",
      "        [9.7104e-05],\n",
      "        [2.5089e-04],\n",
      "        [1.9763e-04],\n",
      "        [6.0870e-05],\n",
      "        [3.1587e-05],\n",
      "        [1.2532e-04],\n",
      "        [1.5017e-03],\n",
      "        [1.9092e-04],\n",
      "        [7.3395e-04],\n",
      "        [6.5441e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.508378982543945: \n",
      "target probs tensor([[1.0432e-04],\n",
      "        [1.4115e-04],\n",
      "        [6.3483e-05],\n",
      "        [1.8399e-04],\n",
      "        [7.5848e-05],\n",
      "        [3.0100e-04],\n",
      "        [1.5091e-04],\n",
      "        [3.5805e-04],\n",
      "        [1.0820e-04],\n",
      "        [4.6290e-04],\n",
      "        [9.1770e-06],\n",
      "        [8.6081e-04],\n",
      "        [4.0415e-05],\n",
      "        [3.8987e-05],\n",
      "        [1.8516e-04],\n",
      "        [1.8022e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.973634719848633: \n",
      "target probs tensor([[1.4531e-04],\n",
      "        [1.8585e-04],\n",
      "        [4.8935e-05],\n",
      "        [6.5938e-04],\n",
      "        [4.2983e-04],\n",
      "        [2.9608e-04],\n",
      "        [2.7842e-04],\n",
      "        [1.4378e-01],\n",
      "        [1.9995e-04],\n",
      "        [2.2081e-04],\n",
      "        [1.6837e-04],\n",
      "        [1.0297e-04],\n",
      "        [3.6002e-05],\n",
      "        [3.9653e-04],\n",
      "        [6.8840e-04],\n",
      "        [6.3547e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.012018203735352: \n",
      "target probs tensor([[6.1918e-05],\n",
      "        [3.2324e-04],\n",
      "        [1.1493e-03],\n",
      "        [3.8640e-05],\n",
      "        [5.1434e-04],\n",
      "        [2.5488e-04],\n",
      "        [5.7215e-05],\n",
      "        [4.6863e-04],\n",
      "        [2.0029e-04],\n",
      "        [1.1858e-03],\n",
      "        [1.5544e-04],\n",
      "        [1.3309e-03],\n",
      "        [2.5941e-05],\n",
      "        [9.1213e-04],\n",
      "        [1.8721e-03],\n",
      "        [8.6211e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.092428207397461: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[6.6398e-06],\n",
      "        [1.0327e-03],\n",
      "        [6.5044e-04],\n",
      "        [1.5593e-05],\n",
      "        [2.5316e-04],\n",
      "        [3.9345e-04],\n",
      "        [1.5794e-04],\n",
      "        [6.1751e-05],\n",
      "        [3.8304e-04],\n",
      "        [6.9107e-05],\n",
      "        [8.6159e-04],\n",
      "        [6.7821e-04],\n",
      "        [1.5523e-03],\n",
      "        [5.6448e-04],\n",
      "        [2.8659e-04],\n",
      "        [1.8806e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.247092247009277: \n",
      "target probs tensor([[6.7627e-05],\n",
      "        [3.0552e-04],\n",
      "        [3.5294e-05],\n",
      "        [3.3093e-04],\n",
      "        [4.1539e-04],\n",
      "        [5.0215e-04],\n",
      "        [9.1268e-04],\n",
      "        [5.4533e-04],\n",
      "        [4.9521e-05],\n",
      "        [2.1947e-03],\n",
      "        [2.0671e-03],\n",
      "        [9.8142e-05],\n",
      "        [9.3789e-05],\n",
      "        [6.1089e-04],\n",
      "        [1.1428e-03],\n",
      "        [3.8967e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.037603378295898: \n",
      "target probs tensor([[1.8438e-04],\n",
      "        [1.7027e-04],\n",
      "        [3.0473e-04],\n",
      "        [1.5087e-04],\n",
      "        [1.9728e-04],\n",
      "        [1.8123e-04],\n",
      "        [6.9429e-04],\n",
      "        [2.8714e-04],\n",
      "        [4.1046e-04],\n",
      "        [1.0104e-04],\n",
      "        [7.0536e-04],\n",
      "        [5.3479e-05],\n",
      "        [8.1901e-06],\n",
      "        [9.9822e-05],\n",
      "        [1.2062e-04],\n",
      "        [4.4277e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.656742095947266: \n",
      "target probs tensor([[1.8027e-04],\n",
      "        [1.3647e-03],\n",
      "        [4.7662e-04],\n",
      "        [2.6435e-04],\n",
      "        [4.1718e-05],\n",
      "        [4.9361e-04],\n",
      "        [3.1628e-04],\n",
      "        [6.7818e-04],\n",
      "        [3.7686e-04],\n",
      "        [1.9884e-04],\n",
      "        [3.4086e-04],\n",
      "        [1.8870e-04],\n",
      "        [8.5820e-05],\n",
      "        [2.8034e-04],\n",
      "        [1.2722e-03],\n",
      "        [8.1072e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.172142028808594: \n",
      "target probs tensor([[3.8390e-04],\n",
      "        [2.2185e-04],\n",
      "        [1.4315e-04],\n",
      "        [1.0789e-03],\n",
      "        [7.0011e-04],\n",
      "        [1.0881e-04],\n",
      "        [2.8241e-04],\n",
      "        [3.0959e-06],\n",
      "        [5.7173e-04],\n",
      "        [2.1305e-04],\n",
      "        [2.3629e-03],\n",
      "        [1.3258e-04],\n",
      "        [3.5868e-05],\n",
      "        [1.7284e-04],\n",
      "        [3.2295e-04],\n",
      "        [4.9565e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.415833473205566: \n",
      "target probs tensor([[0.0004],\n",
      "        [0.0021],\n",
      "        [0.0010],\n",
      "        [0.0006],\n",
      "        [0.0001],\n",
      "        [0.0010],\n",
      "        [0.0007],\n",
      "        [0.0013]], device='cuda:0'), loss: 7.308033466339111: \n",
      "target probs tensor([[8.9906e-04],\n",
      "        [7.7980e-04],\n",
      "        [1.9165e-04],\n",
      "        [4.5176e-04],\n",
      "        [1.1337e-04],\n",
      "        [6.3735e-05],\n",
      "        [4.5368e-05],\n",
      "        [1.3281e-05],\n",
      "        [1.3848e-03],\n",
      "        [9.6124e-05],\n",
      "        [4.5290e-04],\n",
      "        [6.3622e-04],\n",
      "        [2.3610e-04],\n",
      "        [7.4691e-04],\n",
      "        [1.1735e-03],\n",
      "        [4.5548e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.205806732177734: \n",
      "target probs tensor([[1.0573e-04],\n",
      "        [4.6871e-04],\n",
      "        [2.7856e-04],\n",
      "        [1.1874e-04],\n",
      "        [7.9234e-03],\n",
      "        [1.7557e-04],\n",
      "        [1.1550e-03],\n",
      "        [1.1438e-03],\n",
      "        [1.6332e-04],\n",
      "        [3.3542e-04],\n",
      "        [4.0904e-04],\n",
      "        [1.7714e-05],\n",
      "        [1.8078e-04],\n",
      "        [8.4990e-04],\n",
      "        [6.4814e-05],\n",
      "        [1.3329e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.174083709716797: \n",
      "target probs tensor([[1.5900e-05],\n",
      "        [1.6043e-04],\n",
      "        [5.5666e-05],\n",
      "        [1.4376e-04],\n",
      "        [3.0056e-05],\n",
      "        [5.7502e-05],\n",
      "        [7.6800e-04],\n",
      "        [8.7870e-05],\n",
      "        [4.8841e-04],\n",
      "        [1.4830e-04],\n",
      "        [4.1081e-04],\n",
      "        [1.5298e-04],\n",
      "        [3.2512e-04],\n",
      "        [6.7736e-04],\n",
      "        [1.2204e-03],\n",
      "        [1.2353e-04]], device='cuda:0'), loss: 8.698579788208008: \n",
      "target probs tensor([[4.6980e-04],\n",
      "        [2.6701e-03],\n",
      "        [1.5766e-04],\n",
      "        [7.4734e-04],\n",
      "        [1.1593e-04],\n",
      "        [1.0022e-07],\n",
      "        [4.1077e-04],\n",
      "        [3.1164e-04],\n",
      "        [2.1455e-06],\n",
      "        [6.5404e-05],\n",
      "        [3.1986e-04],\n",
      "        [5.6991e-04],\n",
      "        [7.5485e-04],\n",
      "        [1.7333e-04],\n",
      "        [1.1941e-04],\n",
      "        [5.9479e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.819114685058594: \n",
      "target probs tensor([[1.6211e-05],\n",
      "        [5.7014e-05],\n",
      "        [4.3836e-04],\n",
      "        [6.2873e-04],\n",
      "        [9.4761e-04],\n",
      "        [1.8074e-05],\n",
      "        [8.8843e-05],\n",
      "        [2.6806e-04],\n",
      "        [3.5378e-04],\n",
      "        [1.8880e-04],\n",
      "        [2.9051e-04],\n",
      "        [3.5513e-04],\n",
      "        [7.1063e-05],\n",
      "        [8.6286e-04],\n",
      "        [7.5632e-03],\n",
      "        [2.6864e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.210100173950195: \n",
      "target probs tensor([[1.6536e-03],\n",
      "        [1.0460e-04],\n",
      "        [1.2420e-03],\n",
      "        [1.9190e-05],\n",
      "        [4.9429e-04],\n",
      "        [1.4819e-03],\n",
      "        [3.6459e-04],\n",
      "        [2.0403e-04],\n",
      "        [1.3194e-06],\n",
      "        [8.5663e-03],\n",
      "        [1.0346e-02],\n",
      "        [2.2080e-03],\n",
      "        [5.1829e-04],\n",
      "        [4.0038e-04],\n",
      "        [4.4783e-05],\n",
      "        [7.2190e-04]], device='cuda:0'), loss: 7.830212593078613: \n",
      "target probs tensor([[1.1718e-04],\n",
      "        [1.6691e-03],\n",
      "        [5.1123e-03],\n",
      "        [8.2323e-04],\n",
      "        [1.4622e-04],\n",
      "        [1.5269e-05],\n",
      "        [6.2212e-04],\n",
      "        [3.1061e-04],\n",
      "        [3.1042e-04],\n",
      "        [7.8420e-05],\n",
      "        [7.1421e-04],\n",
      "        [9.4346e-04],\n",
      "        [3.8006e-07],\n",
      "        [7.4360e-04],\n",
      "        [9.6221e-04],\n",
      "        [1.3764e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.154227256774902: \n",
      "target probs tensor([[1.1240e-04],\n",
      "        [5.6183e-04],\n",
      "        [2.2364e-03],\n",
      "        [4.6753e-04],\n",
      "        [1.9184e-03],\n",
      "        [6.9015e-04],\n",
      "        [3.9731e-04],\n",
      "        [7.4994e-04],\n",
      "        [8.8112e-04],\n",
      "        [5.8469e-05],\n",
      "        [1.4597e-04],\n",
      "        [5.9280e-04],\n",
      "        [1.2349e-04],\n",
      "        [3.6680e-04],\n",
      "        [4.1399e-04],\n",
      "        [1.7473e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.543797969818115: \n",
      "target probs tensor([[3.5568e-04],\n",
      "        [5.1807e-04],\n",
      "        [8.4628e-04],\n",
      "        [3.8229e-04],\n",
      "        [4.0001e-04],\n",
      "        [1.2454e-03],\n",
      "        [3.0448e-04],\n",
      "        [2.3899e-04],\n",
      "        [2.8452e-04],\n",
      "        [9.6507e-04],\n",
      "        [5.4801e-04],\n",
      "        [1.4799e-04],\n",
      "        [1.8266e-04],\n",
      "        [3.4257e-05],\n",
      "        [1.4814e-05],\n",
      "        [5.0933e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.15166187286377: \n",
      "Better model found at epoch 34 with validation value: 0.8270000219345093.\n",
      "target probs tensor([[2.6066e-04],\n",
      "        [1.2724e-04],\n",
      "        [2.1520e-04],\n",
      "        [8.9248e-04],\n",
      "        [6.0788e-04],\n",
      "        [5.6547e-05],\n",
      "        [1.6985e-03],\n",
      "        [1.0357e-05],\n",
      "        [8.3922e-06],\n",
      "        [8.1166e-04],\n",
      "        [4.7695e-04],\n",
      "        [9.6054e-05],\n",
      "        [6.6054e-04],\n",
      "        [6.4850e-05],\n",
      "        [4.3650e-04],\n",
      "        [2.1103e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.393492698669434: \n",
      "target probs tensor([[5.6105e-04],\n",
      "        [4.0001e-04],\n",
      "        [1.6987e-05],\n",
      "        [4.7367e-04],\n",
      "        [4.3957e-04],\n",
      "        [5.3551e-04],\n",
      "        [1.0829e-03],\n",
      "        [8.4209e-06],\n",
      "        [5.9808e-04],\n",
      "        [6.9938e-05],\n",
      "        [1.9459e-04],\n",
      "        [6.3876e-04],\n",
      "        [1.9763e-04],\n",
      "        [2.6674e-04],\n",
      "        [1.7818e-05],\n",
      "        [2.3466e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.685409545898438: \n",
      "target probs tensor([[3.5952e-04],\n",
      "        [8.6228e-05],\n",
      "        [5.2080e-04],\n",
      "        [3.2555e-04],\n",
      "        [4.7694e-04],\n",
      "        [3.7884e-04],\n",
      "        [1.4768e-04],\n",
      "        [2.9808e-03],\n",
      "        [3.2491e-05],\n",
      "        [9.4074e-05],\n",
      "        [3.4806e-04],\n",
      "        [2.3691e-05],\n",
      "        [9.7824e-03],\n",
      "        [3.9195e-04],\n",
      "        [9.9885e-05],\n",
      "        [1.3597e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.096553802490234: \n",
      "target probs tensor([[6.8434e-04],\n",
      "        [2.9739e-04],\n",
      "        [2.9684e-04],\n",
      "        [6.3981e-04],\n",
      "        [5.0873e-04],\n",
      "        [3.2616e-05],\n",
      "        [2.1909e-03],\n",
      "        [1.9036e-05],\n",
      "        [1.1035e-03],\n",
      "        [4.1474e-04],\n",
      "        [8.5921e-04],\n",
      "        [2.5726e-04],\n",
      "        [2.3866e-05],\n",
      "        [3.6591e-04],\n",
      "        [1.6599e-04],\n",
      "        [1.3517e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.098705291748047: \n",
      "target probs tensor([[1.5761e-04],\n",
      "        [1.1668e-05],\n",
      "        [6.9514e-05],\n",
      "        [2.7140e-05],\n",
      "        [1.3811e-03],\n",
      "        [1.9637e-04],\n",
      "        [3.7588e-04],\n",
      "        [8.2566e-05],\n",
      "        [8.9298e-04],\n",
      "        [5.0089e-04],\n",
      "        [2.0812e-05],\n",
      "        [7.9645e-07],\n",
      "        [7.4333e-05],\n",
      "        [1.0748e-03],\n",
      "        [9.8173e-04],\n",
      "        [1.3182e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.872138023376465: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[7.4929e-05],\n",
      "        [4.3402e-04],\n",
      "        [8.0062e-04],\n",
      "        [1.3668e-04],\n",
      "        [2.9895e-06],\n",
      "        [6.9025e-04],\n",
      "        [1.9021e-04],\n",
      "        [2.4434e-04],\n",
      "        [9.0360e-04],\n",
      "        [4.3755e-03],\n",
      "        [3.0453e-04],\n",
      "        [2.8087e-04],\n",
      "        [5.1399e-05],\n",
      "        [9.3759e-06],\n",
      "        [3.7107e-04],\n",
      "        [1.7260e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.555291175842285: \n",
      "target probs tensor([[2.3620e-04],\n",
      "        [1.4272e-04],\n",
      "        [3.0679e-04],\n",
      "        [2.2707e-04],\n",
      "        [1.5030e-04],\n",
      "        [1.4572e-04],\n",
      "        [1.0620e-04],\n",
      "        [6.9600e-05],\n",
      "        [2.9499e-04],\n",
      "        [5.0899e-04],\n",
      "        [1.9500e-03],\n",
      "        [2.9612e-04],\n",
      "        [3.8870e-04],\n",
      "        [5.0546e-04],\n",
      "        [5.7994e-04],\n",
      "        [4.7398e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.16692066192627: \n",
      "target probs tensor([[6.1350e-05],\n",
      "        [4.0165e-04],\n",
      "        [1.9116e-04],\n",
      "        [5.2966e-06],\n",
      "        [4.6843e-04],\n",
      "        [4.3090e-05],\n",
      "        [2.7960e-04],\n",
      "        [1.6619e-04],\n",
      "        [3.3889e-04],\n",
      "        [3.6732e-04],\n",
      "        [2.5561e-02],\n",
      "        [3.2437e-03],\n",
      "        [2.2770e-03],\n",
      "        [6.9200e-05],\n",
      "        [9.7189e-06],\n",
      "        [5.7223e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.30001449584961: \n",
      "target probs tensor([[5.3418e-04],\n",
      "        [5.3548e-06],\n",
      "        [1.5614e-03],\n",
      "        [3.8912e-04],\n",
      "        [6.4095e-05],\n",
      "        [3.9887e-06],\n",
      "        [5.3427e-04],\n",
      "        [1.1307e-04],\n",
      "        [3.1982e-03],\n",
      "        [6.4458e-06],\n",
      "        [2.2115e-04],\n",
      "        [5.3562e-05],\n",
      "        [4.6648e-05],\n",
      "        [2.6990e-05],\n",
      "        [2.3166e-04],\n",
      "        [6.4710e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.053129196166992: \n",
      "target probs tensor([[1.1718e-05],\n",
      "        [6.0965e-04],\n",
      "        [5.9949e-05],\n",
      "        [2.3492e-04],\n",
      "        [1.3074e-03],\n",
      "        [8.3013e-06],\n",
      "        [1.2993e-03],\n",
      "        [1.3277e-05],\n",
      "        [1.0215e-05],\n",
      "        [4.8057e-04],\n",
      "        [6.0576e-04],\n",
      "        [8.5323e-04],\n",
      "        [6.3449e-06],\n",
      "        [7.1712e-04],\n",
      "        [1.3004e-03],\n",
      "        [2.7033e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.651481628417969: \n",
      "target probs tensor([[5.2447e-04],\n",
      "        [1.4323e-05],\n",
      "        [2.4492e-04],\n",
      "        [2.6916e-04],\n",
      "        [3.9908e-04],\n",
      "        [6.1410e-04],\n",
      "        [5.3440e-05],\n",
      "        [9.5050e-05],\n",
      "        [1.1986e-04],\n",
      "        [1.7958e-04],\n",
      "        [3.6504e-04],\n",
      "        [3.9099e-04],\n",
      "        [2.4338e-04],\n",
      "        [2.9265e-03],\n",
      "        [6.8942e-04],\n",
      "        [4.6512e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.255338668823242: \n",
      "target probs tensor([[6.9523e-06],\n",
      "        [1.5199e-03],\n",
      "        [5.3528e-04],\n",
      "        [1.1038e-03],\n",
      "        [5.4915e-04],\n",
      "        [7.3862e-05],\n",
      "        [5.7521e-04],\n",
      "        [1.0649e-03],\n",
      "        [2.9640e-04],\n",
      "        [2.0335e-04],\n",
      "        [6.4147e-04],\n",
      "        [4.6927e-04],\n",
      "        [1.2504e-03],\n",
      "        [1.5777e-03],\n",
      "        [5.2341e-04],\n",
      "        [7.6273e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.721479415893555: \n",
      "target probs tensor([[2.6671e-03],\n",
      "        [1.3907e-04],\n",
      "        [5.6343e-05],\n",
      "        [1.5274e-03],\n",
      "        [2.4904e-04],\n",
      "        [1.3204e-04],\n",
      "        [2.4716e-04],\n",
      "        [3.7535e-04],\n",
      "        [2.3968e-04],\n",
      "        [2.2350e-04],\n",
      "        [2.5389e-05],\n",
      "        [3.1280e-04],\n",
      "        [1.2469e-04],\n",
      "        [5.0455e-04],\n",
      "        [2.4373e-04],\n",
      "        [6.7140e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.400129318237305: \n",
      "target probs tensor([[4.3665e-04],\n",
      "        [1.4956e-04],\n",
      "        [3.3321e-04],\n",
      "        [6.0109e-05],\n",
      "        [1.8110e-03],\n",
      "        [5.5670e-05],\n",
      "        [2.0515e-04],\n",
      "        [2.7033e-04],\n",
      "        [3.7969e-04],\n",
      "        [9.7018e-04],\n",
      "        [1.0958e-04],\n",
      "        [2.5178e-04],\n",
      "        [9.6232e-06],\n",
      "        [4.6849e-04],\n",
      "        [2.2274e-03],\n",
      "        [1.1064e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.502740859985352: \n",
      "target probs tensor([[5.1820e-04],\n",
      "        [4.4275e-04],\n",
      "        [3.7023e-05],\n",
      "        [1.1919e-04],\n",
      "        [1.2339e-03],\n",
      "        [7.7659e-04],\n",
      "        [4.2311e-04],\n",
      "        [1.8080e-04],\n",
      "        [6.7853e-05],\n",
      "        [3.4945e-04],\n",
      "        [9.5824e-04],\n",
      "        [5.0877e-08],\n",
      "        [8.9940e-05],\n",
      "        [5.8744e-06],\n",
      "        [1.0293e-04],\n",
      "        [6.8561e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.993636131286621: \n",
      "target probs tensor([[0.0003],\n",
      "        [0.0319],\n",
      "        [0.0016],\n",
      "        [0.0005],\n",
      "        [0.0001],\n",
      "        [0.0007],\n",
      "        [0.0005],\n",
      "        [0.0015]], device='cuda:0'), loss: 7.008087158203125: \n",
      "target probs tensor([[1.3636e-03],\n",
      "        [8.5386e-04],\n",
      "        [2.6158e-04],\n",
      "        [3.0532e-04],\n",
      "        [4.7837e-04],\n",
      "        [1.1117e-04],\n",
      "        [1.4361e-04],\n",
      "        [5.3444e-04],\n",
      "        [2.5612e-04],\n",
      "        [2.7110e-03],\n",
      "        [2.5615e-04],\n",
      "        [6.5560e-04],\n",
      "        [1.2790e-05],\n",
      "        [1.4063e-03],\n",
      "        [6.8799e-03],\n",
      "        [1.3544e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.645988464355469: \n",
      "target probs tensor([[3.1953e-03],\n",
      "        [1.0869e-04],\n",
      "        [1.1767e-04],\n",
      "        [7.9519e-04],\n",
      "        [7.0999e-04],\n",
      "        [1.5998e-04],\n",
      "        [6.0442e-05],\n",
      "        [2.2501e-04],\n",
      "        [1.9349e-05],\n",
      "        [4.6038e-04],\n",
      "        [6.7613e-04],\n",
      "        [1.9439e-04],\n",
      "        [1.1590e-04],\n",
      "        [9.5492e-05],\n",
      "        [2.2958e-04],\n",
      "        [7.0839e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.343347549438477: \n",
      "target probs tensor([[2.2788e-05],\n",
      "        [1.4673e-04],\n",
      "        [4.7439e-05],\n",
      "        [1.2487e-04],\n",
      "        [3.2637e-05],\n",
      "        [4.6959e-05],\n",
      "        [8.1929e-04],\n",
      "        [1.3248e-04],\n",
      "        [6.6750e-04],\n",
      "        [2.0334e-04],\n",
      "        [4.7014e-04],\n",
      "        [1.1769e-04],\n",
      "        [3.6496e-04],\n",
      "        [9.5622e-04],\n",
      "        [1.0974e-03],\n",
      "        [1.7709e-04]], device='cuda:0'), loss: 8.602334976196289: \n",
      "target probs tensor([[5.8373e-04],\n",
      "        [1.1658e-03],\n",
      "        [5.6795e-03],\n",
      "        [1.1071e-05],\n",
      "        [2.9251e-03],\n",
      "        [1.1244e-03],\n",
      "        [1.6754e-03],\n",
      "        [1.1263e-04],\n",
      "        [9.8859e-05],\n",
      "        [3.7292e-03],\n",
      "        [1.7570e-04],\n",
      "        [2.9085e-02],\n",
      "        [2.1580e-03],\n",
      "        [2.9937e-04],\n",
      "        [4.9471e-04],\n",
      "        [1.4697e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.28608512878418: \n",
      "target probs tensor([[6.3491e-04],\n",
      "        [7.3948e-05],\n",
      "        [5.6368e-05],\n",
      "        [2.0049e-04],\n",
      "        [7.4073e-04],\n",
      "        [3.4261e-04],\n",
      "        [1.0552e-04],\n",
      "        [8.1297e-05],\n",
      "        [1.7866e-04],\n",
      "        [4.7731e-04],\n",
      "        [2.1290e-04],\n",
      "        [1.4130e-04],\n",
      "        [5.3015e-04],\n",
      "        [3.3910e-04],\n",
      "        [3.8211e-03],\n",
      "        [5.4509e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.196454048156738: \n",
      "target probs tensor([[2.1111e-03],\n",
      "        [1.3282e-04],\n",
      "        [2.0483e-03],\n",
      "        [2.1675e-05],\n",
      "        [2.9668e-04],\n",
      "        [1.7372e-03],\n",
      "        [3.2433e-04],\n",
      "        [1.8250e-04],\n",
      "        [1.0137e-06],\n",
      "        [1.5445e-02],\n",
      "        [1.5403e-02],\n",
      "        [2.4692e-03],\n",
      "        [5.6242e-04],\n",
      "        [5.7329e-04],\n",
      "        [5.6654e-05],\n",
      "        [1.0549e-02]], device='cuda:0'), loss: 7.535309791564941: \n",
      "target probs tensor([[5.1618e-04],\n",
      "        [1.2409e-04],\n",
      "        [7.4184e-05],\n",
      "        [4.4796e-04],\n",
      "        [8.4303e-04],\n",
      "        [1.0102e-04],\n",
      "        [1.7990e-03],\n",
      "        [2.3831e-05],\n",
      "        [5.8412e-05],\n",
      "        [1.7406e-04],\n",
      "        [1.3627e-04],\n",
      "        [5.5317e-04],\n",
      "        [3.1373e-04],\n",
      "        [8.3705e-05],\n",
      "        [8.2596e-04],\n",
      "        [2.7298e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.412010192871094: \n",
      "target probs tensor([[2.5416e-05],\n",
      "        [7.3539e-04],\n",
      "        [1.0273e-03],\n",
      "        [5.1998e-06],\n",
      "        [2.0368e-04],\n",
      "        [2.7529e-04],\n",
      "        [8.3270e-04],\n",
      "        [8.4282e-04],\n",
      "        [9.8107e-05],\n",
      "        [5.4244e-06],\n",
      "        [5.9167e-04],\n",
      "        [1.6282e-04],\n",
      "        [5.2620e-04],\n",
      "        [1.3181e-03],\n",
      "        [2.2252e-04],\n",
      "        [5.0050e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.463155746459961: \n",
      "target probs tensor([[2.6719e-05],\n",
      "        [3.8881e-04],\n",
      "        [5.9918e-06],\n",
      "        [1.6394e-04],\n",
      "        [4.9385e-05],\n",
      "        [8.2121e-05],\n",
      "        [8.8100e-04],\n",
      "        [1.1212e-04],\n",
      "        [3.6783e-04],\n",
      "        [1.1028e-04],\n",
      "        [3.3962e-04],\n",
      "        [1.8409e-04],\n",
      "        [4.5181e-04],\n",
      "        [2.8918e-04],\n",
      "        [3.9964e-04],\n",
      "        [4.8800e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.717891693115234: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[5.4741e-04],\n",
      "        [4.3694e-03],\n",
      "        [8.2616e-05],\n",
      "        [7.0859e-04],\n",
      "        [5.9543e-05],\n",
      "        [5.5255e-04],\n",
      "        [2.8941e-04],\n",
      "        [7.7586e-04],\n",
      "        [4.3214e-04],\n",
      "        [9.3383e-04],\n",
      "        [4.4560e-05],\n",
      "        [2.8606e-06],\n",
      "        [8.9039e-04],\n",
      "        [7.1083e-04],\n",
      "        [6.1956e-04],\n",
      "        [1.5306e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.130386352539062: \n",
      "target probs tensor([[9.8211e-04],\n",
      "        [6.4735e-05],\n",
      "        [6.5774e-04],\n",
      "        [4.7590e-04],\n",
      "        [1.3836e-04],\n",
      "        [6.5243e-04],\n",
      "        [1.3526e-04],\n",
      "        [5.5253e-04],\n",
      "        [1.3557e-03],\n",
      "        [7.1451e-04],\n",
      "        [1.1070e-04],\n",
      "        [2.2111e-03],\n",
      "        [6.4002e-04],\n",
      "        [1.7278e-04],\n",
      "        [7.4624e-04],\n",
      "        [1.0085e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.998156547546387: \n",
      "target probs tensor([[1.4036e-04],\n",
      "        [1.0162e-03],\n",
      "        [9.4025e-04],\n",
      "        [3.9397e-04],\n",
      "        [9.3512e-06],\n",
      "        [3.2900e-04],\n",
      "        [3.1002e-04],\n",
      "        [9.5396e-04],\n",
      "        [8.9224e-07],\n",
      "        [4.0928e-04],\n",
      "        [1.2447e-05],\n",
      "        [1.0576e-04],\n",
      "        [5.9406e-04],\n",
      "        [2.5297e-04],\n",
      "        [1.1273e-04],\n",
      "        [7.8867e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.852043151855469: \n",
      "target probs tensor([[3.8942e-04],\n",
      "        [3.9325e-04],\n",
      "        [2.7864e-04],\n",
      "        [6.2103e-04],\n",
      "        [2.3070e-04],\n",
      "        [1.0968e-02],\n",
      "        [1.4394e-03],\n",
      "        [1.4781e-04],\n",
      "        [1.1251e-04],\n",
      "        [4.4433e-04],\n",
      "        [2.8437e-04],\n",
      "        [1.5957e-03],\n",
      "        [6.0926e-05],\n",
      "        [3.0389e-04],\n",
      "        [2.7422e-04],\n",
      "        [6.3036e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.7690324783325195: \n",
      "target probs tensor([[2.8322e-04],\n",
      "        [9.8248e-04],\n",
      "        [2.3138e-04],\n",
      "        [8.8256e-06],\n",
      "        [3.8754e-04],\n",
      "        [3.9589e-05],\n",
      "        [1.5264e-04],\n",
      "        [6.9782e-05],\n",
      "        [2.2112e-04],\n",
      "        [1.5324e-04],\n",
      "        [1.8854e-04],\n",
      "        [3.9613e-04],\n",
      "        [1.3065e-03],\n",
      "        [1.6824e-04],\n",
      "        [3.6775e-04],\n",
      "        [5.1771e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.491827011108398: \n",
      "target probs tensor([[3.2660e-04],\n",
      "        [5.1400e-04],\n",
      "        [3.7975e-04],\n",
      "        [6.8224e-04],\n",
      "        [1.4028e-03],\n",
      "        [1.4686e-04],\n",
      "        [2.5385e-04],\n",
      "        [1.1084e-03],\n",
      "        [2.7757e-04],\n",
      "        [2.4790e-03],\n",
      "        [6.0163e-05],\n",
      "        [6.1011e-04],\n",
      "        [3.7689e-04],\n",
      "        [3.0561e-03],\n",
      "        [7.5659e-05],\n",
      "        [1.9293e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.623046875: \n",
      "target probs tensor([[5.2411e-04],\n",
      "        [9.2126e-06],\n",
      "        [5.1226e-04],\n",
      "        [1.0520e-03],\n",
      "        [2.0264e-03],\n",
      "        [2.0740e-04],\n",
      "        [9.3559e-04],\n",
      "        [1.7963e-04],\n",
      "        [1.6943e-03],\n",
      "        [3.2787e-04],\n",
      "        [3.5425e-04],\n",
      "        [8.6934e-02],\n",
      "        [3.7204e-04],\n",
      "        [1.0837e-03],\n",
      "        [1.9892e-05],\n",
      "        [2.6300e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.652985572814941: \n",
      "target probs tensor([[3.3631e-03],\n",
      "        [8.9687e-04],\n",
      "        [7.6323e-05],\n",
      "        [6.0296e-06],\n",
      "        [6.4061e-04],\n",
      "        [4.3905e-04],\n",
      "        [8.3850e-04],\n",
      "        [7.4726e-04],\n",
      "        [4.1851e-04],\n",
      "        [1.7907e-03],\n",
      "        [7.3342e-05],\n",
      "        [2.3345e-04],\n",
      "        [1.8031e-04],\n",
      "        [9.5659e-04],\n",
      "        [3.2024e-03],\n",
      "        [4.2749e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.789939880371094: \n",
      "target probs tensor([[9.3687e-04],\n",
      "        [2.4986e-07],\n",
      "        [3.8892e-03],\n",
      "        [7.2411e-04],\n",
      "        [3.6312e-03],\n",
      "        [6.9820e-04],\n",
      "        [1.0583e-04],\n",
      "        [5.9700e-04],\n",
      "        [8.3561e-05],\n",
      "        [5.3802e-03],\n",
      "        [1.1612e-04],\n",
      "        [1.0736e-06],\n",
      "        [1.1127e-03],\n",
      "        [2.1777e-04],\n",
      "        [2.2295e-04],\n",
      "        [2.0211e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.374131202697754: \n",
      "target probs tensor([[3.1379e-06],\n",
      "        [1.6353e-04],\n",
      "        [2.0118e-04],\n",
      "        [2.7453e-04],\n",
      "        [1.5078e-04],\n",
      "        [2.9070e-03],\n",
      "        [1.6892e-04],\n",
      "        [4.1684e-04],\n",
      "        [1.6171e-04],\n",
      "        [7.6526e-04],\n",
      "        [6.3130e-03],\n",
      "        [1.6282e-03],\n",
      "        [3.5760e-04],\n",
      "        [3.8184e-05],\n",
      "        [2.0076e-07],\n",
      "        [2.5075e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.363601684570312: \n",
      "target probs tensor([[3.5406e-04],\n",
      "        [3.2348e-04],\n",
      "        [1.8931e-04],\n",
      "        [5.7359e-04],\n",
      "        [3.9537e-02],\n",
      "        [1.0318e-04],\n",
      "        [1.3181e-03],\n",
      "        [2.1928e-02],\n",
      "        [3.7590e-04],\n",
      "        [2.6737e-04],\n",
      "        [6.0585e-04],\n",
      "        [6.4599e-04],\n",
      "        [4.0745e-05],\n",
      "        [3.5101e-04],\n",
      "        [5.9704e-03],\n",
      "        [3.3919e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.163506507873535: \n",
      "target probs tensor([[5.7417e-04],\n",
      "        [1.4922e-03],\n",
      "        [1.0101e-05],\n",
      "        [3.2813e-04],\n",
      "        [1.0369e-03],\n",
      "        [1.2334e-03],\n",
      "        [7.4927e-04],\n",
      "        [4.5701e-04],\n",
      "        [2.4215e-02],\n",
      "        [2.3688e-04],\n",
      "        [1.2154e-04],\n",
      "        [6.7412e-04],\n",
      "        [1.8471e-03],\n",
      "        [2.2777e-03],\n",
      "        [2.5012e-04],\n",
      "        [2.8599e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.592021942138672: \n",
      "target probs tensor([[4.3649e-03],\n",
      "        [2.7539e-04],\n",
      "        [1.8898e-04],\n",
      "        [5.7155e-03],\n",
      "        [1.0561e-03],\n",
      "        [2.2743e-04],\n",
      "        [8.9437e-07],\n",
      "        [4.9042e-04],\n",
      "        [1.9859e-03],\n",
      "        [5.8112e-04],\n",
      "        [1.5277e-04],\n",
      "        [8.1912e-04],\n",
      "        [2.3700e-04],\n",
      "        [1.0199e-03],\n",
      "        [1.2199e-04],\n",
      "        [1.5130e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.779117107391357: \n",
      "target probs tensor([[8.0082e-04],\n",
      "        [9.1289e-04],\n",
      "        [7.0523e-04],\n",
      "        [2.6132e-05],\n",
      "        [2.2789e-04],\n",
      "        [1.3149e-05],\n",
      "        [2.1593e-05],\n",
      "        [8.9701e-04],\n",
      "        [2.3392e-04],\n",
      "        [9.8768e-06],\n",
      "        [6.1064e-04],\n",
      "        [1.2079e-04],\n",
      "        [5.9134e-05],\n",
      "        [3.6495e-04],\n",
      "        [1.0052e-03],\n",
      "        [4.1660e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.62307357788086: \n",
      "target probs tensor([[7.4785e-04],\n",
      "        [3.7702e-04],\n",
      "        [1.1588e-03],\n",
      "        [5.4822e-05],\n",
      "        [6.3041e-03],\n",
      "        [2.0478e-05],\n",
      "        [5.5662e-04],\n",
      "        [9.3905e-05],\n",
      "        [2.6491e-04],\n",
      "        [6.7460e-05],\n",
      "        [7.2820e-04],\n",
      "        [6.7488e-05],\n",
      "        [2.1313e-04],\n",
      "        [5.3422e-04],\n",
      "        [2.7458e-04],\n",
      "        [4.7192e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.174921989440918: \n",
      "target probs tensor([[3.6187e-04],\n",
      "        [8.5306e-01],\n",
      "        [1.9798e-03],\n",
      "        [6.5147e-04],\n",
      "        [8.7323e-05],\n",
      "        [8.1300e-04],\n",
      "        [1.2351e-03],\n",
      "        [2.0296e-03]], device='cuda:0'), loss: 6.375176906585693: \n",
      "target probs tensor([[4.0836e-03],\n",
      "        [5.8736e-05],\n",
      "        [6.4290e-04],\n",
      "        [4.9980e-04],\n",
      "        [2.1152e-02],\n",
      "        [1.9945e-04],\n",
      "        [8.6961e-04],\n",
      "        [1.1209e-02],\n",
      "        [5.1931e-04],\n",
      "        [4.8950e-04],\n",
      "        [1.5293e-04],\n",
      "        [1.3546e-03],\n",
      "        [4.7372e-05],\n",
      "        [3.2651e-04],\n",
      "        [6.7444e-04],\n",
      "        [1.7065e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.2714385986328125: \n",
      "target probs tensor([[5.8876e-04],\n",
      "        [2.0825e-04],\n",
      "        [2.5361e-03],\n",
      "        [1.0649e-04],\n",
      "        [1.4469e-03],\n",
      "        [6.4022e-04],\n",
      "        [3.4450e-04],\n",
      "        [5.1475e-05],\n",
      "        [6.2843e-06],\n",
      "        [1.0812e-03],\n",
      "        [2.7421e-04],\n",
      "        [2.9358e-06],\n",
      "        [4.5778e-03],\n",
      "        [1.9584e-05],\n",
      "        [6.0497e-05],\n",
      "        [2.0281e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.56059455871582: \n",
      "target probs tensor([[4.2974e-05],\n",
      "        [1.6473e-04],\n",
      "        [2.3348e-05],\n",
      "        [1.4751e-04],\n",
      "        [3.4754e-05],\n",
      "        [5.7646e-05],\n",
      "        [8.9858e-04],\n",
      "        [1.6734e-04],\n",
      "        [5.3760e-04],\n",
      "        [3.1489e-04],\n",
      "        [4.4378e-04],\n",
      "        [1.3738e-04],\n",
      "        [2.6520e-04],\n",
      "        [1.1964e-03],\n",
      "        [1.3470e-03],\n",
      "        [1.4064e-04]], device='cuda:0'), loss: 8.539912223815918: \n",
      "target probs tensor([[7.8324e-04],\n",
      "        [3.2374e-03],\n",
      "        [2.2681e-02],\n",
      "        [1.9817e-02],\n",
      "        [6.0040e-04],\n",
      "        [8.8699e-04],\n",
      "        [6.6568e-04],\n",
      "        [8.0644e-05],\n",
      "        [3.9513e-04],\n",
      "        [2.5946e-04],\n",
      "        [1.8658e-04],\n",
      "        [4.8044e-04],\n",
      "        [8.5743e-05],\n",
      "        [1.2349e-04],\n",
      "        [1.3969e-04],\n",
      "        [4.5324e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.4398512840271: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[4.9313e-04],\n",
      "        [2.1797e-03],\n",
      "        [8.6004e-03],\n",
      "        [1.1553e-04],\n",
      "        [4.5550e-04],\n",
      "        [2.4262e-02],\n",
      "        [7.2628e-05],\n",
      "        [8.2069e-06],\n",
      "        [1.3908e-05],\n",
      "        [9.3335e-04],\n",
      "        [1.4921e-04],\n",
      "        [4.7290e-04],\n",
      "        [1.2509e-04],\n",
      "        [2.7899e-04],\n",
      "        [3.5589e-05],\n",
      "        [2.3471e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.307458877563477: \n",
      "target probs tensor([[2.8879e-03],\n",
      "        [1.8627e-04],\n",
      "        [2.3813e-03],\n",
      "        [2.2028e-05],\n",
      "        [3.3088e-04],\n",
      "        [2.4329e-03],\n",
      "        [3.2174e-04],\n",
      "        [2.0013e-04],\n",
      "        [6.5980e-07],\n",
      "        [3.8127e-02],\n",
      "        [1.8826e-02],\n",
      "        [2.6544e-03],\n",
      "        [5.9583e-04],\n",
      "        [6.6454e-04],\n",
      "        [6.0681e-05],\n",
      "        [7.5917e-02]], device='cuda:0'), loss: 7.263852596282959: \n",
      "target probs tensor([[3.7349e-04],\n",
      "        [4.2228e-04],\n",
      "        [1.0940e-03],\n",
      "        [7.3464e-04],\n",
      "        [1.2093e-02],\n",
      "        [2.1872e-04],\n",
      "        [8.1806e-04],\n",
      "        [6.2079e-06],\n",
      "        [1.7228e-03],\n",
      "        [3.1449e-04],\n",
      "        [5.3677e-04],\n",
      "        [1.4127e-03],\n",
      "        [2.1813e-04],\n",
      "        [2.0768e-03],\n",
      "        [3.3915e-03],\n",
      "        [1.1709e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.325125694274902: \n",
      "target probs tensor([[1.4104e-04],\n",
      "        [1.4296e-04],\n",
      "        [9.3282e-05],\n",
      "        [3.8352e-03],\n",
      "        [4.2614e-05],\n",
      "        [2.7619e-05],\n",
      "        [1.7088e-04],\n",
      "        [1.9346e-04],\n",
      "        [1.4971e-04],\n",
      "        [2.3474e-04],\n",
      "        [4.7539e-05],\n",
      "        [5.3489e-04],\n",
      "        [4.7178e-04],\n",
      "        [1.0729e-03],\n",
      "        [3.6233e-04],\n",
      "        [6.2242e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.425041198730469: \n",
      "target probs tensor([[1.2749e-03],\n",
      "        [4.9239e-04],\n",
      "        [5.6751e-04],\n",
      "        [6.1641e-05],\n",
      "        [6.8084e-05],\n",
      "        [7.5662e-04],\n",
      "        [2.2668e-04],\n",
      "        [1.5314e-03],\n",
      "        [1.2658e-04],\n",
      "        [3.4757e-04],\n",
      "        [1.3133e-04],\n",
      "        [3.0740e-05],\n",
      "        [4.6339e-04],\n",
      "        [5.7270e-04],\n",
      "        [9.8268e-04],\n",
      "        [1.4613e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.141862869262695: \n",
      "target probs tensor([[1.9363e-04],\n",
      "        [7.2135e-03],\n",
      "        [8.4901e-05],\n",
      "        [4.3576e-06],\n",
      "        [6.3867e-03],\n",
      "        [2.5764e-04],\n",
      "        [4.3426e-04],\n",
      "        [3.7983e-05],\n",
      "        [5.0763e-05],\n",
      "        [2.8528e-04],\n",
      "        [9.7070e-05],\n",
      "        [1.1387e-04],\n",
      "        [6.2443e-04],\n",
      "        [4.4631e-04],\n",
      "        [4.9014e-04],\n",
      "        [2.7527e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.357458114624023: \n",
      "target probs tensor([[5.8012e-04],\n",
      "        [3.0308e-03],\n",
      "        [6.5627e-06],\n",
      "        [1.0441e-03],\n",
      "        [3.6176e-03],\n",
      "        [2.9442e-04],\n",
      "        [1.0079e-03],\n",
      "        [6.7774e-05],\n",
      "        [2.1206e-06],\n",
      "        [2.0783e-04],\n",
      "        [1.8637e-03],\n",
      "        [6.6699e-03],\n",
      "        [2.0410e-04],\n",
      "        [2.5917e-04],\n",
      "        [3.6242e-07],\n",
      "        [2.6986e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.577795028686523: \n",
      "target probs tensor([[1.9224e-04],\n",
      "        [8.0472e-05],\n",
      "        [2.9800e-04],\n",
      "        [2.2631e-04],\n",
      "        [2.4105e-03],\n",
      "        [5.6906e-04],\n",
      "        [2.1592e-06],\n",
      "        [8.0217e-06],\n",
      "        [1.6356e-04],\n",
      "        [9.1368e-05],\n",
      "        [1.8147e-04],\n",
      "        [1.4657e-03],\n",
      "        [1.4090e-05],\n",
      "        [4.6116e-04],\n",
      "        [4.0950e-05],\n",
      "        [1.1359e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.99821662902832: \n",
      "target probs tensor([[6.3784e-05],\n",
      "        [2.2178e-04],\n",
      "        [2.0615e-03],\n",
      "        [6.2605e-03],\n",
      "        [2.5825e-04],\n",
      "        [5.8543e-05],\n",
      "        [2.0102e-05],\n",
      "        [4.1381e-02],\n",
      "        [5.1638e-06],\n",
      "        [7.8538e-05],\n",
      "        [9.1250e-05],\n",
      "        [4.8450e-04],\n",
      "        [1.1072e-03],\n",
      "        [1.4684e-03],\n",
      "        [6.9836e-04],\n",
      "        [2.9184e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.03963851928711: \n",
      "target probs tensor([[1.4644e-06],\n",
      "        [1.8631e-04],\n",
      "        [3.1499e-04],\n",
      "        [9.2889e-04],\n",
      "        [4.6331e-04],\n",
      "        [1.4855e-04],\n",
      "        [3.9569e-04],\n",
      "        [4.8731e-06],\n",
      "        [3.1903e-04],\n",
      "        [3.4992e-05],\n",
      "        [3.4994e-05],\n",
      "        [3.1262e-04],\n",
      "        [1.8005e-03],\n",
      "        [5.8569e-04],\n",
      "        [8.0211e-04],\n",
      "        [3.0735e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.55887222290039: \n",
      "target probs tensor([[1.2586e-04],\n",
      "        [5.0178e-04],\n",
      "        [2.9800e-05],\n",
      "        [1.7626e-04],\n",
      "        [2.4563e-05],\n",
      "        [5.6927e-03],\n",
      "        [1.4651e-04],\n",
      "        [5.1906e-04],\n",
      "        [9.0010e-05],\n",
      "        [6.3401e-04],\n",
      "        [2.4109e-04],\n",
      "        [1.0624e-05],\n",
      "        [2.0700e-04],\n",
      "        [5.8479e-03],\n",
      "        [4.5014e-04],\n",
      "        [6.0602e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.313606262207031: \n",
      "target probs tensor([[3.7333e-04],\n",
      "        [1.5015e-03],\n",
      "        [4.8664e-04],\n",
      "        [9.1493e-04],\n",
      "        [4.2646e-03],\n",
      "        [3.7299e-05],\n",
      "        [1.1814e-03],\n",
      "        [7.9228e-05],\n",
      "        [3.9920e-04],\n",
      "        [8.0227e-04],\n",
      "        [3.7089e-04],\n",
      "        [2.3831e-04],\n",
      "        [2.1677e-04],\n",
      "        [4.6369e-05],\n",
      "        [5.2042e-05],\n",
      "        [2.8552e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.030814170837402: \n",
      "target probs tensor([[3.0400e-06],\n",
      "        [3.2901e-05],\n",
      "        [1.0744e-03],\n",
      "        [1.9500e-04],\n",
      "        [2.8442e-04],\n",
      "        [6.6083e-04],\n",
      "        [6.9308e-04],\n",
      "        [4.8249e-02],\n",
      "        [1.0069e-03],\n",
      "        [3.4505e-03],\n",
      "        [7.3543e-05],\n",
      "        [2.7881e-05],\n",
      "        [1.3234e-03],\n",
      "        [1.6487e-05],\n",
      "        [1.1295e-04],\n",
      "        [1.6356e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.975905418395996: \n",
      "target probs tensor([[3.1545e-04],\n",
      "        [5.1123e-03],\n",
      "        [2.1571e-02],\n",
      "        [5.2136e-04],\n",
      "        [4.4389e-04],\n",
      "        [9.9856e-04],\n",
      "        [1.8444e-04],\n",
      "        [2.1317e-03],\n",
      "        [5.7035e-04],\n",
      "        [4.7020e-04],\n",
      "        [2.1335e-04],\n",
      "        [1.2325e-03],\n",
      "        [3.3276e-04],\n",
      "        [9.8985e-05],\n",
      "        [1.4544e-03],\n",
      "        [2.2507e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.284695625305176: \n",
      "target probs tensor([[5.9044e-04],\n",
      "        [8.5812e-05],\n",
      "        [7.2834e-04],\n",
      "        [2.5098e-05],\n",
      "        [2.6592e-05],\n",
      "        [5.5575e-07],\n",
      "        [2.4105e-04],\n",
      "        [2.1692e-03],\n",
      "        [1.9335e-03],\n",
      "        [6.6883e-04],\n",
      "        [1.4882e-03],\n",
      "        [2.1820e-04],\n",
      "        [5.2511e-04],\n",
      "        [1.0310e-01],\n",
      "        [3.6446e-01],\n",
      "        [4.3551e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.567999839782715: \n",
      "target probs tensor([[2.3215e-05],\n",
      "        [1.5280e-04],\n",
      "        [4.7039e-04],\n",
      "        [2.3678e-03],\n",
      "        [4.8096e-04],\n",
      "        [8.7785e-04],\n",
      "        [3.9996e-04],\n",
      "        [5.4069e-03],\n",
      "        [5.7145e-05],\n",
      "        [1.4132e-04],\n",
      "        [4.5224e-04],\n",
      "        [5.4839e-04],\n",
      "        [7.1157e-04],\n",
      "        [2.7656e-04],\n",
      "        [1.3186e-04],\n",
      "        [1.2077e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.864054203033447: \n",
      "target probs tensor([[6.6200e-04],\n",
      "        [2.3138e-04],\n",
      "        [2.9816e-04],\n",
      "        [1.3294e-05],\n",
      "        [6.2952e-03],\n",
      "        [2.0186e-06],\n",
      "        [1.5805e-05],\n",
      "        [3.3515e-04],\n",
      "        [1.0882e-03],\n",
      "        [6.1229e-03],\n",
      "        [6.1969e-05],\n",
      "        [9.4888e-03],\n",
      "        [1.4355e-03],\n",
      "        [5.6702e-04],\n",
      "        [9.3532e-04],\n",
      "        [6.9629e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.925378799438477: \n",
      "target probs tensor([[8.2936e-05],\n",
      "        [6.5348e-04],\n",
      "        [1.7382e-03],\n",
      "        [3.2463e-03],\n",
      "        [4.1906e-04],\n",
      "        [7.4372e-04],\n",
      "        [8.4964e-05],\n",
      "        [1.0616e-04],\n",
      "        [2.2474e-03],\n",
      "        [3.8482e-04],\n",
      "        [1.0959e-04],\n",
      "        [8.6221e-04],\n",
      "        [4.7645e-04],\n",
      "        [1.5543e-04],\n",
      "        [4.1506e-04],\n",
      "        [1.1569e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.85791015625: \n",
      "target probs tensor([[1.6893e-03],\n",
      "        [6.9123e-04],\n",
      "        [2.4560e-04],\n",
      "        [1.3426e-03],\n",
      "        [5.8234e-04],\n",
      "        [1.6753e-03],\n",
      "        [2.8501e-03],\n",
      "        [1.3511e-04],\n",
      "        [4.6505e-04],\n",
      "        [5.3174e-05],\n",
      "        [2.5055e-04],\n",
      "        [5.3384e-04],\n",
      "        [1.3144e-03],\n",
      "        [2.6129e-04],\n",
      "        [5.9492e-04],\n",
      "        [7.5975e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.7898173332214355: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[5.8803e-02],\n",
      "        [1.0476e-04],\n",
      "        [6.3725e-04],\n",
      "        [8.3168e-04],\n",
      "        [2.2202e-04],\n",
      "        [6.7694e-04],\n",
      "        [2.4027e-04],\n",
      "        [2.1091e-03],\n",
      "        [1.2060e-03],\n",
      "        [3.4819e-04],\n",
      "        [6.7659e-04],\n",
      "        [7.1514e-04],\n",
      "        [6.7984e-04],\n",
      "        [1.7042e-04],\n",
      "        [6.4854e-05],\n",
      "        [2.4787e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.487201690673828: \n",
      "target probs tensor([[2.9285e-04],\n",
      "        [8.7627e-01],\n",
      "        [2.1399e-03],\n",
      "        [9.7281e-04],\n",
      "        [9.0471e-05],\n",
      "        [5.8488e-04],\n",
      "        [1.6011e-03],\n",
      "        [1.7645e-03]], device='cuda:0'), loss: 6.3602166175842285: \n",
      "target probs tensor([[1.0039e-04],\n",
      "        [9.6448e-04],\n",
      "        [8.4303e-04],\n",
      "        [1.2637e-05],\n",
      "        [6.6718e-05],\n",
      "        [1.3927e-03],\n",
      "        [7.8987e-04],\n",
      "        [1.1577e-04],\n",
      "        [1.0940e-04],\n",
      "        [1.1290e-04],\n",
      "        [3.4987e-04],\n",
      "        [1.2777e-03],\n",
      "        [5.2049e-04],\n",
      "        [2.8717e-04],\n",
      "        [9.5903e-04],\n",
      "        [5.4048e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.120367050170898: \n",
      "target probs tensor([[0.0026],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0106],\n",
      "        [0.0003],\n",
      "        [0.0009],\n",
      "        [0.0009],\n",
      "        [0.0001],\n",
      "        [0.0003],\n",
      "        [0.0009],\n",
      "        [0.0004],\n",
      "        [0.0061],\n",
      "        [0.0005],\n",
      "        [0.0001],\n",
      "        [0.0023],\n",
      "        [0.0006]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.351128101348877: \n",
      "target probs tensor([[4.3466e-05],\n",
      "        [1.3100e-04],\n",
      "        [5.3323e-05],\n",
      "        [1.3343e-04],\n",
      "        [2.5657e-05],\n",
      "        [6.7635e-05],\n",
      "        [1.0547e-03],\n",
      "        [2.1207e-04],\n",
      "        [5.4918e-04],\n",
      "        [5.5601e-04],\n",
      "        [4.8264e-04],\n",
      "        [1.5120e-04],\n",
      "        [4.0939e-04],\n",
      "        [1.2090e-03],\n",
      "        [1.5259e-03],\n",
      "        [1.1880e-04]], device='cuda:0'), loss: 8.419191360473633: \n",
      "target probs tensor([[1.3166e-03],\n",
      "        [4.3108e-02],\n",
      "        [1.6986e-04],\n",
      "        [3.3451e-04],\n",
      "        [7.1980e-04],\n",
      "        [1.0397e-02],\n",
      "        [4.2982e-02],\n",
      "        [1.4615e-03],\n",
      "        [7.9033e-04],\n",
      "        [4.9823e-04],\n",
      "        [1.8588e-04],\n",
      "        [1.1463e-04],\n",
      "        [1.1667e-03],\n",
      "        [2.3765e-05],\n",
      "        [1.0688e-04],\n",
      "        [8.1801e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.269108772277832: \n",
      "target probs tensor([[3.1989e-04],\n",
      "        [5.8719e-04],\n",
      "        [1.1921e-04],\n",
      "        [3.4475e-04],\n",
      "        [2.8311e-04],\n",
      "        [6.4182e-03],\n",
      "        [3.4287e-06],\n",
      "        [5.5694e-04],\n",
      "        [1.2514e-03],\n",
      "        [6.4794e-04],\n",
      "        [2.9711e-04],\n",
      "        [5.9275e-04],\n",
      "        [1.5561e-04],\n",
      "        [1.6126e-04],\n",
      "        [1.0845e-03],\n",
      "        [1.0026e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.912464141845703: \n",
      "target probs tensor([[2.4605e-03],\n",
      "        [2.6454e-04],\n",
      "        [2.1130e-03],\n",
      "        [5.7092e-05],\n",
      "        [3.8289e-04],\n",
      "        [1.1558e-03],\n",
      "        [3.8575e-04],\n",
      "        [2.9481e-04],\n",
      "        [4.7686e-07],\n",
      "        [5.3608e-02],\n",
      "        [1.8340e-02],\n",
      "        [3.3821e-03],\n",
      "        [5.7873e-04],\n",
      "        [1.1028e-03],\n",
      "        [4.3508e-05],\n",
      "        [1.0865e-01]], device='cuda:0'), loss: 7.155763626098633: \n",
      "target probs tensor([[2.4577e-05],\n",
      "        [1.0589e-04],\n",
      "        [3.8047e-04],\n",
      "        [1.0606e-03],\n",
      "        [2.0886e-05],\n",
      "        [3.8757e-04],\n",
      "        [4.5593e-02],\n",
      "        [1.5015e-04],\n",
      "        [2.3256e-06],\n",
      "        [1.9285e-05],\n",
      "        [5.6666e-04],\n",
      "        [1.5433e-04],\n",
      "        [6.0812e-05],\n",
      "        [9.8186e-04],\n",
      "        [8.3397e-05],\n",
      "        [1.8562e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.44411849975586: \n",
      "target probs tensor([[2.2267e-05],\n",
      "        [5.2270e-04],\n",
      "        [1.1579e-03],\n",
      "        [1.3082e-03],\n",
      "        [1.2238e-02],\n",
      "        [1.1112e-03],\n",
      "        [9.6649e-04],\n",
      "        [3.6148e-03],\n",
      "        [2.3528e-05],\n",
      "        [1.0471e-04],\n",
      "        [3.6033e-05],\n",
      "        [1.1057e-07],\n",
      "        [6.7342e-04],\n",
      "        [2.6329e-03],\n",
      "        [7.1924e-03],\n",
      "        [3.6475e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.975153923034668: \n",
      "target probs tensor([[1.5574e-03],\n",
      "        [3.3892e-03],\n",
      "        [5.6685e-05],\n",
      "        [6.0868e-01],\n",
      "        [4.1179e-04],\n",
      "        [3.7011e-02],\n",
      "        [3.7649e-03],\n",
      "        [4.7220e-04],\n",
      "        [8.9014e-03],\n",
      "        [3.0541e-04],\n",
      "        [2.4777e-03],\n",
      "        [2.2923e-04],\n",
      "        [7.5592e-04],\n",
      "        [3.8398e-04],\n",
      "        [8.5772e-04],\n",
      "        [2.9399e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.36860990524292: \n",
      "target probs tensor([[3.7800e-04],\n",
      "        [1.0609e-03],\n",
      "        [2.2610e-06],\n",
      "        [1.4931e-03],\n",
      "        [4.5092e-05],\n",
      "        [1.1727e-03],\n",
      "        [1.5929e-04],\n",
      "        [9.6659e-04],\n",
      "        [4.1914e-03],\n",
      "        [1.0088e-04],\n",
      "        [1.2206e-04],\n",
      "        [3.4844e-05],\n",
      "        [3.6457e-05],\n",
      "        [1.9231e-04],\n",
      "        [2.9373e-05],\n",
      "        [7.4043e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.709476470947266: \n",
      "target probs tensor([[8.2325e-05],\n",
      "        [1.7488e-04],\n",
      "        [1.1601e-03],\n",
      "        [1.3577e-03],\n",
      "        [9.8151e-04],\n",
      "        [4.1576e-03],\n",
      "        [2.2176e-03],\n",
      "        [6.4956e-04],\n",
      "        [1.6335e-04],\n",
      "        [8.3221e-05],\n",
      "        [4.5334e-04],\n",
      "        [9.2999e-04],\n",
      "        [1.2417e-04],\n",
      "        [3.4706e-04],\n",
      "        [8.4308e-05],\n",
      "        [8.9532e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.002147674560547: \n",
      "target probs tensor([[1.8182e-04],\n",
      "        [1.3021e-03],\n",
      "        [1.1147e-03],\n",
      "        [6.8357e-05],\n",
      "        [2.6971e-04],\n",
      "        [8.6449e-04],\n",
      "        [1.7796e-05],\n",
      "        [3.5291e-04],\n",
      "        [1.9937e-03],\n",
      "        [6.8770e-04],\n",
      "        [4.6864e-05],\n",
      "        [2.9497e-03],\n",
      "        [8.4542e-04],\n",
      "        [3.0875e-05],\n",
      "        [1.2542e-01],\n",
      "        [2.8545e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.8186936378479: \n",
      "target probs tensor([[1.1890e-04],\n",
      "        [1.2515e-03],\n",
      "        [8.7791e-04],\n",
      "        [1.4877e-04],\n",
      "        [5.8127e-04],\n",
      "        [6.0611e-04],\n",
      "        [2.7608e-04],\n",
      "        [1.8876e-04],\n",
      "        [6.9607e-04],\n",
      "        [2.3107e-03],\n",
      "        [1.2558e-01],\n",
      "        [1.7310e-04],\n",
      "        [1.0205e-04],\n",
      "        [2.1286e-04],\n",
      "        [5.2920e-04],\n",
      "        [9.5923e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.60737419128418: \n",
      "target probs tensor([[1.3662e-03],\n",
      "        [3.2398e-04],\n",
      "        [3.4872e-03],\n",
      "        [1.7031e-05],\n",
      "        [4.5297e-04],\n",
      "        [3.3810e-05],\n",
      "        [1.5367e-04],\n",
      "        [9.1158e-05],\n",
      "        [1.3317e-03],\n",
      "        [9.0056e-05],\n",
      "        [6.4806e-05],\n",
      "        [2.3332e-04],\n",
      "        [2.6386e-04],\n",
      "        [1.3077e-04],\n",
      "        [4.8500e-04],\n",
      "        [5.1267e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.21113395690918: \n",
      "target probs tensor([[6.2035e-04],\n",
      "        [5.7086e-04],\n",
      "        [3.0484e-04],\n",
      "        [8.7180e-04],\n",
      "        [2.6705e-06],\n",
      "        [1.0445e-04],\n",
      "        [3.4453e-03],\n",
      "        [3.3168e-04],\n",
      "        [5.3615e-04],\n",
      "        [3.8023e-04],\n",
      "        [7.7988e-05],\n",
      "        [2.0721e-05],\n",
      "        [5.1283e-04],\n",
      "        [3.8856e-04],\n",
      "        [4.4935e-05],\n",
      "        [1.5436e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.183467864990234: \n",
      "target probs tensor([[4.3056e-04],\n",
      "        [1.2617e-03],\n",
      "        [8.6488e-05],\n",
      "        [1.7604e-04],\n",
      "        [7.6774e-04],\n",
      "        [2.4357e-03],\n",
      "        [7.1715e-04],\n",
      "        [7.6612e-03],\n",
      "        [2.6716e-04],\n",
      "        [1.5122e-03],\n",
      "        [6.0100e-05],\n",
      "        [9.8131e-05],\n",
      "        [1.5337e-05],\n",
      "        [1.2431e-03],\n",
      "        [2.4273e-03],\n",
      "        [7.2249e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.65169620513916: \n",
      "target probs tensor([[4.9244e-03],\n",
      "        [1.1231e-03],\n",
      "        [9.1912e-06],\n",
      "        [4.4237e-04],\n",
      "        [1.1790e-04],\n",
      "        [1.9173e-04],\n",
      "        [9.3462e-04],\n",
      "        [2.7410e-04],\n",
      "        [2.0500e-03],\n",
      "        [3.9454e-05],\n",
      "        [5.4023e-04],\n",
      "        [3.0269e-06],\n",
      "        [9.4821e-05],\n",
      "        [1.0417e-03],\n",
      "        [3.9559e-04],\n",
      "        [1.1323e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.219944953918457: \n",
      "target probs tensor([[3.5616e-05],\n",
      "        [2.0568e-04],\n",
      "        [5.6078e-04],\n",
      "        [4.7746e-04],\n",
      "        [6.0658e-04],\n",
      "        [1.5844e-04],\n",
      "        [7.8339e-04],\n",
      "        [6.8138e-02],\n",
      "        [1.5235e-04],\n",
      "        [9.4052e-05],\n",
      "        [5.2424e-04],\n",
      "        [5.9800e-04],\n",
      "        [5.0736e-04],\n",
      "        [3.6167e-03],\n",
      "        [3.2632e-04],\n",
      "        [3.5045e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.630618095397949: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[1.7286e-04],\n",
      "        [2.3747e-04],\n",
      "        [9.0484e-04],\n",
      "        [4.8573e-05],\n",
      "        [9.2374e-04],\n",
      "        [6.5692e-04],\n",
      "        [1.6631e-04],\n",
      "        [1.4358e-04],\n",
      "        [2.4418e-04],\n",
      "        [9.4860e-04],\n",
      "        [2.7583e-04],\n",
      "        [1.2186e-03],\n",
      "        [6.5581e-04],\n",
      "        [9.8171e-02],\n",
      "        [3.4922e-04],\n",
      "        [9.9884e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.5323309898376465: \n",
      "target probs tensor([[3.1932e-03],\n",
      "        [1.0105e-04],\n",
      "        [2.9952e-03],\n",
      "        [6.6061e-04],\n",
      "        [2.2650e-03],\n",
      "        [3.8055e-04],\n",
      "        [5.8046e-04],\n",
      "        [2.5048e-04],\n",
      "        [6.9051e-04],\n",
      "        [9.2473e-05],\n",
      "        [4.9584e-04],\n",
      "        [2.3956e-05],\n",
      "        [1.4637e-03],\n",
      "        [1.0939e-03],\n",
      "        [4.4026e-05],\n",
      "        [5.8774e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.713598728179932: \n",
      "target probs tensor([[5.9945e-05],\n",
      "        [3.7843e-01],\n",
      "        [5.5072e-04],\n",
      "        [1.5659e-03],\n",
      "        [1.2996e-03],\n",
      "        [1.7202e-04],\n",
      "        [3.3930e-04],\n",
      "        [8.7094e-04],\n",
      "        [4.3055e-03],\n",
      "        [2.5570e-03],\n",
      "        [1.8093e-03],\n",
      "        [6.2468e-04],\n",
      "        [5.2373e-04],\n",
      "        [3.7171e-05],\n",
      "        [1.2659e-04],\n",
      "        [6.7037e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.1345109939575195: \n",
      "target probs tensor([[2.6365e-04],\n",
      "        [9.8974e-04],\n",
      "        [1.7608e-04],\n",
      "        [2.2170e-04],\n",
      "        [1.8421e-04],\n",
      "        [9.7409e-05],\n",
      "        [1.7479e-04],\n",
      "        [1.7496e-06],\n",
      "        [3.6084e-03],\n",
      "        [9.3401e-04],\n",
      "        [1.5575e-03],\n",
      "        [8.8419e-04],\n",
      "        [1.1909e-04],\n",
      "        [2.1608e-05],\n",
      "        [1.1194e-04],\n",
      "        [3.2364e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.43557357788086: \n",
      "target probs tensor([[6.4446e-05],\n",
      "        [1.1823e-04],\n",
      "        [1.0064e-03],\n",
      "        [2.1648e-02],\n",
      "        [2.0640e-04],\n",
      "        [3.4425e-06],\n",
      "        [7.2385e-05],\n",
      "        [7.2571e-04],\n",
      "        [4.0143e-03],\n",
      "        [9.0044e-04],\n",
      "        [2.9657e-02],\n",
      "        [7.0025e-04],\n",
      "        [5.5398e-04],\n",
      "        [1.7153e-03],\n",
      "        [3.1935e-04],\n",
      "        [9.9845e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.318046569824219: \n",
      "target probs tensor([[1.2508e-03],\n",
      "        [2.8108e-04],\n",
      "        [3.1575e-04],\n",
      "        [9.1355e-03],\n",
      "        [5.1229e-04],\n",
      "        [5.5141e-04],\n",
      "        [2.5922e-04],\n",
      "        [5.4738e-04],\n",
      "        [7.0587e-04],\n",
      "        [3.5481e-04],\n",
      "        [7.2178e-04],\n",
      "        [2.7123e-04],\n",
      "        [7.8781e-05],\n",
      "        [1.7665e-03],\n",
      "        [3.8827e-03],\n",
      "        [2.5491e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.276371002197266: \n",
      "target probs tensor([[3.4984e-04],\n",
      "        [9.3393e-01],\n",
      "        [2.0987e-03],\n",
      "        [6.6721e-04],\n",
      "        [1.8706e-04],\n",
      "        [5.4786e-04],\n",
      "        [2.0251e-03],\n",
      "        [1.9534e-03]], device='cuda:0'), loss: 6.254889488220215: \n",
      "target probs tensor([[1.6050e-04],\n",
      "        [6.0108e-04],\n",
      "        [1.6781e-06],\n",
      "        [8.6965e-03],\n",
      "        [1.1195e-04],\n",
      "        [2.6634e-04],\n",
      "        [1.5589e-02],\n",
      "        [2.5488e-03],\n",
      "        [1.4531e-04],\n",
      "        [1.1194e-03],\n",
      "        [6.9706e-04],\n",
      "        [3.8059e-04],\n",
      "        [7.5662e-03],\n",
      "        [3.9782e-04],\n",
      "        [4.5971e-06],\n",
      "        [3.2905e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.697030067443848: \n",
      "target probs tensor([[4.7962e-04],\n",
      "        [1.0261e-03],\n",
      "        [5.6438e-06],\n",
      "        [7.2083e-06],\n",
      "        [2.5395e-02],\n",
      "        [1.2574e-04],\n",
      "        [1.7117e-04],\n",
      "        [2.1854e-04],\n",
      "        [3.4188e-04],\n",
      "        [2.7548e-04],\n",
      "        [5.3727e-04],\n",
      "        [1.5587e-02],\n",
      "        [4.1530e-04],\n",
      "        [3.6367e-03],\n",
      "        [1.3572e-03],\n",
      "        [3.6177e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.750250816345215: \n",
      "target probs tensor([[4.3903e-05],\n",
      "        [1.6077e-04],\n",
      "        [4.9921e-05],\n",
      "        [2.4260e-04],\n",
      "        [3.5441e-05],\n",
      "        [8.0227e-05],\n",
      "        [1.3814e-03],\n",
      "        [2.6883e-04],\n",
      "        [7.7494e-04],\n",
      "        [6.0917e-04],\n",
      "        [4.7317e-04],\n",
      "        [2.2979e-04],\n",
      "        [4.7933e-04],\n",
      "        [1.2745e-03],\n",
      "        [2.1174e-03],\n",
      "        [1.5647e-04]], device='cuda:0'), loss: 8.206974983215332: \n",
      "target probs tensor([[1.7682e-06],\n",
      "        [2.9484e-02],\n",
      "        [2.6182e-04],\n",
      "        [9.3588e-04],\n",
      "        [3.7317e-04],\n",
      "        [3.4011e-04],\n",
      "        [6.3597e-04],\n",
      "        [6.1651e-04],\n",
      "        [7.8210e-06],\n",
      "        [1.1286e-03],\n",
      "        [1.4940e-03],\n",
      "        [1.6900e-04],\n",
      "        [9.4176e-02],\n",
      "        [9.9337e-05],\n",
      "        [2.6941e-04],\n",
      "        [4.0870e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.035417556762695: \n",
      "target probs tensor([[8.3698e-04],\n",
      "        [1.7324e-04],\n",
      "        [1.4271e-03],\n",
      "        [1.1022e-03],\n",
      "        [4.7671e-05],\n",
      "        [1.6170e-06],\n",
      "        [3.8532e-04],\n",
      "        [3.0426e-04],\n",
      "        [1.6406e-03],\n",
      "        [1.5334e-02],\n",
      "        [6.5100e-01],\n",
      "        [3.9006e-04],\n",
      "        [2.0674e-04],\n",
      "        [5.8717e-04],\n",
      "        [1.6035e-04],\n",
      "        [5.5186e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.7495574951171875: \n",
      "target probs tensor([[2.3711e-03],\n",
      "        [2.7378e-04],\n",
      "        [1.9321e-03],\n",
      "        [4.2400e-05],\n",
      "        [3.6635e-04],\n",
      "        [2.0687e-03],\n",
      "        [4.2946e-04],\n",
      "        [5.2881e-04],\n",
      "        [6.3107e-07],\n",
      "        [1.0609e-01],\n",
      "        [1.4024e-02],\n",
      "        [3.3144e-03],\n",
      "        [5.6157e-04],\n",
      "        [9.8158e-04],\n",
      "        [6.9554e-05],\n",
      "        [1.9572e-01]], device='cuda:0'), loss: 7.004185199737549: \n",
      "target probs tensor([[2.8731e-04],\n",
      "        [2.4315e-04],\n",
      "        [3.9704e-04],\n",
      "        [5.8064e-06],\n",
      "        [1.1799e-02],\n",
      "        [5.1962e-03],\n",
      "        [3.2676e-03],\n",
      "        [1.7714e-03],\n",
      "        [2.5752e-05],\n",
      "        [4.6086e-04],\n",
      "        [6.9236e-06],\n",
      "        [8.6721e-04],\n",
      "        [6.9249e-05],\n",
      "        [6.0389e-04],\n",
      "        [2.0881e-06],\n",
      "        [1.0385e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.553030014038086: \n",
      "target probs tensor([[7.2518e-04],\n",
      "        [4.5720e-01],\n",
      "        [3.8266e-04],\n",
      "        [9.3123e-04],\n",
      "        [9.2867e-04],\n",
      "        [1.1233e-03],\n",
      "        [7.0815e-04],\n",
      "        [8.3778e-04],\n",
      "        [5.3967e-04],\n",
      "        [1.4748e-04],\n",
      "        [4.3352e-03],\n",
      "        [3.4019e-01],\n",
      "        [5.9128e-04],\n",
      "        [6.9744e-05],\n",
      "        [7.3357e-05],\n",
      "        [4.1644e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.758963584899902: \n",
      "target probs tensor([[6.0162e-04],\n",
      "        [2.9361e-04],\n",
      "        [4.8812e-03],\n",
      "        [6.3217e-05],\n",
      "        [1.5719e-04],\n",
      "        [7.5727e-04],\n",
      "        [1.4479e-02],\n",
      "        [7.9850e-04],\n",
      "        [6.4209e-04],\n",
      "        [1.5700e-04],\n",
      "        [8.3973e-05],\n",
      "        [1.1723e-03],\n",
      "        [4.9770e-04],\n",
      "        [6.5078e-04],\n",
      "        [1.7736e-03],\n",
      "        [2.1483e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.344786167144775: \n",
      "target probs tensor([[7.7447e-04],\n",
      "        [1.1514e-03],\n",
      "        [2.6953e-04],\n",
      "        [2.1618e-04],\n",
      "        [2.0858e-05],\n",
      "        [1.4106e-03],\n",
      "        [1.2576e-04],\n",
      "        [8.6332e-03],\n",
      "        [1.7472e-05],\n",
      "        [1.2751e-03],\n",
      "        [3.4342e-04],\n",
      "        [1.4391e-04],\n",
      "        [1.4954e-04],\n",
      "        [1.4464e-04],\n",
      "        [4.6798e-04],\n",
      "        [3.9484e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.078685760498047: \n",
      "target probs tensor([[4.8306e-05],\n",
      "        [5.9033e-05],\n",
      "        [1.5748e-06],\n",
      "        [5.2385e-04],\n",
      "        [1.0640e-04],\n",
      "        [4.6915e-04],\n",
      "        [5.2253e-04],\n",
      "        [6.1943e-03],\n",
      "        [1.6014e-05],\n",
      "        [9.8853e-04],\n",
      "        [7.6721e-03],\n",
      "        [3.4270e-04],\n",
      "        [1.6953e-05],\n",
      "        [6.3897e-02],\n",
      "        [3.5681e-01],\n",
      "        [9.6669e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.804079055786133: \n",
      "target probs tensor([[6.7119e-04],\n",
      "        [1.0576e-03],\n",
      "        [1.0987e-04],\n",
      "        [1.0112e-03],\n",
      "        [6.6616e-02],\n",
      "        [2.3013e-05],\n",
      "        [4.9466e-04],\n",
      "        [2.0472e-04],\n",
      "        [4.2962e-04],\n",
      "        [8.6893e-04],\n",
      "        [5.5788e-04],\n",
      "        [3.3959e-03],\n",
      "        [1.6212e-04],\n",
      "        [8.9611e-04],\n",
      "        [6.0728e-04],\n",
      "        [1.2342e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.343168258666992: \n",
      "target probs tensor([[3.1000e-04],\n",
      "        [2.8824e-04],\n",
      "        [1.1693e-03],\n",
      "        [1.5583e-04],\n",
      "        [3.1350e-02],\n",
      "        [2.8887e-04],\n",
      "        [9.3682e-04],\n",
      "        [4.1657e-05],\n",
      "        [1.6045e-03],\n",
      "        [2.1023e-03],\n",
      "        [2.0528e-04],\n",
      "        [4.5543e-04],\n",
      "        [1.9435e-04],\n",
      "        [2.7919e-03],\n",
      "        [1.9237e-04],\n",
      "        [4.8406e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.488825798034668: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[7.5416e-04],\n",
      "        [1.8843e-03],\n",
      "        [1.2667e-03],\n",
      "        [1.5359e-04],\n",
      "        [1.0646e-03],\n",
      "        [5.0134e-05],\n",
      "        [4.7724e-04],\n",
      "        [1.8237e-02],\n",
      "        [8.4336e-04],\n",
      "        [1.3245e-03],\n",
      "        [7.1075e-04],\n",
      "        [1.0149e-03],\n",
      "        [4.8060e-02],\n",
      "        [1.1414e-04],\n",
      "        [6.5947e-05],\n",
      "        [7.0092e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.135278701782227: \n",
      "target probs tensor([[7.0018e-03],\n",
      "        [8.3227e-04],\n",
      "        [1.1198e-04],\n",
      "        [5.5517e-04],\n",
      "        [9.0903e-04],\n",
      "        [4.5556e-05],\n",
      "        [3.1826e-04],\n",
      "        [4.7643e-03],\n",
      "        [5.8417e-04],\n",
      "        [1.5052e-04],\n",
      "        [1.5085e-03],\n",
      "        [1.0234e-04],\n",
      "        [2.3103e-03],\n",
      "        [2.7396e-03],\n",
      "        [5.2350e-04],\n",
      "        [2.0286e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.437758922576904: \n",
      "target probs tensor([[1.4024e-03],\n",
      "        [4.2470e-04],\n",
      "        [1.7402e-04],\n",
      "        [3.5146e-03],\n",
      "        [1.7547e-03],\n",
      "        [7.9307e-04],\n",
      "        [2.5777e-04],\n",
      "        [4.1054e-04],\n",
      "        [1.0147e-03],\n",
      "        [3.9306e-04],\n",
      "        [1.2528e-03],\n",
      "        [1.7537e-03],\n",
      "        [1.0988e-04],\n",
      "        [1.8251e-05],\n",
      "        [1.4089e-03],\n",
      "        [1.4649e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.585712909698486: \n",
      "target probs tensor([[2.3236e-01],\n",
      "        [6.6692e-04],\n",
      "        [1.5623e-03],\n",
      "        [9.4548e-05],\n",
      "        [1.5231e-03],\n",
      "        [2.2419e-05],\n",
      "        [4.9294e-05],\n",
      "        [6.6054e-05],\n",
      "        [3.9172e-07],\n",
      "        [7.3958e-04],\n",
      "        [9.4508e-05],\n",
      "        [7.5016e-03],\n",
      "        [2.9942e-04],\n",
      "        [3.6672e-04],\n",
      "        [5.5762e-05],\n",
      "        [9.8859e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.27486515045166: \n",
      "target probs tensor([[4.5923e-05],\n",
      "        [4.5285e-05],\n",
      "        [4.2312e-04],\n",
      "        [1.7424e-05],\n",
      "        [4.1564e-05],\n",
      "        [1.3027e-03],\n",
      "        [1.3220e-03],\n",
      "        [8.9995e-04],\n",
      "        [1.6155e-01],\n",
      "        [2.4175e-04],\n",
      "        [6.3467e-06],\n",
      "        [4.0549e-01],\n",
      "        [9.4357e-04],\n",
      "        [1.4217e-03],\n",
      "        [7.4644e-04],\n",
      "        [4.0339e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.540515422821045: \n",
      "target probs tensor([[1.1772e-04],\n",
      "        [1.8029e-04],\n",
      "        [9.0097e-04],\n",
      "        [4.9788e-04],\n",
      "        [2.9225e-04],\n",
      "        [9.7647e-04],\n",
      "        [4.2422e-03],\n",
      "        [2.3847e-04],\n",
      "        [9.1112e-05],\n",
      "        [4.2532e-04],\n",
      "        [1.3729e-01],\n",
      "        [1.3382e-03],\n",
      "        [5.4011e-04],\n",
      "        [1.2868e-03],\n",
      "        [6.6010e-05],\n",
      "        [2.4264e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.578662872314453: \n",
      "target probs tensor([[2.3795e-04],\n",
      "        [1.9725e-03],\n",
      "        [2.0397e-04],\n",
      "        [3.0051e-03],\n",
      "        [7.4490e-04],\n",
      "        [3.0352e-03],\n",
      "        [9.5621e-04],\n",
      "        [2.2068e-04],\n",
      "        [1.2058e-03],\n",
      "        [7.9338e-06],\n",
      "        [2.2321e-04],\n",
      "        [7.9116e-03],\n",
      "        [3.7870e-05],\n",
      "        [6.1661e-04],\n",
      "        [2.3523e-04],\n",
      "        [2.1816e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.707348823547363: \n",
      "target probs tensor([[5.3003e-04],\n",
      "        [2.8928e-04],\n",
      "        [1.8755e-04],\n",
      "        [1.2694e-05],\n",
      "        [1.5002e-04],\n",
      "        [1.1998e-04],\n",
      "        [3.4976e-03],\n",
      "        [2.7443e-03],\n",
      "        [7.3384e-04],\n",
      "        [7.0400e-03],\n",
      "        [5.5533e-02],\n",
      "        [3.2566e-05],\n",
      "        [2.9281e-05],\n",
      "        [3.4218e-03],\n",
      "        [8.9051e-04],\n",
      "        [9.1145e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.529382705688477: \n",
      "target probs tensor([[9.3794e-05],\n",
      "        [1.9171e-04],\n",
      "        [1.4502e-04],\n",
      "        [2.0450e-03],\n",
      "        [7.2623e-04],\n",
      "        [2.5148e-05],\n",
      "        [2.3256e-03],\n",
      "        [3.0810e-04],\n",
      "        [5.5372e-05],\n",
      "        [3.5956e-03],\n",
      "        [4.0527e-06],\n",
      "        [5.0824e-04],\n",
      "        [4.8639e-05],\n",
      "        [3.5708e-05],\n",
      "        [3.1488e-05],\n",
      "        [2.2491e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.55604362487793: \n",
      "target probs tensor([[4.7852e-04],\n",
      "        [3.3631e-02],\n",
      "        [3.8578e-04],\n",
      "        [9.5772e-04],\n",
      "        [6.6172e-05],\n",
      "        [3.7225e-02],\n",
      "        [1.1159e-03],\n",
      "        [4.9398e-04],\n",
      "        [3.5177e-04],\n",
      "        [9.2202e-04],\n",
      "        [1.1691e-03],\n",
      "        [3.4434e-06],\n",
      "        [2.1197e-04],\n",
      "        [8.2717e-02],\n",
      "        [7.6432e-04],\n",
      "        [2.8419e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.108698844909668: \n",
      "target probs tensor([[5.5877e-04],\n",
      "        [7.5955e-04],\n",
      "        [1.0507e-04],\n",
      "        [5.8511e-05],\n",
      "        [2.7093e-03],\n",
      "        [2.0117e-04],\n",
      "        [2.0003e-03],\n",
      "        [5.2576e-02],\n",
      "        [8.3630e-03],\n",
      "        [9.1502e-04],\n",
      "        [1.4746e-03],\n",
      "        [7.3185e-06],\n",
      "        [3.8434e-04],\n",
      "        [6.5536e-04],\n",
      "        [2.5751e-04],\n",
      "        [8.0429e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.654781818389893: \n",
      "target probs tensor([[3.2451e-04],\n",
      "        [9.6946e-01],\n",
      "        [1.4757e-03],\n",
      "        [1.0890e-03],\n",
      "        [1.9644e-04],\n",
      "        [5.5463e-04],\n",
      "        [2.1196e-03],\n",
      "        [1.7269e-03]], device='cuda:0'), loss: 6.244467735290527: \n",
      "target probs tensor([[4.7083e-05],\n",
      "        [1.2000e-04],\n",
      "        [1.5383e-04],\n",
      "        [5.9980e-04],\n",
      "        [1.9270e-04],\n",
      "        [4.9987e-04],\n",
      "        [6.9698e-05],\n",
      "        [7.0713e-05],\n",
      "        [8.8420e-04],\n",
      "        [1.3667e-06],\n",
      "        [4.2511e-04],\n",
      "        [1.0526e-03],\n",
      "        [4.1988e-04],\n",
      "        [1.4514e-03],\n",
      "        [4.2115e-04],\n",
      "        [4.0544e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.326166152954102: \n",
      "target probs tensor([[1.8262e-05],\n",
      "        [1.0199e-06],\n",
      "        [4.1655e-04],\n",
      "        [3.2118e-04],\n",
      "        [9.6199e-04],\n",
      "        [1.9122e-04],\n",
      "        [2.6407e-03],\n",
      "        [4.9695e-04],\n",
      "        [1.8820e-04],\n",
      "        [2.6488e-02],\n",
      "        [6.7947e-04],\n",
      "        [2.7202e-02],\n",
      "        [2.4929e-04],\n",
      "        [4.0264e-04],\n",
      "        [1.8066e-03],\n",
      "        [1.0386e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.768528938293457: \n",
      "target probs tensor([[3.6854e-05],\n",
      "        [1.4926e-04],\n",
      "        [5.9665e-05],\n",
      "        [2.3434e-04],\n",
      "        [3.6513e-05],\n",
      "        [8.0634e-05],\n",
      "        [1.2281e-03],\n",
      "        [2.8026e-04],\n",
      "        [6.5951e-04],\n",
      "        [7.6835e-04],\n",
      "        [4.4894e-04],\n",
      "        [2.3973e-04],\n",
      "        [4.4822e-04],\n",
      "        [1.3462e-03],\n",
      "        [1.6350e-03],\n",
      "        [1.5174e-04]], device='cuda:0'), loss: 8.231220245361328: \n",
      "target probs tensor([[4.0989e-04],\n",
      "        [2.4399e-04],\n",
      "        [2.2644e-04],\n",
      "        [8.5281e-05],\n",
      "        [2.6943e-04],\n",
      "        [2.1271e-03],\n",
      "        [7.6161e-04],\n",
      "        [2.9271e-03],\n",
      "        [5.4437e-02],\n",
      "        [6.8609e-04],\n",
      "        [9.0839e-05],\n",
      "        [5.0874e-04],\n",
      "        [1.4636e-04],\n",
      "        [7.2662e-04],\n",
      "        [9.5858e-04],\n",
      "        [1.9150e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.494941234588623: \n",
      "target probs tensor([[1.8799e-04],\n",
      "        [7.0204e-05],\n",
      "        [5.8546e-04],\n",
      "        [2.5321e-02],\n",
      "        [4.4151e-03],\n",
      "        [1.9497e-04],\n",
      "        [9.9356e-04],\n",
      "        [4.5784e-03],\n",
      "        [3.9074e-03],\n",
      "        [4.7122e-04],\n",
      "        [1.8932e-01],\n",
      "        [8.4444e-04],\n",
      "        [1.0309e-04],\n",
      "        [2.4569e-04],\n",
      "        [2.2171e-04],\n",
      "        [6.1953e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.922924041748047: \n",
      "target probs tensor([[2.3836e-03],\n",
      "        [2.6677e-04],\n",
      "        [2.1410e-03],\n",
      "        [6.8573e-05],\n",
      "        [3.0069e-04],\n",
      "        [2.3016e-03],\n",
      "        [3.1174e-04],\n",
      "        [6.3647e-04],\n",
      "        [2.2683e-06],\n",
      "        [1.5120e-01],\n",
      "        [2.3210e-02],\n",
      "        [4.5145e-03],\n",
      "        [6.5048e-04],\n",
      "        [6.8332e-04],\n",
      "        [7.8431e-05],\n",
      "        [2.3039e-01]], device='cuda:0'), loss: 6.825982093811035: \n",
      "target probs tensor([[1.1665e-03],\n",
      "        [2.1309e-03],\n",
      "        [5.5845e-04],\n",
      "        [6.4077e-05],\n",
      "        [1.3658e-03],\n",
      "        [6.0351e-05],\n",
      "        [2.2092e-04],\n",
      "        [1.9039e-02],\n",
      "        [1.5150e-04],\n",
      "        [4.0102e-05],\n",
      "        [1.8308e-05],\n",
      "        [6.8859e-05],\n",
      "        [4.2238e-04],\n",
      "        [4.0169e-04],\n",
      "        [1.0869e-02],\n",
      "        [3.3745e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.891067028045654: \n",
      "target probs tensor([[5.5846e-02],\n",
      "        [1.2253e-04],\n",
      "        [2.9357e-01],\n",
      "        [1.3183e-04],\n",
      "        [1.2336e-03],\n",
      "        [8.3004e-05],\n",
      "        [1.5638e-04],\n",
      "        [5.3111e-04],\n",
      "        [7.9396e-04],\n",
      "        [9.5280e-03],\n",
      "        [6.1075e-04],\n",
      "        [2.8852e-04],\n",
      "        [4.7771e-04],\n",
      "        [2.2352e-04],\n",
      "        [2.7365e-04],\n",
      "        [1.1981e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.0485520362854: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[1.6397e-04],\n",
      "        [3.8075e-03],\n",
      "        [1.1927e-03],\n",
      "        [1.2700e-03],\n",
      "        [5.4969e-05],\n",
      "        [4.6576e-04],\n",
      "        [6.5762e-04],\n",
      "        [9.8412e-04],\n",
      "        [1.3568e-03],\n",
      "        [1.3189e-03],\n",
      "        [1.1372e-04],\n",
      "        [2.2751e-03],\n",
      "        [3.9984e-04],\n",
      "        [5.1082e-05],\n",
      "        [2.6666e-04],\n",
      "        [3.5218e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.606661319732666: \n",
      "target probs tensor([[1.8274e-03],\n",
      "        [5.7772e-03],\n",
      "        [1.4845e-04],\n",
      "        [2.9703e-04],\n",
      "        [2.0121e-03],\n",
      "        [7.4022e-06],\n",
      "        [1.1615e-03],\n",
      "        [1.6878e-03],\n",
      "        [5.4788e-04],\n",
      "        [2.7291e-01],\n",
      "        [5.8262e-05],\n",
      "        [1.1724e-03],\n",
      "        [3.8107e-04],\n",
      "        [1.0011e-04],\n",
      "        [6.0940e-05],\n",
      "        [8.7931e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.418210983276367: \n",
      "target probs tensor([[1.1207e-02],\n",
      "        [9.6191e-04],\n",
      "        [3.6513e-04],\n",
      "        [5.1633e-03],\n",
      "        [1.1185e-04],\n",
      "        [8.5734e-02],\n",
      "        [8.3436e-05],\n",
      "        [2.5959e-05],\n",
      "        [5.3643e-05],\n",
      "        [1.0223e-03],\n",
      "        [8.8149e-04],\n",
      "        [1.1269e-05],\n",
      "        [3.7129e-04],\n",
      "        [3.9433e-03],\n",
      "        [2.8906e-07],\n",
      "        [1.0292e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.771105766296387: \n",
      "target probs tensor([[2.5417e-05],\n",
      "        [1.7670e-05],\n",
      "        [1.5052e-03],\n",
      "        [1.3230e-04],\n",
      "        [2.0390e-05],\n",
      "        [1.7096e-05],\n",
      "        [1.3546e-03],\n",
      "        [2.1919e-04],\n",
      "        [3.8014e-03],\n",
      "        [1.1226e-03],\n",
      "        [1.6012e-01],\n",
      "        [7.0907e-04],\n",
      "        [4.8643e-04],\n",
      "        [7.3882e-04],\n",
      "        [4.2784e-03],\n",
      "        [1.1365e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.642541408538818: \n",
      "target probs tensor([[9.8067e-05],\n",
      "        [1.9282e-03],\n",
      "        [4.9437e-05],\n",
      "        [1.6306e-04],\n",
      "        [1.1232e-03],\n",
      "        [6.5371e-03],\n",
      "        [5.7412e-04],\n",
      "        [6.0984e-03],\n",
      "        [2.8155e-04],\n",
      "        [4.8297e-04],\n",
      "        [8.2554e-04],\n",
      "        [3.1104e-03],\n",
      "        [3.2812e-06],\n",
      "        [1.6387e-05],\n",
      "        [6.8434e-04],\n",
      "        [9.1847e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.675519943237305: \n",
      "target probs tensor([[4.6921e-05],\n",
      "        [1.6387e-03],\n",
      "        [1.3137e-03],\n",
      "        [6.7965e-04],\n",
      "        [1.1940e-05],\n",
      "        [2.4518e-03],\n",
      "        [2.3529e-05],\n",
      "        [5.6699e-04],\n",
      "        [1.0542e-03],\n",
      "        [2.5724e-02],\n",
      "        [2.1135e-02],\n",
      "        [8.4976e-03],\n",
      "        [1.7149e-04],\n",
      "        [1.1400e-02],\n",
      "        [3.8100e-04],\n",
      "        [1.0107e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.052733898162842: \n",
      "target probs tensor([[2.0756e-04],\n",
      "        [5.4993e-03],\n",
      "        [4.1620e-04],\n",
      "        [2.2139e-04],\n",
      "        [1.0530e-03],\n",
      "        [1.2280e-02],\n",
      "        [5.1228e-04],\n",
      "        [2.0943e-03],\n",
      "        [9.1639e-04],\n",
      "        [7.1928e-05],\n",
      "        [4.0094e-05],\n",
      "        [1.1754e-04],\n",
      "        [1.7064e-03],\n",
      "        [6.7339e-03],\n",
      "        [1.3189e-05],\n",
      "        [1.3870e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.486427307128906: \n",
      "target probs tensor([[3.8657e-02],\n",
      "        [3.6546e-03],\n",
      "        [1.7340e-03],\n",
      "        [3.8676e-04],\n",
      "        [5.9650e-04],\n",
      "        [1.6120e-05],\n",
      "        [1.2512e-05],\n",
      "        [1.1642e-03],\n",
      "        [1.0712e-06],\n",
      "        [1.7284e-02],\n",
      "        [7.6031e-03],\n",
      "        [1.3130e-02],\n",
      "        [2.0189e-04],\n",
      "        [1.2090e-03],\n",
      "        [3.2851e-05],\n",
      "        [8.0012e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.598967552185059: \n",
      "target probs tensor([[4.3657e-06],\n",
      "        [1.4770e-03],\n",
      "        [2.7882e-03],\n",
      "        [2.2885e-04],\n",
      "        [1.1236e-04],\n",
      "        [6.5503e-04],\n",
      "        [1.2722e-04],\n",
      "        [7.5833e-04],\n",
      "        [9.3929e-04],\n",
      "        [8.9479e-03],\n",
      "        [1.3585e-03],\n",
      "        [2.1221e-04],\n",
      "        [3.3347e-04],\n",
      "        [1.1513e-04],\n",
      "        [8.5848e-04],\n",
      "        [1.6962e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.685253143310547: \n",
      "target probs tensor([[4.5290e-04],\n",
      "        [1.9464e-02],\n",
      "        [1.5976e-04],\n",
      "        [1.6809e-08],\n",
      "        [1.3672e-04],\n",
      "        [2.8712e-05],\n",
      "        [5.6725e-05],\n",
      "        [3.5144e-04],\n",
      "        [8.4326e-04],\n",
      "        [1.3656e-03],\n",
      "        [1.1268e-03],\n",
      "        [8.7320e-02],\n",
      "        [2.6355e-04],\n",
      "        [4.6690e-04],\n",
      "        [9.4045e-03],\n",
      "        [4.9886e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.046669006347656: \n",
      "target probs tensor([[4.1709e-03],\n",
      "        [7.5436e-04],\n",
      "        [6.7353e-04],\n",
      "        [1.5236e-03],\n",
      "        [1.7116e-03],\n",
      "        [5.4151e-04],\n",
      "        [7.8431e-04],\n",
      "        [3.3972e-04],\n",
      "        [2.8528e-04],\n",
      "        [2.3121e-03],\n",
      "        [3.9688e-01],\n",
      "        [8.1560e-05],\n",
      "        [2.0444e-04],\n",
      "        [2.0708e-04],\n",
      "        [1.1626e-03],\n",
      "        [5.1304e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.9605183601379395: \n",
      "target probs tensor([[1.2144e-03],\n",
      "        [6.4782e-04],\n",
      "        [7.4397e-05],\n",
      "        [1.6700e-05],\n",
      "        [3.4478e-04],\n",
      "        [5.0199e-04],\n",
      "        [3.1633e-03],\n",
      "        [3.5579e-04],\n",
      "        [3.0246e-02],\n",
      "        [1.3462e-03],\n",
      "        [2.8470e-04],\n",
      "        [5.2813e-04],\n",
      "        [3.2912e-05],\n",
      "        [3.6647e-04],\n",
      "        [2.3982e-03],\n",
      "        [3.8899e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.610361576080322: \n",
      "target probs tensor([[2.9244e-03],\n",
      "        [4.0610e-03],\n",
      "        [8.4771e-06],\n",
      "        [1.2247e-03],\n",
      "        [1.0964e-04],\n",
      "        [4.6773e-03],\n",
      "        [1.0207e-03],\n",
      "        [7.6026e-05],\n",
      "        [1.1604e-04],\n",
      "        [1.6897e-03],\n",
      "        [1.2163e-03],\n",
      "        [1.3595e-03],\n",
      "        [9.6001e-04],\n",
      "        [2.0164e-04],\n",
      "        [2.6899e-03],\n",
      "        [2.0751e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.305633544921875: \n",
      "target probs tensor([[9.6137e-06],\n",
      "        [2.8739e-03],\n",
      "        [1.3483e-06],\n",
      "        [2.4784e-03],\n",
      "        [2.3453e-04],\n",
      "        [1.2298e-05],\n",
      "        [2.0877e-05],\n",
      "        [5.5453e-04],\n",
      "        [1.1098e-04],\n",
      "        [8.1656e-04],\n",
      "        [4.1835e-04],\n",
      "        [7.6462e-03],\n",
      "        [1.7196e-04],\n",
      "        [1.1497e-03],\n",
      "        [2.0419e-04],\n",
      "        [2.6489e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.349708557128906: \n",
      "target probs tensor([[3.9026e-04],\n",
      "        [1.2491e-04],\n",
      "        [7.2311e-03],\n",
      "        [1.5058e-05],\n",
      "        [8.0531e-04],\n",
      "        [1.1254e-02],\n",
      "        [4.1611e-02],\n",
      "        [8.4528e-04],\n",
      "        [8.4377e-04],\n",
      "        [3.5101e-04],\n",
      "        [7.1143e-04],\n",
      "        [3.0549e-02],\n",
      "        [1.2191e-03],\n",
      "        [8.2924e-04],\n",
      "        [5.0819e-04],\n",
      "        [1.3884e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.923534393310547: \n",
      "target probs tensor([[1.9679e-05],\n",
      "        [1.4927e-05],\n",
      "        [5.5451e-05],\n",
      "        [4.4969e-05],\n",
      "        [2.8027e-04],\n",
      "        [2.1993e-03],\n",
      "        [2.6726e-06],\n",
      "        [8.0884e-04],\n",
      "        [6.2966e-04],\n",
      "        [4.1157e-03],\n",
      "        [1.4642e-05],\n",
      "        [2.3675e-03],\n",
      "        [7.4195e-03],\n",
      "        [2.5880e-04],\n",
      "        [6.7748e-04],\n",
      "        [3.7544e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.399873733520508: \n",
      "target probs tensor([[3.4148e-04],\n",
      "        [9.8544e-01],\n",
      "        [5.7343e-03],\n",
      "        [3.1154e-04],\n",
      "        [1.9007e-04],\n",
      "        [8.8199e-04],\n",
      "        [1.3547e-03],\n",
      "        [1.9482e-03]], device='cuda:0'), loss: 6.2098236083984375: \n",
      "target probs tensor([[2.0044e-04],\n",
      "        [3.4034e-02],\n",
      "        [2.3278e-04],\n",
      "        [8.2915e-04],\n",
      "        [9.1629e-04],\n",
      "        [4.3745e-05],\n",
      "        [1.4131e-03],\n",
      "        [1.8454e-04],\n",
      "        [2.8771e-04],\n",
      "        [2.7521e-05],\n",
      "        [6.5187e-04],\n",
      "        [7.9200e-05],\n",
      "        [2.8407e-02],\n",
      "        [1.5378e-04],\n",
      "        [8.2268e-02],\n",
      "        [1.3408e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.27716064453125: \n",
      "target probs tensor([[8.1811e-04],\n",
      "        [4.5164e-04],\n",
      "        [7.6671e-05],\n",
      "        [1.0853e-03],\n",
      "        [7.0633e-03],\n",
      "        [4.3981e-04],\n",
      "        [2.8724e-04],\n",
      "        [1.0646e-02],\n",
      "        [1.7387e-03],\n",
      "        [6.6532e-04],\n",
      "        [3.7031e-04],\n",
      "        [3.3953e-03],\n",
      "        [3.2455e-04],\n",
      "        [2.2303e-03],\n",
      "        [3.7529e-05],\n",
      "        [1.4478e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.307407855987549: \n",
      "target probs tensor([[4.2304e-05],\n",
      "        [1.5585e-04],\n",
      "        [6.6141e-05],\n",
      "        [2.1618e-04],\n",
      "        [3.2928e-05],\n",
      "        [4.4010e-05],\n",
      "        [2.1503e-03],\n",
      "        [1.8614e-04],\n",
      "        [5.8213e-04],\n",
      "        [1.7949e-03],\n",
      "        [5.5485e-04],\n",
      "        [3.0911e-04],\n",
      "        [5.5096e-04],\n",
      "        [1.6554e-03],\n",
      "        [1.8259e-03],\n",
      "        [2.0667e-04]], device='cuda:0'), loss: 8.12697696685791: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 80 with validation value: 0.828000009059906.\n",
      "target probs tensor([[2.6994e-04],\n",
      "        [1.6601e-04],\n",
      "        [2.2327e-04],\n",
      "        [6.3430e-04],\n",
      "        [1.2770e-03],\n",
      "        [4.1858e-05],\n",
      "        [5.7421e-03],\n",
      "        [4.6681e-06],\n",
      "        [7.5443e-03],\n",
      "        [3.3046e-05],\n",
      "        [4.7390e-04],\n",
      "        [1.5452e-04],\n",
      "        [8.8917e-04],\n",
      "        [2.0717e-02],\n",
      "        [5.4705e-04],\n",
      "        [1.4461e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.716006278991699: \n",
      "target probs tensor([[1.1478e-03],\n",
      "        [1.6632e-03],\n",
      "        [3.1807e-04],\n",
      "        [1.0040e-03],\n",
      "        [4.5658e-04],\n",
      "        [1.2786e-03],\n",
      "        [3.0319e-04],\n",
      "        [3.6869e-04],\n",
      "        [1.4042e-03],\n",
      "        [1.4151e-04],\n",
      "        [4.0443e-03],\n",
      "        [9.0224e-04],\n",
      "        [2.1669e-05],\n",
      "        [1.8591e-04],\n",
      "        [4.2910e-04],\n",
      "        [1.7422e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.636107444763184: \n",
      "target probs tensor([[2.3078e-03],\n",
      "        [3.9538e-04],\n",
      "        [4.4577e-03],\n",
      "        [7.6668e-05],\n",
      "        [3.1934e-04],\n",
      "        [2.7352e-03],\n",
      "        [5.6256e-04],\n",
      "        [6.6632e-04],\n",
      "        [1.2942e-06],\n",
      "        [1.9664e-01],\n",
      "        [1.8588e-02],\n",
      "        [3.6062e-03],\n",
      "        [6.1796e-04],\n",
      "        [1.1461e-03],\n",
      "        [8.2586e-05],\n",
      "        [2.5500e-01]], device='cuda:0'), loss: 6.704172134399414: \n",
      "target probs tensor([[1.7744e-03],\n",
      "        [5.9925e-04],\n",
      "        [4.8838e-05],\n",
      "        [7.8468e-03],\n",
      "        [4.4852e-04],\n",
      "        [6.8891e-05],\n",
      "        [4.2232e-05],\n",
      "        [6.6892e-03],\n",
      "        [2.4657e-05],\n",
      "        [3.4837e-05],\n",
      "        [9.4973e-03],\n",
      "        [1.6778e-03],\n",
      "        [6.2541e-05],\n",
      "        [3.2949e-05],\n",
      "        [1.5952e-04],\n",
      "        [1.1096e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.879240036010742: \n",
      "target probs tensor([[4.4396e-03],\n",
      "        [1.0301e-03],\n",
      "        [4.4151e-04],\n",
      "        [6.2845e-04],\n",
      "        [5.7915e-03],\n",
      "        [2.2270e-03],\n",
      "        [4.1748e-01],\n",
      "        [1.4105e-02],\n",
      "        [5.8594e-04],\n",
      "        [3.3401e-03],\n",
      "        [1.5304e-04],\n",
      "        [2.3683e-05],\n",
      "        [6.2116e-04],\n",
      "        [1.4668e-03],\n",
      "        [1.2343e-04],\n",
      "        [3.5642e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.556933403015137: \n",
      "target probs tensor([[0.0018],\n",
      "        [0.0001],\n",
      "        [0.0076],\n",
      "        [0.0004],\n",
      "        [0.0006],\n",
      "        [0.0002],\n",
      "        [0.0023],\n",
      "        [0.0001],\n",
      "        [0.0004],\n",
      "        [0.0004],\n",
      "        [0.0377],\n",
      "        [0.0005],\n",
      "        [0.0063],\n",
      "        [0.0004],\n",
      "        [0.0010],\n",
      "        [0.0007]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.066532135009766: \n",
      "target probs tensor([[9.4570e-05],\n",
      "        [6.3262e-04],\n",
      "        [1.1033e-04],\n",
      "        [1.6123e-04],\n",
      "        [6.4006e-05],\n",
      "        [1.0296e-03],\n",
      "        [9.5220e-04],\n",
      "        [5.1721e-03],\n",
      "        [1.5913e-03],\n",
      "        [3.4167e-04],\n",
      "        [5.6800e-03],\n",
      "        [1.3102e-03],\n",
      "        [3.8606e-04],\n",
      "        [2.8747e-04],\n",
      "        [1.5667e-02],\n",
      "        [4.7867e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.186123847961426: \n",
      "target probs tensor([[4.2745e-04],\n",
      "        [4.7725e-08],\n",
      "        [9.0506e-04],\n",
      "        [3.8596e-04],\n",
      "        [7.8705e-05],\n",
      "        [1.9829e-06],\n",
      "        [7.9469e-04],\n",
      "        [6.1467e-05],\n",
      "        [1.2705e-03],\n",
      "        [1.9863e-04],\n",
      "        [2.7199e-04],\n",
      "        [2.8424e-03],\n",
      "        [7.2589e-04],\n",
      "        [8.3179e-04],\n",
      "        [5.4316e-04],\n",
      "        [5.8946e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.589729309082031: \n",
      "target probs tensor([[1.0154e-04],\n",
      "        [2.2681e-01],\n",
      "        [5.5068e-04],\n",
      "        [5.8849e-03],\n",
      "        [2.9667e-03],\n",
      "        [2.8866e-04],\n",
      "        [5.9457e-01],\n",
      "        [2.1023e-08],\n",
      "        [8.1288e-04],\n",
      "        [1.6197e-02],\n",
      "        [5.4551e-04],\n",
      "        [1.5184e-04],\n",
      "        [1.6785e-03],\n",
      "        [3.9115e-03],\n",
      "        [3.0642e-05],\n",
      "        [6.0460e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.6160078048706055: \n",
      "target probs tensor([[6.8201e-02],\n",
      "        [2.5840e-02],\n",
      "        [2.4048e-02],\n",
      "        [3.4846e-04],\n",
      "        [1.8560e-01],\n",
      "        [1.4390e-03],\n",
      "        [2.6063e-03],\n",
      "        [5.4350e-03],\n",
      "        [1.2461e-04],\n",
      "        [7.1602e-01],\n",
      "        [1.6939e-03],\n",
      "        [1.2506e-03],\n",
      "        [6.3376e-03],\n",
      "        [3.0540e-03],\n",
      "        [1.2668e-03],\n",
      "        [7.2319e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 5.285518169403076: \n",
      "target probs tensor([[1.1878e-03],\n",
      "        [7.3253e-04],\n",
      "        [4.6591e-05],\n",
      "        [4.6006e-04],\n",
      "        [4.7957e-04],\n",
      "        [1.4843e-03],\n",
      "        [2.8722e-03],\n",
      "        [5.2201e-06],\n",
      "        [7.7555e-03],\n",
      "        [1.2537e-04],\n",
      "        [9.9184e-05],\n",
      "        [5.7764e-04],\n",
      "        [1.1031e-04],\n",
      "        [7.5213e-04],\n",
      "        [1.1272e-04],\n",
      "        [1.5337e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.030044555664062: \n",
      "target probs tensor([[8.2921e-04],\n",
      "        [4.4782e-04],\n",
      "        [2.7405e-04],\n",
      "        [1.2199e-04],\n",
      "        [1.6198e-03],\n",
      "        [7.7844e-03],\n",
      "        [3.7429e-04],\n",
      "        [2.4884e-03],\n",
      "        [6.8429e-05],\n",
      "        [5.0358e-03],\n",
      "        [4.8445e-04],\n",
      "        [1.0666e-04],\n",
      "        [3.9096e-05],\n",
      "        [6.5139e-04],\n",
      "        [1.4118e-03],\n",
      "        [6.4817e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.083073616027832: \n",
      "target probs tensor([[4.6306e-03],\n",
      "        [1.0076e-05],\n",
      "        [7.2127e-04],\n",
      "        [1.5572e-04],\n",
      "        [7.5082e-05],\n",
      "        [6.2892e-03],\n",
      "        [1.2412e-03],\n",
      "        [4.2230e-04],\n",
      "        [2.9332e-04],\n",
      "        [1.1956e-05],\n",
      "        [4.2888e-04],\n",
      "        [6.3432e-05],\n",
      "        [1.0864e-05],\n",
      "        [1.0905e-03],\n",
      "        [3.0685e-05],\n",
      "        [5.3104e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.43635368347168: \n",
      "target probs tensor([[1.5974e-04],\n",
      "        [2.7366e-05],\n",
      "        [7.3939e-04],\n",
      "        [4.2453e-04],\n",
      "        [3.4095e-04],\n",
      "        [1.4001e-02],\n",
      "        [3.7192e-04],\n",
      "        [5.1443e-01],\n",
      "        [1.3569e-04],\n",
      "        [1.4888e-04],\n",
      "        [2.9864e-04],\n",
      "        [3.8341e-03],\n",
      "        [2.2005e-04],\n",
      "        [8.2934e-04],\n",
      "        [6.3196e-04],\n",
      "        [1.4446e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.384974479675293: \n",
      "target probs tensor([[3.2397e-04],\n",
      "        [4.7269e-04],\n",
      "        [7.1711e-04],\n",
      "        [3.2267e-03],\n",
      "        [1.0964e-03],\n",
      "        [1.3649e-03],\n",
      "        [8.4215e-04],\n",
      "        [1.1756e-03],\n",
      "        [2.2011e-05],\n",
      "        [1.9018e-03],\n",
      "        [7.2412e-05],\n",
      "        [5.5289e-04],\n",
      "        [3.9058e-04],\n",
      "        [2.8260e-05],\n",
      "        [7.6634e-04],\n",
      "        [7.9692e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.659963607788086: \n",
      "target probs tensor([[1.9335e-04],\n",
      "        [1.8804e-04],\n",
      "        [5.2245e-04],\n",
      "        [6.8854e-04],\n",
      "        [7.5238e-04],\n",
      "        [7.9875e-05],\n",
      "        [2.1574e-04],\n",
      "        [4.8120e-04],\n",
      "        [2.2556e-04],\n",
      "        [5.6184e-04],\n",
      "        [4.9494e-03],\n",
      "        [6.4187e-04],\n",
      "        [6.3884e-03],\n",
      "        [3.2860e-04],\n",
      "        [9.3420e-04],\n",
      "        [2.7700e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.591116905212402: \n",
      "target probs tensor([[1.7228e-02],\n",
      "        [4.7352e-05],\n",
      "        [7.4095e-02],\n",
      "        [1.6147e-03],\n",
      "        [3.1416e-04],\n",
      "        [2.6343e-04],\n",
      "        [2.4403e-06],\n",
      "        [9.6563e-04],\n",
      "        [9.4945e-04],\n",
      "        [4.7383e-06],\n",
      "        [3.8083e-02],\n",
      "        [5.3759e-05],\n",
      "        [6.8988e-05],\n",
      "        [4.3732e-04],\n",
      "        [8.1459e-03],\n",
      "        [6.8127e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.560011863708496: \n",
      "target probs tensor([[2.5293e-03],\n",
      "        [9.9569e-04],\n",
      "        [4.2994e-03],\n",
      "        [4.7659e-02],\n",
      "        [2.5024e-04],\n",
      "        [1.1658e-02],\n",
      "        [2.9095e-04],\n",
      "        [6.2710e-06],\n",
      "        [3.5727e-04],\n",
      "        [1.6170e-02],\n",
      "        [1.8366e-03],\n",
      "        [1.8090e-04],\n",
      "        [1.5954e-04],\n",
      "        [1.2074e-03],\n",
      "        [9.1144e-05],\n",
      "        [3.9395e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.114713668823242: \n",
      "target probs tensor([[2.1589e-03],\n",
      "        [2.5104e-06],\n",
      "        [8.9224e-04],\n",
      "        [8.2650e-04],\n",
      "        [1.1463e-04],\n",
      "        [1.9701e-03],\n",
      "        [2.4116e-04],\n",
      "        [1.8222e-04],\n",
      "        [1.3103e-04],\n",
      "        [1.7649e-04],\n",
      "        [1.7319e-04],\n",
      "        [6.9880e-05],\n",
      "        [2.8402e-06],\n",
      "        [1.3758e-04],\n",
      "        [6.1779e-04],\n",
      "        [2.1311e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.381877899169922: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[4.1569e-04],\n",
      "        [5.0971e-03],\n",
      "        [1.2484e-03],\n",
      "        [2.1596e-03],\n",
      "        [1.0446e-03],\n",
      "        [3.6532e-05],\n",
      "        [1.4824e-03],\n",
      "        [1.4264e-03],\n",
      "        [1.8415e-03],\n",
      "        [1.1068e-03],\n",
      "        [1.0952e-05],\n",
      "        [6.7628e-03],\n",
      "        [9.5989e-04],\n",
      "        [7.5031e-04],\n",
      "        [2.4112e-04],\n",
      "        [1.2193e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.45918083190918: \n",
      "target probs tensor([[0.0012],\n",
      "        [0.0001],\n",
      "        [0.0003],\n",
      "        [0.0010],\n",
      "        [0.0011],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0004],\n",
      "        [0.0003],\n",
      "        [0.0003],\n",
      "        [0.0003],\n",
      "        [0.0148],\n",
      "        [0.0004],\n",
      "        [0.0004],\n",
      "        [0.0002],\n",
      "        [0.0003]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.6758012771606445: \n",
      "target probs tensor([[3.4746e-04],\n",
      "        [9.8974e-01],\n",
      "        [5.0231e-03],\n",
      "        [5.5570e-04],\n",
      "        [2.3700e-04],\n",
      "        [1.0665e-03],\n",
      "        [1.3320e-03],\n",
      "        [1.8333e-03]], device='cuda:0'), loss: 6.109705924987793: \n",
      "target probs tensor([[5.6662e-03],\n",
      "        [7.1833e-04],\n",
      "        [3.6710e-05],\n",
      "        [2.0910e-01],\n",
      "        [3.1038e-02],\n",
      "        [1.5129e-04],\n",
      "        [4.0131e-03],\n",
      "        [8.7540e-03],\n",
      "        [1.4694e-03],\n",
      "        [4.3038e-04],\n",
      "        [5.9179e-04],\n",
      "        [5.4773e-05],\n",
      "        [5.0446e-04],\n",
      "        [2.8941e-04],\n",
      "        [6.8230e-04],\n",
      "        [9.7740e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.618168354034424: \n",
      "target probs tensor([[3.1771e-04],\n",
      "        [2.6703e-04],\n",
      "        [5.5377e-04],\n",
      "        [3.5796e-05],\n",
      "        [3.8923e-04],\n",
      "        [1.7719e-03],\n",
      "        [4.0712e-03],\n",
      "        [1.5993e-03],\n",
      "        [2.1087e-05],\n",
      "        [4.3990e-04],\n",
      "        [1.1268e-04],\n",
      "        [3.6483e-03],\n",
      "        [6.7106e-04],\n",
      "        [7.5947e-04],\n",
      "        [3.1000e-05],\n",
      "        [1.1843e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.097692489624023: \n",
      "target probs tensor([[2.7171e-05],\n",
      "        [1.8172e-04],\n",
      "        [6.4149e-05],\n",
      "        [2.6893e-04],\n",
      "        [3.4560e-05],\n",
      "        [5.5176e-05],\n",
      "        [4.7797e-03],\n",
      "        [1.8582e-04],\n",
      "        [7.1400e-04],\n",
      "        [1.3119e-03],\n",
      "        [5.3362e-04],\n",
      "        [2.2551e-04],\n",
      "        [5.0083e-04],\n",
      "        [1.6465e-03],\n",
      "        [2.4017e-03],\n",
      "        [1.9496e-04]], device='cuda:0'), loss: 8.08813190460205: \n",
      "target probs tensor([[6.2521e-03],\n",
      "        [7.7359e-05],\n",
      "        [2.5053e-04],\n",
      "        [2.2505e-03],\n",
      "        [5.0606e-04],\n",
      "        [8.1019e-04],\n",
      "        [1.5202e-04],\n",
      "        [4.1257e-04],\n",
      "        [5.3407e-05],\n",
      "        [5.0017e-02],\n",
      "        [5.5798e-04],\n",
      "        [1.3711e-02],\n",
      "        [3.1502e-03],\n",
      "        [8.5698e-06],\n",
      "        [7.5272e-01],\n",
      "        [5.9283e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.161448955535889: \n",
      "target probs tensor([[0.0002],\n",
      "        [0.0743],\n",
      "        [0.0007],\n",
      "        [0.0003],\n",
      "        [0.0001],\n",
      "        [0.0006],\n",
      "        [0.0104],\n",
      "        [0.0011],\n",
      "        [0.0003],\n",
      "        [0.0001],\n",
      "        [0.0751],\n",
      "        [0.0042],\n",
      "        [0.0021],\n",
      "        [0.0001],\n",
      "        [0.0007],\n",
      "        [0.1101]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.476531028747559: \n",
      "target probs tensor([[2.9312e-03],\n",
      "        [3.7755e-04],\n",
      "        [3.8598e-03],\n",
      "        [8.8234e-05],\n",
      "        [2.4903e-04],\n",
      "        [2.4901e-03],\n",
      "        [8.1138e-04],\n",
      "        [1.0850e-03],\n",
      "        [2.0182e-06],\n",
      "        [2.3855e-01],\n",
      "        [2.0769e-02],\n",
      "        [4.0757e-03],\n",
      "        [5.9449e-04],\n",
      "        [1.2354e-03],\n",
      "        [6.2250e-05],\n",
      "        [3.1515e-01]], device='cuda:0'), loss: 6.608115196228027: \n",
      "target probs tensor([[2.0930e-04],\n",
      "        [3.4558e-03],\n",
      "        [4.4071e-01],\n",
      "        [2.1380e-01],\n",
      "        [7.0997e-04],\n",
      "        [2.0540e-03],\n",
      "        [2.2427e-04],\n",
      "        [5.2183e-03],\n",
      "        [2.9217e-02],\n",
      "        [2.2691e-04],\n",
      "        [1.1037e-05],\n",
      "        [2.7590e-03],\n",
      "        [1.0187e-04],\n",
      "        [7.2411e-04],\n",
      "        [1.5399e-03],\n",
      "        [7.6714e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.575183391571045: \n",
      "target probs tensor([[2.0389e-02],\n",
      "        [1.7177e-03],\n",
      "        [6.2413e-05],\n",
      "        [3.1607e-04],\n",
      "        [7.8250e-04],\n",
      "        [1.8811e-04],\n",
      "        [3.3774e-04],\n",
      "        [1.5849e-04],\n",
      "        [1.9740e-04],\n",
      "        [4.2273e-04],\n",
      "        [1.7446e-03],\n",
      "        [1.5123e-01],\n",
      "        [4.2234e-04],\n",
      "        [1.2387e-02],\n",
      "        [1.1859e-03],\n",
      "        [4.4607e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.832820892333984: \n",
      "target probs tensor([[2.5750e-03],\n",
      "        [1.4687e-03],\n",
      "        [2.0537e-05],\n",
      "        [1.2258e-03],\n",
      "        [5.8527e-04],\n",
      "        [3.6336e-06],\n",
      "        [1.4319e-01],\n",
      "        [6.7880e-05],\n",
      "        [6.3026e-04],\n",
      "        [1.1191e-04],\n",
      "        [6.3538e-05],\n",
      "        [8.3352e-04],\n",
      "        [1.7602e-04],\n",
      "        [2.0940e-05],\n",
      "        [4.9751e-03],\n",
      "        [1.4758e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.8721442222595215: \n",
      "target probs tensor([[5.8589e-05],\n",
      "        [3.7893e-03],\n",
      "        [5.1104e-04],\n",
      "        [2.1061e-06],\n",
      "        [2.4233e-03],\n",
      "        [1.5986e-03],\n",
      "        [7.8227e-04],\n",
      "        [8.2931e-01],\n",
      "        [4.2476e-03],\n",
      "        [2.9108e-03],\n",
      "        [2.8133e-04],\n",
      "        [4.5297e-02],\n",
      "        [4.1062e-04],\n",
      "        [3.2010e-04],\n",
      "        [1.3182e-03],\n",
      "        [4.2609e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.786267280578613: \n",
      "target probs tensor([[4.2830e-03],\n",
      "        [4.5022e-03],\n",
      "        [5.1846e-02],\n",
      "        [2.4932e-03],\n",
      "        [6.0459e-04],\n",
      "        [2.5982e-01],\n",
      "        [4.9925e-03],\n",
      "        [4.1947e-03],\n",
      "        [8.0715e-06],\n",
      "        [3.9908e-03],\n",
      "        [4.2451e-05],\n",
      "        [3.0814e-04],\n",
      "        [4.3160e-04],\n",
      "        [7.1899e-03],\n",
      "        [3.7165e-04],\n",
      "        [2.8901e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.323285102844238: \n",
      "target probs tensor([[1.3591e-04],\n",
      "        [1.9829e-03],\n",
      "        [8.9895e-06],\n",
      "        [2.1891e-04],\n",
      "        [3.0142e-04],\n",
      "        [3.6540e-04],\n",
      "        [1.4315e-04],\n",
      "        [1.9008e-04],\n",
      "        [4.2679e-01],\n",
      "        [1.0495e-04],\n",
      "        [1.0409e-03],\n",
      "        [8.1194e-04],\n",
      "        [8.0274e-04],\n",
      "        [6.0346e-04],\n",
      "        [9.4910e-05],\n",
      "        [4.2108e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.761691093444824: \n",
      "target probs tensor([[8.6103e-03],\n",
      "        [1.4730e-04],\n",
      "        [5.8248e-10],\n",
      "        [7.2107e-04],\n",
      "        [3.2304e-04],\n",
      "        [2.1185e-04],\n",
      "        [2.4449e-04],\n",
      "        [8.2895e-05],\n",
      "        [1.3678e-03],\n",
      "        [1.1801e-03],\n",
      "        [1.9604e-04],\n",
      "        [2.1398e-03],\n",
      "        [1.5224e-05],\n",
      "        [1.6048e-03],\n",
      "        [5.3483e-06],\n",
      "        [1.7485e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.904258728027344: \n",
      "target probs tensor([[1.5086e-04],\n",
      "        [3.2118e-02],\n",
      "        [4.0767e-04],\n",
      "        [1.4902e-03],\n",
      "        [1.0449e-02],\n",
      "        [2.5360e-01],\n",
      "        [4.1554e-04],\n",
      "        [1.4984e-03],\n",
      "        [3.7998e-04],\n",
      "        [1.6467e-03],\n",
      "        [8.0400e-05],\n",
      "        [2.4049e-03],\n",
      "        [4.3026e-03],\n",
      "        [3.2016e-04],\n",
      "        [6.2285e-07],\n",
      "        [1.9942e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.907412528991699: \n",
      "target probs tensor([[4.9726e-05],\n",
      "        [3.7289e-04],\n",
      "        [5.0519e-04],\n",
      "        [3.0223e-04],\n",
      "        [2.8381e-03],\n",
      "        [3.7529e-07],\n",
      "        [2.5778e-06],\n",
      "        [1.6388e-04],\n",
      "        [3.7482e-03],\n",
      "        [2.2390e-02],\n",
      "        [1.4863e-04],\n",
      "        [1.4589e-04],\n",
      "        [1.0393e-04],\n",
      "        [1.4827e-03],\n",
      "        [9.0750e-01],\n",
      "        [4.4405e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.74843692779541: \n",
      "target probs tensor([[2.7571e-05],\n",
      "        [1.2548e-03],\n",
      "        [4.1120e-04],\n",
      "        [1.9452e-04],\n",
      "        [6.0000e-02],\n",
      "        [9.4374e-02],\n",
      "        [4.3920e-04],\n",
      "        [7.1791e-04],\n",
      "        [3.2606e-04],\n",
      "        [1.1394e-01],\n",
      "        [5.8897e-04],\n",
      "        [2.2613e-03],\n",
      "        [8.0296e-04],\n",
      "        [6.8053e-06],\n",
      "        [4.1059e-04],\n",
      "        [5.7204e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.980198860168457: \n",
      "target probs tensor([[7.4937e-04],\n",
      "        [2.3442e-02],\n",
      "        [5.3157e-04],\n",
      "        [4.1447e-04],\n",
      "        [4.0467e-04],\n",
      "        [1.6435e-04],\n",
      "        [7.4666e-04],\n",
      "        [1.0811e-03],\n",
      "        [3.1030e-04],\n",
      "        [4.1640e-04],\n",
      "        [2.0871e-04],\n",
      "        [6.8549e-02],\n",
      "        [1.0322e-03],\n",
      "        [5.6728e-05],\n",
      "        [3.0175e-04],\n",
      "        [5.6648e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.255324363708496: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[0.0226],\n",
      "        [0.0002],\n",
      "        [0.0014],\n",
      "        [0.0001],\n",
      "        [0.0006],\n",
      "        [0.0020],\n",
      "        [0.0003],\n",
      "        [0.0011],\n",
      "        [0.0036],\n",
      "        [0.0011],\n",
      "        [0.0002],\n",
      "        [0.0027],\n",
      "        [0.0023],\n",
      "        [0.0006],\n",
      "        [0.0046],\n",
      "        [0.0003]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.88395881652832: \n",
      "target probs tensor([[2.1217e-03],\n",
      "        [4.4777e-04],\n",
      "        [1.2917e-04],\n",
      "        [9.2063e-04],\n",
      "        [1.1260e-03],\n",
      "        [7.2014e-04],\n",
      "        [5.1768e-04],\n",
      "        [4.1124e-04],\n",
      "        [9.0985e-01],\n",
      "        [1.2210e-04],\n",
      "        [3.9167e-01],\n",
      "        [3.0724e-01],\n",
      "        [1.2971e-03],\n",
      "        [1.7944e-03],\n",
      "        [2.1361e-04],\n",
      "        [4.6847e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.219372749328613: \n"
     ]
    }
   ],
   "source": [
    "# RUN SITE\n",
    "if 'x' in env.save_filename and mode != 'sanity_check':\n",
    "  raise ValueError('save_filename contains x')\n",
    "\n",
    "\n",
    "saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=env.save_filename + \"-best\")\n",
    "saver_every_epoch = SaveModelCallback(learn, every='epoch', name=env.save_filename)\n",
    "# fooling_weight_scheduler = FoolingWeightScheduler(learn)\n",
    "# lr_anneal = LRAnneal(learn, 1e-4)\n",
    "# file_ctrl = FileControl(learn, '/root/Derakhshani/adversarial/ctrl', learn.model)\n",
    "# cyclical_sched = CyclicalLRScheduler(learn, 3e-2, 6e-4, 4)\n",
    "\n",
    "callbacks = [saver_best, saver_every_epoch]\n",
    "# callbacks.append(lr_anneal)\n",
    "# callbacks.append(fooling_weight_scheduler)\n",
    "# callbacks.append(file_ctrl)\n",
    "# callbacks.append(cyclical_sched)\n",
    "\n",
    "learn.fit(100, lr=1e-2, wd = 0., callbacks=callbacks)\n",
    "\n",
    "# with Hooks(gen, append_stats_normal) as hooks:\n",
    "#   learn.fit(1, lr=5e-03, wd = 0., callbacks=[saver_best, saver_every_epoch])\n",
    "\n",
    "# for i in range(10):\n",
    "#   learn.fit_one_cycle(7, wd = 0.,max_lr=1., div_factor = 1000.) \n",
    "\n",
    "shutil.copyfile(env.temp_csv_path + '/' + env.save_filename + \".csv\", env.get_csv_path() + '.csv')\n",
    "shutil.copytree(env.data_path/env.get_learner_models_dir(), env.get_models_path())\n",
    "shutil.rmtree(env.data_path/env.get_learner_models_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanup import cleanup_models_folder\n",
    "cleanup_models_folder('/root/Derakhshani/adversarial/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x1440 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram\n",
    "fig, axes = plt.subplots(len(hooks),1, figsize=(30,12))\n",
    "for ax,h in zip(axes.flatten(), hooks):\n",
    "  ax.imshow(get_hist(h), origin='lower')\n",
    "  ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mean and std\n",
    "fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "for h in hooks:\n",
    "  ms, ss, _ = h.stats\n",
    "  ax0.plot(ms[:100])\n",
    "  ax1.plot(ss[:100])\n",
    "plt.legend(range(len(hooks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "for h in hooks:\n",
    "  ms, ss, _ = h.stats\n",
    "  ax0.plot(ms)\n",
    "  ax1.plot(ss)\n",
    "plt.legend(range(len(hooks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero precentage:\n",
    "fig,axes = plt.subplots(len(hooks),1, figsize=(30,30))\n",
    "for ax,h in zip(axes.flatten(), hooks):\n",
    "    ax.plot(get_min(h))\n",
    "    ax.set_ylim(0,1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MO2fZ-hSSUzJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# z1 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "# z2 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "z1 = torch.tensor([0.8, -0.5] * 5).cuda()\n",
    "z2 = torch.tensor([-1.] * 10).cuda()\n",
    "print(\"z1: \", z1)\n",
    "print(\"z2: \", z2)\n",
    "print(\"distance: \", torch.norm(z1-z2,p=2))\n",
    "model = learn.model.eval()\n",
    "\n",
    "z_s = interpolate(z1, z2, 0.1)\n",
    "print(len(z_s))\n",
    "\n",
    "for i,z in enumerate(z_s):\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n",
    "  #img.save('./pics/' + str(i) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea : have 200 noises (1 for each class), then start iterating the dataset, and for each image, randomly apply one noise and record the result\n",
    "def targeted_diversity(learn, n_perturbations = 200, percentage = 95):\n",
    "  model = learn.model.eval()\n",
    "\n",
    "  one_hot_conditions = [torch.empty(z_dim).uniform_(0,1).cuda().detach() for _ in range(n_perturbations)]\n",
    "#   for i in range(z_dim):\n",
    "#     one_hot_conditions[i][i] = 1.\n",
    "\n",
    "  perturbations = [model.forward_single_z(z) for z in one_hot_conditions]\n",
    "\n",
    "  hist = [0.] * 1000\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 : print(\"at batch_no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbations[np.random.randint(0,len(perturbations))][None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      hist[pred] += 1\n",
    "\n",
    "  pred_histogram_sum = np.sum(hist)\n",
    "  indexed_pred_histogram = [(i, hist_element) for i,hist_element in  \n",
    "                            enumerate(hist)]\n",
    "\n",
    "  indexed_pred_histogram.sort(key=lambda x: x[1], reverse = True)\n",
    "\n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = indexed_pred_histogram[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / pred_histogram_sum) * 100.\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, indexed_pred_histogram\n",
    "\n",
    "def targeted_diversity_average(learn, n_perturbations = 200, percentage = 95, average_over = 4):\n",
    "  results = []\n",
    "  for i in range(average_over):\n",
    "    n, _ = targeted_diversity(learn, n_perturbations, percentage)\n",
    "    print(f'done with the {i}th calculation: {n}')\n",
    "    results.append(n)\n",
    "  return np.mean(results)\n",
    "\n",
    "def diversity_average(learn, n_perturbations = 10, percentage = 95, average_over = 4):\n",
    "  results = []\n",
    "  for i in range(average_over):\n",
    "    n, _ = diversity(learn, n_perturbations, percentage, verbose = False)\n",
    "    print(f'done with the {i}th calculation: {n}')\n",
    "    results.append(n)\n",
    "  return np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 5\n",
      "at batch no 10\n",
      "at batch no 15\n",
      "at batch no 20\n",
      "at batch no 25\n",
      "at batch no 30\n",
      "at batch no 35\n",
      "at batch no 40\n",
      "at batch no 45\n",
      "at batch no 50\n",
      "at batch no 55\n",
      "at batch no 60\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(566,\n",
       " [(794, 50.099998474121094),\n",
       "  (599, 21.100000381469727),\n",
       "  (668, 20.200000762939453),\n",
       "  (904, 15.0),\n",
       "  (973, 13.600000381469727),\n",
       "  (490, 12.899999618530273),\n",
       "  (39, 12.699999809265137),\n",
       "  (770, 12.600000381469727),\n",
       "  (741, 11.300000190734863),\n",
       "  (828, 11.100000381469727),\n",
       "  (109, 9.199999809265137),\n",
       "  (556, 8.600000381469727),\n",
       "  (489, 8.399999618530273),\n",
       "  (955, 8.399999618530273),\n",
       "  (887, 8.300000190734863),\n",
       "  (669, 8.100000381469727),\n",
       "  (84, 7.400000095367432),\n",
       "  (855, 7.0),\n",
       "  (538, 6.800000190734863),\n",
       "  (108, 6.599999904632568),\n",
       "  (124, 6.0),\n",
       "  (397, 5.900000095367432),\n",
       "  (48, 5.800000190734863),\n",
       "  (61, 5.5),\n",
       "  (777, 4.900000095367432),\n",
       "  (721, 4.800000190734863),\n",
       "  (401, 4.5),\n",
       "  (971, 4.400000095367432),\n",
       "  (893, 4.300000190734863),\n",
       "  (857, 4.099999904632568),\n",
       "  (591, 4.0),\n",
       "  (711, 3.9000000953674316),\n",
       "  (709, 3.799999952316284),\n",
       "  (455, 3.700000047683716),\n",
       "  (55, 3.5999999046325684),\n",
       "  (414, 3.5),\n",
       "  (906, 3.5),\n",
       "  (151, 3.4000000953674316),\n",
       "  (389, 3.4000000953674316),\n",
       "  (406, 3.4000000953674316),\n",
       "  (865, 3.299999952316284),\n",
       "  (750, 3.200000047683716),\n",
       "  (581, 3.0),\n",
       "  (0, 2.9000000953674316),\n",
       "  (1, 2.9000000953674316),\n",
       "  (476, 2.9000000953674316),\n",
       "  (915, 2.9000000953674316),\n",
       "  (363, 2.799999952316284),\n",
       "  (837, 2.799999952316284),\n",
       "  (982, 2.799999952316284),\n",
       "  (110, 2.700000047683716),\n",
       "  (46, 2.5999999046325684),\n",
       "  (62, 2.5999999046325684),\n",
       "  (593, 2.5999999046325684),\n",
       "  (640, 2.5999999046325684),\n",
       "  (735, 2.5999999046325684),\n",
       "  (819, 2.5999999046325684),\n",
       "  (94, 2.5),\n",
       "  (126, 2.5),\n",
       "  (431, 2.5),\n",
       "  (611, 2.4000000953674316),\n",
       "  (864, 2.4000000953674316),\n",
       "  (868, 2.4000000953674316),\n",
       "  (222, 2.299999952316284),\n",
       "  (558, 2.299999952316284),\n",
       "  (572, 2.299999952316284),\n",
       "  (850, 2.299999952316284),\n",
       "  (115, 2.200000047683716),\n",
       "  (410, 2.200000047683716),\n",
       "  (698, 2.200000047683716),\n",
       "  (763, 2.200000047683716),\n",
       "  (772, 2.200000047683716),\n",
       "  (779, 2.200000047683716),\n",
       "  (97, 2.0999999046325684),\n",
       "  (238, 2.0999999046325684),\n",
       "  (440, 2.0999999046325684),\n",
       "  (447, 2.0999999046325684),\n",
       "  (464, 2.0999999046325684),\n",
       "  (872, 2.0999999046325684),\n",
       "  (898, 2.0999999046325684),\n",
       "  (68, 2.0),\n",
       "  (118, 2.0),\n",
       "  (182, 2.0),\n",
       "  (242, 2.0),\n",
       "  (292, 2.0),\n",
       "  (393, 2.0),\n",
       "  (520, 2.0),\n",
       "  (621, 2.0),\n",
       "  (60, 1.899999976158142),\n",
       "  (188, 1.899999976158142),\n",
       "  (189, 1.899999976158142),\n",
       "  (192, 1.899999976158142),\n",
       "  (342, 1.899999976158142),\n",
       "  (411, 1.899999976158142),\n",
       "  (472, 1.899999976158142),\n",
       "  (570, 1.899999976158142),\n",
       "  (619, 1.899999976158142),\n",
       "  (620, 1.899999976158142),\n",
       "  (724, 1.899999976158142),\n",
       "  (791, 1.899999976158142),\n",
       "  (800, 1.899999976158142),\n",
       "  (199, 1.7999999523162842),\n",
       "  (290, 1.7999999523162842),\n",
       "  (348, 1.7999999523162842),\n",
       "  (423, 1.7999999523162842),\n",
       "  (761, 1.7999999523162842),\n",
       "  (762, 1.7999999523162842),\n",
       "  (870, 1.7999999523162842),\n",
       "  (920, 1.7999999523162842),\n",
       "  (72, 1.7000000476837158),\n",
       "  (128, 1.7000000476837158),\n",
       "  (155, 1.7000000476837158),\n",
       "  (195, 1.7000000476837158),\n",
       "  (334, 1.7000000476837158),\n",
       "  (526, 1.7000000476837158),\n",
       "  (547, 1.7000000476837158),\n",
       "  (671, 1.7000000476837158),\n",
       "  (725, 1.7000000476837158),\n",
       "  (775, 1.7000000476837158),\n",
       "  (783, 1.7000000476837158),\n",
       "  (824, 1.7000000476837158),\n",
       "  (871, 1.7000000476837158),\n",
       "  (123, 1.600000023841858),\n",
       "  (375, 1.600000023841858),\n",
       "  (457, 1.600000023841858),\n",
       "  (468, 1.600000023841858),\n",
       "  (492, 1.600000023841858),\n",
       "  (508, 1.600000023841858),\n",
       "  (550, 1.600000023841858),\n",
       "  (562, 1.600000023841858),\n",
       "  (803, 1.600000023841858),\n",
       "  (817, 1.600000023841858),\n",
       "  (953, 1.600000023841858),\n",
       "  (963, 1.600000023841858),\n",
       "  (107, 1.5),\n",
       "  (119, 1.5),\n",
       "  (202, 1.5),\n",
       "  (230, 1.5),\n",
       "  (420, 1.5),\n",
       "  (477, 1.5),\n",
       "  (564, 1.5),\n",
       "  (748, 1.5),\n",
       "  (815, 1.5),\n",
       "  (907, 1.5),\n",
       "  (76, 1.399999976158142),\n",
       "  (83, 1.399999976158142),\n",
       "  (193, 1.399999976158142),\n",
       "  (231, 1.399999976158142),\n",
       "  (274, 1.399999976158142),\n",
       "  (293, 1.399999976158142),\n",
       "  (305, 1.399999976158142),\n",
       "  (314, 1.399999976158142),\n",
       "  (336, 1.399999976158142),\n",
       "  (552, 1.399999976158142),\n",
       "  (565, 1.399999976158142),\n",
       "  (579, 1.399999976158142),\n",
       "  (597, 1.399999976158142),\n",
       "  (624, 1.399999976158142),\n",
       "  (679, 1.399999976158142),\n",
       "  (784, 1.399999976158142),\n",
       "  (786, 1.399999976158142),\n",
       "  (801, 1.399999976158142),\n",
       "  (891, 1.399999976158142),\n",
       "  (902, 1.399999976158142),\n",
       "  (33, 1.2999999523162842),\n",
       "  (57, 1.2999999523162842),\n",
       "  (96, 1.2999999523162842),\n",
       "  (120, 1.2999999523162842),\n",
       "  (247, 1.2999999523162842),\n",
       "  (275, 1.2999999523162842),\n",
       "  (328, 1.2999999523162842),\n",
       "  (355, 1.2999999523162842),\n",
       "  (409, 1.2999999523162842),\n",
       "  (441, 1.2999999523162842),\n",
       "  (505, 1.2999999523162842),\n",
       "  (586, 1.2999999523162842),\n",
       "  (588, 1.2999999523162842),\n",
       "  (633, 1.2999999523162842),\n",
       "  (638, 1.2999999523162842),\n",
       "  (821, 1.2999999523162842),\n",
       "  (842, 1.2999999523162842),\n",
       "  (892, 1.2999999523162842),\n",
       "  (58, 1.2000000476837158),\n",
       "  (65, 1.2000000476837158),\n",
       "  (171, 1.2000000476837158),\n",
       "  (204, 1.2000000476837158),\n",
       "  (205, 1.2000000476837158),\n",
       "  (219, 1.2000000476837158),\n",
       "  (307, 1.2000000476837158),\n",
       "  (308, 1.2000000476837158),\n",
       "  (327, 1.2000000476837158),\n",
       "  (331, 1.2000000476837158),\n",
       "  (353, 1.2000000476837158),\n",
       "  (366, 1.2000000476837158),\n",
       "  (443, 1.2000000476837158),\n",
       "  (454, 1.2000000476837158),\n",
       "  (474, 1.2000000476837158),\n",
       "  (495, 1.2000000476837158),\n",
       "  (563, 1.2000000476837158),\n",
       "  (574, 1.2000000476837158),\n",
       "  (602, 1.2000000476837158),\n",
       "  (641, 1.2000000476837158),\n",
       "  (654, 1.2000000476837158),\n",
       "  (658, 1.2000000476837158),\n",
       "  (781, 1.2000000476837158),\n",
       "  (823, 1.2000000476837158),\n",
       "  (848, 1.2000000476837158),\n",
       "  (854, 1.2000000476837158),\n",
       "  (858, 1.2000000476837158),\n",
       "  (883, 1.2000000476837158),\n",
       "  (24, 1.100000023841858),\n",
       "  (41, 1.100000023841858),\n",
       "  (51, 1.100000023841858),\n",
       "  (113, 1.100000023841858),\n",
       "  (116, 1.100000023841858),\n",
       "  (236, 1.100000023841858),\n",
       "  (249, 1.100000023841858),\n",
       "  (253, 1.100000023841858),\n",
       "  (271, 1.100000023841858),\n",
       "  (281, 1.100000023841858),\n",
       "  (300, 1.100000023841858),\n",
       "  (310, 1.100000023841858),\n",
       "  (317, 1.100000023841858),\n",
       "  (318, 1.100000023841858),\n",
       "  (319, 1.100000023841858),\n",
       "  (350, 1.100000023841858),\n",
       "  (381, 1.100000023841858),\n",
       "  (395, 1.100000023841858),\n",
       "  (396, 1.100000023841858),\n",
       "  (491, 1.100000023841858),\n",
       "  (496, 1.100000023841858),\n",
       "  (497, 1.100000023841858),\n",
       "  (506, 1.100000023841858),\n",
       "  (507, 1.100000023841858),\n",
       "  (527, 1.100000023841858),\n",
       "  (544, 1.100000023841858),\n",
       "  (575, 1.100000023841858),\n",
       "  (609, 1.100000023841858),\n",
       "  (626, 1.100000023841858),\n",
       "  (759, 1.100000023841858),\n",
       "  (787, 1.100000023841858),\n",
       "  (796, 1.100000023841858),\n",
       "  (806, 1.100000023841858),\n",
       "  (820, 1.100000023841858),\n",
       "  (834, 1.100000023841858),\n",
       "  (863, 1.100000023841858),\n",
       "  (882, 1.100000023841858),\n",
       "  (894, 1.100000023841858),\n",
       "  (918, 1.100000023841858),\n",
       "  (981, 1.100000023841858),\n",
       "  (996, 1.100000023841858),\n",
       "  (7, 1.0),\n",
       "  (8, 1.0),\n",
       "  (10, 1.0),\n",
       "  (15, 1.0),\n",
       "  (17, 1.0),\n",
       "  (19, 1.0),\n",
       "  (25, 1.0),\n",
       "  (28, 1.0),\n",
       "  (37, 1.0),\n",
       "  (42, 1.0),\n",
       "  (45, 1.0),\n",
       "  (49, 1.0),\n",
       "  (52, 1.0),\n",
       "  (53, 1.0),\n",
       "  (63, 1.0),\n",
       "  (70, 1.0),\n",
       "  (75, 1.0),\n",
       "  (79, 1.0),\n",
       "  (86, 1.0),\n",
       "  (87, 1.0),\n",
       "  (90, 1.0),\n",
       "  (91, 1.0),\n",
       "  (92, 1.0),\n",
       "  (98, 1.0),\n",
       "  (102, 1.0),\n",
       "  (105, 1.0),\n",
       "  (117, 1.0),\n",
       "  (134, 1.0),\n",
       "  (139, 1.0),\n",
       "  (140, 1.0),\n",
       "  (141, 1.0),\n",
       "  (144, 1.0),\n",
       "  (158, 1.0),\n",
       "  (161, 1.0),\n",
       "  (162, 1.0),\n",
       "  (163, 1.0),\n",
       "  (164, 1.0),\n",
       "  (173, 1.0),\n",
       "  (183, 1.0),\n",
       "  (186, 1.0),\n",
       "  (196, 1.0),\n",
       "  (197, 1.0),\n",
       "  (198, 1.0),\n",
       "  (206, 1.0),\n",
       "  (213, 1.0),\n",
       "  (218, 1.0),\n",
       "  (228, 1.0),\n",
       "  (235, 1.0),\n",
       "  (260, 1.0),\n",
       "  (273, 1.0),\n",
       "  (284, 1.0),\n",
       "  (289, 1.0),\n",
       "  (291, 1.0),\n",
       "  (301, 1.0),\n",
       "  (304, 1.0),\n",
       "  (306, 1.0),\n",
       "  (312, 1.0),\n",
       "  (313, 1.0),\n",
       "  (316, 1.0),\n",
       "  (321, 1.0),\n",
       "  (323, 1.0),\n",
       "  (337, 1.0),\n",
       "  (347, 1.0),\n",
       "  (360, 1.0),\n",
       "  (376, 1.0),\n",
       "  (378, 1.0),\n",
       "  (387, 1.0),\n",
       "  (392, 1.0),\n",
       "  (398, 1.0),\n",
       "  (417, 1.0),\n",
       "  (425, 1.0),\n",
       "  (428, 1.0),\n",
       "  (429, 1.0),\n",
       "  (433, 1.0),\n",
       "  (445, 1.0),\n",
       "  (451, 1.0),\n",
       "  (483, 1.0),\n",
       "  (488, 1.0),\n",
       "  (498, 1.0),\n",
       "  (518, 1.0),\n",
       "  (528, 1.0),\n",
       "  (530, 1.0),\n",
       "  (531, 1.0),\n",
       "  (533, 1.0),\n",
       "  (566, 1.0),\n",
       "  (580, 1.0),\n",
       "  (608, 1.0),\n",
       "  (612, 1.0),\n",
       "  (616, 1.0),\n",
       "  (625, 1.0),\n",
       "  (629, 1.0),\n",
       "  (637, 1.0),\n",
       "  (645, 1.0),\n",
       "  (646, 1.0),\n",
       "  (651, 1.0),\n",
       "  (655, 1.0),\n",
       "  (661, 1.0),\n",
       "  (684, 1.0),\n",
       "  (687, 1.0),\n",
       "  (691, 1.0),\n",
       "  (692, 1.0),\n",
       "  (694, 1.0),\n",
       "  (716, 1.0),\n",
       "  (719, 1.0),\n",
       "  (734, 1.0),\n",
       "  (738, 1.0),\n",
       "  (746, 1.0),\n",
       "  (753, 1.0),\n",
       "  (768, 1.0),\n",
       "  (793, 1.0),\n",
       "  (802, 1.0),\n",
       "  (816, 1.0),\n",
       "  (826, 1.0),\n",
       "  (830, 1.0),\n",
       "  (831, 1.0),\n",
       "  (847, 1.0),\n",
       "  (873, 1.0),\n",
       "  (884, 1.0),\n",
       "  (905, 1.0),\n",
       "  (923, 1.0),\n",
       "  (932, 1.0),\n",
       "  (934, 1.0),\n",
       "  (937, 1.0),\n",
       "  (939, 1.0),\n",
       "  (944, 1.0),\n",
       "  (946, 1.0),\n",
       "  (957, 1.0),\n",
       "  (959, 1.0),\n",
       "  (984, 1.0),\n",
       "  (985, 1.0),\n",
       "  (987, 1.0),\n",
       "  (989, 1.0),\n",
       "  (992, 1.0),\n",
       "  (9, 0.8999999761581421),\n",
       "  (18, 0.8999999761581421),\n",
       "  (21, 0.8999999761581421),\n",
       "  (36, 0.8999999761581421),\n",
       "  (47, 0.8999999761581421),\n",
       "  (85, 0.8999999761581421),\n",
       "  (135, 0.8999999761581421),\n",
       "  (142, 0.8999999761581421),\n",
       "  (145, 0.8999999761581421),\n",
       "  (176, 0.8999999761581421),\n",
       "  (187, 0.8999999761581421),\n",
       "  (263, 0.8999999761581421),\n",
       "  (266, 0.8999999761581421),\n",
       "  (303, 0.8999999761581421),\n",
       "  (315, 0.8999999761581421),\n",
       "  (326, 0.8999999761581421),\n",
       "  (365, 0.8999999761581421),\n",
       "  (384, 0.8999999761581421),\n",
       "  (390, 0.8999999761581421),\n",
       "  (391, 0.8999999761581421),\n",
       "  (408, 0.8999999761581421),\n",
       "  (459, 0.8999999761581421),\n",
       "  (463, 0.8999999761581421),\n",
       "  (482, 0.8999999761581421),\n",
       "  (503, 0.8999999761581421),\n",
       "  (534, 0.8999999761581421),\n",
       "  (535, 0.8999999761581421),\n",
       "  (555, 0.8999999761581421),\n",
       "  (577, 0.8999999761581421),\n",
       "  (635, 0.8999999761581421),\n",
       "  (663, 0.8999999761581421),\n",
       "  (674, 0.8999999761581421),\n",
       "  (702, 0.8999999761581421),\n",
       "  (703, 0.8999999761581421),\n",
       "  (712, 0.8999999761581421),\n",
       "  (743, 0.8999999761581421),\n",
       "  (757, 0.8999999761581421),\n",
       "  (764, 0.8999999761581421),\n",
       "  (776, 0.8999999761581421),\n",
       "  (788, 0.8999999761581421),\n",
       "  (808, 0.8999999761581421),\n",
       "  (832, 0.8999999761581421),\n",
       "  (833, 0.8999999761581421),\n",
       "  (900, 0.8999999761581421),\n",
       "  (968, 0.8999999761581421),\n",
       "  (988, 0.8999999761581421),\n",
       "  (997, 0.8999999761581421),\n",
       "  (38, 0.800000011920929),\n",
       "  (77, 0.800000011920929),\n",
       "  (93, 0.800000011920929),\n",
       "  (100, 0.800000011920929),\n",
       "  (160, 0.800000011920929),\n",
       "  (246, 0.800000011920929),\n",
       "  (254, 0.800000011920929),\n",
       "  (280, 0.800000011920929),\n",
       "  (294, 0.800000011920929),\n",
       "  (344, 0.800000011920929),\n",
       "  (372, 0.800000011920929),\n",
       "  (377, 0.800000011920929),\n",
       "  (399, 0.800000011920929),\n",
       "  (407, 0.800000011920929),\n",
       "  (415, 0.800000011920929),\n",
       "  (430, 0.800000011920929),\n",
       "  (432, 0.800000011920929),\n",
       "  (439, 0.800000011920929),\n",
       "  (458, 0.800000011920929),\n",
       "  (514, 0.800000011920929),\n",
       "  (545, 0.800000011920929),\n",
       "  (546, 0.800000011920929),\n",
       "  (595, 0.800000011920929),\n",
       "  (603, 0.800000011920929),\n",
       "  (643, 0.800000011920929),\n",
       "  (644, 0.800000011920929),\n",
       "  (672, 0.800000011920929),\n",
       "  (696, 0.800000011920929),\n",
       "  (729, 0.800000011920929),\n",
       "  (732, 0.800000011920929),\n",
       "  (809, 0.800000011920929),\n",
       "  (822, 0.800000011920929),\n",
       "  (829, 0.800000011920929),\n",
       "  (838, 0.800000011920929),\n",
       "  (843, 0.800000011920929),\n",
       "  (852, 0.800000011920929),\n",
       "  (885, 0.800000011920929),\n",
       "  (889, 0.800000011920929),\n",
       "  (901, 0.800000011920929),\n",
       "  (956, 0.800000011920929),\n",
       "  (34, 0.699999988079071),\n",
       "  (50, 0.699999988079071),\n",
       "  (99, 0.699999988079071),\n",
       "  (112, 0.699999988079071),\n",
       "  (168, 0.699999988079071),\n",
       "  (184, 0.699999988079071),\n",
       "  (214, 0.699999988079071),\n",
       "  (216, 0.699999988079071),\n",
       "  (217, 0.699999988079071),\n",
       "  (232, 0.699999988079071),\n",
       "  (270, 0.699999988079071),\n",
       "  (320, 0.699999988079071),\n",
       "  (330, 0.699999988079071),\n",
       "  (335, 0.699999988079071),\n",
       "  (345, 0.699999988079071),\n",
       "  (361, 0.699999988079071),\n",
       "  (388, 0.699999988079071),\n",
       "  (412, 0.699999988079071),\n",
       "  (512, 0.699999988079071),\n",
       "  (561, 0.699999988079071),\n",
       "  (576, 0.699999988079071),\n",
       "  (632, 0.699999988079071),\n",
       "  (695, 0.699999988079071),\n",
       "  (805, 0.699999988079071),\n",
       "  (879, 0.699999988079071),\n",
       "  (886, 0.699999988079071),\n",
       "  (952, 0.699999988079071),\n",
       "  (32, 0.6000000238418579),\n",
       "  (88, 0.6000000238418579),\n",
       "  (146, 0.6000000238418579),\n",
       "  (148, 0.6000000238418579),\n",
       "  (209, 0.6000000238418579),\n",
       "  (211, 0.6000000238418579),\n",
       "  (322, 0.6000000238418579),\n",
       "  (340, 0.6000000238418579),\n",
       "  (343, 0.6000000238418579),\n",
       "  (442, 0.6000000238418579),\n",
       "  (450, 0.6000000238418579),\n",
       "  (470, 0.6000000238418579),\n",
       "  (532, 0.6000000238418579),\n",
       "  (539, 0.6000000238418579),\n",
       "  (554, 0.6000000238418579),\n",
       "  (560, 0.6000000238418579),\n",
       "  (571, 0.6000000238418579),\n",
       "  (584, 0.6000000238418579),\n",
       "  (589, 0.6000000238418579),\n",
       "  (656, 0.6000000238418579),\n",
       "  (699, 0.6000000238418579),\n",
       "  (736, 0.6000000238418579),\n",
       "  (754, 0.6000000238418579),\n",
       "  (758, 0.6000000238418579),\n",
       "  (765, 0.6000000238418579),\n",
       "  (790, 0.6000000238418579),\n",
       "  (888, 0.6000000238418579),\n",
       "  (890, 0.6000000238418579),\n",
       "  (972, 0.6000000238418579),\n",
       "  (979, 0.6000000238418579),\n",
       "  (12, 0.5),\n",
       "  (16, 0.5),\n",
       "  (22, 0.5),\n",
       "  (143, 0.5),\n",
       "  (157, 0.5),\n",
       "  (166, 0.5),\n",
       "  (203, 0.5),\n",
       "  (212, 0.5),\n",
       "  (221, 0.5),\n",
       "  (224, 0.5),\n",
       "  (229, 0.5),\n",
       "  (234, 0.5),\n",
       "  (237, 0.5),\n",
       "  (252, 0.5),\n",
       "  (276, 0.5),\n",
       "  (282, 0.5),\n",
       "  (288, 0.5),\n",
       "  (295, 0.5),\n",
       "  (296, 0.5),\n",
       "  (309, 0.5),\n",
       "  (329, 0.5),\n",
       "  (402, 0.5),\n",
       "  (413, 0.5),\n",
       "  (436, 0.5),\n",
       "  (444, 0.5),\n",
       "  (448, 0.5),\n",
       "  (471, 0.5),\n",
       "  (540, 0.5),\n",
       "  (604, 0.5),\n",
       "  (639, 0.5),\n",
       "  (647, 0.5),\n",
       "  (683, 0.5),\n",
       "  (697, 0.5),\n",
       "  (715, 0.5),\n",
       "  (752, 0.5),\n",
       "  (755, 0.5),\n",
       "  (795, 0.5),\n",
       "  (811, 0.5),\n",
       "  (839, 0.5),\n",
       "  (853, 0.5),\n",
       "  (862, 0.5),\n",
       "  (877, 0.5),\n",
       "  (897, 0.5),\n",
       "  (910, 0.5),\n",
       "  (911, 0.5),\n",
       "  (927, 0.5),\n",
       "  (977, 0.5),\n",
       "  (20, 0.4000000059604645),\n",
       "  (29, 0.4000000059604645),\n",
       "  (69, 0.4000000059604645),\n",
       "  (122, 0.4000000059604645),\n",
       "  (125, 0.4000000059604645),\n",
       "  (130, 0.4000000059604645),\n",
       "  (132, 0.4000000059604645),\n",
       "  (178, 0.4000000059604645),\n",
       "  (180, 0.4000000059604645),\n",
       "  (256, 0.4000000059604645),\n",
       "  (264, 0.4000000059604645),\n",
       "  (298, 0.4000000059604645),\n",
       "  (370, 0.4000000059604645),\n",
       "  (419, 0.4000000059604645),\n",
       "  (424, 0.4000000059604645),\n",
       "  (480, 0.4000000059604645),\n",
       "  (484, 0.4000000059604645),\n",
       "  (509, 0.4000000059604645),\n",
       "  (511, 0.4000000059604645),\n",
       "  (537, 0.4000000059604645),\n",
       "  (582, 0.4000000059604645),\n",
       "  (587, 0.4000000059604645),\n",
       "  (590, 0.4000000059604645),\n",
       "  (606, 0.4000000059604645),\n",
       "  (615, 0.4000000059604645),\n",
       "  (652, 0.4000000059604645),\n",
       "  (670, 0.4000000059604645),\n",
       "  (689, 0.4000000059604645),\n",
       "  (700, 0.4000000059604645),\n",
       "  (707, 0.4000000059604645),\n",
       "  (720, 0.4000000059604645),\n",
       "  (727, 0.4000000059604645),\n",
       "  (745, 0.4000000059604645),\n",
       "  (804, 0.4000000059604645),\n",
       "  (844, 0.4000000059604645),\n",
       "  (866, 0.4000000059604645),\n",
       "  (878, 0.4000000059604645),\n",
       "  (881, 0.4000000059604645),\n",
       "  (896, 0.4000000059604645),\n",
       "  (925, 0.4000000059604645),\n",
       "  (2, 0.30000001192092896),\n",
       "  (5, 0.30000001192092896),\n",
       "  (74, 0.30000001192092896),\n",
       "  (78, 0.30000001192092896),\n",
       "  (81, 0.30000001192092896),\n",
       "  (89, 0.30000001192092896),\n",
       "  (101, 0.30000001192092896),\n",
       "  (136, 0.30000001192092896),\n",
       "  (138, 0.30000001192092896),\n",
       "  (169, 0.30000001192092896),\n",
       "  (190, 0.30000001192092896),\n",
       "  (207, 0.30000001192092896),\n",
       "  (208, 0.30000001192092896),\n",
       "  (215, 0.30000001192092896),\n",
       "  (243, 0.30000001192092896),\n",
       "  (250, 0.30000001192092896),\n",
       "  (285, 0.30000001192092896),\n",
       "  (286, 0.30000001192092896),\n",
       "  (302, 0.30000001192092896),\n",
       "  (332, 0.30000001192092896),\n",
       "  (341, 0.30000001192092896),\n",
       "  (357, 0.30000001192092896),\n",
       "  (364, 0.30000001192092896),\n",
       "  (369, 0.30000001192092896),\n",
       "  (386, 0.30000001192092896),\n",
       "  (403, 0.30000001192092896),\n",
       "  (438, 0.30000001192092896),\n",
       "  (456, 0.30000001192092896),\n",
       "  (461, 0.30000001192092896),\n",
       "  (487, 0.30000001192092896),\n",
       "  (523, 0.30000001192092896),\n",
       "  (541, 0.30000001192092896),\n",
       "  (542, 0.30000001192092896),\n",
       "  (607, 0.30000001192092896),\n",
       "  (613, 0.30000001192092896),\n",
       "  (636, 0.30000001192092896),\n",
       "  (650, 0.30000001192092896),\n",
       "  (704, 0.30000001192092896),\n",
       "  (740, 0.30000001192092896),\n",
       "  (818, 0.30000001192092896),\n",
       "  (867, 0.30000001192092896),\n",
       "  (6, 0.20000000298023224),\n",
       "  (11, 0.20000000298023224),\n",
       "  (14, 0.20000000298023224),\n",
       "  (23, 0.20000000298023224),\n",
       "  (35, 0.20000000298023224),\n",
       "  (40, 0.20000000298023224),\n",
       "  (44, 0.20000000298023224),\n",
       "  (56, 0.20000000298023224),\n",
       "  (71, 0.20000000298023224),\n",
       "  (114, 0.20000000298023224),\n",
       "  (159, 0.20000000298023224),\n",
       "  (201, 0.20000000298023224),\n",
       "  (223, 0.20000000298023224),\n",
       "  (241, 0.20000000298023224),\n",
       "  (248, 0.20000000298023224),\n",
       "  (261, 0.20000000298023224),\n",
       "  (265, 0.20000000298023224),\n",
       "  (269, 0.20000000298023224),\n",
       "  (272, 0.20000000298023224),\n",
       "  (352, 0.20000000298023224),\n",
       "  (394, 0.20000000298023224),\n",
       "  (453, 0.20000000298023224),\n",
       "  (486, 0.20000000298023224),\n",
       "  (502, 0.20000000298023224),\n",
       "  (513, 0.20000000298023224),\n",
       "  (516, 0.20000000298023224),\n",
       "  (524, 0.20000000298023224),\n",
       "  (529, 0.20000000298023224),\n",
       "  (567, 0.20000000298023224),\n",
       "  (592, 0.20000000298023224),\n",
       "  (601, 0.20000000298023224),\n",
       "  (614, 0.20000000298023224),\n",
       "  (634, 0.20000000298023224),\n",
       "  (642, 0.20000000298023224),\n",
       "  (649, 0.20000000298023224),\n",
       "  (662, 0.20000000298023224),\n",
       "  (664, 0.20000000298023224),\n",
       "  (667, 0.20000000298023224),\n",
       "  (678, 0.20000000298023224),\n",
       "  (680, 0.20000000298023224),\n",
       "  (688, 0.20000000298023224),\n",
       "  (706, 0.20000000298023224),\n",
       "  (751, 0.20000000298023224),\n",
       "  (766, 0.20000000298023224),\n",
       "  (773, 0.20000000298023224),\n",
       "  (799, 0.20000000298023224),\n",
       "  (827, 0.20000000298023224),\n",
       "  (859, 0.20000000298023224),\n",
       "  (962, 0.20000000298023224),\n",
       "  (976, 0.20000000298023224),\n",
       "  (978, 0.20000000298023224),\n",
       "  (994, 0.20000000298023224),\n",
       "  (4, 0.10000000149011612),\n",
       "  (31, 0.10000000149011612),\n",
       "  (67, 0.10000000149011612),\n",
       "  (95, 0.10000000149011612),\n",
       "  (121, 0.10000000149011612),\n",
       "  (150, 0.10000000149011612),\n",
       "  (170, 0.10000000149011612),\n",
       "  (181, 0.10000000149011612),\n",
       "  (226, 0.10000000149011612),\n",
       "  (227, 0.10000000149011612),\n",
       "  (245, 0.10000000149011612),\n",
       "  (257, 0.10000000149011612),\n",
       "  (259, 0.10000000149011612),\n",
       "  (267, 0.10000000149011612),\n",
       "  (279, 0.10000000149011612),\n",
       "  (297, 0.10000000149011612),\n",
       "  (311, 0.10000000149011612),\n",
       "  (324, 0.10000000149011612),\n",
       "  (354, 0.10000000149011612),\n",
       "  (358, 0.10000000149011612),\n",
       "  (359, 0.10000000149011612),\n",
       "  (362, 0.10000000149011612),\n",
       "  (379, 0.10000000149011612),\n",
       "  (380, 0.10000000149011612),\n",
       "  (382, 0.10000000149011612),\n",
       "  (383, 0.10000000149011612),\n",
       "  (385, 0.10000000149011612),\n",
       "  (400, 0.10000000149011612),\n",
       "  (422, 0.10000000149011612),\n",
       "  (434, 0.10000000149011612),\n",
       "  (452, 0.10000000149011612),\n",
       "  (501, 0.10000000149011612),\n",
       "  (517, 0.10000000149011612),\n",
       "  (568, 0.10000000149011612),\n",
       "  (578, 0.10000000149011612),\n",
       "  (585, 0.10000000149011612),\n",
       "  (605, 0.10000000149011612),\n",
       "  (618, 0.10000000149011612),\n",
       "  (628, 0.10000000149011612),\n",
       "  (657, 0.10000000149011612),\n",
       "  (665, 0.10000000149011612),\n",
       "  (676, 0.10000000149011612),\n",
       "  (685, 0.10000000149011612),\n",
       "  (690, 0.10000000149011612),\n",
       "  (701, 0.10000000149011612),\n",
       "  (710, 0.10000000149011612),\n",
       "  (749, 0.10000000149011612),\n",
       "  (771, 0.10000000149011612),\n",
       "  (774, 0.10000000149011612),\n",
       "  (778, 0.10000000149011612),\n",
       "  (782, 0.10000000149011612),\n",
       "  (797, 0.10000000149011612),\n",
       "  (814, 0.10000000149011612),\n",
       "  (825, 0.10000000149011612),\n",
       "  (849, 0.10000000149011612),\n",
       "  (869, 0.10000000149011612),\n",
       "  (874, 0.10000000149011612),\n",
       "  (875, 0.10000000149011612),\n",
       "  (903, 0.10000000149011612),\n",
       "  (919, 0.10000000149011612),\n",
       "  (921, 0.10000000149011612),\n",
       "  (933, 0.10000000149011612),\n",
       "  (938, 0.10000000149011612),\n",
       "  (943, 0.10000000149011612),\n",
       "  (949, 0.10000000149011612),\n",
       "  (983, 0.10000000149011612),\n",
       "  (990, 0.10000000149011612),\n",
       "  (3, 0.0),\n",
       "  (13, 0.0),\n",
       "  (26, 0.0),\n",
       "  (27, 0.0),\n",
       "  (30, 0.0),\n",
       "  (43, 0.0),\n",
       "  (54, 0.0),\n",
       "  (59, 0.0),\n",
       "  (64, 0.0),\n",
       "  (66, 0.0),\n",
       "  (73, 0.0),\n",
       "  (80, 0.0),\n",
       "  (82, 0.0),\n",
       "  (103, 0.0),\n",
       "  (104, 0.0),\n",
       "  (106, 0.0),\n",
       "  (111, 0.0),\n",
       "  (127, 0.0),\n",
       "  (129, 0.0),\n",
       "  (131, 0.0),\n",
       "  (133, 0.0),\n",
       "  (137, 0.0),\n",
       "  (147, 0.0),\n",
       "  (149, 0.0),\n",
       "  (152, 0.0),\n",
       "  (153, 0.0),\n",
       "  (154, 0.0),\n",
       "  (156, 0.0),\n",
       "  (165, 0.0),\n",
       "  (167, 0.0),\n",
       "  (172, 0.0),\n",
       "  (174, 0.0),\n",
       "  (175, 0.0),\n",
       "  (177, 0.0),\n",
       "  (179, 0.0),\n",
       "  (185, 0.0),\n",
       "  (191, 0.0),\n",
       "  (194, 0.0),\n",
       "  (200, 0.0),\n",
       "  (210, 0.0),\n",
       "  (220, 0.0),\n",
       "  (225, 0.0),\n",
       "  (233, 0.0),\n",
       "  (239, 0.0),\n",
       "  (240, 0.0),\n",
       "  (244, 0.0),\n",
       "  (251, 0.0),\n",
       "  (255, 0.0),\n",
       "  (258, 0.0),\n",
       "  (262, 0.0),\n",
       "  (268, 0.0),\n",
       "  (277, 0.0),\n",
       "  (278, 0.0),\n",
       "  (283, 0.0),\n",
       "  (287, 0.0),\n",
       "  (299, 0.0),\n",
       "  (325, 0.0),\n",
       "  (333, 0.0),\n",
       "  (338, 0.0),\n",
       "  (339, 0.0),\n",
       "  (346, 0.0),\n",
       "  (349, 0.0),\n",
       "  (351, 0.0),\n",
       "  (356, 0.0),\n",
       "  (367, 0.0),\n",
       "  (368, 0.0),\n",
       "  (371, 0.0),\n",
       "  (373, 0.0),\n",
       "  (374, 0.0),\n",
       "  (404, 0.0),\n",
       "  (405, 0.0),\n",
       "  (416, 0.0),\n",
       "  (418, 0.0),\n",
       "  (421, 0.0),\n",
       "  (426, 0.0),\n",
       "  (427, 0.0),\n",
       "  (435, 0.0),\n",
       "  (437, 0.0),\n",
       "  (446, 0.0),\n",
       "  (449, 0.0),\n",
       "  (460, 0.0),\n",
       "  (462, 0.0),\n",
       "  (465, 0.0),\n",
       "  (466, 0.0),\n",
       "  (467, 0.0),\n",
       "  (469, 0.0),\n",
       "  (473, 0.0),\n",
       "  (475, 0.0),\n",
       "  (478, 0.0),\n",
       "  (479, 0.0),\n",
       "  (481, 0.0),\n",
       "  (485, 0.0),\n",
       "  (493, 0.0),\n",
       "  (494, 0.0),\n",
       "  (499, 0.0),\n",
       "  (500, 0.0),\n",
       "  (504, 0.0),\n",
       "  (510, 0.0),\n",
       "  (515, 0.0),\n",
       "  (519, 0.0),\n",
       "  (521, 0.0),\n",
       "  (522, 0.0),\n",
       "  (525, 0.0),\n",
       "  (536, 0.0),\n",
       "  (543, 0.0),\n",
       "  (548, 0.0),\n",
       "  (549, 0.0),\n",
       "  (551, 0.0),\n",
       "  (553, 0.0),\n",
       "  (557, 0.0),\n",
       "  (559, 0.0),\n",
       "  (569, 0.0),\n",
       "  (573, 0.0),\n",
       "  (583, 0.0),\n",
       "  (594, 0.0),\n",
       "  (596, 0.0),\n",
       "  (598, 0.0),\n",
       "  (600, 0.0),\n",
       "  (610, 0.0),\n",
       "  (617, 0.0),\n",
       "  (622, 0.0),\n",
       "  (623, 0.0),\n",
       "  (627, 0.0),\n",
       "  (630, 0.0),\n",
       "  (631, 0.0),\n",
       "  (648, 0.0),\n",
       "  (653, 0.0),\n",
       "  (659, 0.0),\n",
       "  (660, 0.0),\n",
       "  (666, 0.0),\n",
       "  (673, 0.0),\n",
       "  (675, 0.0),\n",
       "  (677, 0.0),\n",
       "  (681, 0.0),\n",
       "  (682, 0.0),\n",
       "  (686, 0.0),\n",
       "  (693, 0.0),\n",
       "  (705, 0.0),\n",
       "  (708, 0.0),\n",
       "  (713, 0.0),\n",
       "  (714, 0.0),\n",
       "  (717, 0.0),\n",
       "  (718, 0.0),\n",
       "  (722, 0.0),\n",
       "  (723, 0.0),\n",
       "  (726, 0.0),\n",
       "  (728, 0.0),\n",
       "  (730, 0.0),\n",
       "  (731, 0.0),\n",
       "  (733, 0.0),\n",
       "  (737, 0.0),\n",
       "  (739, 0.0),\n",
       "  (742, 0.0),\n",
       "  (744, 0.0),\n",
       "  (747, 0.0),\n",
       "  (756, 0.0),\n",
       "  (760, 0.0),\n",
       "  (767, 0.0),\n",
       "  (769, 0.0),\n",
       "  (780, 0.0),\n",
       "  (785, 0.0),\n",
       "  (789, 0.0),\n",
       "  (792, 0.0),\n",
       "  (798, 0.0),\n",
       "  (807, 0.0),\n",
       "  (810, 0.0),\n",
       "  (812, 0.0),\n",
       "  (813, 0.0),\n",
       "  (835, 0.0),\n",
       "  (836, 0.0),\n",
       "  (840, 0.0),\n",
       "  (841, 0.0),\n",
       "  (845, 0.0),\n",
       "  (846, 0.0),\n",
       "  (851, 0.0),\n",
       "  (856, 0.0),\n",
       "  (860, 0.0),\n",
       "  (861, 0.0),\n",
       "  (876, 0.0),\n",
       "  (880, 0.0),\n",
       "  (895, 0.0),\n",
       "  (899, 0.0),\n",
       "  (908, 0.0),\n",
       "  (909, 0.0),\n",
       "  (912, 0.0),\n",
       "  (913, 0.0),\n",
       "  (914, 0.0),\n",
       "  (916, 0.0),\n",
       "  (917, 0.0),\n",
       "  (922, 0.0),\n",
       "  (924, 0.0),\n",
       "  (926, 0.0),\n",
       "  (928, 0.0),\n",
       "  (929, 0.0),\n",
       "  (930, 0.0),\n",
       "  (931, 0.0),\n",
       "  (935, 0.0),\n",
       "  (936, 0.0),\n",
       "  (940, 0.0),\n",
       "  (941, 0.0),\n",
       "  (942, 0.0),\n",
       "  (945, 0.0),\n",
       "  (947, 0.0),\n",
       "  (948, 0.0),\n",
       "  (950, 0.0),\n",
       "  (951, 0.0),\n",
       "  (954, 0.0),\n",
       "  (958, 0.0),\n",
       "  (960, 0.0),\n",
       "  (961, 0.0),\n",
       "  (964, 0.0),\n",
       "  (965, 0.0),\n",
       "  (966, 0.0),\n",
       "  (967, 0.0),\n",
       "  (969, 0.0),\n",
       "  (970, 0.0),\n",
       "  (974, 0.0),\n",
       "  (975, 0.0),\n",
       "  (980, 0.0),\n",
       "  (986, 0.0),\n",
       "  (991, 0.0),\n",
       "  (993, 0.0),\n",
       "  (995, 0.0),\n",
       "  (998, 0.0),\n",
       "  (999, 0.0)])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch_no 0\n",
      "done with the 0th calculation: 717\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 718\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 720\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 718\n",
      "result for n_pert: 10 is 718.25\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 713\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 724\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 715\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 719\n",
      "result for n_pert: 20 is 717.75\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 712\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 722\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 720\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 717\n",
      "result for n_pert: 30 is 717.75\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 716\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 717\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 713\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 714\n",
      "result for n_pert: 40 is 715.0\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 717\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 723\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 722\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 720\n",
      "result for n_pert: 50 is 720.5\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 719\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 712\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 712\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 720\n",
      "result for n_pert: 60 is 715.75\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 723\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 721\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 722\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 711\n",
      "result for n_pert: 70 is 719.25\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 715\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 715\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 717\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 715\n",
      "result for n_pert: 80 is 715.5\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 721\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 716\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 716\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 714\n",
      "result for n_pert: 90 is 716.75\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 718\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 719\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 724\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 723\n",
      "result for n_pert: 100 is 721.0\n"
     ]
    }
   ],
   "source": [
    "results_1 = []\n",
    "for n_pert in range(10, 110, 10):\n",
    "  n = targeted_diversity_average(learn, n_pert, 95, 4)\n",
    "  print(f'result for n_pert: {n_pert} is {n}')\n",
    "  results_1.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7feac1262b70>]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(10, 110, 10))\n",
    "plt.ylim(0, 800)\n",
    "plt.plot(x, results_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 548\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 524\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 519\n",
      "result for n_pert: 10 is 530.3333333333334\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 524\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 546\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 518\n",
      "result for n_pert: 20 is 529.3333333333334\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 561\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 543\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 523\n",
      "result for n_pert: 30 is 542.3333333333334\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 571\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 548\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 548\n",
      "result for n_pert: 40 is 555.6666666666666\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 552\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 535\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 538\n",
      "result for n_pert: 50 is 541.6666666666666\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 558\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 540\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 540\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 534\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 534\n",
      "result for n_pert: 70 is 536.0\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 547\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 521\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 542\n",
      "result for n_pert: 80 is 536.6666666666666\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 539\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 564\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 530\n",
      "result for n_pert: 90 is 544.3333333333334\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 550\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 540\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 540\n",
      "result for n_pert: 100 is 543.3333333333334\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for n_pert in range(10, 110, 10):\n",
    "  n = diversity_average(learn, n_pert, 95, 3)\n",
    "  print(f'result for n_pert: {n_pert} is {n}')\n",
    "  results.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fea6868afd0>]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(10, 110, 10))\n",
    "plt.ylim(0, 600)\n",
    "plt.plot(x, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#targeted:\n",
    "targeted_div_metrics = results_1\n",
    "div_metrics = results\n",
    "\n",
    "#non-targeted:\n",
    "n_targeted_div_metrics = [244.0, 247.0, 265.3333333333333, 246.66666666666666, 241.0, 231.33333333333334, \n",
    "                          247.66666666666666, 229.0, 222.33333333333334, 236.0]\n",
    "n_div_metrics = [132, 118, 122, 135, 133, 129, 136, 132, 124, 143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHjlJREFUeJzt3XuQZGd93vHvM91z2/tV0mp2xUqwEiDQjZEsTJKSkTCWQpDiIBtMQFHkbFJFDLaTYEE5sYlxAlW2EaqkVFEki4XiJgRYQoUJskAxOJasWUksuqLVdWevI+195z7zyx/v2zs9szM7PffZM8+n6tQ55z1vd7/nbO9z3n779BlFBGZmVlx1c90AMzObWQ56M7OCc9CbmRWcg97MrOAc9GZmBeegNzMrOAe9LSiSrpDUPonHfUnSZ/PyP5b03PS3bmZJekrSFXPdDpt9DnqbMEkvS7pqjl77eODOlYj4SUScN5dtqFbrMYmI8yPioVloks0zDnqbVZJKc92GhUZSea7bYHPLQW8TIukrwFnA9yQdlfRJSd+StEfSIUl/K+n8qvpfknSbpO9LOgb8iqTVkr4n6bCkRyV9VtJPqx7zZkkPSNov6TlJv5HLNwMfBj6ZX/t7ufxMSd+W1CHpJUkfr3qu5tyGA5KeBi6tcT8vlvSYpCOSvgk0VW07Pvwj6WZJ94x47Bcl3TrO8z+U9/v/VfYlH5evVh2XjVM4Ji9L+gNJ24BjksrVn8QklSR9WtILeR+3StpQy7GxU1BEePI0oQl4Gbiqav1fA0uBRuAW4ImqbV8CDgHvInUsmoBv5GkR8FZgB/DTXH9xXr8RKAOXAK8B51c932ernr8O2Ar8F6ABOAd4EXhv3v454CfAKmAD8CTQPs7+NQCvAL8H1AMfAPoqrwtcUXkO4A1AJ7Asr5eA3cDl47zGQ8B24I3AcuBp4BfAVXm/vwzcNZljUvVv9ETe5+aR/27AfwJ+DpwHCLgQWD3X7y1PMzO5R29TFhF/GRFHIqIH+GPgQknLq6rcGxF/FxGDpMD8F8AfRURnRDwNbKmq+z7g5Yi4KyL6I+Ix4NuksB3NpcDaiPivEdEbES8C/xv4YN7+G8CfRsT+iNgBnLSnnV1OCvhbIqIvIu4BHh1j318BHgOuy0XvBjoj4uEaXueuiHghIg4Bfw28EBF/ExH9wLeAi3O9iR6TilsjYkdEdI2y7beBP4yI5yL5WUS8XkOb7RTksTubkjzm/qfA9cBaYDBvWkPqyUPqjVasJb3vqsuql98A/JKkg1VlZeArYzThDcCZI+qXSL14gDNHPP8rJ9ufqsfsjIjqO/6d7HFfAz5E6oX/Vl6vxd6q5a5R1pfk5Ykek4odJ9m2AXihxnbaKc5Bb5NRHYC/BVxLGnJ4mTQMcYA0HDBa/Q6gH1hPGqqAFDoVO4D/GxHvqeG1K/VfiohNY9TfnZ//qbx+1hj1Rj6mRZKqwv4sxg7GbwF/Lmk98M+Bd9bwGhMx0WMyXnnlOd9IGsqygvPQjU3GXtJYOKSx+R7gddKY+3872QMjYgD4DvDHkhZJejPw0aoq9wPnSvqIpPo8XSrpLaO8NsA/AIfzF4/N+UvGt0mqfOl6N/ApSStzEP9ODfv396ST0cfzl5i/Dlx2kn3qII2530U66TxTw2tMxESPSS3uAP5E0iYlF0haPa2ttnnDQW+T8d+BP8xDCatIwxo7SV8o1jI2/e9JPf89pOGHr5NOFkTEEeBXSWPsu3Kdz5O+6AW4E3irpIOS/iqfOP4ZcBHwEulLyjvy8wN8JrfvJeCHjD/cQUT0Ar8O/CvSp5PfJJ2cTuZrpE81tQ7b1Gyix6TGp/0L0knwh8Dh/BzN09lumz80fBjSbPZJ+jxwRkTcMNdtMSsi9+ht1uVrwi/IQwaXATcB353rdpkVlb+MtbmwlDRccyawD/hz4N7ZbICks0hDTaN5a0S8Og2vcXSMTVdHxE/G2GY27Tx0Y2ZWcB66MTMruHkxdLNmzZrYuHHjXDfDzOyUsnXr1tciYu149eZF0G/cuJG2tra5boaZ2SlFUi2/9PbQjZlZ0TnozcwKzkFvZlZwDnozs4Jz0JuZFZyD3sys4Bz0ZmYFNy+uozebqojgWO8Ah7v6ONzdx6HOPo729NNUX2JJY5mlTWWWNJVZ2lhPU30dksZ/UrNJGhwMjvX2c6xnIM/7OdqT13v6q8oGuOotp3HB+hUz2p5TOuif3HmIx149gABJSFAnIdKc6vU6EKmOJOqU1uvE8bLK4yrPw4jnU6Xu8ccNPZ783HOtXCcayyWa6utoLJdoLNfRmJdLdfOggSfRNzDI4a4+DnX1cbi7P82Pr+d5V/9QmA/b3s/AYG33bSrXiSVNZZY0pmlZU/3QelM6KSxtrKzXD63nOktzWWN5Zk8YA4NBd98Anb0Dx+ddfQN09vYPreeyrt4T63VV1e/qG2RgcJBF9WWaG0osaijR3FBicUP5+PKihhKL8noqG1peVF2vvkS5VKzBgIEczJ09AzmQh8K5s3d42bHegargHlqvBHdnb3pMrU5b2uigP5mfbn+Nz/31s3PdjFNGfUnHw7+pPs0bynU01pdoyvPGct2w7Y3lEo31dTTleaVs5Imkun5DuY7uvoETgriyfnhEcFeCfLz/HA2lOpY117O8ucyy5npWLW5g4+rFLG+uZ1lzmeXN9Wm5Kc0XN5bp7kv/SY/29HO4u5+j3f0c7enjSF4+0pPm+45082JHqneku5+e/sGTtgXSCaPySWFJYz1Lqz45VE4ay5rqaSzX0dM/WBW8lUBOAdzV2z9U3jtAZw7pWtowUlN9Hc31KZib6utY1FCmub7EiuZ6SnWiq3eAg5297DqYXq8SShN9rYZyXToB1JdY1JhPAvX5pNBYTuVjnCzKJTEwGPQPBAODwUAE/YPBwMAgAwEDg4N5PW0bGMzbq6b+wcGq5ZHbxq5bvb2rb+B4D7urr/ZgXtxQYnFjOU/pZHnGsiYWNZZZktePb8sdhsUNZRY1pk+XlbLKMZmNDti8uHtla2trTOYWCF35jToYEAQREAGDEQTp4xPk9aryOL6eHjc4OMbjo1I29HyVcqofn+vMB6kXOEhPf/rP29M3QHf/ID1VZd19eduw7cPL0vLA8efqG5ie/VvaVB4WxpWAHlofu7ypvjQtbahFT38KgaPd/RzuTsNA6STRz5HuvuMniMqJ4Uj1CaRn6CTSWxWgdSIHcOl4MDZXBWRTDs7mhuHlqV55qF79UI+88hyLGko0lUvUTTI0KsHXmXuw6VNBGmqoXq58cujsS73fyrbO3oG0Xr2cT2BTfe9I6aRaqhPlujrqBOVSXV4XdRLlkkZZr0uPU96W65Qk6upEc30liHNwVwV0CuIc0pX1fAKb7DGeCZK2RkTrePVO6R595T+EzbyBwaB32ElixEmjb/iJobm+NBTaOayXNM1O72U6pE8rJVYtbpjS8/T0D9DdO0hTQx0Npfn73UCpTseHsqZbb/40UzkJ9A8E5dJoIV2XQjlvqw5lm5pTOuht9pTq5BPrJFROGAtZQx4iXE79XDdlwRr3GxVJ50l6omo6LOl3Ja2S9ICk5/N8Za4vSbdK2i5pm6RLZn43zMxsLOMGfUQ8FxEXRcRFwDuATtLf97wZeDAiNgEP5nWAq4FNedoM3DYTDTczs9pM9BqpK4EXIuIV4FpgSy7fAlyXl68FvhzJw8AKSeumpbVmZjZhEw36D5L+qDPA6RGxGyDPT8vlLcCOqse05zIzM5sDNQe9pAbg/cC3xqs6StkJ11dJ2iypTVJbR0dHrc0wM7MJmkiP/mrgsYjYm9f3VoZk8nxfLm8HNlQ9bj2wa+STRcTtEdEaEa1r1477Jw/NzGySJhL0H2Jo2AbgPuCGvHwDcG9V+Ufz1TeXA4cqQzxmZjb7arqOXtIi4D3Av60q/hxwt6SbgFeB63P594FrgO2kK3RunLbWmpnZhNUU9BHRCaweUfY66SqckXUD+Ni0tM7MzKasWLegMzOzEzjozcwKzkFvZlZwDnozs4Jz0JuZFZyD3sys4Bz0ZmYF56A3Mys4B72ZWcE56M3MCs5Bb2ZWcA56M7OCc9CbmRWcg97MrOAc9GZmBeegNzMrOAe9mVnBOejNzArOQW9mVnAOejOzgnPQm5kVnIPezKzgHPRmZgXnoDczKzgHvZlZwTnozcwKzkFvZlZwDnozs4Jz0JuZFZyD3sys4Bz0ZmYFV1PQS1oh6R5Jz0p6RtI7Ja2S9ICk5/N8Za4rSbdK2i5pm6RLZnYXzMzsZGrt0X8R+EFEvBm4EHgGuBl4MCI2AQ/mdYCrgU152gzcNq0tNjOzCRk36CUtA/4JcCdARPRGxEHgWmBLrrYFuC4vXwt8OZKHgRWS1k17y83MrCa19OjPATqAuyQ9LukOSYuB0yNiN0Cen5brtwA7qh7fnsuGkbRZUpukto6OjinthJmZja2WoC8DlwC3RcTFwDGGhmlGo1HK4oSCiNsjojUiWteuXVtTY83MbOJqCfp2oD0iHsnr95CCf29lSCbP91XV31D1+PXArulprpmZTdS4QR8Re4Adks7LRVcCTwP3ATfkshuAe/PyfcBH89U3lwOHKkM8ZmY2+8o11vsd4KuSGoAXgRtJJ4m7Jd0EvApcn+t+H7gG2A505rpmZjZHagr6iHgCaB1l05Wj1A3gY1Nsl5mZTRP/MtbMrOAc9GZmBeegNzMrOAe9mVnBOejNzArOQW9mVnAOejOzgnPQm5kVnIPezKzgHPRmZgXnoDczKzgHvZlZwTnozcwKzkFvZlZwDnozs4Jz0JuZFZyD3sys4Bz0ZmYF56A3Mys4B72ZWcE56M3MCs5Bb2ZWcA56M7OCc9CbmRWcg97MrOAc9GZmBeegNzMrOAe9mVnBOejNzArOQW9mVnAOejOzgqsp6CW9LOnnkp6Q1JbLVkl6QNLzeb4yl0vSrZK2S9om6ZKZ3AEzMzu5ifTofyUiLoqI1rx+M/BgRGwCHszrAFcDm/K0GbhtuhprZmYTN5Whm2uBLXl5C3BdVfmXI3kYWCFp3RRex8zMpqDWoA/gh5K2Stqcy06PiN0AeX5aLm8BdlQ9tj2XDSNps6Q2SW0dHR2Ta72ZmY2rXGO9d0XELkmnAQ9IevYkdTVKWZxQEHE7cDtAa2vrCdvNzGx61NSjj4hdeb4P+C5wGbC3MiST5/ty9XZgQ9XD1wO7pqvBZmY2MeMGvaTFkpZWloFfBZ4E7gNuyNVuAO7Ny/cBH81X31wOHKoM8ZiZ2eyrZejmdOC7kir1vxYRP5D0KHC3pJuAV4Hrc/3vA9cA24FO4MZpb7WZmdVs3KCPiBeBC0cpfx24cpTyAD42La0zM7Mp8y9jzcwKzkFvZlZwDnozs4Jz0JuZFZyD3sys4Bz0ZmYF56A3Mys4B72ZWcE56M3MCs5Bb2ZWcA56M7OCc9CbmRWcg97MrOAc9GZmBeegNzMrOAe9mVnBOejNzArOQW9mVnAOejOzgnPQm5kVnIPezKzgHPRmZgXnoDczKzgHvZlZwTnozcwKzkFvZlZwDnozs4Jz0JuZFZyD3sys4Bz0ZmYFV3PQSypJelzS/Xn9bEmPSHpe0jclNeTyxry+PW/fODNNNzOzWkykR/8J4Jmq9c8DX4iITcAB4KZcfhNwICLeBHwh1zMzszlSU9BLWg/8U+COvC7g3cA9ucoW4Lq8fG1eJ2+/Mtc3M7M5UGuP/hbgk8BgXl8NHIyI/rzeDrTk5RZgB0DefijXH0bSZkltkto6Ojom2XwzMxvPuEEv6X3AvojYWl08StWoYdtQQcTtEdEaEa1r166tqbFmZjZx5RrqvAt4v6RrgCZgGamHv0JSOffa1wO7cv12YAPQLqkMLAf2T3vLzcysJuP26CPiUxGxPiI2Ah8EfhQRHwZ+DHwgV7sBuDcv35fXydt/FBEn9OjNzGx2TOU6+j8Afl/SdtIY/J25/E5gdS7/feDmqTXRzMymopahm+Mi4iHgobz8InDZKHW6geunoW1mZjYN/MtYM7OCc9CbmRWcg97MrOAc9GZmBeegNzMrOAe9mVnBOejNzArOQW9mVnAOejOzgnPQm5kVnIPezKzgHPRmZgXnoDczKzgHvZlZwTnozcwKzkFvZlZwDnozs4Jz0JuZFZyD3sys4Bz0ZmYF56A3Mys4B72ZWcE56M3MCs5Bb2ZWcA56M7OCc9CbmRWcg97MrOAc9GZmBeegNzMrOAe9mVnBOejNzApu3KCX1CTpHyT9TNJTkj6Ty8+W9Iik5yV9U1JDLm/M69vz9o0zuwtmZnYytfToe4B3R8SFwEXAr0m6HPg88IWI2AQcAG7K9W8CDkTEm4Av5HpmZjZHxg36SI7m1fo8BfBu4J5cvgW4Li9fm9fJ26+UpGlrsZmZTUhNY/SSSpKeAPYBDwAvAAcjoj9XaQda8nILsAMgbz8ErB7lOTdLapPU1tHRMbW9MDOzMdUU9BExEBEXAeuBy4C3jFYtz0frvccJBRG3R0RrRLSuXbu21vaamdkETeiqm4g4CDwEXA6skFTOm9YDu/JyO7ABIG9fDuyfjsaamdnE1XLVzVpJK/JyM3AV8AzwY+ADudoNwL15+b68Tt7+o4g4oUdvZmazozx+FdYBWySVSCeGuyPifklPA9+Q9FngceDOXP9O4CuStpN68h+cgXabmVmNxg36iNgGXDxK+Yuk8fqR5d3A9dPSOjMzmzL/MtbMrOAc9GZmBeegNzMrOAe9mVnBOejNzArOQW9mVnAOejOzgqvlB1M2Xw0Owmu/gJ1t0N4Gux6DUiOsvxTWt6b58vXgm4eaLWgO+lPJ0X0p0I8H++PQczhta1wGLZdAfw+03QkP/89UvuSModBffymceRE0LJ67fTCzWeegn6/6umD3z6qCfSscejVtUwlOPx/e/gFoaU1BvnoT1OWRuIE+2Ptkemz7o2l69v7hj60E//pLYfUb3es3KzDNh/uNtba2Rltb21w3Y+4MDsLr24d66jvbYO9TMJhv9798A7S8IwV6SyusuxAaFk3sNY69np8/B3/7Vug9krY1r8wnjDzk0/IOaF4xvftoZtNO0taIaB2vnnv0c+HYayOGYB6D7kNpW8NSaLkYfvnjQ8G+9PSpv+bi1XDue9MEMDiQxvePB38bPPQ3HP/TAWvOGz7Wf9pboK409XaY2axzj36m9XXDnm3Dg/3gK2mb6uC082H9O4aGYNacO3eB2n04nXQqwd/+KHS+nrY1LIEzL4YNl6Xgb2mFJf6DMWZzyT36uRABr78wfAhmz5Mw2Je2L2tJwyKX3pSCcr59Mdq0DM65Ik2Q9ufAS8PH+v/ui0NDSis3Vo31t8Lpb4dyw5w03czG5h59RLpSpb8r9b7HnHenL0jHmh/aATu3QteB9Lz1i9NVMNVj68vWzc0+Tqe+Ltj1RNWQz6NwZHfaVmpMJ6+WVlixAZpWpLH+kfP65rndh4Wgvxdeew52b0ufKHdvS1donXnR0Ml57Zs9HHeKq7VHf2oH/a4n4NW/P3kA93ePH+An/knb2tSVodwM9U2w5PQc7HkIZiH9Jzq0c/hY/+4n8nEdQ6lx9BNALfP6Zl8hNFLP0fTl/Z5t6UqtPdtg3zMw0Ju21y+C098GjUvT0FylM9KwJL1nq6/AWrxm7vbDJmxhDN289LfwwH8eWi83pam++cR50wpYWllvGgroqcxLp/bhmzbLW9J0/nVpfXAgfbncdQC6D0LXwZPPj+yGjmeg6xD0HDr5a5Uaaj8xNK+EpWekIbNy48wfh9lw7HXY87PhPfXXt3O8s9K8CtZdAL/079LVWWdckC6frXQ6ImD/i8M/kf30FoiBtH3l2SOG497m4bgCOLV79L3H0rBLJeDrfEeHU17lJFHLCWLkvPswY346W7w2Bf7y9XneMnx96br5deKOSMOB1YG+Zxsc3jlUZ/mGFOTrLhiaL2uZ+Cee3s70KawS/DsehaN70rZyE6y7aPiP7pa3TN9+2pQsjKEbs2qDg+kTQSX4O/enTwuHdsLh9jzfmeaV3xBUqC79injkCWB5Cyxbn+aLT5uZzsTgALz2/PChl93b0j5U2rZ60/BAP+MCWLRq+tsC6SRzeOfwq692PQEDPWn70jNP/LW1v3eZuGOvp+9RVm6EZWdO6ikc9GYn032oKvjbh04A1SeEkd8z1NWnL9QrwT/aCWHRqpP3qPu6Yd9Tw3vqe59K3xlB+v7i9LdWBfqF6ZfME/2B3HTr74W9Px9+BdaBl9O2unIa4qke8ll1jr9LgfzJrD0Fescvhs8rly5f82dw2b+Z1NM76M2mIiJ9Ihj2SWDECeHw7qFLZyvKzal3Vv1JoGFJ+nJ0zzboeG5oPLxxOZzx9uE99TXnQql+9vd3Mo52DP+19c7HoPdo2ta8anjwt7wjXb5bVAN9sP+lHOTPpR8jdjyXPqn1HRuq17wy/Rhx7bl5fl4aGpvkb1Ic9GYzbXAQju0bZWio6oRwdA/EYBoWGjn0snJjsXq9gwPQ8ezwK7A6ns0bla5Eq1yRtngNLFqdPgEtWp2mhiXz/3j0HkvhfTzIcw99/wtDvy+B9ClvzbkpyI/Pz0v7PY376KA3mw8G+lMvd6HeO6jrYPp9SfWQT+W7h5FKDUOhX30CGDaNKJ+p7wY69w8P8sq8cmNBSDcIXHX2iB76uSnYG5fOTLtGWBiXV5rNd6Xywg15SPv+pivTBGlIrOdwGp/u3J/nI6f96X5Qe55M610HGPNqqvpFo58AmleNcbJYNXSpbeVL52FDLXne+drQa5SbYc2b0u0/LvnIUA991RtPmUtPHfRmNnskaFqeplXn1PaYwYH0yWDUk8KIE8b+l9L6yX6P0bAUFq1M9SrfKUD6/cXa8+C8q4eGWtaeC8vPOuUv3XbQm9n8VldKd19dvLr2x/T3pk8CJzspNK8c/qXo4rXz/zuCSXLQm1nxlBvS7b2n4xbfBXBqfx4xM7NxOejNzArOQW9mVnAOejOzghs36CVtkPRjSc9IekrSJ3L5KkkPSHo+z1fmckm6VdJ2SdskXTLTO2FmZmOrpUffD/yHiHgLcDnwMUlvBW4GHoyITcCDeR3gamBTnjYDt017q83MrGbjBn1E7I6Ix/LyEeAZoAW4FtiSq20B8l+d4Frgy5E8DKyQVIC/oWdmdmqa0Bi9pI3AxcAjwOkRsRvSyQA4LVdrAXZUPaw9l418rs2S2iS1dXR0TLzlZmZWk5p/MCVpCfBt4Hcj4rDG/gXZaBtOuFFFRNwO3J6fu0PSK7W2ZZ5aA7w2bq2Fw8djiI/FcD4ew03leLyhlko1Bb2kelLIfzUivpOL90paFxG789DMvlzeDmyoevh6YNfJnj8iJncz5nlEUlstd5FbKHw8hvhYDOfjMdxsHI9arroRcCfwTET8RdWm+4Ab8vINwL1V5R/NV99cDhyqDPGYmdnsq6VH/y7gI8DPJT2Ryz4NfA64W9JNwKvA9Xnb94FrgO1AJ3DjtLbYzMwmZNygj4ifMvq4O8CVo9QP4GNTbNep6Pa5bsA84+MxxMdiOB+P4Wb8eMyLvzBlZmYzx7dAMDMrOAe9mVnBOegnYaL3/1kIJJUkPS7p/rx+tqRH8rH4pqRT449rTgNJKyTdI+nZ/B5550J9b0j6vfx/5ElJX5fUtJDeG5L+UtI+SU9Wlc36fcIc9JMz0fv/LASfIN0eo+LzwBfysTgA3DQnrZobXwR+EBFvBi4kHZcF996Q1AJ8HGiNiLcBJeCDLKz3xpeAXxtRNvv3CYsIT1OcSL8heA/wHLAul60Dnpvrts3S/q/Pb9h3A/eTrtJ6DSjn7e8E/s9ct3OWjsUy4CXyhQ5V5QvuvcHQ7VBWka7wux9470J7bwAbgSfHey8A/wv40Gj1pjq5Rz9FNd7/p+huAT4JDOb11cDBiOjP66Pe76igzgE6gLvyUNYdkhazAN8bEbET+DPS72x2A4eArSzc90bFlO4TNhkO+ikYef+fuW7PXJD0PmBfRGytLh6l6kK5jrcMXALcFhEXA8dYAMM0o8ljz9cCZwNnAotJwxMjLZT3xnhm7P+Ng36STnb/n7y9+v4/RfYu4P2SXga+QRq+uYV0e+rKD/LGvd9RgbQD7RHxSF6/hxT8C/G9cRXwUkR0REQf8B3gl1m4742Ksd4LE75PWK0c9JMwifv/FFZEfCoi1kfERtIXbT+KiA8DPwY+kKstiGMBEBF7gB2SzstFVwJPswDfG6Qhm8slLcr/ZyrHYkG+N6rM+n3C/MvYSZD0j4CfAD9naFz606Rx+ruBs8j3/4mI/XPSyDkg6QrgP0bE+ySdQ+rhrwIeB/5lRPTMZftmi6SLgDuABuBF0v2e6liA7w1JnwF+k3Sl2uPAb5PGnRfEe0PS14ErSLci3gv8EfBXjPJeyCfD/0G6SqcTuDEi2qalHQ56M7Ni89CNmVnBOejNzArOQW9mVnAOejOzgnPQm5kVnIPezKzgHPRmZgX3/wEuwr9JEiJ0MQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, targeted_div_metrics)\n",
    "plt.plot(x, n_targeted_div_metrics)\n",
    "plt.title('targeted_div_metric')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, div_metrics)\n",
    "plt.plot(x, n_div_metrics)\n",
    "plt.title('div_metric')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 5\n",
      "at batch no 10\n",
      "at batch no 15\n",
      "at batch no 20\n",
      "at batch no 25\n",
      "at batch no 30\n",
      "at batch no 35\n",
      "at batch no 40\n",
      "at batch no 45\n",
      "at batch no 50\n",
      "at batch no 55\n",
      "at batch no 60\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(364,\n",
       " [(721, 143.20),\n",
       "  (750, 49.40),\n",
       "  (971, 48.40),\n",
       "  (794, 47.10),\n",
       "  (431, 35.10),\n",
       "  (669, 35.10),\n",
       "  (414, 30.60),\n",
       "  (588, 28.40),\n",
       "  (520, 24.90),\n",
       "  (61, 24.20),\n",
       "  (904, 18.30),\n",
       "  (411, 14.30),\n",
       "  (828, 13.50),\n",
       "  (39, 11.70),\n",
       "  (556, 11.40),\n",
       "  (581, 9.20),\n",
       "  (651, 8.30),\n",
       "  (489, 7.80),\n",
       "  (599, 6.60),\n",
       "  (84, 5.50),\n",
       "  (572, 4.80),\n",
       "  (907, 4.70),\n",
       "  (987, 4.40),\n",
       "  (401, 4.30),\n",
       "  (490, 4.30),\n",
       "  (614, 4.30),\n",
       "  (60, 4.00),\n",
       "  (955, 3.50),\n",
       "  (711, 3.30),\n",
       "  (48, 3.20),\n",
       "  (691, 3.10),\n",
       "  (709, 3.00),\n",
       "  (419, 2.90),\n",
       "  (770, 2.90),\n",
       "  (815, 2.90),\n",
       "  (864, 2.80),\n",
       "  (879, 2.80),\n",
       "  (108, 2.70),\n",
       "  (441, 2.70),\n",
       "  (632, 2.70),\n",
       "  (56, 2.50),\n",
       "  (604, 2.50),\n",
       "  (55, 2.40),\n",
       "  (464, 2.30),\n",
       "  (549, 2.30),\n",
       "  (575, 2.20),\n",
       "  (893, 2.20),\n",
       "  (973, 2.20),\n",
       "  (96, 2.10),\n",
       "  (496, 2.10),\n",
       "  (518, 2.10),\n",
       "  (412, 2.00),\n",
       "  (641, 2.00),\n",
       "  (762, 2.00),\n",
       "  (801, 2.00),\n",
       "  (872, 2.00),\n",
       "  (858, 1.90),\n",
       "  (871, 1.90),\n",
       "  (580, 1.80),\n",
       "  (621, 1.80),\n",
       "  (746, 1.80),\n",
       "  (806, 1.80),\n",
       "  (46, 1.70),\n",
       "  (151, 1.70),\n",
       "  (171, 1.70),\n",
       "  (633, 1.70),\n",
       "  (781, 1.70),\n",
       "  (790, 1.70),\n",
       "  (850, 1.70),\n",
       "  (424, 1.60),\n",
       "  (443, 1.60),\n",
       "  (453, 1.60),\n",
       "  (646, 1.60),\n",
       "  (981, 1.60),\n",
       "  (128, 1.50),\n",
       "  (360, 1.50),\n",
       "  (457, 1.50),\n",
       "  (545, 1.50),\n",
       "  (680, 1.50),\n",
       "  (722, 1.50),\n",
       "  (743, 1.50),\n",
       "  (1, 1.40),\n",
       "  (94, 1.40),\n",
       "  (230, 1.40),\n",
       "  (292, 1.40),\n",
       "  (406, 1.40),\n",
       "  (410, 1.40),\n",
       "  (440, 1.40),\n",
       "  (506, 1.40),\n",
       "  (847, 1.40),\n",
       "  (897, 1.40),\n",
       "  (67, 1.30),\n",
       "  (68, 1.30),\n",
       "  (124, 1.30),\n",
       "  (417, 1.30),\n",
       "  (538, 1.30),\n",
       "  (706, 1.30),\n",
       "  (779, 1.30),\n",
       "  (791, 1.30),\n",
       "  (826, 1.30),\n",
       "  (865, 1.30),\n",
       "  (898, 1.30),\n",
       "  (937, 1.30),\n",
       "  (953, 1.30),\n",
       "  (242, 1.20),\n",
       "  (310, 1.20),\n",
       "  (408, 1.20),\n",
       "  (582, 1.20),\n",
       "  (591, 1.20),\n",
       "  (698, 1.20),\n",
       "  (857, 1.20),\n",
       "  (896, 1.20),\n",
       "  (963, 1.20),\n",
       "  (83, 1.10),\n",
       "  (90, 1.10),\n",
       "  (92, 1.10),\n",
       "  (123, 1.10),\n",
       "  (293, 1.10),\n",
       "  (300, 1.10),\n",
       "  (316, 1.10),\n",
       "  (393, 1.10),\n",
       "  (407, 1.10),\n",
       "  (468, 1.10),\n",
       "  (612, 1.10),\n",
       "  (619, 1.10),\n",
       "  (645, 1.10),\n",
       "  (656, 1.10),\n",
       "  (738, 1.10),\n",
       "  (783, 1.10),\n",
       "  (805, 1.10),\n",
       "  (817, 1.10),\n",
       "  (906, 1.10),\n",
       "  (7, 1.00),\n",
       "  (25, 1.00),\n",
       "  (31, 1.00),\n",
       "  (33, 1.00),\n",
       "  (37, 1.00),\n",
       "  (57, 1.00),\n",
       "  (75, 1.00),\n",
       "  (88, 1.00),\n",
       "  (91, 1.00),\n",
       "  (102, 1.00),\n",
       "  (115, 1.00),\n",
       "  (120, 1.00),\n",
       "  (163, 1.00),\n",
       "  (176, 1.00),\n",
       "  (186, 1.00),\n",
       "  (195, 1.00),\n",
       "  (218, 1.00),\n",
       "  (231, 1.00),\n",
       "  (247, 1.00),\n",
       "  (260, 1.00),\n",
       "  (274, 1.00),\n",
       "  (281, 1.00),\n",
       "  (290, 1.00),\n",
       "  (306, 1.00),\n",
       "  (307, 1.00),\n",
       "  (314, 1.00),\n",
       "  (318, 1.00),\n",
       "  (334, 1.00),\n",
       "  (376, 1.00),\n",
       "  (392, 1.00),\n",
       "  (429, 1.00),\n",
       "  (474, 1.00),\n",
       "  (483, 1.00),\n",
       "  (488, 1.00),\n",
       "  (492, 1.00),\n",
       "  (498, 1.00),\n",
       "  (507, 1.00),\n",
       "  (516, 1.00),\n",
       "  (528, 1.00),\n",
       "  (530, 1.00),\n",
       "  (533, 1.00),\n",
       "  (562, 1.00),\n",
       "  (566, 1.00),\n",
       "  (602, 1.00),\n",
       "  (625, 1.00),\n",
       "  (637, 1.00),\n",
       "  (661, 1.00),\n",
       "  (684, 1.00),\n",
       "  (694, 1.00),\n",
       "  (716, 1.00),\n",
       "  (725, 1.00),\n",
       "  (734, 1.00),\n",
       "  (787, 1.00),\n",
       "  (816, 1.00),\n",
       "  (822, 1.00),\n",
       "  (824, 1.00),\n",
       "  (837, 1.00),\n",
       "  (873, 1.00),\n",
       "  (923, 1.00),\n",
       "  (934, 1.00),\n",
       "  (944, 1.00),\n",
       "  (992, 1.00),\n",
       "  (997, 1.00),\n",
       "  (40, 0.90),\n",
       "  (62, 0.90),\n",
       "  (105, 0.90),\n",
       "  (164, 0.90),\n",
       "  (301, 0.90),\n",
       "  (347, 0.90),\n",
       "  (369, 0.90),\n",
       "  (397, 0.90),\n",
       "  (425, 0.90),\n",
       "  (444, 0.90),\n",
       "  (753, 0.90),\n",
       "  (819, 0.90),\n",
       "  (868, 0.90),\n",
       "  (889, 0.90),\n",
       "  (982, 0.90),\n",
       "  (47, 0.80),\n",
       "  (72, 0.80),\n",
       "  (86, 0.80),\n",
       "  (97, 0.80),\n",
       "  (118, 0.80),\n",
       "  (254, 0.80),\n",
       "  (275, 0.80),\n",
       "  (304, 0.80),\n",
       "  (331, 0.80),\n",
       "  (463, 0.80),\n",
       "  (509, 0.80),\n",
       "  (532, 0.80),\n",
       "  (576, 0.80),\n",
       "  (586, 0.80),\n",
       "  (606, 0.80),\n",
       "  (638, 0.80),\n",
       "  (703, 0.80),\n",
       "  (732, 0.80),\n",
       "  (772, 0.80),\n",
       "  (786, 0.80),\n",
       "  (803, 0.80),\n",
       "  (829, 0.80),\n",
       "  (878, 0.80),\n",
       "  (899, 0.80),\n",
       "  (905, 0.80),\n",
       "  (915, 0.80),\n",
       "  (984, 0.80),\n",
       "  (8, 0.70),\n",
       "  (15, 0.70),\n",
       "  (38, 0.70),\n",
       "  (42, 0.70),\n",
       "  (109, 0.70),\n",
       "  (116, 0.70),\n",
       "  (144, 0.70),\n",
       "  (155, 0.70),\n",
       "  (189, 0.70),\n",
       "  (235, 0.70),\n",
       "  (313, 0.70),\n",
       "  (341, 0.70),\n",
       "  (353, 0.70),\n",
       "  (355, 0.70),\n",
       "  (387, 0.70),\n",
       "  (438, 0.70),\n",
       "  (476, 0.70),\n",
       "  (497, 0.70),\n",
       "  (508, 0.70),\n",
       "  (517, 0.70),\n",
       "  (526, 0.70),\n",
       "  (547, 0.70),\n",
       "  (552, 0.70),\n",
       "  (564, 0.70),\n",
       "  (570, 0.70),\n",
       "  (579, 0.70),\n",
       "  (620, 0.70),\n",
       "  (629, 0.70),\n",
       "  (727, 0.70),\n",
       "  (741, 0.70),\n",
       "  (752, 0.70),\n",
       "  (778, 0.70),\n",
       "  (788, 0.70),\n",
       "  (814, 0.70),\n",
       "  (925, 0.70),\n",
       "  (17, 0.60),\n",
       "  (19, 0.60),\n",
       "  (87, 0.60),\n",
       "  (289, 0.60),\n",
       "  (291, 0.60),\n",
       "  (294, 0.60),\n",
       "  (327, 0.60),\n",
       "  (375, 0.60),\n",
       "  (398, 0.60),\n",
       "  (409, 0.60),\n",
       "  (433, 0.60),\n",
       "  (445, 0.60),\n",
       "  (472, 0.60),\n",
       "  (482, 0.60),\n",
       "  (535, 0.60),\n",
       "  (544, 0.60),\n",
       "  (565, 0.60),\n",
       "  (671, 0.60),\n",
       "  (679, 0.60),\n",
       "  (696, 0.60),\n",
       "  (701, 0.60),\n",
       "  (751, 0.60),\n",
       "  (760, 0.60),\n",
       "  (796, 0.60),\n",
       "  (797, 0.60),\n",
       "  (820, 0.60),\n",
       "  (867, 0.60),\n",
       "  (892, 0.60),\n",
       "  (902, 0.60),\n",
       "  (920, 0.60),\n",
       "  (998, 0.60),\n",
       "  (0, 0.50),\n",
       "  (45, 0.50),\n",
       "  (50, 0.50),\n",
       "  (52, 0.50),\n",
       "  (107, 0.50),\n",
       "  (149, 0.50),\n",
       "  (159, 0.50),\n",
       "  (336, 0.50),\n",
       "  (342, 0.50),\n",
       "  (348, 0.50),\n",
       "  (391, 0.50),\n",
       "  (495, 0.50),\n",
       "  (515, 0.50),\n",
       "  (523, 0.50),\n",
       "  (527, 0.50),\n",
       "  (539, 0.50),\n",
       "  (555, 0.50),\n",
       "  (605, 0.50),\n",
       "  (607, 0.50),\n",
       "  (609, 0.50),\n",
       "  (626, 0.50),\n",
       "  (654, 0.50),\n",
       "  (655, 0.50),\n",
       "  (664, 0.50),\n",
       "  (674, 0.50),\n",
       "  (705, 0.50),\n",
       "  (719, 0.50),\n",
       "  (745, 0.50),\n",
       "  (754, 0.50),\n",
       "  (759, 0.50),\n",
       "  (768, 0.50),\n",
       "  (823, 0.50),\n",
       "  (853, 0.50),\n",
       "  (900, 0.50),\n",
       "  (917, 0.50),\n",
       "  (985, 0.50),\n",
       "  (9, 0.40),\n",
       "  (23, 0.40),\n",
       "  (24, 0.40),\n",
       "  (28, 0.40),\n",
       "  (63, 0.40),\n",
       "  (77, 0.40),\n",
       "  (126, 0.40),\n",
       "  (134, 0.40),\n",
       "  (140, 0.40),\n",
       "  (168, 0.40),\n",
       "  (192, 0.40),\n",
       "  (197, 0.40),\n",
       "  (205, 0.40),\n",
       "  (219, 0.40),\n",
       "  (224, 0.40),\n",
       "  (236, 0.40),\n",
       "  (249, 0.40),\n",
       "  (305, 0.40),\n",
       "  (319, 0.40),\n",
       "  (321, 0.40),\n",
       "  (332, 0.40),\n",
       "  (363, 0.40),\n",
       "  (381, 0.40),\n",
       "  (383, 0.40),\n",
       "  (388, 0.40),\n",
       "  (389, 0.40),\n",
       "  (396, 0.40),\n",
       "  (473, 0.40),\n",
       "  (481, 0.40),\n",
       "  (491, 0.40),\n",
       "  (522, 0.40),\n",
       "  (546, 0.40),\n",
       "  (574, 0.40),\n",
       "  (584, 0.40),\n",
       "  (672, 0.40),\n",
       "  (692, 0.40),\n",
       "  (697, 0.40),\n",
       "  (712, 0.40),\n",
       "  (802, 0.40),\n",
       "  (843, 0.40),\n",
       "  (854, 0.40),\n",
       "  (870, 0.40),\n",
       "  (880, 0.40),\n",
       "  (882, 0.40),\n",
       "  (890, 0.40),\n",
       "  (918, 0.40),\n",
       "  (938, 0.40),\n",
       "  (991, 0.40),\n",
       "  (41, 0.30),\n",
       "  (65, 0.30),\n",
       "  (71, 0.30),\n",
       "  (74, 0.30),\n",
       "  (113, 0.30),\n",
       "  (183, 0.30),\n",
       "  (191, 0.30),\n",
       "  (196, 0.30),\n",
       "  (202, 0.30),\n",
       "  (228, 0.30),\n",
       "  (232, 0.30),\n",
       "  (253, 0.30),\n",
       "  (303, 0.30),\n",
       "  (320, 0.30),\n",
       "  (337, 0.30),\n",
       "  (350, 0.30),\n",
       "  (358, 0.30),\n",
       "  (364, 0.30),\n",
       "  (399, 0.30),\n",
       "  (452, 0.30),\n",
       "  (454, 0.30),\n",
       "  (477, 0.30),\n",
       "  (480, 0.30),\n",
       "  (512, 0.30),\n",
       "  (537, 0.30),\n",
       "  (550, 0.30),\n",
       "  (554, 0.30),\n",
       "  (560, 0.30),\n",
       "  (593, 0.30),\n",
       "  (640, 0.30),\n",
       "  (644, 0.30),\n",
       "  (683, 0.30),\n",
       "  (707, 0.30),\n",
       "  (720, 0.30),\n",
       "  (729, 0.30),\n",
       "  (748, 0.30),\n",
       "  (757, 0.30),\n",
       "  (758, 0.30),\n",
       "  (777, 0.30),\n",
       "  (811, 0.30),\n",
       "  (831, 0.30),\n",
       "  (840, 0.30),\n",
       "  (846, 0.30),\n",
       "  (875, 0.30),\n",
       "  (883, 0.30),\n",
       "  (932, 0.30),\n",
       "  (939, 0.30),\n",
       "  (951, 0.30),\n",
       "  (957, 0.30),\n",
       "  (968, 0.30),\n",
       "  (995, 0.30),\n",
       "  (58, 0.20),\n",
       "  (82, 0.20),\n",
       "  (93, 0.20),\n",
       "  (98, 0.20),\n",
       "  (99, 0.20),\n",
       "  (100, 0.20),\n",
       "  (117, 0.20),\n",
       "  (122, 0.20),\n",
       "  (131, 0.20),\n",
       "  (138, 0.20),\n",
       "  (153, 0.20),\n",
       "  (161, 0.20),\n",
       "  (178, 0.20),\n",
       "  (188, 0.20),\n",
       "  (198, 0.20),\n",
       "  (206, 0.20),\n",
       "  (238, 0.20),\n",
       "  (283, 0.20),\n",
       "  (284, 0.20),\n",
       "  (317, 0.20),\n",
       "  (323, 0.20),\n",
       "  (326, 0.20),\n",
       "  (340, 0.20),\n",
       "  (344, 0.20),\n",
       "  (379, 0.20),\n",
       "  (395, 0.20),\n",
       "  (420, 0.20),\n",
       "  (432, 0.20),\n",
       "  (435, 0.20),\n",
       "  (436, 0.20),\n",
       "  (455, 0.20),\n",
       "  (484, 0.20),\n",
       "  (485, 0.20),\n",
       "  (487, 0.20),\n",
       "  (502, 0.20),\n",
       "  (505, 0.20),\n",
       "  (514, 0.20),\n",
       "  (534, 0.20),\n",
       "  (557, 0.20),\n",
       "  (585, 0.20),\n",
       "  (595, 0.20),\n",
       "  (618, 0.20),\n",
       "  (635, 0.20),\n",
       "  (643, 0.20),\n",
       "  (681, 0.20),\n",
       "  (700, 0.20),\n",
       "  (717, 0.20),\n",
       "  (723, 0.20),\n",
       "  (728, 0.20),\n",
       "  (736, 0.20),\n",
       "  (747, 0.20),\n",
       "  (785, 0.20),\n",
       "  (800, 0.20),\n",
       "  (808, 0.20),\n",
       "  (809, 0.20),\n",
       "  (818, 0.20),\n",
       "  (821, 0.20),\n",
       "  (825, 0.20),\n",
       "  (832, 0.20),\n",
       "  (844, 0.20),\n",
       "  (852, 0.20),\n",
       "  (876, 0.20),\n",
       "  (881, 0.20),\n",
       "  (926, 0.20),\n",
       "  (962, 0.20),\n",
       "  (966, 0.20),\n",
       "  (996, 0.20),\n",
       "  (11, 0.10),\n",
       "  (18, 0.10),\n",
       "  (21, 0.10),\n",
       "  (22, 0.10),\n",
       "  (26, 0.10),\n",
       "  (66, 0.10),\n",
       "  (70, 0.10),\n",
       "  (76, 0.10),\n",
       "  (79, 0.10),\n",
       "  (95, 0.10),\n",
       "  (111, 0.10),\n",
       "  (114, 0.10),\n",
       "  (121, 0.10),\n",
       "  (125, 0.10),\n",
       "  (129, 0.10),\n",
       "  (132, 0.10),\n",
       "  (139, 0.10),\n",
       "  (141, 0.10),\n",
       "  (142, 0.10),\n",
       "  (146, 0.10),\n",
       "  (158, 0.10),\n",
       "  (173, 0.10),\n",
       "  (179, 0.10),\n",
       "  (193, 0.10),\n",
       "  (214, 0.10),\n",
       "  (222, 0.10),\n",
       "  (229, 0.10),\n",
       "  (237, 0.10),\n",
       "  (252, 0.10),\n",
       "  (256, 0.10),\n",
       "  (268, 0.10),\n",
       "  (273, 0.10),\n",
       "  (276, 0.10),\n",
       "  (278, 0.10),\n",
       "  (282, 0.10),\n",
       "  (295, 0.10),\n",
       "  (298, 0.10),\n",
       "  (308, 0.10),\n",
       "  (309, 0.10),\n",
       "  (311, 0.10),\n",
       "  (315, 0.10),\n",
       "  (330, 0.10),\n",
       "  (343, 0.10),\n",
       "  (346, 0.10),\n",
       "  (351, 0.10),\n",
       "  (352, 0.10),\n",
       "  (356, 0.10),\n",
       "  (361, 0.10),\n",
       "  (365, 0.10),\n",
       "  (366, 0.10),\n",
       "  (377, 0.10),\n",
       "  (413, 0.10),\n",
       "  (423, 0.10),\n",
       "  (427, 0.10),\n",
       "  (428, 0.10),\n",
       "  (439, 0.10),\n",
       "  (442, 0.10),\n",
       "  (447, 0.10),\n",
       "  (448, 0.10),\n",
       "  (450, 0.10),\n",
       "  (451, 0.10),\n",
       "  (459, 0.10),\n",
       "  (475, 0.10),\n",
       "  (486, 0.10),\n",
       "  (493, 0.10),\n",
       "  (504, 0.10),\n",
       "  (521, 0.10),\n",
       "  (540, 0.10),\n",
       "  (551, 0.10),\n",
       "  (563, 0.10),\n",
       "  (569, 0.10),\n",
       "  (577, 0.10),\n",
       "  (603, 0.10),\n",
       "  (611, 0.10),\n",
       "  (613, 0.10),\n",
       "  (615, 0.10),\n",
       "  (616, 0.10),\n",
       "  (636, 0.10),\n",
       "  (639, 0.10),\n",
       "  (650, 0.10),\n",
       "  (665, 0.10),\n",
       "  (668, 0.10),\n",
       "  (673, 0.10),\n",
       "  (730, 0.10),\n",
       "  (735, 0.10),\n",
       "  (755, 0.10),\n",
       "  (756, 0.10),\n",
       "  (761, 0.10),\n",
       "  (763, 0.10),\n",
       "  (764, 0.10),\n",
       "  (765, 0.10),\n",
       "  (775, 0.10),\n",
       "  (792, 0.10),\n",
       "  (804, 0.10),\n",
       "  (813, 0.10),\n",
       "  (830, 0.10),\n",
       "  (833, 0.10),\n",
       "  (834, 0.10),\n",
       "  (836, 0.10),\n",
       "  (839, 0.10),\n",
       "  (842, 0.10),\n",
       "  (855, 0.10),\n",
       "  (856, 0.10),\n",
       "  (863, 0.10),\n",
       "  (866, 0.10),\n",
       "  (884, 0.10),\n",
       "  (885, 0.10),\n",
       "  (901, 0.10),\n",
       "  (927, 0.10),\n",
       "  (946, 0.10),\n",
       "  (950, 0.10),\n",
       "  (952, 0.10),\n",
       "  (954, 0.10),\n",
       "  (978, 0.10),\n",
       "  (983, 0.10),\n",
       "  (988, 0.10),\n",
       "  (999, 0.10),\n",
       "  (2, 0.00),\n",
       "  (3, 0.00),\n",
       "  (4, 0.00),\n",
       "  (5, 0.00),\n",
       "  (6, 0.00),\n",
       "  (10, 0.00),\n",
       "  (12, 0.00),\n",
       "  (13, 0.00),\n",
       "  (14, 0.00),\n",
       "  (16, 0.00),\n",
       "  (20, 0.00),\n",
       "  (27, 0.00),\n",
       "  (29, 0.00),\n",
       "  (30, 0.00),\n",
       "  (32, 0.00),\n",
       "  (34, 0.00),\n",
       "  (35, 0.00),\n",
       "  (36, 0.00),\n",
       "  (43, 0.00),\n",
       "  (44, 0.00),\n",
       "  (49, 0.00),\n",
       "  (51, 0.00),\n",
       "  (53, 0.00),\n",
       "  (54, 0.00),\n",
       "  (59, 0.00),\n",
       "  (64, 0.00),\n",
       "  (69, 0.00),\n",
       "  (73, 0.00),\n",
       "  (78, 0.00),\n",
       "  (80, 0.00),\n",
       "  (81, 0.00),\n",
       "  (85, 0.00),\n",
       "  (89, 0.00),\n",
       "  (101, 0.00),\n",
       "  (103, 0.00),\n",
       "  (104, 0.00),\n",
       "  (106, 0.00),\n",
       "  (110, 0.00),\n",
       "  (112, 0.00),\n",
       "  (119, 0.00),\n",
       "  (127, 0.00),\n",
       "  (130, 0.00),\n",
       "  (133, 0.00),\n",
       "  (135, 0.00),\n",
       "  (136, 0.00),\n",
       "  (137, 0.00),\n",
       "  (143, 0.00),\n",
       "  (145, 0.00),\n",
       "  (147, 0.00),\n",
       "  (148, 0.00),\n",
       "  (150, 0.00),\n",
       "  (152, 0.00),\n",
       "  (154, 0.00),\n",
       "  (156, 0.00),\n",
       "  (157, 0.00),\n",
       "  (160, 0.00),\n",
       "  (162, 0.00),\n",
       "  (165, 0.00),\n",
       "  (166, 0.00),\n",
       "  (167, 0.00),\n",
       "  (169, 0.00),\n",
       "  (170, 0.00),\n",
       "  (172, 0.00),\n",
       "  (174, 0.00),\n",
       "  (175, 0.00),\n",
       "  (177, 0.00),\n",
       "  (180, 0.00),\n",
       "  (181, 0.00),\n",
       "  (182, 0.00),\n",
       "  (184, 0.00),\n",
       "  (185, 0.00),\n",
       "  (187, 0.00),\n",
       "  (190, 0.00),\n",
       "  (194, 0.00),\n",
       "  (199, 0.00),\n",
       "  (200, 0.00),\n",
       "  (201, 0.00),\n",
       "  (203, 0.00),\n",
       "  (204, 0.00),\n",
       "  (207, 0.00),\n",
       "  (208, 0.00),\n",
       "  (209, 0.00),\n",
       "  (210, 0.00),\n",
       "  (211, 0.00),\n",
       "  (212, 0.00),\n",
       "  (213, 0.00),\n",
       "  (215, 0.00),\n",
       "  (216, 0.00),\n",
       "  (217, 0.00),\n",
       "  (220, 0.00),\n",
       "  (221, 0.00),\n",
       "  (223, 0.00),\n",
       "  (225, 0.00),\n",
       "  (226, 0.00),\n",
       "  (227, 0.00),\n",
       "  (233, 0.00),\n",
       "  (234, 0.00),\n",
       "  (239, 0.00),\n",
       "  (240, 0.00),\n",
       "  (241, 0.00),\n",
       "  (243, 0.00),\n",
       "  (244, 0.00),\n",
       "  (245, 0.00),\n",
       "  (246, 0.00),\n",
       "  (248, 0.00),\n",
       "  (250, 0.00),\n",
       "  (251, 0.00),\n",
       "  (255, 0.00),\n",
       "  (257, 0.00),\n",
       "  (258, 0.00),\n",
       "  (259, 0.00),\n",
       "  (261, 0.00),\n",
       "  (262, 0.00),\n",
       "  (263, 0.00),\n",
       "  (264, 0.00),\n",
       "  (265, 0.00),\n",
       "  (266, 0.00),\n",
       "  (267, 0.00),\n",
       "  (269, 0.00),\n",
       "  (270, 0.00),\n",
       "  (271, 0.00),\n",
       "  (272, 0.00),\n",
       "  (277, 0.00),\n",
       "  (279, 0.00),\n",
       "  (280, 0.00),\n",
       "  (285, 0.00),\n",
       "  (286, 0.00),\n",
       "  (287, 0.00),\n",
       "  (288, 0.00),\n",
       "  (296, 0.00),\n",
       "  (297, 0.00),\n",
       "  (299, 0.00),\n",
       "  (302, 0.00),\n",
       "  (312, 0.00),\n",
       "  (322, 0.00),\n",
       "  (324, 0.00),\n",
       "  (325, 0.00),\n",
       "  (328, 0.00),\n",
       "  (329, 0.00),\n",
       "  (333, 0.00),\n",
       "  (335, 0.00),\n",
       "  (338, 0.00),\n",
       "  (339, 0.00),\n",
       "  (345, 0.00),\n",
       "  (349, 0.00),\n",
       "  (354, 0.00),\n",
       "  (357, 0.00),\n",
       "  (359, 0.00),\n",
       "  (362, 0.00),\n",
       "  (367, 0.00),\n",
       "  (368, 0.00),\n",
       "  (370, 0.00),\n",
       "  (371, 0.00),\n",
       "  (372, 0.00),\n",
       "  (373, 0.00),\n",
       "  (374, 0.00),\n",
       "  (378, 0.00),\n",
       "  (380, 0.00),\n",
       "  (382, 0.00),\n",
       "  (384, 0.00),\n",
       "  (385, 0.00),\n",
       "  (386, 0.00),\n",
       "  (390, 0.00),\n",
       "  (394, 0.00),\n",
       "  (400, 0.00),\n",
       "  (402, 0.00),\n",
       "  (403, 0.00),\n",
       "  (404, 0.00),\n",
       "  (405, 0.00),\n",
       "  (415, 0.00),\n",
       "  (416, 0.00),\n",
       "  (418, 0.00),\n",
       "  (421, 0.00),\n",
       "  (422, 0.00),\n",
       "  (426, 0.00),\n",
       "  (430, 0.00),\n",
       "  (434, 0.00),\n",
       "  (437, 0.00),\n",
       "  (446, 0.00),\n",
       "  (449, 0.00),\n",
       "  (456, 0.00),\n",
       "  (458, 0.00),\n",
       "  (460, 0.00),\n",
       "  (461, 0.00),\n",
       "  (462, 0.00),\n",
       "  (465, 0.00),\n",
       "  (466, 0.00),\n",
       "  (467, 0.00),\n",
       "  (469, 0.00),\n",
       "  (470, 0.00),\n",
       "  (471, 0.00),\n",
       "  (478, 0.00),\n",
       "  (479, 0.00),\n",
       "  (494, 0.00),\n",
       "  (499, 0.00),\n",
       "  (500, 0.00),\n",
       "  (501, 0.00),\n",
       "  (503, 0.00),\n",
       "  (510, 0.00),\n",
       "  (511, 0.00),\n",
       "  (513, 0.00),\n",
       "  (519, 0.00),\n",
       "  (524, 0.00),\n",
       "  (525, 0.00),\n",
       "  (529, 0.00),\n",
       "  (531, 0.00),\n",
       "  (536, 0.00),\n",
       "  (541, 0.00),\n",
       "  (542, 0.00),\n",
       "  (543, 0.00),\n",
       "  (548, 0.00),\n",
       "  (553, 0.00),\n",
       "  (558, 0.00),\n",
       "  (559, 0.00),\n",
       "  (561, 0.00),\n",
       "  (567, 0.00),\n",
       "  (568, 0.00),\n",
       "  (571, 0.00),\n",
       "  (573, 0.00),\n",
       "  (578, 0.00),\n",
       "  (583, 0.00),\n",
       "  (587, 0.00),\n",
       "  (589, 0.00),\n",
       "  (590, 0.00),\n",
       "  (592, 0.00),\n",
       "  (594, 0.00),\n",
       "  (596, 0.00),\n",
       "  (597, 0.00),\n",
       "  (598, 0.00),\n",
       "  (600, 0.00),\n",
       "  (601, 0.00),\n",
       "  (608, 0.00),\n",
       "  (610, 0.00),\n",
       "  (617, 0.00),\n",
       "  (622, 0.00),\n",
       "  (623, 0.00),\n",
       "  (624, 0.00),\n",
       "  (627, 0.00),\n",
       "  (628, 0.00),\n",
       "  (630, 0.00),\n",
       "  (631, 0.00),\n",
       "  (634, 0.00),\n",
       "  (642, 0.00),\n",
       "  (647, 0.00),\n",
       "  (648, 0.00),\n",
       "  (649, 0.00),\n",
       "  (652, 0.00),\n",
       "  (653, 0.00),\n",
       "  (657, 0.00),\n",
       "  (658, 0.00),\n",
       "  (659, 0.00),\n",
       "  (660, 0.00),\n",
       "  (662, 0.00),\n",
       "  (663, 0.00),\n",
       "  (666, 0.00),\n",
       "  (667, 0.00),\n",
       "  (670, 0.00),\n",
       "  (675, 0.00),\n",
       "  (676, 0.00),\n",
       "  (677, 0.00),\n",
       "  (678, 0.00),\n",
       "  (682, 0.00),\n",
       "  (685, 0.00),\n",
       "  (686, 0.00),\n",
       "  (687, 0.00),\n",
       "  (688, 0.00),\n",
       "  (689, 0.00),\n",
       "  (690, 0.00),\n",
       "  (693, 0.00),\n",
       "  (695, 0.00),\n",
       "  (699, 0.00),\n",
       "  (702, 0.00),\n",
       "  (704, 0.00),\n",
       "  (708, 0.00),\n",
       "  (710, 0.00),\n",
       "  (713, 0.00),\n",
       "  (714, 0.00),\n",
       "  (715, 0.00),\n",
       "  (718, 0.00),\n",
       "  (724, 0.00),\n",
       "  (726, 0.00),\n",
       "  (731, 0.00),\n",
       "  (733, 0.00),\n",
       "  (737, 0.00),\n",
       "  (739, 0.00),\n",
       "  (740, 0.00),\n",
       "  (742, 0.00),\n",
       "  (744, 0.00),\n",
       "  (749, 0.00),\n",
       "  (766, 0.00),\n",
       "  (767, 0.00),\n",
       "  (769, 0.00),\n",
       "  (771, 0.00),\n",
       "  (773, 0.00),\n",
       "  (774, 0.00),\n",
       "  (776, 0.00),\n",
       "  (780, 0.00),\n",
       "  (782, 0.00),\n",
       "  (784, 0.00),\n",
       "  (789, 0.00),\n",
       "  (793, 0.00),\n",
       "  (795, 0.00),\n",
       "  (798, 0.00),\n",
       "  (799, 0.00),\n",
       "  (807, 0.00),\n",
       "  (810, 0.00),\n",
       "  (812, 0.00),\n",
       "  (827, 0.00),\n",
       "  (835, 0.00),\n",
       "  (838, 0.00),\n",
       "  (841, 0.00),\n",
       "  (845, 0.00),\n",
       "  (848, 0.00),\n",
       "  (849, 0.00),\n",
       "  (851, 0.00),\n",
       "  (859, 0.00),\n",
       "  (860, 0.00),\n",
       "  (861, 0.00),\n",
       "  (862, 0.00),\n",
       "  (869, 0.00),\n",
       "  (874, 0.00),\n",
       "  (877, 0.00),\n",
       "  (886, 0.00),\n",
       "  (887, 0.00),\n",
       "  (888, 0.00),\n",
       "  (891, 0.00),\n",
       "  (894, 0.00),\n",
       "  (895, 0.00),\n",
       "  (903, 0.00),\n",
       "  (908, 0.00),\n",
       "  (909, 0.00),\n",
       "  (910, 0.00),\n",
       "  (911, 0.00),\n",
       "  (912, 0.00),\n",
       "  (913, 0.00),\n",
       "  (914, 0.00),\n",
       "  (916, 0.00),\n",
       "  (919, 0.00),\n",
       "  (921, 0.00),\n",
       "  (922, 0.00),\n",
       "  (924, 0.00),\n",
       "  (928, 0.00),\n",
       "  (929, 0.00),\n",
       "  (930, 0.00),\n",
       "  (931, 0.00),\n",
       "  (933, 0.00),\n",
       "  (935, 0.00),\n",
       "  (936, 0.00),\n",
       "  (940, 0.00),\n",
       "  (941, 0.00),\n",
       "  (942, 0.00),\n",
       "  (943, 0.00),\n",
       "  (945, 0.00),\n",
       "  (947, 0.00),\n",
       "  (948, 0.00),\n",
       "  (949, 0.00),\n",
       "  (956, 0.00),\n",
       "  (958, 0.00),\n",
       "  (959, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (964, 0.00),\n",
       "  (965, 0.00),\n",
       "  (967, 0.00),\n",
       "  (969, 0.00),\n",
       "  (970, 0.00),\n",
       "  (972, 0.00),\n",
       "  (974, 0.00),\n",
       "  (975, 0.00),\n",
       "  (976, 0.00),\n",
       "  (977, 0.00),\n",
       "  (979, 0.00),\n",
       "  (980, 0.00),\n",
       "  (986, 0.00),\n",
       "  (989, 0.00),\n",
       "  (990, 0.00),\n",
       "  (993, 0.00),\n",
       "  (994, 0.00)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f59cf2afe48>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_hist = sorted(hist, key=lambda x: x[0], reverse = False)\n",
    "values = [elem[1] for elem in sorted_hist]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.6468)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch_no 0\n",
      "at batch_no 100\n",
      "at batch_no 200\n",
      "at batch_no 300\n",
      "at batch_no 400\n",
      "at batch_no 500\n",
      "at batch_no 600\n",
      "at batch_no 700\n",
      "at batch_no 800\n",
      "at batch_no 900\n",
      "at batch_no 1000\n",
      "at batch_no 1100\n",
      "at batch_no 1200\n",
      "at batch_no 1300\n",
      "at batch_no 1400\n",
      "at batch_no 1500\n",
      "at batch_no 1600\n",
      "at batch_no 1700\n",
      "at batch_no 1800\n",
      "at batch_no 1900\n",
      "at batch_no 2000\n",
      "at batch_no 2100\n",
      "at batch_no 2200\n",
      "at batch_no 2300\n",
      "at batch_no 2400\n",
      "at batch_no 2500\n",
      "at batch_no 2600\n",
      "at batch_no 2700\n",
      "at batch_no 2800\n",
      "at batch_no 2900\n",
      "at batch_no 3000\n",
      "at batch_no 3100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(885,\n",
       " [(815, 234.00),\n",
       "  (652, 177.00),\n",
       "  (646, 163.00),\n",
       "  (580, 160.00),\n",
       "  (611, 156.00),\n",
       "  (489, 145.00),\n",
       "  (591, 144.00),\n",
       "  (621, 141.00),\n",
       "  (737, 139.00),\n",
       "  (904, 130.00),\n",
       "  (94, 129.00),\n",
       "  (868, 127.00),\n",
       "  (582, 125.00),\n",
       "  (497, 123.00),\n",
       "  (893, 120.00),\n",
       "  (794, 118.00),\n",
       "  (116, 115.00),\n",
       "  (955, 115.00),\n",
       "  (979, 114.00),\n",
       "  (679, 112.00),\n",
       "  (721, 109.00),\n",
       "  (39, 108.00),\n",
       "  (565, 108.00),\n",
       "  (741, 106.00),\n",
       "  (491, 105.00),\n",
       "  (562, 103.00),\n",
       "  (839, 102.00),\n",
       "  (109, 101.00),\n",
       "  (162, 101.00),\n",
       "  (549, 99.00),\n",
       "  (46, 97.00),\n",
       "  (48, 96.00),\n",
       "  (84, 95.00),\n",
       "  (750, 95.00),\n",
       "  (82, 94.00),\n",
       "  (973, 94.00),\n",
       "  (151, 93.00),\n",
       "  (492, 93.00),\n",
       "  (695, 93.00),\n",
       "  (199, 91.00),\n",
       "  (843, 89.00),\n",
       "  (51, 88.00),\n",
       "  (971, 88.00),\n",
       "  (640, 87.00),\n",
       "  (424, 86.00),\n",
       "  (669, 86.00),\n",
       "  (692, 86.00),\n",
       "  (879, 86.00),\n",
       "  (281, 85.00),\n",
       "  (47, 84.00),\n",
       "  (783, 84.00),\n",
       "  (203, 83.00),\n",
       "  (310, 83.00),\n",
       "  (382, 83.00),\n",
       "  (411, 83.00),\n",
       "  (866, 83.00),\n",
       "  (743, 82.00),\n",
       "  (364, 81.00),\n",
       "  (577, 81.00),\n",
       "  (197, 80.00),\n",
       "  (318, 80.00),\n",
       "  (319, 80.00),\n",
       "  (406, 80.00),\n",
       "  (725, 80.00),\n",
       "  (828, 80.00),\n",
       "  (180, 79.00),\n",
       "  (189, 79.00),\n",
       "  (208, 79.00),\n",
       "  (298, 79.00),\n",
       "  (703, 79.00),\n",
       "  (754, 79.00),\n",
       "  (905, 79.00),\n",
       "  (956, 79.00),\n",
       "  (847, 78.00),\n",
       "  (76, 77.00),\n",
       "  (182, 77.00),\n",
       "  (342, 77.00),\n",
       "  (455, 77.00),\n",
       "  (572, 77.00),\n",
       "  (711, 77.00),\n",
       "  (762, 77.00),\n",
       "  (896, 77.00),\n",
       "  (963, 77.00),\n",
       "  (217, 76.00),\n",
       "  (440, 76.00),\n",
       "  (824, 76.00),\n",
       "  (830, 76.00),\n",
       "  (55, 75.00),\n",
       "  (219, 75.00),\n",
       "  (270, 75.00),\n",
       "  (700, 75.00),\n",
       "  (730, 75.00),\n",
       "  (791, 75.00),\n",
       "  (128, 74.00),\n",
       "  (849, 74.00),\n",
       "  (938, 74.00),\n",
       "  (982, 74.00),\n",
       "  (237, 73.00),\n",
       "  (343, 73.00),\n",
       "  (363, 73.00),\n",
       "  (454, 73.00),\n",
       "  (570, 73.00),\n",
       "  (645, 73.00),\n",
       "  (735, 73.00),\n",
       "  (778, 73.00),\n",
       "  (825, 73.00),\n",
       "  (222, 72.00),\n",
       "  (304, 72.00),\n",
       "  (436, 72.00),\n",
       "  (483, 72.00),\n",
       "  (800, 72.00),\n",
       "  (826, 72.00),\n",
       "  (61, 71.00),\n",
       "  (74, 71.00),\n",
       "  (471, 71.00),\n",
       "  (472, 71.00),\n",
       "  (805, 71.00),\n",
       "  (806, 71.00),\n",
       "  (916, 71.00),\n",
       "  (30, 70.00),\n",
       "  (496, 70.00),\n",
       "  (716, 70.00),\n",
       "  (23, 69.00),\n",
       "  (341, 69.00),\n",
       "  (425, 69.00),\n",
       "  (775, 69.00),\n",
       "  (808, 69.00),\n",
       "  (878, 69.00),\n",
       "  (556, 68.00),\n",
       "  (620, 68.00),\n",
       "  (845, 68.00),\n",
       "  (184, 67.00),\n",
       "  (192, 67.00),\n",
       "  (195, 67.00),\n",
       "  (300, 67.00),\n",
       "  (361, 67.00),\n",
       "  (671, 67.00),\n",
       "  (887, 67.00),\n",
       "  (912, 67.00),\n",
       "  (37, 66.00),\n",
       "  (178, 66.00),\n",
       "  (313, 66.00),\n",
       "  (458, 66.00),\n",
       "  (654, 66.00),\n",
       "  (772, 66.00),\n",
       "  (820, 66.00),\n",
       "  (863, 66.00),\n",
       "  (870, 66.00),\n",
       "  (77, 65.00),\n",
       "  (124, 65.00),\n",
       "  (423, 65.00),\n",
       "  (655, 65.00),\n",
       "  (949, 65.00),\n",
       "  (988, 65.00),\n",
       "  (6, 64.00),\n",
       "  (211, 64.00),\n",
       "  (506, 64.00),\n",
       "  (688, 64.00),\n",
       "  (926, 64.00),\n",
       "  (972, 64.00),\n",
       "  (234, 63.00),\n",
       "  (272, 63.00),\n",
       "  (293, 63.00),\n",
       "  (481, 63.00),\n",
       "  (595, 63.00),\n",
       "  (852, 63.00),\n",
       "  (871, 63.00),\n",
       "  (895, 63.00),\n",
       "  (953, 63.00),\n",
       "  (975, 63.00),\n",
       "  (992, 63.00),\n",
       "  (90, 62.00),\n",
       "  (463, 62.00),\n",
       "  (505, 62.00),\n",
       "  (561, 62.00),\n",
       "  (697, 62.00),\n",
       "  (809, 62.00),\n",
       "  (864, 62.00),\n",
       "  (944, 62.00),\n",
       "  (987, 62.00),\n",
       "  (97, 61.00),\n",
       "  (99, 61.00),\n",
       "  (118, 61.00),\n",
       "  (125, 61.00),\n",
       "  (135, 61.00),\n",
       "  (155, 61.00),\n",
       "  (238, 61.00),\n",
       "  (292, 61.00),\n",
       "  (331, 61.00),\n",
       "  (468, 61.00),\n",
       "  (474, 61.00),\n",
       "  (597, 61.00),\n",
       "  (788, 61.00),\n",
       "  (834, 61.00),\n",
       "  (913, 61.00),\n",
       "  (50, 60.00),\n",
       "  (311, 60.00),\n",
       "  (401, 60.00),\n",
       "  (404, 60.00),\n",
       "  (420, 60.00),\n",
       "  (457, 60.00),\n",
       "  (476, 60.00),\n",
       "  (515, 60.00),\n",
       "  (532, 60.00),\n",
       "  (586, 60.00),\n",
       "  (594, 60.00),\n",
       "  (635, 60.00),\n",
       "  (649, 60.00),\n",
       "  (781, 60.00),\n",
       "  (850, 60.00),\n",
       "  (880, 60.00),\n",
       "  (892, 60.00),\n",
       "  (937, 60.00),\n",
       "  (946, 60.00),\n",
       "  (33, 59.00),\n",
       "  (129, 59.00),\n",
       "  (263, 59.00),\n",
       "  (372, 59.00),\n",
       "  (519, 59.00),\n",
       "  (564, 59.00),\n",
       "  (607, 59.00),\n",
       "  (724, 59.00),\n",
       "  (766, 59.00),\n",
       "  (770, 59.00),\n",
       "  (875, 59.00),\n",
       "  (957, 59.00),\n",
       "  (85, 58.00),\n",
       "  (119, 58.00),\n",
       "  (161, 58.00),\n",
       "  (181, 58.00),\n",
       "  (249, 58.00),\n",
       "  (259, 58.00),\n",
       "  (280, 58.00),\n",
       "  (348, 58.00),\n",
       "  (383, 58.00),\n",
       "  (441, 58.00),\n",
       "  (522, 58.00),\n",
       "  (539, 58.00),\n",
       "  (552, 58.00),\n",
       "  (563, 58.00),\n",
       "  (601, 58.00),\n",
       "  (619, 58.00),\n",
       "  (696, 58.00),\n",
       "  (748, 58.00),\n",
       "  (816, 58.00),\n",
       "  (21, 57.00),\n",
       "  (69, 57.00),\n",
       "  (89, 57.00),\n",
       "  (113, 57.00),\n",
       "  (115, 57.00),\n",
       "  (218, 57.00),\n",
       "  (232, 57.00),\n",
       "  (250, 57.00),\n",
       "  (284, 57.00),\n",
       "  (316, 57.00),\n",
       "  (407, 57.00),\n",
       "  (428, 57.00),\n",
       "  (488, 57.00),\n",
       "  (581, 57.00),\n",
       "  (603, 57.00),\n",
       "  (614, 57.00),\n",
       "  (626, 57.00),\n",
       "  (698, 57.00),\n",
       "  (774, 57.00),\n",
       "  (777, 57.00),\n",
       "  (784, 57.00),\n",
       "  (790, 57.00),\n",
       "  (842, 57.00),\n",
       "  (962, 57.00),\n",
       "  (8, 56.00),\n",
       "  (31, 56.00),\n",
       "  (57, 56.00),\n",
       "  (60, 56.00),\n",
       "  (171, 56.00),\n",
       "  (225, 56.00),\n",
       "  (275, 56.00),\n",
       "  (317, 56.00),\n",
       "  (334, 56.00),\n",
       "  (391, 56.00),\n",
       "  (443, 56.00),\n",
       "  (490, 56.00),\n",
       "  (527, 56.00),\n",
       "  (569, 56.00),\n",
       "  (593, 56.00),\n",
       "  (609, 56.00),\n",
       "  (642, 56.00),\n",
       "  (792, 56.00),\n",
       "  (819, 56.00),\n",
       "  (857, 56.00),\n",
       "  (924, 56.00),\n",
       "  (952, 56.00),\n",
       "  (24, 55.00),\n",
       "  (25, 55.00),\n",
       "  (67, 55.00),\n",
       "  (229, 55.00),\n",
       "  (231, 55.00),\n",
       "  (283, 55.00),\n",
       "  (291, 55.00),\n",
       "  (308, 55.00),\n",
       "  (328, 55.00),\n",
       "  (386, 55.00),\n",
       "  (396, 55.00),\n",
       "  (410, 55.00),\n",
       "  (509, 55.00),\n",
       "  (512, 55.00),\n",
       "  (612, 55.00),\n",
       "  (661, 55.00),\n",
       "  (822, 55.00),\n",
       "  (858, 55.00),\n",
       "  (884, 55.00),\n",
       "  (950, 55.00),\n",
       "  (985, 55.00),\n",
       "  (986, 55.00),\n",
       "  (991, 55.00),\n",
       "  (88, 54.00),\n",
       "  (159, 54.00),\n",
       "  (170, 54.00),\n",
       "  (206, 54.00),\n",
       "  (228, 54.00),\n",
       "  (241, 54.00),\n",
       "  (269, 54.00),\n",
       "  (276, 54.00),\n",
       "  (285, 54.00),\n",
       "  (327, 54.00),\n",
       "  (487, 54.00),\n",
       "  (547, 54.00),\n",
       "  (657, 54.00),\n",
       "  (709, 54.00),\n",
       "  (768, 54.00),\n",
       "  (780, 54.00),\n",
       "  (801, 54.00),\n",
       "  (832, 54.00),\n",
       "  (855, 54.00),\n",
       "  (936, 54.00),\n",
       "  (990, 54.00),\n",
       "  (995, 54.00),\n",
       "  (3, 53.00),\n",
       "  (35, 53.00),\n",
       "  (58, 53.00),\n",
       "  (70, 53.00),\n",
       "  (104, 53.00),\n",
       "  (138, 53.00),\n",
       "  (177, 53.00),\n",
       "  (251, 53.00),\n",
       "  (254, 53.00),\n",
       "  (274, 53.00),\n",
       "  (307, 53.00),\n",
       "  (367, 53.00),\n",
       "  (444, 53.00),\n",
       "  (452, 53.00),\n",
       "  (477, 53.00),\n",
       "  (508, 53.00),\n",
       "  (524, 53.00),\n",
       "  (526, 53.00),\n",
       "  (528, 53.00),\n",
       "  (533, 53.00),\n",
       "  (641, 53.00),\n",
       "  (653, 53.00),\n",
       "  (665, 53.00),\n",
       "  (668, 53.00),\n",
       "  (739, 53.00),\n",
       "  (818, 53.00),\n",
       "  (835, 53.00),\n",
       "  (888, 53.00),\n",
       "  (903, 53.00),\n",
       "  (922, 53.00),\n",
       "  (1, 52.00),\n",
       "  (92, 52.00),\n",
       "  (164, 52.00),\n",
       "  (176, 52.00),\n",
       "  (216, 52.00),\n",
       "  (239, 52.00),\n",
       "  (431, 52.00),\n",
       "  (448, 52.00),\n",
       "  (478, 52.00),\n",
       "  (701, 52.00),\n",
       "  (738, 52.00),\n",
       "  (752, 52.00),\n",
       "  (779, 52.00),\n",
       "  (787, 52.00),\n",
       "  (829, 52.00),\n",
       "  (833, 52.00),\n",
       "  (840, 52.00),\n",
       "  (877, 52.00),\n",
       "  (917, 52.00),\n",
       "  (939, 52.00),\n",
       "  (943, 52.00),\n",
       "  (133, 51.00),\n",
       "  (160, 51.00),\n",
       "  (188, 51.00),\n",
       "  (196, 51.00),\n",
       "  (212, 51.00),\n",
       "  (221, 51.00),\n",
       "  (286, 51.00),\n",
       "  (362, 51.00),\n",
       "  (377, 51.00),\n",
       "  (416, 51.00),\n",
       "  (419, 51.00),\n",
       "  (514, 51.00),\n",
       "  (545, 51.00),\n",
       "  (592, 51.00),\n",
       "  (636, 51.00),\n",
       "  (637, 51.00),\n",
       "  (746, 51.00),\n",
       "  (757, 51.00),\n",
       "  (764, 51.00),\n",
       "  (776, 51.00),\n",
       "  (902, 51.00),\n",
       "  (927, 51.00),\n",
       "  (13, 50.00),\n",
       "  (36, 50.00),\n",
       "  (102, 50.00),\n",
       "  (114, 50.00),\n",
       "  (126, 50.00),\n",
       "  (261, 50.00),\n",
       "  (277, 50.00),\n",
       "  (289, 50.00),\n",
       "  (294, 50.00),\n",
       "  (295, 50.00),\n",
       "  (301, 50.00),\n",
       "  (352, 50.00),\n",
       "  (358, 50.00),\n",
       "  (373, 50.00),\n",
       "  (449, 50.00),\n",
       "  (467, 50.00),\n",
       "  (604, 50.00),\n",
       "  (608, 50.00),\n",
       "  (618, 50.00),\n",
       "  (639, 50.00),\n",
       "  (659, 50.00),\n",
       "  (685, 50.00),\n",
       "  (765, 50.00),\n",
       "  (997, 50.00),\n",
       "  (0, 49.00),\n",
       "  (9, 49.00),\n",
       "  (123, 49.00),\n",
       "  (172, 49.00),\n",
       "  (267, 49.00),\n",
       "  (325, 49.00),\n",
       "  (375, 49.00),\n",
       "  (376, 49.00),\n",
       "  (378, 49.00),\n",
       "  (387, 49.00),\n",
       "  (388, 49.00),\n",
       "  (398, 49.00),\n",
       "  (523, 49.00),\n",
       "  (535, 49.00),\n",
       "  (541, 49.00),\n",
       "  (566, 49.00),\n",
       "  (583, 49.00),\n",
       "  (602, 49.00),\n",
       "  (667, 49.00),\n",
       "  (704, 49.00),\n",
       "  (763, 49.00),\n",
       "  (771, 49.00),\n",
       "  (874, 49.00),\n",
       "  (11, 48.00),\n",
       "  (12, 48.00),\n",
       "  (14, 48.00),\n",
       "  (15, 48.00),\n",
       "  (16, 48.00),\n",
       "  (18, 48.00),\n",
       "  (41, 48.00),\n",
       "  (71, 48.00),\n",
       "  (78, 48.00),\n",
       "  (134, 48.00),\n",
       "  (137, 48.00),\n",
       "  (141, 48.00),\n",
       "  (156, 48.00),\n",
       "  (194, 48.00),\n",
       "  (209, 48.00),\n",
       "  (214, 48.00),\n",
       "  (255, 48.00),\n",
       "  (264, 48.00),\n",
       "  (279, 48.00),\n",
       "  (288, 48.00),\n",
       "  (290, 48.00),\n",
       "  (312, 48.00),\n",
       "  (336, 48.00),\n",
       "  (340, 48.00),\n",
       "  (349, 48.00),\n",
       "  (354, 48.00),\n",
       "  (413, 48.00),\n",
       "  (451, 48.00),\n",
       "  (517, 48.00),\n",
       "  (628, 48.00),\n",
       "  (683, 48.00),\n",
       "  (684, 48.00),\n",
       "  (758, 48.00),\n",
       "  (797, 48.00),\n",
       "  (865, 48.00),\n",
       "  (872, 48.00),\n",
       "  (881, 48.00),\n",
       "  (890, 48.00),\n",
       "  (891, 48.00),\n",
       "  (915, 48.00),\n",
       "  (951, 48.00),\n",
       "  (994, 48.00),\n",
       "  (40, 47.00),\n",
       "  (53, 47.00),\n",
       "  (100, 47.00),\n",
       "  (101, 47.00),\n",
       "  (110, 47.00),\n",
       "  (130, 47.00),\n",
       "  (136, 47.00),\n",
       "  (227, 47.00),\n",
       "  (243, 47.00),\n",
       "  (253, 47.00),\n",
       "  (256, 47.00),\n",
       "  (265, 47.00),\n",
       "  (321, 47.00),\n",
       "  (333, 47.00),\n",
       "  (335, 47.00),\n",
       "  (337, 47.00),\n",
       "  (350, 47.00),\n",
       "  (351, 47.00),\n",
       "  (395, 47.00),\n",
       "  (426, 47.00),\n",
       "  (503, 47.00),\n",
       "  (518, 47.00),\n",
       "  (560, 47.00),\n",
       "  (576, 47.00),\n",
       "  (606, 47.00),\n",
       "  (707, 47.00),\n",
       "  (732, 47.00),\n",
       "  (759, 47.00),\n",
       "  (769, 47.00),\n",
       "  (807, 47.00),\n",
       "  (886, 47.00),\n",
       "  (5, 46.00),\n",
       "  (22, 46.00),\n",
       "  (56, 46.00),\n",
       "  (63, 46.00),\n",
       "  (65, 46.00),\n",
       "  (72, 46.00),\n",
       "  (79, 46.00),\n",
       "  (87, 46.00),\n",
       "  (95, 46.00),\n",
       "  (108, 46.00),\n",
       "  (149, 46.00),\n",
       "  (157, 46.00),\n",
       "  (201, 46.00),\n",
       "  (215, 46.00),\n",
       "  (266, 46.00),\n",
       "  (320, 46.00),\n",
       "  (323, 46.00),\n",
       "  (324, 46.00),\n",
       "  (366, 46.00),\n",
       "  (370, 46.00),\n",
       "  (397, 46.00),\n",
       "  (422, 46.00),\n",
       "  (432, 46.00),\n",
       "  (433, 46.00),\n",
       "  (434, 46.00),\n",
       "  (530, 46.00),\n",
       "  (616, 46.00),\n",
       "  (658, 46.00),\n",
       "  (664, 46.00),\n",
       "  (751, 46.00),\n",
       "  (753, 46.00),\n",
       "  (796, 46.00),\n",
       "  (823, 46.00),\n",
       "  (848, 46.00),\n",
       "  (867, 46.00),\n",
       "  (882, 46.00),\n",
       "  (918, 46.00),\n",
       "  (983, 46.00),\n",
       "  (989, 46.00),\n",
       "  (993, 46.00),\n",
       "  (2, 45.00),\n",
       "  (28, 45.00),\n",
       "  (66, 45.00),\n",
       "  (96, 45.00),\n",
       "  (105, 45.00),\n",
       "  (121, 45.00),\n",
       "  (139, 45.00),\n",
       "  (144, 45.00),\n",
       "  (169, 45.00),\n",
       "  (191, 45.00),\n",
       "  (247, 45.00),\n",
       "  (273, 45.00),\n",
       "  (299, 45.00),\n",
       "  (326, 45.00),\n",
       "  (332, 45.00),\n",
       "  (339, 45.00),\n",
       "  (344, 45.00),\n",
       "  (355, 45.00),\n",
       "  (365, 45.00),\n",
       "  (389, 45.00),\n",
       "  (392, 45.00),\n",
       "  (445, 45.00),\n",
       "  (571, 45.00),\n",
       "  (599, 45.00),\n",
       "  (625, 45.00),\n",
       "  (674, 45.00),\n",
       "  (734, 45.00),\n",
       "  (755, 45.00),\n",
       "  (817, 45.00),\n",
       "  (853, 45.00),\n",
       "  (900, 45.00),\n",
       "  (910, 45.00),\n",
       "  (75, 44.00),\n",
       "  (131, 44.00),\n",
       "  (186, 44.00),\n",
       "  (223, 44.00),\n",
       "  (244, 44.00),\n",
       "  (252, 44.00),\n",
       "  (405, 44.00),\n",
       "  (412, 44.00),\n",
       "  (417, 44.00),\n",
       "  (473, 44.00),\n",
       "  (475, 44.00),\n",
       "  (486, 44.00),\n",
       "  (579, 44.00),\n",
       "  (613, 44.00),\n",
       "  (682, 44.00),\n",
       "  (702, 44.00),\n",
       "  (706, 44.00),\n",
       "  (727, 44.00),\n",
       "  (821, 44.00),\n",
       "  (941, 44.00),\n",
       "  (968, 44.00),\n",
       "  (984, 44.00),\n",
       "  (44, 43.00),\n",
       "  (83, 43.00),\n",
       "  (107, 43.00),\n",
       "  (142, 43.00),\n",
       "  (198, 43.00),\n",
       "  (268, 43.00),\n",
       "  (330, 43.00),\n",
       "  (353, 43.00),\n",
       "  (357, 43.00),\n",
       "  (360, 43.00),\n",
       "  (381, 43.00),\n",
       "  (384, 43.00),\n",
       "  (414, 43.00),\n",
       "  (495, 43.00),\n",
       "  (537, 43.00),\n",
       "  (542, 43.00),\n",
       "  (717, 43.00),\n",
       "  (782, 43.00),\n",
       "  (844, 43.00),\n",
       "  (873, 43.00),\n",
       "  (907, 43.00),\n",
       "  (920, 43.00),\n",
       "  (959, 43.00),\n",
       "  (998, 43.00),\n",
       "  (7, 42.00),\n",
       "  (17, 42.00),\n",
       "  (19, 42.00),\n",
       "  (42, 42.00),\n",
       "  (80, 42.00),\n",
       "  (93, 42.00),\n",
       "  (112, 42.00),\n",
       "  (132, 42.00),\n",
       "  (154, 42.00),\n",
       "  (174, 42.00),\n",
       "  (193, 42.00),\n",
       "  (245, 42.00),\n",
       "  (260, 42.00),\n",
       "  (306, 42.00),\n",
       "  (309, 42.00),\n",
       "  (421, 42.00),\n",
       "  (429, 42.00),\n",
       "  (437, 42.00),\n",
       "  (447, 42.00),\n",
       "  (464, 42.00),\n",
       "  (485, 42.00),\n",
       "  (507, 42.00),\n",
       "  (510, 42.00),\n",
       "  (538, 42.00),\n",
       "  (575, 42.00),\n",
       "  (633, 42.00),\n",
       "  (656, 42.00),\n",
       "  (666, 42.00),\n",
       "  (690, 42.00),\n",
       "  (710, 42.00),\n",
       "  (799, 42.00),\n",
       "  (889, 42.00),\n",
       "  (894, 42.00),\n",
       "  (919, 42.00),\n",
       "  (932, 42.00),\n",
       "  (933, 42.00),\n",
       "  (981, 42.00),\n",
       "  (999, 42.00),\n",
       "  (117, 41.00),\n",
       "  (127, 41.00),\n",
       "  (153, 41.00),\n",
       "  (190, 41.00),\n",
       "  (224, 41.00),\n",
       "  (258, 41.00),\n",
       "  (278, 41.00),\n",
       "  (322, 41.00),\n",
       "  (390, 41.00),\n",
       "  (393, 41.00),\n",
       "  (430, 41.00),\n",
       "  (574, 41.00),\n",
       "  (712, 41.00),\n",
       "  (723, 41.00),\n",
       "  (795, 41.00),\n",
       "  (854, 41.00),\n",
       "  (898, 41.00),\n",
       "  (929, 41.00),\n",
       "  (27, 40.00),\n",
       "  (230, 40.00),\n",
       "  (242, 40.00),\n",
       "  (257, 40.00),\n",
       "  (287, 40.00),\n",
       "  (418, 40.00),\n",
       "  (500, 40.00),\n",
       "  (546, 40.00),\n",
       "  (548, 40.00),\n",
       "  (558, 40.00),\n",
       "  (573, 40.00),\n",
       "  (584, 40.00),\n",
       "  (600, 40.00),\n",
       "  (610, 40.00),\n",
       "  (713, 40.00),\n",
       "  (736, 40.00),\n",
       "  (749, 40.00),\n",
       "  (837, 40.00),\n",
       "  (862, 40.00),\n",
       "  (921, 40.00),\n",
       "  (925, 40.00),\n",
       "  (996, 40.00),\n",
       "  (173, 39.00),\n",
       "  (183, 39.00),\n",
       "  (262, 39.00),\n",
       "  (296, 39.00),\n",
       "  (297, 39.00),\n",
       "  (302, 39.00),\n",
       "  (329, 39.00),\n",
       "  (368, 39.00),\n",
       "  (435, 39.00),\n",
       "  (470, 39.00),\n",
       "  (480, 39.00),\n",
       "  (513, 39.00),\n",
       "  (520, 39.00),\n",
       "  (529, 39.00),\n",
       "  (553, 39.00),\n",
       "  (670, 39.00),\n",
       "  (672, 39.00),\n",
       "  (677, 39.00),\n",
       "  (694, 39.00),\n",
       "  (722, 39.00),\n",
       "  (726, 39.00),\n",
       "  (756, 39.00),\n",
       "  (812, 39.00),\n",
       "  (945, 39.00),\n",
       "  (43, 38.00),\n",
       "  (91, 38.00),\n",
       "  (210, 38.00),\n",
       "  (235, 38.00),\n",
       "  (236, 38.00),\n",
       "  (305, 38.00),\n",
       "  (314, 38.00),\n",
       "  (439, 38.00),\n",
       "  (442, 38.00),\n",
       "  (456, 38.00),\n",
       "  (462, 38.00),\n",
       "  (466, 38.00),\n",
       "  (501, 38.00),\n",
       "  (540, 38.00),\n",
       "  (555, 38.00),\n",
       "  (559, 38.00),\n",
       "  (643, 38.00),\n",
       "  (687, 38.00),\n",
       "  (699, 38.00),\n",
       "  (708, 38.00),\n",
       "  (719, 38.00),\n",
       "  (38, 37.00),\n",
       "  (52, 37.00),\n",
       "  (111, 37.00),\n",
       "  (140, 37.00),\n",
       "  (175, 37.00),\n",
       "  (204, 37.00),\n",
       "  (248, 37.00),\n",
       "  (338, 37.00),\n",
       "  (346, 37.00),\n",
       "  (379, 37.00),\n",
       "  (450, 37.00),\n",
       "  (494, 37.00),\n",
       "  (554, 37.00),\n",
       "  (615, 37.00),\n",
       "  (630, 37.00),\n",
       "  (650, 37.00),\n",
       "  (678, 37.00),\n",
       "  (693, 37.00),\n",
       "  (714, 37.00),\n",
       "  (761, 37.00),\n",
       "  (814, 37.00),\n",
       "  (831, 37.00),\n",
       "  (934, 37.00),\n",
       "  (10, 36.00),\n",
       "  (26, 36.00),\n",
       "  (45, 36.00),\n",
       "  (62, 36.00),\n",
       "  (98, 36.00),\n",
       "  (120, 36.00),\n",
       "  (145, 36.00),\n",
       "  (200, 36.00),\n",
       "  (207, 36.00),\n",
       "  (408, 36.00),\n",
       "  (427, 36.00),\n",
       "  (453, 36.00),\n",
       "  (511, 36.00),\n",
       "  (588, 36.00),\n",
       "  (827, 36.00),\n",
       "  (851, 36.00),\n",
       "  (20, 35.00),\n",
       "  (143, 35.00),\n",
       "  (148, 35.00),\n",
       "  (220, 35.00),\n",
       "  (347, 35.00),\n",
       "  (374, 35.00),\n",
       "  (380, 35.00),\n",
       "  (409, 35.00),\n",
       "  (415, 35.00),\n",
       "  (543, 35.00),\n",
       "  (605, 35.00),\n",
       "  (627, 35.00),\n",
       "  (745, 35.00),\n",
       "  (760, 35.00),\n",
       "  (793, 35.00),\n",
       "  (798, 35.00),\n",
       "  (846, 35.00),\n",
       "  (958, 35.00),\n",
       "  (73, 34.00),\n",
       "  (226, 34.00),\n",
       "  (399, 34.00),\n",
       "  (465, 34.00),\n",
       "  (720, 34.00),\n",
       "  (786, 34.00),\n",
       "  (802, 34.00),\n",
       "  (861, 34.00),\n",
       "  (897, 34.00),\n",
       "  (928, 34.00),\n",
       "  (81, 33.00),\n",
       "  (152, 33.00),\n",
       "  (158, 33.00),\n",
       "  (205, 33.00),\n",
       "  (213, 33.00),\n",
       "  (385, 33.00),\n",
       "  (647, 33.00),\n",
       "  (803, 33.00),\n",
       "  (859, 33.00),\n",
       "  (964, 33.00),\n",
       "  (59, 32.00),\n",
       "  (86, 32.00),\n",
       "  (146, 32.00),\n",
       "  (356, 32.00),\n",
       "  (359, 32.00),\n",
       "  (371, 32.00),\n",
       "  (402, 32.00),\n",
       "  (629, 32.00),\n",
       "  (883, 32.00),\n",
       "  (947, 32.00),\n",
       "  (980, 32.00),\n",
       "  (106, 31.00),\n",
       "  (179, 31.00),\n",
       "  (233, 31.00),\n",
       "  (403, 31.00),\n",
       "  (459, 31.00),\n",
       "  (502, 31.00),\n",
       "  (521, 31.00),\n",
       "  (744, 31.00),\n",
       "  (914, 31.00),\n",
       "  (923, 31.00),\n",
       "  (954, 31.00),\n",
       "  (64, 30.00),\n",
       "  (166, 30.00),\n",
       "  (202, 30.00),\n",
       "  (345, 30.00),\n",
       "  (469, 30.00),\n",
       "  (484, 30.00),\n",
       "  (531, 30.00),\n",
       "  (551, 30.00),\n",
       "  (568, 30.00),\n",
       "  (578, 30.00),\n",
       "  (589, 30.00),\n",
       "  (634, 30.00),\n",
       "  (663, 30.00),\n",
       "  (705, 30.00),\n",
       "  (948, 30.00),\n",
       "  (187, 29.00),\n",
       "  (461, 29.00),\n",
       "  (498, 29.00),\n",
       "  (557, 29.00),\n",
       "  (585, 29.00),\n",
       "  (617, 29.00),\n",
       "  (860, 29.00),\n",
       "  (966, 29.00),\n",
       "  (4, 28.00),\n",
       "  (29, 28.00),\n",
       "  (122, 28.00),\n",
       "  (596, 28.00),\n",
       "  (632, 28.00),\n",
       "  (676, 28.00),\n",
       "  (691, 28.00),\n",
       "  (733, 28.00),\n",
       "  (747, 28.00),\n",
       "  (49, 27.00),\n",
       "  (54, 27.00),\n",
       "  (147, 27.00),\n",
       "  (150, 27.00),\n",
       "  (271, 27.00),\n",
       "  (303, 27.00),\n",
       "  (369, 27.00),\n",
       "  (567, 27.00),\n",
       "  (587, 27.00),\n",
       "  (624, 27.00),\n",
       "  (644, 27.00),\n",
       "  (660, 27.00),\n",
       "  (718, 27.00),\n",
       "  (767, 27.00),\n",
       "  (789, 27.00),\n",
       "  (32, 26.00),\n",
       "  (516, 26.00),\n",
       "  (544, 26.00),\n",
       "  (631, 26.00),\n",
       "  (731, 26.00),\n",
       "  (804, 26.00),\n",
       "  (869, 26.00),\n",
       "  (965, 26.00),\n",
       "  (163, 25.00),\n",
       "  (168, 25.00),\n",
       "  (394, 25.00),\n",
       "  (479, 25.00),\n",
       "  (482, 25.00),\n",
       "  (536, 25.00),\n",
       "  (590, 25.00),\n",
       "  (623, 25.00),\n",
       "  (686, 25.00),\n",
       "  (838, 25.00),\n",
       "  (908, 25.00),\n",
       "  (165, 24.00),\n",
       "  (785, 24.00),\n",
       "  (901, 24.00),\n",
       "  (931, 24.00),\n",
       "  (942, 24.00),\n",
       "  (185, 23.00),\n",
       "  (240, 23.00),\n",
       "  (499, 23.00),\n",
       "  (638, 23.00),\n",
       "  (680, 23.00),\n",
       "  (876, 23.00),\n",
       "  (974, 23.00),\n",
       "  (68, 22.00),\n",
       "  (525, 22.00),\n",
       "  (740, 22.00),\n",
       "  (773, 22.00),\n",
       "  (811, 22.00),\n",
       "  (856, 22.00),\n",
       "  (909, 22.00),\n",
       "  (911, 22.00),\n",
       "  (246, 21.00),\n",
       "  (438, 21.00),\n",
       "  (675, 21.00),\n",
       "  (885, 21.00),\n",
       "  (315, 20.00),\n",
       "  (400, 20.00),\n",
       "  (662, 20.00),\n",
       "  (715, 20.00),\n",
       "  (977, 20.00),\n",
       "  (598, 19.00),\n",
       "  (729, 19.00),\n",
       "  (622, 18.00),\n",
       "  (651, 18.00),\n",
       "  (103, 17.00),\n",
       "  (550, 17.00),\n",
       "  (648, 17.00),\n",
       "  (728, 17.00),\n",
       "  (841, 17.00),\n",
       "  (976, 17.00),\n",
       "  (460, 16.00),\n",
       "  (504, 16.00),\n",
       "  (836, 16.00),\n",
       "  (930, 16.00),\n",
       "  (970, 16.00),\n",
       "  (282, 15.00),\n",
       "  (446, 15.00),\n",
       "  (493, 15.00),\n",
       "  (534, 15.00),\n",
       "  (906, 15.00),\n",
       "  (813, 14.00),\n",
       "  (969, 13.00),\n",
       "  (742, 12.00),\n",
       "  (940, 12.00),\n",
       "  (34, 11.00),\n",
       "  (167, 11.00),\n",
       "  (689, 11.00),\n",
       "  (967, 11.00),\n",
       "  (978, 11.00),\n",
       "  (681, 9.00),\n",
       "  (899, 9.00),\n",
       "  (960, 9.00),\n",
       "  (673, 8.00),\n",
       "  (810, 7.00),\n",
       "  (935, 6.00),\n",
       "  (961, 2.00)])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = targeted_diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch_no 0\n",
      "at batch_no 100\n",
      "at batch_no 200\n",
      "at batch_no 300\n",
      "at batch_no 400\n",
      "at batch_no 500\n",
      "at batch_no 600\n",
      "at batch_no 700\n",
      "at batch_no 800\n",
      "at batch_no 900\n",
      "at batch_no 1000\n",
      "at batch_no 1100\n",
      "at batch_no 1200\n",
      "at batch_no 1300\n",
      "at batch_no 1400\n",
      "at batch_no 1500\n",
      "at batch_no 1600\n",
      "at batch_no 1700\n",
      "at batch_no 1800\n",
      "at batch_no 1900\n",
      "at batch_no 2000\n",
      "at batch_no 2100\n",
      "at batch_no 2200\n",
      "at batch_no 2300\n",
      "at batch_no 2400\n",
      "at batch_no 2500\n",
      "at batch_no 2600\n",
      "at batch_no 2700\n",
      "at batch_no 2800\n",
      "at batch_no 2900\n",
      "at batch_no 3000\n",
      "at batch_no 3100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(885,\n",
       " [(815, 239.00),\n",
       "  (652, 181.00),\n",
       "  (646, 168.00),\n",
       "  (580, 163.00),\n",
       "  (611, 155.00),\n",
       "  (591, 148.00),\n",
       "  (737, 143.00),\n",
       "  (489, 142.00),\n",
       "  (621, 139.00),\n",
       "  (904, 134.00),\n",
       "  (794, 130.00),\n",
       "  (497, 127.00),\n",
       "  (893, 127.00),\n",
       "  (582, 125.00),\n",
       "  (94, 123.00),\n",
       "  (955, 120.00),\n",
       "  (116, 115.00),\n",
       "  (868, 115.00),\n",
       "  (39, 112.00),\n",
       "  (679, 112.00),\n",
       "  (565, 110.00),\n",
       "  (162, 109.00),\n",
       "  (491, 108.00),\n",
       "  (721, 108.00),\n",
       "  (979, 108.00),\n",
       "  (741, 105.00),\n",
       "  (839, 105.00),\n",
       "  (109, 104.00),\n",
       "  (562, 104.00),\n",
       "  (84, 103.00),\n",
       "  (549, 102.00),\n",
       "  (48, 99.00),\n",
       "  (82, 99.00),\n",
       "  (492, 99.00),\n",
       "  (51, 98.00),\n",
       "  (973, 96.00),\n",
       "  (199, 95.00),\n",
       "  (750, 95.00),\n",
       "  (46, 93.00),\n",
       "  (640, 92.00),\n",
       "  (695, 92.00),\n",
       "  (151, 88.00),\n",
       "  (971, 88.00),\n",
       "  (783, 87.00),\n",
       "  (843, 87.00),\n",
       "  (203, 86.00),\n",
       "  (669, 86.00),\n",
       "  (692, 86.00),\n",
       "  (424, 84.00),\n",
       "  (956, 84.00),\n",
       "  (281, 83.00),\n",
       "  (577, 83.00),\n",
       "  (828, 82.00),\n",
       "  (866, 82.00),\n",
       "  (47, 81.00),\n",
       "  (310, 81.00),\n",
       "  (743, 81.00),\n",
       "  (847, 81.00),\n",
       "  (61, 80.00),\n",
       "  (208, 80.00),\n",
       "  (703, 80.00),\n",
       "  (180, 79.00),\n",
       "  (182, 79.00),\n",
       "  (382, 79.00),\n",
       "  (830, 79.00),\n",
       "  (189, 78.00),\n",
       "  (219, 78.00),\n",
       "  (319, 78.00),\n",
       "  (343, 78.00),\n",
       "  (411, 78.00),\n",
       "  (725, 78.00),\n",
       "  (879, 78.00),\n",
       "  (342, 77.00),\n",
       "  (406, 77.00),\n",
       "  (440, 77.00),\n",
       "  (454, 77.00),\n",
       "  (778, 77.00),\n",
       "  (128, 76.00),\n",
       "  (197, 76.00),\n",
       "  (270, 76.00),\n",
       "  (298, 76.00),\n",
       "  (570, 76.00),\n",
       "  (824, 76.00),\n",
       "  (982, 76.00),\n",
       "  (711, 75.00),\n",
       "  (754, 75.00),\n",
       "  (762, 75.00),\n",
       "  (791, 75.00),\n",
       "  (963, 75.00),\n",
       "  (55, 74.00),\n",
       "  (217, 74.00),\n",
       "  (237, 74.00),\n",
       "  (364, 74.00),\n",
       "  (455, 74.00),\n",
       "  (572, 74.00),\n",
       "  (671, 74.00),\n",
       "  (805, 74.00),\n",
       "  (905, 74.00),\n",
       "  (318, 73.00),\n",
       "  (436, 73.00),\n",
       "  (735, 73.00),\n",
       "  (30, 72.00),\n",
       "  (304, 72.00),\n",
       "  (363, 72.00),\n",
       "  (472, 72.00),\n",
       "  (645, 72.00),\n",
       "  (730, 72.00),\n",
       "  (800, 72.00),\n",
       "  (845, 72.00),\n",
       "  (76, 71.00),\n",
       "  (483, 71.00),\n",
       "  (496, 71.00),\n",
       "  (716, 71.00),\n",
       "  (878, 71.00),\n",
       "  (938, 71.00),\n",
       "  (23, 70.00),\n",
       "  (50, 70.00),\n",
       "  (313, 70.00),\n",
       "  (471, 70.00),\n",
       "  (654, 70.00),\n",
       "  (700, 70.00),\n",
       "  (806, 70.00),\n",
       "  (826, 70.00),\n",
       "  (192, 69.00),\n",
       "  (820, 69.00),\n",
       "  (825, 69.00),\n",
       "  (896, 69.00),\n",
       "  (195, 68.00),\n",
       "  (425, 68.00),\n",
       "  (849, 68.00),\n",
       "  (916, 68.00),\n",
       "  (74, 67.00),\n",
       "  (222, 67.00),\n",
       "  (341, 67.00),\n",
       "  (468, 67.00),\n",
       "  (808, 67.00),\n",
       "  (949, 67.00),\n",
       "  (37, 66.00),\n",
       "  (77, 66.00),\n",
       "  (556, 66.00),\n",
       "  (688, 66.00),\n",
       "  (850, 66.00),\n",
       "  (912, 66.00),\n",
       "  (944, 66.00),\n",
       "  (972, 66.00),\n",
       "  (655, 65.00),\n",
       "  (775, 65.00),\n",
       "  (926, 65.00),\n",
       "  (97, 64.00),\n",
       "  (124, 64.00),\n",
       "  (178, 64.00),\n",
       "  (293, 64.00),\n",
       "  (772, 64.00),\n",
       "  (863, 64.00),\n",
       "  (870, 64.00),\n",
       "  (880, 64.00),\n",
       "  (992, 64.00),\n",
       "  (125, 63.00),\n",
       "  (181, 63.00),\n",
       "  (184, 63.00),\n",
       "  (211, 63.00),\n",
       "  (238, 63.00),\n",
       "  (272, 63.00),\n",
       "  (300, 63.00),\n",
       "  (311, 63.00),\n",
       "  (331, 63.00),\n",
       "  (420, 63.00),\n",
       "  (458, 63.00),\n",
       "  (506, 63.00),\n",
       "  (515, 63.00),\n",
       "  (620, 63.00),\n",
       "  (864, 63.00),\n",
       "  (892, 63.00),\n",
       "  (953, 63.00),\n",
       "  (90, 62.00),\n",
       "  (135, 62.00),\n",
       "  (155, 62.00),\n",
       "  (161, 62.00),\n",
       "  (234, 62.00),\n",
       "  (505, 62.00),\n",
       "  (561, 62.00),\n",
       "  (619, 62.00),\n",
       "  (784, 62.00),\n",
       "  (819, 62.00),\n",
       "  (871, 62.00),\n",
       "  (975, 62.00),\n",
       "  (988, 62.00),\n",
       "  (6, 61.00),\n",
       "  (249, 61.00),\n",
       "  (292, 61.00),\n",
       "  (476, 61.00),\n",
       "  (527, 61.00),\n",
       "  (635, 61.00),\n",
       "  (724, 61.00),\n",
       "  (788, 61.00),\n",
       "  (895, 61.00),\n",
       "  (937, 61.00),\n",
       "  (946, 61.00),\n",
       "  (99, 60.00),\n",
       "  (225, 60.00),\n",
       "  (423, 60.00),\n",
       "  (463, 60.00),\n",
       "  (481, 60.00),\n",
       "  (586, 60.00),\n",
       "  (595, 60.00),\n",
       "  (626, 60.00),\n",
       "  (748, 60.00),\n",
       "  (781, 60.00),\n",
       "  (792, 60.00),\n",
       "  (875, 60.00),\n",
       "  (884, 60.00),\n",
       "  (887, 60.00),\n",
       "  (231, 59.00),\n",
       "  (316, 59.00),\n",
       "  (327, 59.00),\n",
       "  (361, 59.00),\n",
       "  (391, 59.00),\n",
       "  (401, 59.00),\n",
       "  (474, 59.00),\n",
       "  (509, 59.00),\n",
       "  (593, 59.00),\n",
       "  (597, 59.00),\n",
       "  (641, 59.00),\n",
       "  (697, 59.00),\n",
       "  (790, 59.00),\n",
       "  (952, 59.00),\n",
       "  (8, 58.00),\n",
       "  (115, 58.00),\n",
       "  (118, 58.00),\n",
       "  (348, 58.00),\n",
       "  (404, 58.00),\n",
       "  (410, 58.00),\n",
       "  (478, 58.00),\n",
       "  (532, 58.00),\n",
       "  (594, 58.00),\n",
       "  (609, 58.00),\n",
       "  (614, 58.00),\n",
       "  (770, 58.00),\n",
       "  (809, 58.00),\n",
       "  (957, 58.00),\n",
       "  (962, 58.00),\n",
       "  (991, 58.00),\n",
       "  (24, 57.00),\n",
       "  (33, 57.00),\n",
       "  (113, 57.00),\n",
       "  (129, 57.00),\n",
       "  (275, 57.00),\n",
       "  (308, 57.00),\n",
       "  (407, 57.00),\n",
       "  (441, 57.00),\n",
       "  (457, 57.00),\n",
       "  (477, 57.00),\n",
       "  (522, 57.00),\n",
       "  (539, 57.00),\n",
       "  (564, 57.00),\n",
       "  (581, 57.00),\n",
       "  (607, 57.00),\n",
       "  (698, 57.00),\n",
       "  (774, 57.00),\n",
       "  (816, 57.00),\n",
       "  (834, 57.00),\n",
       "  (842, 57.00),\n",
       "  (877, 57.00),\n",
       "  (913, 57.00),\n",
       "  (25, 56.00),\n",
       "  (57, 56.00),\n",
       "  (119, 56.00),\n",
       "  (171, 56.00),\n",
       "  (280, 56.00),\n",
       "  (284, 56.00),\n",
       "  (317, 56.00),\n",
       "  (334, 56.00),\n",
       "  (396, 56.00),\n",
       "  (428, 56.00),\n",
       "  (443, 56.00),\n",
       "  (488, 56.00),\n",
       "  (601, 56.00),\n",
       "  (642, 56.00),\n",
       "  (649, 56.00),\n",
       "  (661, 56.00),\n",
       "  (696, 56.00),\n",
       "  (852, 56.00),\n",
       "  (950, 56.00),\n",
       "  (987, 56.00),\n",
       "  (21, 55.00),\n",
       "  (60, 55.00),\n",
       "  (69, 55.00),\n",
       "  (85, 55.00),\n",
       "  (88, 55.00),\n",
       "  (104, 55.00),\n",
       "  (170, 55.00),\n",
       "  (232, 55.00),\n",
       "  (250, 55.00),\n",
       "  (254, 55.00),\n",
       "  (259, 55.00),\n",
       "  (274, 55.00),\n",
       "  (283, 55.00),\n",
       "  (291, 55.00),\n",
       "  (328, 55.00),\n",
       "  (367, 55.00),\n",
       "  (372, 55.00),\n",
       "  (375, 55.00),\n",
       "  (452, 55.00),\n",
       "  (512, 55.00),\n",
       "  (547, 55.00),\n",
       "  (563, 55.00),\n",
       "  (569, 55.00),\n",
       "  (603, 55.00),\n",
       "  (608, 55.00),\n",
       "  (618, 55.00),\n",
       "  (653, 55.00),\n",
       "  (766, 55.00),\n",
       "  (780, 55.00),\n",
       "  (797, 55.00),\n",
       "  (801, 55.00),\n",
       "  (822, 55.00),\n",
       "  (829, 55.00),\n",
       "  (840, 55.00),\n",
       "  (857, 55.00),\n",
       "  (903, 55.00),\n",
       "  (936, 55.00),\n",
       "  (206, 54.00),\n",
       "  (228, 54.00),\n",
       "  (251, 54.00),\n",
       "  (263, 54.00),\n",
       "  (285, 54.00),\n",
       "  (307, 54.00),\n",
       "  (386, 54.00),\n",
       "  (419, 54.00),\n",
       "  (448, 54.00),\n",
       "  (524, 54.00),\n",
       "  (701, 54.00),\n",
       "  (768, 54.00),\n",
       "  (777, 54.00),\n",
       "  (832, 54.00),\n",
       "  (835, 54.00),\n",
       "  (888, 54.00),\n",
       "  (902, 54.00),\n",
       "  (924, 54.00),\n",
       "  (939, 54.00),\n",
       "  (943, 54.00),\n",
       "  (985, 54.00),\n",
       "  (986, 54.00),\n",
       "  (31, 53.00),\n",
       "  (58, 53.00),\n",
       "  (159, 53.00),\n",
       "  (256, 53.00),\n",
       "  (294, 53.00),\n",
       "  (383, 53.00),\n",
       "  (487, 53.00),\n",
       "  (533, 53.00),\n",
       "  (541, 53.00),\n",
       "  (612, 53.00),\n",
       "  (657, 53.00),\n",
       "  (664, 53.00),\n",
       "  (667, 53.00),\n",
       "  (709, 53.00),\n",
       "  (858, 53.00),\n",
       "  (917, 53.00),\n",
       "  (922, 53.00),\n",
       "  (990, 53.00),\n",
       "  (995, 53.00),\n",
       "  (997, 53.00),\n",
       "  (0, 52.00),\n",
       "  (89, 52.00),\n",
       "  (164, 52.00),\n",
       "  (218, 52.00),\n",
       "  (229, 52.00),\n",
       "  (269, 52.00),\n",
       "  (276, 52.00),\n",
       "  (289, 52.00),\n",
       "  (352, 52.00),\n",
       "  (451, 52.00),\n",
       "  (490, 52.00),\n",
       "  (528, 52.00),\n",
       "  (545, 52.00),\n",
       "  (665, 52.00),\n",
       "  (752, 52.00),\n",
       "  (771, 52.00),\n",
       "  (927, 52.00),\n",
       "  (13, 51.00),\n",
       "  (36, 51.00),\n",
       "  (67, 51.00),\n",
       "  (70, 51.00),\n",
       "  (78, 51.00),\n",
       "  (126, 51.00),\n",
       "  (133, 51.00),\n",
       "  (177, 51.00),\n",
       "  (194, 51.00),\n",
       "  (196, 51.00),\n",
       "  (209, 51.00),\n",
       "  (239, 51.00),\n",
       "  (241, 51.00),\n",
       "  (261, 51.00),\n",
       "  (264, 51.00),\n",
       "  (286, 51.00),\n",
       "  (295, 51.00),\n",
       "  (362, 51.00),\n",
       "  (388, 51.00),\n",
       "  (431, 51.00),\n",
       "  (508, 51.00),\n",
       "  (526, 51.00),\n",
       "  (602, 51.00),\n",
       "  (616, 51.00),\n",
       "  (685, 51.00),\n",
       "  (738, 51.00),\n",
       "  (739, 51.00),\n",
       "  (1, 50.00),\n",
       "  (3, 50.00),\n",
       "  (92, 50.00),\n",
       "  (100, 50.00),\n",
       "  (138, 50.00),\n",
       "  (172, 50.00),\n",
       "  (216, 50.00),\n",
       "  (277, 50.00),\n",
       "  (325, 50.00),\n",
       "  (340, 50.00),\n",
       "  (358, 50.00),\n",
       "  (373, 50.00),\n",
       "  (378, 50.00),\n",
       "  (433, 50.00),\n",
       "  (519, 50.00),\n",
       "  (552, 50.00),\n",
       "  (639, 50.00),\n",
       "  (668, 50.00),\n",
       "  (683, 50.00),\n",
       "  (684, 50.00),\n",
       "  (746, 50.00),\n",
       "  (757, 50.00),\n",
       "  (779, 50.00),\n",
       "  (833, 50.00),\n",
       "  (865, 50.00),\n",
       "  (9, 49.00),\n",
       "  (14, 49.00),\n",
       "  (15, 49.00),\n",
       "  (35, 49.00),\n",
       "  (63, 49.00),\n",
       "  (66, 49.00),\n",
       "  (96, 49.00),\n",
       "  (123, 49.00),\n",
       "  (160, 49.00),\n",
       "  (176, 49.00),\n",
       "  (212, 49.00),\n",
       "  (214, 49.00),\n",
       "  (267, 49.00),\n",
       "  (326, 49.00),\n",
       "  (333, 49.00),\n",
       "  (336, 49.00),\n",
       "  (349, 49.00),\n",
       "  (376, 49.00),\n",
       "  (377, 49.00),\n",
       "  (387, 49.00),\n",
       "  (416, 49.00),\n",
       "  (444, 49.00),\n",
       "  (467, 49.00),\n",
       "  (583, 49.00),\n",
       "  (604, 49.00),\n",
       "  (606, 49.00),\n",
       "  (628, 49.00),\n",
       "  (659, 49.00),\n",
       "  (704, 49.00),\n",
       "  (732, 49.00),\n",
       "  (765, 49.00),\n",
       "  (776, 49.00),\n",
       "  (787, 49.00),\n",
       "  (855, 49.00),\n",
       "  (11, 48.00),\n",
       "  (16, 48.00),\n",
       "  (18, 48.00),\n",
       "  (22, 48.00),\n",
       "  (41, 48.00),\n",
       "  (95, 48.00),\n",
       "  (102, 48.00),\n",
       "  (114, 48.00),\n",
       "  (188, 48.00),\n",
       "  (191, 48.00),\n",
       "  (243, 48.00),\n",
       "  (253, 48.00),\n",
       "  (265, 48.00),\n",
       "  (290, 48.00),\n",
       "  (337, 48.00),\n",
       "  (344, 48.00),\n",
       "  (365, 48.00),\n",
       "  (397, 48.00),\n",
       "  (432, 48.00),\n",
       "  (449, 48.00),\n",
       "  (514, 48.00),\n",
       "  (535, 48.00),\n",
       "  (566, 48.00),\n",
       "  (576, 48.00),\n",
       "  (592, 48.00),\n",
       "  (633, 48.00),\n",
       "  (674, 48.00),\n",
       "  (706, 48.00),\n",
       "  (707, 48.00),\n",
       "  (764, 48.00),\n",
       "  (853, 48.00),\n",
       "  (874, 48.00),\n",
       "  (890, 48.00),\n",
       "  (891, 48.00),\n",
       "  (915, 48.00),\n",
       "  (5, 47.00),\n",
       "  (12, 47.00),\n",
       "  (53, 47.00),\n",
       "  (72, 47.00),\n",
       "  (79, 47.00),\n",
       "  (121, 47.00),\n",
       "  (134, 47.00),\n",
       "  (144, 47.00),\n",
       "  (156, 47.00),\n",
       "  (169, 47.00),\n",
       "  (193, 47.00),\n",
       "  (201, 47.00),\n",
       "  (255, 47.00),\n",
       "  (279, 47.00),\n",
       "  (288, 47.00),\n",
       "  (301, 47.00),\n",
       "  (321, 47.00),\n",
       "  (350, 47.00),\n",
       "  (351, 47.00),\n",
       "  (366, 47.00),\n",
       "  (413, 47.00),\n",
       "  (503, 47.00),\n",
       "  (560, 47.00),\n",
       "  (571, 47.00),\n",
       "  (753, 47.00),\n",
       "  (769, 47.00),\n",
       "  (796, 47.00),\n",
       "  (823, 47.00),\n",
       "  (867, 47.00),\n",
       "  (886, 47.00),\n",
       "  (889, 47.00),\n",
       "  (951, 47.00),\n",
       "  (989, 47.00),\n",
       "  (28, 46.00),\n",
       "  (65, 46.00),\n",
       "  (87, 46.00),\n",
       "  (101, 46.00),\n",
       "  (110, 46.00),\n",
       "  (130, 46.00),\n",
       "  (136, 46.00),\n",
       "  (139, 46.00),\n",
       "  (149, 46.00),\n",
       "  (221, 46.00),\n",
       "  (227, 46.00),\n",
       "  (273, 46.00),\n",
       "  (312, 46.00),\n",
       "  (323, 46.00),\n",
       "  (392, 46.00),\n",
       "  (395, 46.00),\n",
       "  (398, 46.00),\n",
       "  (414, 46.00),\n",
       "  (486, 46.00),\n",
       "  (523, 46.00),\n",
       "  (530, 46.00),\n",
       "  (625, 46.00),\n",
       "  (727, 46.00),\n",
       "  (751, 46.00),\n",
       "  (758, 46.00),\n",
       "  (795, 46.00),\n",
       "  (872, 46.00),\n",
       "  (881, 46.00),\n",
       "  (882, 46.00),\n",
       "  (907, 46.00),\n",
       "  (959, 46.00),\n",
       "  (983, 46.00),\n",
       "  (994, 46.00),\n",
       "  (2, 45.00),\n",
       "  (56, 45.00),\n",
       "  (71, 45.00),\n",
       "  (131, 45.00),\n",
       "  (137, 45.00),\n",
       "  (141, 45.00),\n",
       "  (157, 45.00),\n",
       "  (247, 45.00),\n",
       "  (320, 45.00),\n",
       "  (324, 45.00),\n",
       "  (335, 45.00),\n",
       "  (339, 45.00),\n",
       "  (354, 45.00),\n",
       "  (384, 45.00),\n",
       "  (417, 45.00),\n",
       "  (422, 45.00),\n",
       "  (426, 45.00),\n",
       "  (475, 45.00),\n",
       "  (517, 45.00),\n",
       "  (599, 45.00),\n",
       "  (613, 45.00),\n",
       "  (666, 45.00),\n",
       "  (702, 45.00),\n",
       "  (759, 45.00),\n",
       "  (763, 45.00),\n",
       "  (818, 45.00),\n",
       "  (821, 45.00),\n",
       "  (848, 45.00),\n",
       "  (873, 45.00),\n",
       "  (900, 45.00),\n",
       "  (918, 45.00),\n",
       "  (44, 44.00),\n",
       "  (75, 44.00),\n",
       "  (105, 44.00),\n",
       "  (107, 44.00),\n",
       "  (112, 44.00),\n",
       "  (215, 44.00),\n",
       "  (244, 44.00),\n",
       "  (252, 44.00),\n",
       "  (258, 44.00),\n",
       "  (332, 44.00),\n",
       "  (390, 44.00),\n",
       "  (445, 44.00),\n",
       "  (456, 44.00),\n",
       "  (518, 44.00),\n",
       "  (579, 44.00),\n",
       "  (658, 44.00),\n",
       "  (690, 44.00),\n",
       "  (717, 44.00),\n",
       "  (755, 44.00),\n",
       "  (807, 44.00),\n",
       "  (920, 44.00),\n",
       "  (941, 44.00),\n",
       "  (984, 44.00),\n",
       "  (993, 44.00),\n",
       "  (80, 43.00),\n",
       "  (108, 43.00),\n",
       "  (142, 43.00),\n",
       "  (174, 43.00),\n",
       "  (198, 43.00),\n",
       "  (242, 43.00),\n",
       "  (268, 43.00),\n",
       "  (287, 43.00),\n",
       "  (306, 43.00),\n",
       "  (330, 43.00),\n",
       "  (355, 43.00),\n",
       "  (357, 43.00),\n",
       "  (381, 43.00),\n",
       "  (421, 43.00),\n",
       "  (434, 43.00),\n",
       "  (473, 43.00),\n",
       "  (495, 43.00),\n",
       "  (510, 43.00),\n",
       "  (637, 43.00),\n",
       "  (672, 43.00),\n",
       "  (812, 43.00),\n",
       "  (817, 43.00),\n",
       "  (894, 43.00),\n",
       "  (929, 43.00),\n",
       "  (933, 43.00),\n",
       "  (981, 43.00),\n",
       "  (998, 43.00),\n",
       "  (7, 42.00),\n",
       "  (42, 42.00),\n",
       "  (153, 42.00),\n",
       "  (173, 42.00),\n",
       "  (257, 42.00),\n",
       "  (266, 42.00),\n",
       "  (299, 42.00),\n",
       "  (360, 42.00),\n",
       "  (370, 42.00),\n",
       "  (389, 42.00),\n",
       "  (418, 42.00),\n",
       "  (442, 42.00),\n",
       "  (520, 42.00),\n",
       "  (538, 42.00),\n",
       "  (573, 42.00),\n",
       "  (723, 42.00),\n",
       "  (734, 42.00),\n",
       "  (756, 42.00),\n",
       "  (837, 42.00),\n",
       "  (898, 42.00),\n",
       "  (910, 42.00),\n",
       "  (968, 42.00),\n",
       "  (19, 41.00),\n",
       "  (38, 41.00),\n",
       "  (40, 41.00),\n",
       "  (83, 41.00),\n",
       "  (132, 41.00),\n",
       "  (223, 41.00),\n",
       "  (260, 41.00),\n",
       "  (302, 41.00),\n",
       "  (305, 41.00),\n",
       "  (353, 41.00),\n",
       "  (405, 41.00),\n",
       "  (429, 41.00),\n",
       "  (430, 41.00),\n",
       "  (437, 41.00),\n",
       "  (537, 41.00),\n",
       "  (558, 41.00),\n",
       "  (574, 41.00),\n",
       "  (575, 41.00),\n",
       "  (636, 41.00),\n",
       "  (694, 41.00),\n",
       "  (710, 41.00),\n",
       "  (712, 41.00),\n",
       "  (851, 41.00),\n",
       "  (919, 41.00),\n",
       "  (932, 41.00),\n",
       "  (996, 41.00),\n",
       "  (93, 40.00),\n",
       "  (117, 40.00),\n",
       "  (190, 40.00),\n",
       "  (224, 40.00),\n",
       "  (236, 40.00),\n",
       "  (245, 40.00),\n",
       "  (278, 40.00),\n",
       "  (322, 40.00),\n",
       "  (380, 40.00),\n",
       "  (447, 40.00),\n",
       "  (485, 40.00),\n",
       "  (500, 40.00),\n",
       "  (507, 40.00),\n",
       "  (542, 40.00),\n",
       "  (555, 40.00),\n",
       "  (559, 40.00),\n",
       "  (584, 40.00),\n",
       "  (643, 40.00),\n",
       "  (682, 40.00),\n",
       "  (699, 40.00),\n",
       "  (713, 40.00),\n",
       "  (726, 40.00),\n",
       "  (799, 40.00),\n",
       "  (862, 40.00),\n",
       "  (925, 40.00),\n",
       "  (945, 40.00),\n",
       "  (999, 40.00),\n",
       "  (10, 39.00),\n",
       "  (17, 39.00),\n",
       "  (27, 39.00),\n",
       "  (91, 39.00),\n",
       "  (111, 39.00),\n",
       "  (127, 39.00),\n",
       "  (140, 39.00),\n",
       "  (145, 39.00),\n",
       "  (148, 39.00),\n",
       "  (186, 39.00),\n",
       "  (329, 39.00),\n",
       "  (338, 39.00),\n",
       "  (379, 39.00),\n",
       "  (393, 39.00),\n",
       "  (439, 39.00),\n",
       "  (480, 39.00),\n",
       "  (501, 39.00),\n",
       "  (546, 39.00),\n",
       "  (670, 39.00),\n",
       "  (722, 39.00),\n",
       "  (749, 39.00),\n",
       "  (761, 39.00),\n",
       "  (883, 39.00),\n",
       "  (204, 38.00),\n",
       "  (230, 38.00),\n",
       "  (297, 38.00),\n",
       "  (309, 38.00),\n",
       "  (314, 38.00),\n",
       "  (368, 38.00),\n",
       "  (412, 38.00),\n",
       "  (470, 38.00),\n",
       "  (529, 38.00),\n",
       "  (540, 38.00),\n",
       "  (548, 38.00),\n",
       "  (553, 38.00),\n",
       "  (656, 38.00),\n",
       "  (687, 38.00),\n",
       "  (708, 38.00),\n",
       "  (714, 38.00),\n",
       "  (736, 38.00),\n",
       "  (782, 38.00),\n",
       "  (831, 38.00),\n",
       "  (844, 38.00),\n",
       "  (854, 38.00),\n",
       "  (20, 37.00),\n",
       "  (52, 37.00),\n",
       "  (152, 37.00),\n",
       "  (154, 37.00),\n",
       "  (175, 37.00),\n",
       "  (183, 37.00),\n",
       "  (210, 37.00),\n",
       "  (235, 37.00),\n",
       "  (262, 37.00),\n",
       "  (296, 37.00),\n",
       "  (347, 37.00),\n",
       "  (435, 37.00),\n",
       "  (462, 37.00),\n",
       "  (466, 37.00),\n",
       "  (494, 37.00),\n",
       "  (554, 37.00),\n",
       "  (600, 37.00),\n",
       "  (610, 37.00),\n",
       "  (630, 37.00),\n",
       "  (678, 37.00),\n",
       "  (814, 37.00),\n",
       "  (921, 37.00),\n",
       "  (98, 36.00),\n",
       "  (200, 36.00),\n",
       "  (205, 36.00),\n",
       "  (207, 36.00),\n",
       "  (399, 36.00),\n",
       "  (408, 36.00),\n",
       "  (453, 36.00),\n",
       "  (511, 36.00),\n",
       "  (513, 36.00),\n",
       "  (650, 36.00),\n",
       "  (693, 36.00),\n",
       "  (719, 36.00),\n",
       "  (720, 36.00),\n",
       "  (861, 36.00),\n",
       "  (928, 36.00),\n",
       "  (934, 36.00),\n",
       "  (120, 35.00),\n",
       "  (143, 35.00),\n",
       "  (248, 35.00),\n",
       "  (346, 35.00),\n",
       "  (356, 35.00),\n",
       "  (605, 35.00),\n",
       "  (802, 35.00),\n",
       "  (43, 34.00),\n",
       "  (45, 34.00),\n",
       "  (62, 34.00),\n",
       "  (86, 34.00),\n",
       "  (220, 34.00),\n",
       "  (359, 34.00),\n",
       "  (374, 34.00),\n",
       "  (450, 34.00),\n",
       "  (464, 34.00),\n",
       "  (615, 34.00),\n",
       "  (677, 34.00),\n",
       "  (745, 34.00),\n",
       "  (793, 34.00),\n",
       "  (798, 34.00),\n",
       "  (827, 34.00),\n",
       "  (947, 34.00),\n",
       "  (958, 34.00),\n",
       "  (964, 34.00),\n",
       "  (59, 33.00),\n",
       "  (158, 33.00),\n",
       "  (213, 33.00),\n",
       "  (226, 33.00),\n",
       "  (233, 33.00),\n",
       "  (402, 33.00),\n",
       "  (403, 33.00),\n",
       "  (409, 33.00),\n",
       "  (415, 33.00),\n",
       "  (588, 33.00),\n",
       "  (691, 33.00),\n",
       "  (954, 33.00),\n",
       "  (26, 32.00),\n",
       "  (64, 32.00),\n",
       "  (73, 32.00),\n",
       "  (106, 32.00),\n",
       "  (385, 32.00),\n",
       "  (427, 32.00),\n",
       "  (465, 32.00),\n",
       "  (543, 32.00),\n",
       "  (627, 32.00),\n",
       "  (760, 32.00),\n",
       "  (786, 32.00),\n",
       "  (859, 32.00),\n",
       "  (897, 32.00),\n",
       "  (948, 32.00),\n",
       "  (980, 32.00),\n",
       "  (122, 31.00),\n",
       "  (371, 31.00),\n",
       "  (461, 31.00),\n",
       "  (469, 31.00),\n",
       "  (484, 31.00),\n",
       "  (502, 31.00),\n",
       "  (521, 31.00),\n",
       "  (531, 31.00),\n",
       "  (551, 31.00),\n",
       "  (568, 31.00),\n",
       "  (578, 31.00),\n",
       "  (644, 31.00),\n",
       "  (647, 31.00),\n",
       "  (705, 31.00),\n",
       "  (744, 31.00),\n",
       "  (789, 31.00),\n",
       "  (846, 31.00),\n",
       "  (914, 31.00),\n",
       "  (81, 30.00),\n",
       "  (146, 30.00),\n",
       "  (166, 30.00),\n",
       "  (179, 30.00),\n",
       "  (345, 30.00),\n",
       "  (459, 30.00),\n",
       "  (557, 30.00),\n",
       "  (589, 30.00),\n",
       "  (617, 30.00),\n",
       "  (629, 30.00),\n",
       "  (632, 30.00),\n",
       "  (803, 30.00),\n",
       "  (966, 30.00),\n",
       "  (303, 29.00),\n",
       "  (498, 29.00),\n",
       "  (585, 29.00),\n",
       "  (634, 29.00),\n",
       "  (4, 28.00),\n",
       "  (29, 28.00),\n",
       "  (54, 28.00),\n",
       "  (567, 28.00),\n",
       "  (587, 28.00),\n",
       "  (631, 28.00),\n",
       "  (663, 28.00),\n",
       "  (718, 28.00),\n",
       "  (731, 28.00),\n",
       "  (767, 28.00),\n",
       "  (860, 28.00),\n",
       "  (876, 28.00),\n",
       "  (909, 28.00),\n",
       "  (923, 28.00),\n",
       "  (147, 27.00),\n",
       "  (187, 27.00),\n",
       "  (202, 27.00),\n",
       "  (479, 27.00),\n",
       "  (482, 27.00),\n",
       "  (660, 27.00),\n",
       "  (733, 27.00),\n",
       "  (965, 27.00),\n",
       "  (49, 26.00),\n",
       "  (150, 26.00),\n",
       "  (271, 26.00),\n",
       "  (624, 26.00),\n",
       "  (676, 26.00),\n",
       "  (686, 26.00),\n",
       "  (785, 26.00),\n",
       "  (869, 26.00),\n",
       "  (931, 26.00),\n",
       "  (32, 25.00),\n",
       "  (163, 25.00),\n",
       "  (240, 25.00),\n",
       "  (369, 25.00),\n",
       "  (544, 25.00),\n",
       "  (804, 25.00),\n",
       "  (885, 25.00),\n",
       "  (516, 24.00),\n",
       "  (536, 24.00),\n",
       "  (590, 24.00),\n",
       "  (596, 24.00),\n",
       "  (747, 24.00),\n",
       "  (856, 24.00),\n",
       "  (901, 24.00),\n",
       "  (168, 23.00),\n",
       "  (185, 23.00),\n",
       "  (315, 23.00),\n",
       "  (394, 23.00),\n",
       "  (438, 23.00),\n",
       "  (623, 23.00),\n",
       "  (680, 23.00),\n",
       "  (811, 23.00),\n",
       "  (838, 23.00),\n",
       "  (908, 23.00),\n",
       "  (911, 23.00),\n",
       "  (974, 23.00),\n",
       "  (165, 22.00),\n",
       "  (550, 22.00),\n",
       "  (638, 22.00),\n",
       "  (675, 22.00),\n",
       "  (942, 22.00),\n",
       "  (499, 21.00),\n",
       "  (525, 21.00),\n",
       "  (773, 21.00),\n",
       "  (68, 20.00),\n",
       "  (662, 20.00),\n",
       "  (715, 20.00),\n",
       "  (729, 20.00),\n",
       "  (740, 20.00),\n",
       "  (246, 19.00),\n",
       "  (400, 19.00),\n",
       "  (598, 19.00),\n",
       "  (622, 19.00),\n",
       "  (648, 18.00),\n",
       "  (651, 18.00),\n",
       "  (728, 18.00),\n",
       "  (836, 18.00),\n",
       "  (977, 18.00),\n",
       "  (504, 17.00),\n",
       "  (813, 17.00),\n",
       "  (906, 17.00),\n",
       "  (976, 17.00),\n",
       "  (460, 16.00),\n",
       "  (534, 16.00),\n",
       "  (841, 16.00),\n",
       "  (930, 16.00),\n",
       "  (970, 15.00),\n",
       "  (103, 14.00),\n",
       "  (282, 14.00),\n",
       "  (742, 14.00),\n",
       "  (969, 14.00),\n",
       "  (34, 13.00),\n",
       "  (446, 13.00),\n",
       "  (493, 13.00),\n",
       "  (967, 12.00),\n",
       "  (167, 11.00),\n",
       "  (940, 11.00),\n",
       "  (978, 11.00),\n",
       "  (689, 10.00),\n",
       "  (899, 9.00),\n",
       "  (960, 9.00),\n",
       "  (673, 8.00),\n",
       "  (681, 8.00),\n",
       "  (810, 8.00),\n",
       "  (935, 8.00),\n",
       "  (961, 2.00)])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = targeted_diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch_no 0\n",
      "at batch_no 100\n",
      "at batch_no 200\n",
      "at batch_no 300\n",
      "at batch_no 400\n",
      "at batch_no 500\n",
      "at batch_no 600\n",
      "at batch_no 700\n",
      "at batch_no 800\n",
      "at batch_no 900\n",
      "at batch_no 1000\n",
      "at batch_no 1100\n",
      "at batch_no 1200\n",
      "at batch_no 1300\n",
      "at batch_no 1400\n",
      "at batch_no 1500\n",
      "at batch_no 1600\n",
      "at batch_no 1700\n",
      "at batch_no 1800\n",
      "at batch_no 1900\n",
      "at batch_no 2000\n",
      "at batch_no 2100\n",
      "at batch_no 2200\n",
      "at batch_no 2300\n",
      "at batch_no 2400\n",
      "at batch_no 2500\n",
      "at batch_no 2600\n",
      "at batch_no 2700\n",
      "at batch_no 2800\n",
      "at batch_no 2900\n",
      "at batch_no 3000\n",
      "at batch_no 3100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(885,\n",
       " [(815, 227.00),\n",
       "  (652, 183.00),\n",
       "  (646, 172.00),\n",
       "  (580, 155.00),\n",
       "  (591, 149.00),\n",
       "  (621, 148.00),\n",
       "  (737, 148.00),\n",
       "  (611, 147.00),\n",
       "  (489, 136.00),\n",
       "  (904, 129.00),\n",
       "  (497, 125.00),\n",
       "  (868, 125.00),\n",
       "  (979, 124.00),\n",
       "  (582, 122.00),\n",
       "  (794, 122.00),\n",
       "  (94, 118.00),\n",
       "  (955, 118.00),\n",
       "  (893, 117.00),\n",
       "  (721, 116.00),\n",
       "  (116, 114.00),\n",
       "  (491, 108.00),\n",
       "  (565, 108.00),\n",
       "  (741, 107.00),\n",
       "  (562, 106.00),\n",
       "  (679, 106.00),\n",
       "  (39, 105.00),\n",
       "  (839, 104.00),\n",
       "  (109, 102.00),\n",
       "  (162, 101.00),\n",
       "  (750, 100.00),\n",
       "  (46, 98.00),\n",
       "  (695, 98.00),\n",
       "  (549, 97.00),\n",
       "  (82, 95.00),\n",
       "  (199, 95.00),\n",
       "  (492, 95.00),\n",
       "  (84, 94.00),\n",
       "  (973, 94.00),\n",
       "  (51, 93.00),\n",
       "  (151, 91.00),\n",
       "  (48, 90.00),\n",
       "  (424, 89.00),\n",
       "  (843, 89.00),\n",
       "  (203, 87.00),\n",
       "  (692, 87.00),\n",
       "  (971, 87.00),\n",
       "  (640, 86.00),\n",
       "  (669, 86.00),\n",
       "  (197, 84.00),\n",
       "  (440, 84.00),\n",
       "  (879, 84.00),\n",
       "  (47, 83.00),\n",
       "  (180, 83.00),\n",
       "  (577, 83.00),\n",
       "  (208, 81.00),\n",
       "  (743, 81.00),\n",
       "  (783, 81.00),\n",
       "  (847, 81.00),\n",
       "  (189, 80.00),\n",
       "  (281, 80.00),\n",
       "  (382, 80.00),\n",
       "  (406, 80.00),\n",
       "  (703, 80.00),\n",
       "  (824, 80.00),\n",
       "  (905, 80.00),\n",
       "  (61, 79.00),\n",
       "  (318, 79.00),\n",
       "  (319, 79.00),\n",
       "  (411, 79.00),\n",
       "  (762, 79.00),\n",
       "  (866, 79.00),\n",
       "  (219, 78.00),\n",
       "  (310, 78.00),\n",
       "  (938, 78.00),\n",
       "  (76, 77.00),\n",
       "  (270, 77.00),\n",
       "  (364, 77.00),\n",
       "  (849, 77.00),\n",
       "  (182, 76.00),\n",
       "  (298, 76.00),\n",
       "  (791, 76.00),\n",
       "  (828, 76.00),\n",
       "  (896, 76.00),\n",
       "  (956, 76.00),\n",
       "  (963, 76.00),\n",
       "  (55, 75.00),\n",
       "  (217, 75.00),\n",
       "  (342, 75.00),\n",
       "  (343, 75.00),\n",
       "  (730, 75.00),\n",
       "  (778, 75.00),\n",
       "  (830, 75.00),\n",
       "  (455, 74.00),\n",
       "  (483, 74.00),\n",
       "  (800, 74.00),\n",
       "  (982, 74.00),\n",
       "  (304, 73.00),\n",
       "  (363, 73.00),\n",
       "  (436, 73.00),\n",
       "  (471, 73.00),\n",
       "  (472, 73.00),\n",
       "  (645, 73.00),\n",
       "  (805, 73.00),\n",
       "  (128, 72.00),\n",
       "  (725, 72.00),\n",
       "  (754, 72.00),\n",
       "  (878, 71.00),\n",
       "  (192, 70.00),\n",
       "  (341, 70.00),\n",
       "  (454, 70.00),\n",
       "  (556, 70.00),\n",
       "  (572, 70.00),\n",
       "  (735, 70.00),\n",
       "  (806, 70.00),\n",
       "  (825, 70.00),\n",
       "  (826, 70.00),\n",
       "  (178, 69.00),\n",
       "  (195, 69.00),\n",
       "  (570, 69.00),\n",
       "  (671, 69.00),\n",
       "  (716, 69.00),\n",
       "  (23, 68.00),\n",
       "  (50, 68.00),\n",
       "  (654, 68.00),\n",
       "  (700, 68.00),\n",
       "  (775, 68.00),\n",
       "  (30, 67.00),\n",
       "  (77, 67.00),\n",
       "  (425, 67.00),\n",
       "  (496, 67.00),\n",
       "  (845, 67.00),\n",
       "  (916, 67.00),\n",
       "  (949, 67.00),\n",
       "  (37, 66.00),\n",
       "  (124, 66.00),\n",
       "  (420, 66.00),\n",
       "  (586, 66.00),\n",
       "  (620, 66.00),\n",
       "  (784, 66.00),\n",
       "  (895, 66.00),\n",
       "  (926, 66.00),\n",
       "  (155, 65.00),\n",
       "  (234, 65.00),\n",
       "  (238, 65.00),\n",
       "  (293, 65.00),\n",
       "  (313, 65.00),\n",
       "  (649, 65.00),\n",
       "  (688, 65.00),\n",
       "  (820, 65.00),\n",
       "  (863, 65.00),\n",
       "  (864, 65.00),\n",
       "  (870, 65.00),\n",
       "  (871, 65.00),\n",
       "  (892, 65.00),\n",
       "  (988, 65.00),\n",
       "  (6, 64.00),\n",
       "  (74, 64.00),\n",
       "  (222, 64.00),\n",
       "  (237, 64.00),\n",
       "  (361, 64.00),\n",
       "  (463, 64.00),\n",
       "  (468, 64.00),\n",
       "  (711, 64.00),\n",
       "  (772, 64.00),\n",
       "  (887, 64.00),\n",
       "  (944, 64.00),\n",
       "  (97, 63.00),\n",
       "  (119, 63.00),\n",
       "  (331, 63.00),\n",
       "  (474, 63.00),\n",
       "  (819, 63.00),\n",
       "  (913, 63.00),\n",
       "  (972, 63.00),\n",
       "  (8, 62.00),\n",
       "  (90, 62.00),\n",
       "  (99, 62.00),\n",
       "  (125, 62.00),\n",
       "  (161, 62.00),\n",
       "  (184, 62.00),\n",
       "  (272, 62.00),\n",
       "  (300, 62.00),\n",
       "  (348, 62.00),\n",
       "  (506, 62.00),\n",
       "  (515, 62.00),\n",
       "  (593, 62.00),\n",
       "  (788, 62.00),\n",
       "  (808, 62.00),\n",
       "  (842, 62.00),\n",
       "  (875, 62.00),\n",
       "  (880, 62.00),\n",
       "  (992, 62.00),\n",
       "  (135, 61.00),\n",
       "  (225, 61.00),\n",
       "  (280, 61.00),\n",
       "  (292, 61.00),\n",
       "  (311, 61.00),\n",
       "  (316, 61.00),\n",
       "  (476, 61.00),\n",
       "  (505, 61.00),\n",
       "  (561, 61.00),\n",
       "  (609, 61.00),\n",
       "  (698, 61.00),\n",
       "  (809, 61.00),\n",
       "  (850, 61.00),\n",
       "  (937, 61.00),\n",
       "  (946, 61.00),\n",
       "  (953, 61.00),\n",
       "  (181, 60.00),\n",
       "  (372, 60.00),\n",
       "  (401, 60.00),\n",
       "  (532, 60.00),\n",
       "  (539, 60.00),\n",
       "  (619, 60.00),\n",
       "  (697, 60.00),\n",
       "  (781, 60.00),\n",
       "  (884, 60.00),\n",
       "  (113, 59.00),\n",
       "  (327, 59.00),\n",
       "  (391, 59.00),\n",
       "  (404, 59.00),\n",
       "  (481, 59.00),\n",
       "  (490, 59.00),\n",
       "  (522, 59.00),\n",
       "  (581, 59.00),\n",
       "  (607, 59.00),\n",
       "  (655, 59.00),\n",
       "  (724, 59.00),\n",
       "  (774, 59.00),\n",
       "  (912, 59.00),\n",
       "  (952, 59.00),\n",
       "  (987, 59.00),\n",
       "  (60, 58.00),\n",
       "  (211, 58.00),\n",
       "  (249, 58.00),\n",
       "  (284, 58.00),\n",
       "  (317, 58.00),\n",
       "  (407, 58.00),\n",
       "  (410, 58.00),\n",
       "  (423, 58.00),\n",
       "  (458, 58.00),\n",
       "  (563, 58.00),\n",
       "  (594, 58.00),\n",
       "  (595, 58.00),\n",
       "  (597, 58.00),\n",
       "  (603, 58.00),\n",
       "  (641, 58.00),\n",
       "  (696, 58.00),\n",
       "  (709, 58.00),\n",
       "  (790, 58.00),\n",
       "  (857, 58.00),\n",
       "  (21, 57.00),\n",
       "  (307, 57.00),\n",
       "  (419, 57.00),\n",
       "  (441, 57.00),\n",
       "  (457, 57.00),\n",
       "  (612, 57.00),\n",
       "  (618, 57.00),\n",
       "  (636, 57.00),\n",
       "  (801, 57.00),\n",
       "  (69, 56.00),\n",
       "  (118, 56.00),\n",
       "  (231, 56.00),\n",
       "  (232, 56.00),\n",
       "  (263, 56.00),\n",
       "  (275, 56.00),\n",
       "  (308, 56.00),\n",
       "  (334, 56.00),\n",
       "  (367, 56.00),\n",
       "  (452, 56.00),\n",
       "  (477, 56.00),\n",
       "  (527, 56.00),\n",
       "  (547, 56.00),\n",
       "  (564, 56.00),\n",
       "  (635, 56.00),\n",
       "  (642, 56.00),\n",
       "  (653, 56.00),\n",
       "  (657, 56.00),\n",
       "  (661, 56.00),\n",
       "  (780, 56.00),\n",
       "  (816, 56.00),\n",
       "  (822, 56.00),\n",
       "  (957, 56.00),\n",
       "  (962, 56.00),\n",
       "  (975, 56.00),\n",
       "  (25, 55.00),\n",
       "  (31, 55.00),\n",
       "  (33, 55.00),\n",
       "  (57, 55.00),\n",
       "  (88, 55.00),\n",
       "  (104, 55.00),\n",
       "  (115, 55.00),\n",
       "  (129, 55.00),\n",
       "  (228, 55.00),\n",
       "  (328, 55.00),\n",
       "  (383, 55.00),\n",
       "  (396, 55.00),\n",
       "  (428, 55.00),\n",
       "  (509, 55.00),\n",
       "  (614, 55.00),\n",
       "  (738, 55.00),\n",
       "  (748, 55.00),\n",
       "  (777, 55.00),\n",
       "  (834, 55.00),\n",
       "  (852, 55.00),\n",
       "  (877, 55.00),\n",
       "  (985, 55.00),\n",
       "  (24, 54.00),\n",
       "  (58, 54.00),\n",
       "  (67, 54.00),\n",
       "  (70, 54.00),\n",
       "  (85, 54.00),\n",
       "  (170, 54.00),\n",
       "  (250, 54.00),\n",
       "  (259, 54.00),\n",
       "  (274, 54.00),\n",
       "  (444, 54.00),\n",
       "  (448, 54.00),\n",
       "  (451, 54.00),\n",
       "  (519, 54.00),\n",
       "  (524, 54.00),\n",
       "  (552, 54.00),\n",
       "  (569, 54.00),\n",
       "  (601, 54.00),\n",
       "  (766, 54.00),\n",
       "  (770, 54.00),\n",
       "  (818, 54.00),\n",
       "  (829, 54.00),\n",
       "  (855, 54.00),\n",
       "  (858, 54.00),\n",
       "  (950, 54.00),\n",
       "  (991, 54.00),\n",
       "  (995, 54.00),\n",
       "  (89, 53.00),\n",
       "  (171, 53.00),\n",
       "  (196, 53.00),\n",
       "  (216, 53.00),\n",
       "  (218, 53.00),\n",
       "  (256, 53.00),\n",
       "  (269, 53.00),\n",
       "  (276, 53.00),\n",
       "  (283, 53.00),\n",
       "  (289, 53.00),\n",
       "  (352, 53.00),\n",
       "  (375, 53.00),\n",
       "  (608, 53.00),\n",
       "  (626, 53.00),\n",
       "  (701, 53.00),\n",
       "  (768, 53.00),\n",
       "  (832, 53.00),\n",
       "  (903, 53.00),\n",
       "  (924, 53.00),\n",
       "  (936, 53.00),\n",
       "  (986, 53.00),\n",
       "  (0, 52.00),\n",
       "  (3, 52.00),\n",
       "  (36, 52.00),\n",
       "  (92, 52.00),\n",
       "  (96, 52.00),\n",
       "  (164, 52.00),\n",
       "  (172, 52.00),\n",
       "  (209, 52.00),\n",
       "  (241, 52.00),\n",
       "  (251, 52.00),\n",
       "  (254, 52.00),\n",
       "  (277, 52.00),\n",
       "  (291, 52.00),\n",
       "  (467, 52.00),\n",
       "  (528, 52.00),\n",
       "  (685, 52.00),\n",
       "  (757, 52.00),\n",
       "  (765, 52.00),\n",
       "  (792, 52.00),\n",
       "  (840, 52.00),\n",
       "  (865, 52.00),\n",
       "  (888, 52.00),\n",
       "  (922, 52.00),\n",
       "  (939, 52.00),\n",
       "  (990, 52.00),\n",
       "  (997, 52.00),\n",
       "  (15, 51.00),\n",
       "  (78, 51.00),\n",
       "  (114, 51.00),\n",
       "  (206, 51.00),\n",
       "  (229, 51.00),\n",
       "  (239, 51.00),\n",
       "  (285, 51.00),\n",
       "  (286, 51.00),\n",
       "  (336, 51.00),\n",
       "  (358, 51.00),\n",
       "  (378, 51.00),\n",
       "  (386, 51.00),\n",
       "  (395, 51.00),\n",
       "  (413, 51.00),\n",
       "  (431, 51.00),\n",
       "  (487, 51.00),\n",
       "  (488, 51.00),\n",
       "  (512, 51.00),\n",
       "  (526, 51.00),\n",
       "  (533, 51.00),\n",
       "  (535, 51.00),\n",
       "  (592, 51.00),\n",
       "  (667, 51.00),\n",
       "  (752, 51.00),\n",
       "  (753, 51.00),\n",
       "  (835, 51.00),\n",
       "  (886, 51.00),\n",
       "  (902, 51.00),\n",
       "  (1, 50.00),\n",
       "  (41, 50.00),\n",
       "  (126, 50.00),\n",
       "  (138, 50.00),\n",
       "  (176, 50.00),\n",
       "  (177, 50.00),\n",
       "  (194, 50.00),\n",
       "  (253, 50.00),\n",
       "  (261, 50.00),\n",
       "  (267, 50.00),\n",
       "  (294, 50.00),\n",
       "  (362, 50.00),\n",
       "  (373, 50.00),\n",
       "  (443, 50.00),\n",
       "  (449, 50.00),\n",
       "  (508, 50.00),\n",
       "  (514, 50.00),\n",
       "  (523, 50.00),\n",
       "  (545, 50.00),\n",
       "  (602, 50.00),\n",
       "  (606, 50.00),\n",
       "  (616, 50.00),\n",
       "  (665, 50.00),\n",
       "  (746, 50.00),\n",
       "  (771, 50.00),\n",
       "  (779, 50.00),\n",
       "  (797, 50.00),\n",
       "  (874, 50.00),\n",
       "  (917, 50.00),\n",
       "  (9, 49.00),\n",
       "  (12, 49.00),\n",
       "  (13, 49.00),\n",
       "  (35, 49.00),\n",
       "  (71, 49.00),\n",
       "  (102, 49.00),\n",
       "  (121, 49.00),\n",
       "  (159, 49.00),\n",
       "  (160, 49.00),\n",
       "  (214, 49.00),\n",
       "  (266, 49.00),\n",
       "  (301, 49.00),\n",
       "  (340, 49.00),\n",
       "  (387, 49.00),\n",
       "  (388, 49.00),\n",
       "  (541, 49.00),\n",
       "  (604, 49.00),\n",
       "  (639, 49.00),\n",
       "  (664, 49.00),\n",
       "  (739, 49.00),\n",
       "  (764, 49.00),\n",
       "  (776, 49.00),\n",
       "  (833, 49.00),\n",
       "  (890, 49.00),\n",
       "  (915, 49.00),\n",
       "  (927, 49.00),\n",
       "  (11, 48.00),\n",
       "  (14, 48.00),\n",
       "  (18, 48.00),\n",
       "  (72, 48.00),\n",
       "  (95, 48.00),\n",
       "  (100, 48.00),\n",
       "  (133, 48.00),\n",
       "  (134, 48.00),\n",
       "  (149, 48.00),\n",
       "  (156, 48.00),\n",
       "  (169, 48.00),\n",
       "  (221, 48.00),\n",
       "  (247, 48.00),\n",
       "  (255, 48.00),\n",
       "  (295, 48.00),\n",
       "  (320, 48.00),\n",
       "  (321, 48.00),\n",
       "  (325, 48.00),\n",
       "  (337, 48.00),\n",
       "  (376, 48.00),\n",
       "  (398, 48.00),\n",
       "  (432, 48.00),\n",
       "  (433, 48.00),\n",
       "  (478, 48.00),\n",
       "  (503, 48.00),\n",
       "  (517, 48.00),\n",
       "  (583, 48.00),\n",
       "  (628, 48.00),\n",
       "  (668, 48.00),\n",
       "  (683, 48.00),\n",
       "  (704, 48.00),\n",
       "  (727, 48.00),\n",
       "  (787, 48.00),\n",
       "  (848, 48.00),\n",
       "  (881, 48.00),\n",
       "  (943, 48.00),\n",
       "  (951, 48.00),\n",
       "  (989, 48.00),\n",
       "  (994, 48.00),\n",
       "  (22, 47.00),\n",
       "  (56, 47.00),\n",
       "  (79, 47.00),\n",
       "  (87, 47.00),\n",
       "  (123, 47.00),\n",
       "  (137, 47.00),\n",
       "  (139, 47.00),\n",
       "  (141, 47.00),\n",
       "  (193, 47.00),\n",
       "  (273, 47.00),\n",
       "  (288, 47.00),\n",
       "  (290, 47.00),\n",
       "  (326, 47.00),\n",
       "  (333, 47.00),\n",
       "  (335, 47.00),\n",
       "  (344, 47.00),\n",
       "  (351, 47.00),\n",
       "  (354, 47.00),\n",
       "  (366, 47.00),\n",
       "  (416, 47.00),\n",
       "  (422, 47.00),\n",
       "  (445, 47.00),\n",
       "  (518, 47.00),\n",
       "  (571, 47.00),\n",
       "  (576, 47.00),\n",
       "  (625, 47.00),\n",
       "  (637, 47.00),\n",
       "  (666, 47.00),\n",
       "  (684, 47.00),\n",
       "  (707, 47.00),\n",
       "  (751, 47.00),\n",
       "  (763, 47.00),\n",
       "  (769, 47.00),\n",
       "  (807, 47.00),\n",
       "  (889, 47.00),\n",
       "  (891, 47.00),\n",
       "  (918, 47.00),\n",
       "  (983, 47.00),\n",
       "  (5, 46.00),\n",
       "  (28, 46.00),\n",
       "  (53, 46.00),\n",
       "  (66, 46.00),\n",
       "  (130, 46.00),\n",
       "  (136, 46.00),\n",
       "  (144, 46.00),\n",
       "  (157, 46.00),\n",
       "  (191, 46.00),\n",
       "  (201, 46.00),\n",
       "  (212, 46.00),\n",
       "  (243, 46.00),\n",
       "  (257, 46.00),\n",
       "  (264, 46.00),\n",
       "  (279, 46.00),\n",
       "  (312, 46.00),\n",
       "  (323, 46.00),\n",
       "  (350, 46.00),\n",
       "  (377, 46.00),\n",
       "  (384, 46.00),\n",
       "  (392, 46.00),\n",
       "  (560, 46.00),\n",
       "  (566, 46.00),\n",
       "  (579, 46.00),\n",
       "  (659, 46.00),\n",
       "  (702, 46.00),\n",
       "  (706, 46.00),\n",
       "  (732, 46.00),\n",
       "  (823, 46.00),\n",
       "  (872, 46.00),\n",
       "  (993, 46.00),\n",
       "  (16, 45.00),\n",
       "  (40, 45.00),\n",
       "  (75, 45.00),\n",
       "  (105, 45.00),\n",
       "  (110, 45.00),\n",
       "  (131, 45.00),\n",
       "  (188, 45.00),\n",
       "  (215, 45.00),\n",
       "  (252, 45.00),\n",
       "  (324, 45.00),\n",
       "  (339, 45.00),\n",
       "  (355, 45.00),\n",
       "  (365, 45.00),\n",
       "  (381, 45.00),\n",
       "  (389, 45.00),\n",
       "  (397, 45.00),\n",
       "  (426, 45.00),\n",
       "  (475, 45.00),\n",
       "  (537, 45.00),\n",
       "  (613, 45.00),\n",
       "  (674, 45.00),\n",
       "  (758, 45.00),\n",
       "  (759, 45.00),\n",
       "  (796, 45.00),\n",
       "  (853, 45.00),\n",
       "  (867, 45.00),\n",
       "  (882, 45.00),\n",
       "  (900, 45.00),\n",
       "  (968, 45.00),\n",
       "  (981, 45.00),\n",
       "  (2, 44.00),\n",
       "  (17, 44.00),\n",
       "  (42, 44.00),\n",
       "  (44, 44.00),\n",
       "  (101, 44.00),\n",
       "  (107, 44.00),\n",
       "  (132, 44.00),\n",
       "  (227, 44.00),\n",
       "  (245, 44.00),\n",
       "  (287, 44.00),\n",
       "  (370, 44.00),\n",
       "  (412, 44.00),\n",
       "  (414, 44.00),\n",
       "  (447, 44.00),\n",
       "  (817, 44.00),\n",
       "  (821, 44.00),\n",
       "  (929, 44.00),\n",
       "  (941, 44.00),\n",
       "  (959, 44.00),\n",
       "  (63, 43.00),\n",
       "  (65, 43.00),\n",
       "  (83, 43.00),\n",
       "  (93, 43.00),\n",
       "  (174, 43.00),\n",
       "  (244, 43.00),\n",
       "  (299, 43.00),\n",
       "  (330, 43.00),\n",
       "  (332, 43.00),\n",
       "  (349, 43.00),\n",
       "  (437, 43.00),\n",
       "  (456, 43.00),\n",
       "  (485, 43.00),\n",
       "  (520, 43.00),\n",
       "  (574, 43.00),\n",
       "  (575, 43.00),\n",
       "  (599, 43.00),\n",
       "  (658, 43.00),\n",
       "  (682, 43.00),\n",
       "  (710, 43.00),\n",
       "  (717, 43.00),\n",
       "  (723, 43.00),\n",
       "  (755, 43.00),\n",
       "  (756, 43.00),\n",
       "  (761, 43.00),\n",
       "  (782, 43.00),\n",
       "  (795, 43.00),\n",
       "  (873, 43.00),\n",
       "  (898, 43.00),\n",
       "  (907, 43.00),\n",
       "  (910, 43.00),\n",
       "  (919, 43.00),\n",
       "  (984, 43.00),\n",
       "  (999, 43.00),\n",
       "  (108, 42.00),\n",
       "  (112, 42.00),\n",
       "  (153, 42.00),\n",
       "  (223, 42.00),\n",
       "  (260, 42.00),\n",
       "  (265, 42.00),\n",
       "  (268, 42.00),\n",
       "  (305, 42.00),\n",
       "  (329, 42.00),\n",
       "  (353, 42.00),\n",
       "  (429, 42.00),\n",
       "  (430, 42.00),\n",
       "  (434, 42.00),\n",
       "  (473, 42.00),\n",
       "  (495, 42.00),\n",
       "  (510, 42.00),\n",
       "  (558, 42.00),\n",
       "  (633, 42.00),\n",
       "  (643, 42.00),\n",
       "  (672, 42.00),\n",
       "  (690, 42.00),\n",
       "  (996, 42.00),\n",
       "  (7, 41.00),\n",
       "  (19, 41.00),\n",
       "  (80, 41.00),\n",
       "  (142, 41.00),\n",
       "  (154, 41.00),\n",
       "  (173, 41.00),\n",
       "  (190, 41.00),\n",
       "  (258, 41.00),\n",
       "  (302, 41.00),\n",
       "  (306, 41.00),\n",
       "  (322, 41.00),\n",
       "  (360, 41.00),\n",
       "  (390, 41.00),\n",
       "  (393, 41.00),\n",
       "  (421, 41.00),\n",
       "  (462, 41.00),\n",
       "  (486, 41.00),\n",
       "  (501, 41.00),\n",
       "  (507, 41.00),\n",
       "  (530, 41.00),\n",
       "  (573, 41.00),\n",
       "  (712, 41.00),\n",
       "  (714, 41.00),\n",
       "  (734, 41.00),\n",
       "  (837, 41.00),\n",
       "  (844, 41.00),\n",
       "  (862, 41.00),\n",
       "  (933, 41.00),\n",
       "  (998, 41.00),\n",
       "  (27, 40.00),\n",
       "  (117, 40.00),\n",
       "  (127, 40.00),\n",
       "  (186, 40.00),\n",
       "  (210, 40.00),\n",
       "  (297, 40.00),\n",
       "  (368, 40.00),\n",
       "  (380, 40.00),\n",
       "  (417, 40.00),\n",
       "  (442, 40.00),\n",
       "  (466, 40.00),\n",
       "  (538, 40.00),\n",
       "  (630, 40.00),\n",
       "  (670, 40.00),\n",
       "  (713, 40.00),\n",
       "  (722, 40.00),\n",
       "  (749, 40.00),\n",
       "  (799, 40.00),\n",
       "  (812, 40.00),\n",
       "  (851, 40.00),\n",
       "  (920, 40.00),\n",
       "  (925, 40.00),\n",
       "  (52, 39.00),\n",
       "  (91, 39.00),\n",
       "  (140, 39.00),\n",
       "  (183, 39.00),\n",
       "  (224, 39.00),\n",
       "  (236, 39.00),\n",
       "  (242, 39.00),\n",
       "  (248, 39.00),\n",
       "  (309, 39.00),\n",
       "  (346, 39.00),\n",
       "  (357, 39.00),\n",
       "  (405, 39.00),\n",
       "  (500, 39.00),\n",
       "  (548, 39.00),\n",
       "  (559, 39.00),\n",
       "  (694, 39.00),\n",
       "  (699, 39.00),\n",
       "  (708, 39.00),\n",
       "  (726, 39.00),\n",
       "  (894, 39.00),\n",
       "  (932, 39.00),\n",
       "  (10, 38.00),\n",
       "  (43, 38.00),\n",
       "  (262, 38.00),\n",
       "  (347, 38.00),\n",
       "  (427, 38.00),\n",
       "  (464, 38.00),\n",
       "  (480, 38.00),\n",
       "  (540, 38.00),\n",
       "  (542, 38.00),\n",
       "  (555, 38.00),\n",
       "  (584, 38.00),\n",
       "  (656, 38.00),\n",
       "  (719, 38.00),\n",
       "  (736, 38.00),\n",
       "  (814, 38.00),\n",
       "  (859, 38.00),\n",
       "  (921, 38.00),\n",
       "  (928, 38.00),\n",
       "  (38, 37.00),\n",
       "  (98, 37.00),\n",
       "  (111, 37.00),\n",
       "  (198, 37.00),\n",
       "  (200, 37.00),\n",
       "  (207, 37.00),\n",
       "  (230, 37.00),\n",
       "  (235, 37.00),\n",
       "  (278, 37.00),\n",
       "  (296, 37.00),\n",
       "  (338, 37.00),\n",
       "  (374, 37.00),\n",
       "  (379, 37.00),\n",
       "  (418, 37.00),\n",
       "  (439, 37.00),\n",
       "  (513, 37.00),\n",
       "  (529, 37.00),\n",
       "  (553, 37.00),\n",
       "  (554, 37.00),\n",
       "  (786, 37.00),\n",
       "  (854, 37.00),\n",
       "  (945, 37.00),\n",
       "  (45, 36.00),\n",
       "  (62, 36.00),\n",
       "  (143, 36.00),\n",
       "  (145, 36.00),\n",
       "  (148, 36.00),\n",
       "  (152, 36.00),\n",
       "  (175, 36.00),\n",
       "  (204, 36.00),\n",
       "  (205, 36.00),\n",
       "  (314, 36.00),\n",
       "  (385, 36.00),\n",
       "  (399, 36.00),\n",
       "  (408, 36.00),\n",
       "  (435, 36.00),\n",
       "  (453, 36.00),\n",
       "  (470, 36.00),\n",
       "  (546, 36.00),\n",
       "  (588, 36.00),\n",
       "  (605, 36.00),\n",
       "  (610, 36.00),\n",
       "  (615, 36.00),\n",
       "  (678, 36.00),\n",
       "  (687, 36.00),\n",
       "  (720, 36.00),\n",
       "  (793, 36.00),\n",
       "  (802, 36.00),\n",
       "  (934, 36.00),\n",
       "  (20, 35.00),\n",
       "  (120, 35.00),\n",
       "  (213, 35.00),\n",
       "  (226, 35.00),\n",
       "  (359, 35.00),\n",
       "  (600, 35.00),\n",
       "  (632, 35.00),\n",
       "  (827, 35.00),\n",
       "  (831, 35.00),\n",
       "  (846, 35.00),\n",
       "  (883, 35.00),\n",
       "  (958, 35.00),\n",
       "  (26, 34.00),\n",
       "  (73, 34.00),\n",
       "  (220, 34.00),\n",
       "  (356, 34.00),\n",
       "  (409, 34.00),\n",
       "  (415, 34.00),\n",
       "  (450, 34.00),\n",
       "  (494, 34.00),\n",
       "  (511, 34.00),\n",
       "  (543, 34.00),\n",
       "  (585, 34.00),\n",
       "  (627, 34.00),\n",
       "  (677, 34.00),\n",
       "  (693, 34.00),\n",
       "  (861, 34.00),\n",
       "  (897, 34.00),\n",
       "  (923, 34.00),\n",
       "  (947, 34.00),\n",
       "  (86, 33.00),\n",
       "  (403, 33.00),\n",
       "  (465, 33.00),\n",
       "  (650, 33.00),\n",
       "  (798, 33.00),\n",
       "  (964, 33.00),\n",
       "  (980, 33.00),\n",
       "  (158, 32.00),\n",
       "  (371, 32.00),\n",
       "  (402, 32.00),\n",
       "  (461, 32.00),\n",
       "  (629, 32.00),\n",
       "  (634, 32.00),\n",
       "  (644, 32.00),\n",
       "  (647, 32.00),\n",
       "  (760, 32.00),\n",
       "  (948, 32.00),\n",
       "  (954, 32.00),\n",
       "  (59, 31.00),\n",
       "  (64, 31.00),\n",
       "  (81, 31.00),\n",
       "  (459, 31.00),\n",
       "  (498, 31.00),\n",
       "  (521, 31.00),\n",
       "  (531, 31.00),\n",
       "  (578, 31.00),\n",
       "  (589, 31.00),\n",
       "  (745, 31.00),\n",
       "  (803, 31.00),\n",
       "  (860, 31.00),\n",
       "  (966, 31.00),\n",
       "  (146, 30.00),\n",
       "  (179, 30.00),\n",
       "  (303, 30.00),\n",
       "  (567, 30.00),\n",
       "  (676, 30.00),\n",
       "  (744, 30.00),\n",
       "  (789, 30.00),\n",
       "  (4, 29.00),\n",
       "  (122, 29.00),\n",
       "  (166, 29.00),\n",
       "  (187, 29.00),\n",
       "  (233, 29.00),\n",
       "  (345, 29.00),\n",
       "  (484, 29.00),\n",
       "  (551, 29.00),\n",
       "  (557, 29.00),\n",
       "  (568, 29.00),\n",
       "  (617, 29.00),\n",
       "  (733, 29.00),\n",
       "  (914, 29.00),\n",
       "  (29, 28.00),\n",
       "  (32, 28.00),\n",
       "  (106, 28.00),\n",
       "  (147, 28.00),\n",
       "  (469, 28.00),\n",
       "  (482, 28.00),\n",
       "  (502, 28.00),\n",
       "  (544, 28.00),\n",
       "  (663, 28.00),\n",
       "  (686, 28.00),\n",
       "  (691, 28.00),\n",
       "  (747, 28.00),\n",
       "  (804, 28.00),\n",
       "  (931, 28.00),\n",
       "  (54, 27.00),\n",
       "  (150, 27.00),\n",
       "  (271, 27.00),\n",
       "  (587, 27.00),\n",
       "  (596, 27.00),\n",
       "  (624, 27.00),\n",
       "  (660, 27.00),\n",
       "  (705, 27.00),\n",
       "  (869, 27.00),\n",
       "  (369, 26.00),\n",
       "  (718, 26.00),\n",
       "  (731, 26.00),\n",
       "  (942, 26.00),\n",
       "  (965, 26.00),\n",
       "  (202, 25.00),\n",
       "  (240, 25.00),\n",
       "  (536, 25.00),\n",
       "  (767, 25.00),\n",
       "  (911, 25.00),\n",
       "  (974, 25.00),\n",
       "  (163, 24.00),\n",
       "  (168, 24.00),\n",
       "  (479, 24.00),\n",
       "  (516, 24.00),\n",
       "  (590, 24.00),\n",
       "  (631, 24.00),\n",
       "  (680, 24.00),\n",
       "  (876, 24.00),\n",
       "  (901, 24.00),\n",
       "  (909, 24.00),\n",
       "  (49, 23.00),\n",
       "  (165, 23.00),\n",
       "  (185, 23.00),\n",
       "  (623, 23.00),\n",
       "  (785, 23.00),\n",
       "  (811, 23.00),\n",
       "  (838, 23.00),\n",
       "  (856, 23.00),\n",
       "  (68, 22.00),\n",
       "  (315, 22.00),\n",
       "  (438, 22.00),\n",
       "  (499, 22.00),\n",
       "  (773, 22.00),\n",
       "  (394, 21.00),\n",
       "  (622, 21.00),\n",
       "  (638, 21.00),\n",
       "  (675, 21.00),\n",
       "  (729, 21.00),\n",
       "  (976, 21.00),\n",
       "  (525, 20.00),\n",
       "  (598, 20.00),\n",
       "  (662, 20.00),\n",
       "  (728, 20.00),\n",
       "  (908, 20.00),\n",
       "  (103, 19.00),\n",
       "  (246, 19.00),\n",
       "  (400, 19.00),\n",
       "  (550, 19.00),\n",
       "  (715, 19.00),\n",
       "  (740, 19.00),\n",
       "  (885, 19.00),\n",
       "  (282, 18.00),\n",
       "  (648, 18.00),\n",
       "  (836, 18.00),\n",
       "  (970, 18.00),\n",
       "  (651, 17.00),\n",
       "  (906, 17.00),\n",
       "  (930, 17.00),\n",
       "  (977, 17.00),\n",
       "  (446, 16.00),\n",
       "  (841, 16.00),\n",
       "  (460, 15.00),\n",
       "  (534, 15.00),\n",
       "  (969, 15.00),\n",
       "  (493, 14.00),\n",
       "  (504, 14.00),\n",
       "  (742, 14.00),\n",
       "  (813, 14.00),\n",
       "  (689, 13.00),\n",
       "  (34, 12.00),\n",
       "  (940, 12.00),\n",
       "  (967, 12.00),\n",
       "  (167, 11.00),\n",
       "  (899, 11.00),\n",
       "  (673, 10.00),\n",
       "  (960, 10.00),\n",
       "  (978, 9.00),\n",
       "  (681, 8.00),\n",
       "  (935, 8.00),\n",
       "  (810, 7.00),\n",
       "  (961, 2.00)])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = targeted_diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f59b994b4e0>]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# on test\n",
    "sorted_hist = sorted(hist, key=lambda x: x[0], reverse = False)\n",
    "values = [elem[1] for elem in sorted_hist]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.8493)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "entropy(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fCvFG0VMKts"
   },
   "outputs": [],
   "source": [
    "def make_triplet_samples(z, margin, r2, r3):\n",
    "  positive_sample = z + random_vector_volume(z.shape, 0, margin).cuda() \n",
    "  negative_sample = z + random_vector_volume(z.shape, r2, r3).cuda()\n",
    "  return positive_sample, negative_sample\n",
    "\n",
    "def random_vector_surface(shape, r = 1.):\n",
    "  mat = torch.randn(size=shape).cuda()\n",
    "  norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "  return (mat/norm) * r\n",
    "\n",
    "def random_vector_volume(shape, inner_r, outer_r):\n",
    "  fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "  fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "  fraction.unsqueeze_(-1)\n",
    "  return random_vector_surface(shape, 1) * fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PizmBkGqMKtu"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_frequent(x):\n",
    "  return Counter(x).most_common(1)[0]\n",
    "\n",
    "def preds_around(center, radius, n_preds, model, dummy_img):\n",
    "  z_s = random_vector_volume([n_preds, 10], radius, radius + 0.01) + center[None]\n",
    "  noises = model.forward_z(z_s)\n",
    "  perturbed_imgs = noises + dummy_img \n",
    "  return torch.argmax(arch(perturbed_imgs), 1)\n",
    "  \n",
    "def most_freq_pred_around(center, radius, n_preds, model, dummy_img):\n",
    "  preds = preds_around(center, radius, n_preds, model, dummy_img)\n",
    "  most_freq = most_frequent(preds.tolist())\n",
    "  return (class_index_to_label(most_freq[0]), most_freq[1]/n_preds)\n",
    "\n",
    "def investigate_neighborhood(z, step, model, dummy_img):\n",
    "  with torch.no_grad():\n",
    "    result = []\n",
    "    for radius in np.arange(0.1, 6., step):\n",
    "#       print(\"creating {} more preds\".format(int(10 + 5 * (radius ** 2))))\n",
    "      most_freq_pred = most_freq_pred_around(z, radius, int(10 + 5 * (radius ** 2)), model, dummy_img)\n",
    "      result.append((radius, most_freq_pred))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-btRW4qMKtw",
    "outputId": "90e81f6a-1b9e-45a4-ae82-bda370319bd9"
   },
   "outputs": [],
   "source": [
    "#experiment 1\n",
    "\n",
    "z = torch.tensor([0.5] * 10).cuda()\n",
    "# z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "# z_s = z[None]\n",
    "\n",
    "model = learn.model.eval()\n",
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "  \n",
    "for i in range(6):\n",
    "  z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "  print(\"investigation for: \", z)\n",
    "  for elem in investigate_neighborhood(z, 0.5, model, x_img):\n",
    "    print(elem)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 1-1: modified investigate_z\n",
    "z_investigate_path = '/root/Derakhshani/adversarial/textual_notes/investigate_z_{}.txt'.format(env.save_filename)\n",
    "if Path(z_investigate_path).exists(): raise FileExistsError(\"file already exists\")\n",
    "file = open(str(z_investigate_path), 'w')\n",
    "        \n",
    "for i, (z, noise) in enumerate(zip(pruned_z_s, pruned_noises)):\n",
    "  hist = compute_prediction_histogram(learn, noise)\n",
    "  indexed_hist = [(i, val) for i, val in enumerate(hist)]\n",
    "  sorted_hist = sorted(indexed_hist, key=lambda x: x[1], reverse=True)\n",
    "  labeled_hist = [(class_index_to_label(i), count) for i, count in sorted_hist]\n",
    "  print(\"result {}:\".format(i))\n",
    "  print(big_vector_to_str(z))\n",
    "  print(labeled_hist[:6])\n",
    "  print(\"\\n\\n\")\n",
    "  \n",
    "  file.write(\"result {}:\\n\".format(i))\n",
    "  file.write(big_vector_to_str(z) + \"\\n\")\n",
    "  file.write(str(labeled_hist[:6]))\n",
    "  file.write(\"\\n\\n\\n\")\n",
    "  file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bp6YOnipMKtz"
   },
   "outputs": [],
   "source": [
    "#experiment 2\n",
    "import itertools\n",
    "z_s = [torch.tensor(t).cuda() for t in itertools.product( *([[-0.33, 0.33]] * 10) )]\n",
    "model = learn.model.eval()\n",
    "noises = []\n",
    "with torch.no_grad():\n",
    "  for z in z_s:\n",
    "    noises.append(model.forward_single_z(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "55lErWDyMKt1",
    "outputId": "93d8fb71-3fd5-44a5-d3ec-8013e13f17ba"
   },
   "outputs": [],
   "source": [
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "\n",
    "preds = []\n",
    "for noise in noises:\n",
    "  perturbed_img = x_img + noise\n",
    "  preds.append(torch.argmax(arch(perturbed_img[None]), 1)[0].item())\n",
    "\n",
    "from collections import Counter\n",
    "result = [(class_index_to_label(index), count) for index, count in Counter(preds).most_common(5)]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4WSg-wBFMKt5"
   },
   "outputs": [],
   "source": [
    "#experiment 3\n",
    "import itertools\n",
    "dimension_values = [[-0.9, 0.9]] * z_dim\n",
    "for i in range(z_dim):\n",
    "  if i % 100 != 0:\n",
    "    dimension_values[i] = [0.]\n",
    "# dimension_values[0] = [0.]\n",
    "# dimension_values[3] = [0.]\n",
    "# dimension_values[6] = [0.]\n",
    "# dimension_values[9] = [0.]\n",
    "pruned_z_s = [torch.tensor(t).cuda() for t in itertools.product(*dimension_values)]\n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 3: for the targeted-attack case\n",
    "pruned_z_s = []\n",
    "for i in range(z_dim):\n",
    "  new_z = torch.zeros(z_dim).cuda()\n",
    "  new_z[i] = 1.\n",
    "  pruned_z_s.append(new_z)\n",
    "  \n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 3-1: noises for \n",
    "pruned_z_s = []\n",
    "# for i in range(z_dim):\n",
    "#   new_z = torch.empty(z_dim).uniform_(0,1).cuda().detach()\n",
    "#   pruned_z_s.append(new_z)\n",
    "\n",
    "for i in range(z_dim):\n",
    "  new_z = torch.zeros(z_dim).cuda().detach()\n",
    "  new_z[i] = 1.\n",
    "  pruned_z_s.append(new_z)\n",
    "  \n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise in pruned_noises[0:200]:\n",
    "  img = noise_to_image(noise)\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider web\n",
    "z_values = [\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33],\n",
    "  [-0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33,  0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33,  0.33,  0.33],\n",
    "  [-0.33,  0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_values = [\n",
    "  # window screen\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuAVZzmKMKt9",
    "outputId": "7e6643e0-ce55-438b-e0ae-79bde3ee4cef"
   },
   "outputs": [],
   "source": [
    "#vgg-16_12 most repeated labels:\n",
    "l = [(611, 215.0),\n",
    "  (474, 194.1),\n",
    "  (398, 120.3),\n",
    "  (721, 79.6),\n",
    "  (741, 73.5),\n",
    "  (510, 62.5)]\n",
    "\n",
    "[(class_index_to_label(index), count) for index, count in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAgk-YyWc3rG"
   },
   "outputs": [],
   "source": [
    "# learn.recorder.plot_losses()\n",
    "# learn.recorder.plot_lr()\n",
    "# learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTHG4Bt7VDYp"
   },
   "outputs": [],
   "source": [
    "fooling_rates = []\n",
    "model = learn.model.eval()\n",
    "learn.metrics = [validation_single_perturbation]\n",
    "for i in range(10):\n",
    "  global_perturbations = model(torch.rand(1, 3, 224, 244).cuda())[0]\n",
    "  nag_util.global_perturbations = global_perturbations\n",
    "  fooling_rates.append(learn.validate()[1].cpu().item())\n",
    "  print(\"%d : %f\"%(i, fooling_rates[-1]))\n",
    "\n",
    "mean = np.mean(fooling_rates)\n",
    "stddev = np.std(fooling_rates)\n",
    "print(mean, stddev); print(fooling_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "OFCjzI7UaY3C",
    "outputId": "740185b4-dd54-46f4-b0af-79ee452568e1"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[200][0]\n",
    "x = normalize(x_img.data.cuda())\n",
    "z = torch.tensor([-0.33,  0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33], dtype=torch.float32).cuda()\n",
    "# z = torch.empty(z_dim).uniform_(-1,1).cuda()\n",
    "p = model.forward_single_z(z).detach()\n",
    "\n",
    "p_x = x + p\n",
    "# print(\"img range, noise range\")\n",
    "# print_range(x); print_range(p)\n",
    "adv_label = class_index_to_label(arch(p_x[None]).argmax(1).item())\n",
    "print_big_vector(arch(p_x[None])[0])\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0., 1.])\n",
    "p_img = Image(p)\n",
    "x_img.show()\n",
    "p_img.show()\n",
    "p_x_img.show()\n",
    "\n",
    "\n",
    "# print_range(p)\n",
    "# print_range(denormalize(x))\n",
    "# print_range(p_x)\n",
    "\n",
    "benign_label = class_index_to_label(arch(x[None]).argmax(1).item())\n",
    "\n",
    "print_big_vector(arch(x[None])[0])\n",
    "print(benign_label, adv_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzwsI2P1ZANz"
   },
   "outputs": [],
   "source": [
    "z1 = torch.tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p1 = model.forward_single_z(z1)\n",
    "\n",
    "z2 = torch.tensor([1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p2 = model.forward_single_z(z2)\n",
    "\n",
    "z3 = torch.tensor([1, 1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p3 = model.forward_single_z(z3)\n",
    "\n",
    "l2_distance(p1, p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eroI82OKSnAL"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[4][0]\n",
    "x = x_img.data[None].cuda()\n",
    "p = model(x)[0].squeeze().detach() \n",
    "x = x.squeeze()\n",
    "x = normalize(x)\n",
    "\n",
    "p_x = x + p\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0.,1.])\n",
    "p_img = Image(p)\n",
    "# x_img.show()\n",
    "p_img.show()\n",
    "# p_x_img.show()\n",
    "\n",
    "print_range(p)\n",
    "print_range(x)\n",
    "print_range(p_x)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NAG-tripletLossExperiment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
