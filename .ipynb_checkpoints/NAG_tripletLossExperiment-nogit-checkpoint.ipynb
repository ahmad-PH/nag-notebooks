{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ahmad-PH/nag-notebooks/blob/master/NAG_tripletLossExperiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqeZpz16do4y"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def run_shell_command(cmd):\n",
    "  p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "  print(str(p.communicate()[0], 'utf-8'))\n",
    "\n",
    "\n",
    "def detect_env():\n",
    "    import os\n",
    "    if 'content' in os.listdir('/'):\n",
    "      return \"colab\"\n",
    "    else:\n",
    "      return \"IBM\"\n",
    "  \n",
    "  \n",
    "def create_env():\n",
    "  if detect_env() == \"IBM\":\n",
    "    return IBMEnv()\n",
    "  elif detect_env() == \"colab\":\n",
    "    return ColabEnv()\n",
    "\n",
    "\n",
    "class Env:\n",
    "  def get_nag_util_files(self):\n",
    "      import os\n",
    "      \n",
    "      print(\"\\ngetting git files ...\")\n",
    "      if os.path.isdir(self.python_files_path):\n",
    "        os.chdir(self.python_files_path)\n",
    "        run_shell_command('git pull')\n",
    "        os.chdir(self.root_folder)\n",
    "      else:\n",
    "        run_shell_command('git clone https://github.com/ahmad-PH/nag-public.git')\n",
    "      print(\"done.\")\n",
    "  \n",
    "\n",
    "class IBMEnv(Env):\n",
    "    def __init__(self):\n",
    "      self.root_folder = \"/root/Derakhshani/adversarial\"\n",
    "      self.temp_csv_path = self.root_folder + \"/temp\"\n",
    "      self.python_files_path = self.root_folder + \"/nag-public\"\n",
    "      self.python_files_dir = \"NAG-11May-beforeDenoiser\"\n",
    "      \n",
    "      import sys\n",
    "      sys.path.append('./nag/nag_util')\n",
    "      \n",
    "    def get_csv_path(self):\n",
    "      return self.root_folder + \"/textual_notes/CSVs/\" + self.save_filename\n",
    "    \n",
    "    def get_models_path(self):\n",
    "      return self.root_folder + \"/models/\" + self.save_filename\n",
    "      \n",
    "    def setup(self):\n",
    "      self.get_nag_util_files()\n",
    "      \n",
    "      import os; import torch;\n",
    "      os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "      cuda_index = 0\n",
    "      os.environ['CUDA_VISIBLE_DEVICES']=str(cuda_index)\n",
    "#       defaults.device = torch.device('cuda:' + str(cuda_index))\n",
    "#       print('cuda:' + str(cuda_index))\n",
    "#       torch.cuda.set_device('cuda:1')\n",
    "      \n",
    "    def load_dataset(self, compressed_name, unpacked_name):\n",
    "      pass\n",
    "\n",
    "    def load_test_dataset(self, root_folder):\n",
    "      pass\n",
    "    \n",
    "    def set_data_path(self, path):\n",
    "      self.data_path = Path(self.root_folder + '/datasets/' + path)\n",
    "    \n",
    "        \n",
    "class ColabEnv(Env):\n",
    "    def __init__(self):\n",
    "      self.root_folder = '/content'\n",
    "      self.temp_csv_path = self.root_folder\n",
    "      self.python_files_path = self.root_folder + '/nag-public'\n",
    "      self.python_files_dir = \"NAG-11May-beforeDenoiser\"\n",
    "      self.torchvision_upgraded = False\n",
    "      \n",
    "    def get_csv_path(self):\n",
    "      return self.root_folder + '/gdrive/My Drive/DL/textual_notes/CSVs/' + self.save_filename\n",
    "    \n",
    "    def get_models_path(self):\n",
    "      return self.root_folder + \"/gdrive/My Drive/DL/models/\" + self.save_filename\n",
    "        \n",
    "    def setup(self):\n",
    "        # ######################################################\n",
    "        # # TODO remove this once torchvision 0.3 is present by\n",
    "        # # default in Colab\n",
    "        # ######################################################\n",
    "        global torchvision_upgraded\n",
    "        try:\n",
    "            torchvision_upgraded\n",
    "        except NameError:\n",
    "          !pip uninstall -y torchvision\n",
    "          !pip install https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl\n",
    "          torchvision_upgraded = True\n",
    "        else:\n",
    "          print(\"torchvision already upgraded\")\n",
    "          \n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/gdrive')\n",
    "        \n",
    "        self.get_nag_util_files()\n",
    "        \n",
    "    def load_dataset(self, compressed_name, unpacked_name):\n",
    "      if compressed_name not in os.listdir('.'):\n",
    "        print(compressed_name + ' not found, getting it from drive')\n",
    "        shutil.copyfile(\"/content/gdrive/My Drive/DL/{}.tar.gz\".format(compressed_name), \"./{}.tar.gz\".format(compressed_name))\n",
    "\n",
    "        gunzip_arg = \"./{}.tar.gz\".format(compressed_name)\n",
    "        !gunzip -f $gunzip_arg\n",
    "\n",
    "        tar_arg = \"./{}.tar\".format(compressed_name)\n",
    "        !tar -xvf $tar_arg > /dev/null\n",
    "\n",
    "        os.rename(unpacked_name, compressed_name)\n",
    "\n",
    "    #     ls_arg = \"./{}/train/n01440764\".format(compressed_name)\n",
    "    #     !ls $ls_arg\n",
    "\n",
    "        !rm $tar_arg\n",
    "\n",
    "        print(\"done\") \n",
    "      else:\n",
    "        print(compressed_name + \" found\")\n",
    "        \n",
    "    def load_test_dataset(self, root_folder):\n",
    "      test_folder = root_folder + '/test/'\n",
    "      if 'test' not in os.listdir(root_folder):\n",
    "        print('getting test dataset from drive')\n",
    "        os.mkdir(test_folder)\n",
    "        for i in range(1,11):\n",
    "          shutil.copy(\"/content/gdrive/My Drive/DL/full_test_folder/{}.zip\".format(i), test_folder)\n",
    "          shutil.unpack_archive(test_folder + \"/{}.zip\".format(i), test_folder)\n",
    "          os.remove(test_folder + \"/{}.zip\".format(i))\n",
    "          print(\"done with the {}th fragment\".format(i))\n",
    "      else:\n",
    "        print('test dataset found.')\n",
    "        \n",
    "    def set_data_path(self, path):\n",
    "      self.data_path = Path('./' + path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "YyZUYSjBi9K9",
    "outputId": "5ef25a03-c55f-43de-8460-a1afde369fd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "getting git files ...\n",
      "Already up-to-date.\n",
      "\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "env = create_env()\n",
    "env.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Ev7jcRKoARg"
   },
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.imports import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys; import os; import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3_1aE41PZAMw"
   },
   "outputs": [],
   "source": [
    "sys.path.append(env.python_files_path + '/' + env.python_files_dir)\n",
    "\n",
    "from nag_util import *\n",
    "import nag_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tltucTv2ep9-"
   },
   "outputs": [],
   "source": [
    "# mode = \"sanity_check\"\n",
    "mode = \"normal\"\n",
    "# mode = \"div_metric_calc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SO1h55obXzOv",
    "outputId": "54414cc5-84d5-4f45-ecab-87374a58dd33"
   },
   "outputs": [],
   "source": [
    "if mode == \"normal\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "elif mode == \"sanity_check\":\n",
    "  env.load_dataset('dataset_sanity_check_small', 'dataset_sanity_check_small')  \n",
    "  env.set_data_path('dataset_sanity_check_small')\n",
    "elif mode == \"div_metric_calc\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "  env.load_test_dataset(str(env.data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "koaQZmjMom7w"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "gpu_flag = True\n",
    "nag_util.batch_size = batch_size; nag_util.gpu_flag = gpu_flag;\n",
    "# nag_util.set_globals(gpu_flag, batch_size)\n",
    "tfms = get_transforms(do_flip=False, max_rotate=0)\n",
    "data = (ImageList.from_folder(env.data_path)\n",
    "        .split_by_folder(valid=('test' if mode == 'div_metric_calc' else 'valid'))\n",
    "        .label_from_folder()\n",
    "        .transform(tfms, size=224)\n",
    "        .databunch(bs=batch_size, num_workers=1)\n",
    "        .normalize(imagenet_stats))\n",
    "\n",
    "# data.show_batch(rows=2, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDBkRV8yovwV"
   },
   "outputs": [],
   "source": [
    "# model = models.resnet50\n",
    "# model = models.resnet152\n",
    "model = models.vgg16_bn\n",
    "# model = torchvision.models.googlenet\n",
    "model_name = model.__name__\n",
    "z_dim = 1000\n",
    "\n",
    "class SoftmaxWrapper(nn.Module):\n",
    "  def __init__(self, m):\n",
    "    super().__init__()\n",
    "    self.m = m\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "  def forward(self, inp):\n",
    "    out = self.m(inp)\n",
    "    return self.softmax(out)\n",
    "  \n",
    "arch = SoftmaxWrapper(model(pretrained=True).cuda().eval())\n",
    "nag_util.arch = arch\n",
    "requires_grad(arch, False)\n",
    "\n",
    "# vgg:\n",
    "# layers = []\n",
    "# blocks = [i-1 for i,o in enumerate(children(arch.features)) if isinstance(o, nn.MaxPool2d)]\n",
    "# layers = [arch.features[i] for i in blocks]\n",
    "# layer_weights = [1] * len(layers)\n",
    "\n",
    "layers = [\n",
    "    arch.softmax\n",
    "]\n",
    "\n",
    "layer_weights = [1.] * len(layers)\n",
    "\n",
    "# inception:\n",
    "# layers = [\n",
    "#     arch.Conv2d_1a_3x3,\n",
    "#     arch.Mixed_6e,\n",
    "#     arch.Mixed_7a,\n",
    "#     arch.fc    \n",
    "# ]\n",
    "# layer_weights = [1.0/4.0] * len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M9gFeezWZWan"
   },
   "outputs": [],
   "source": [
    "# class Gen(nn.Module):\n",
    "#   def __init__(self, z_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "#     super(Gen, self).__init__()\n",
    "\n",
    "#     self.bs = None\n",
    "#     self.z_dim = z_dim\n",
    "#     self.gf_dim = gf_dim\n",
    "#     self.y_dim = y_dim\n",
    "#     self.df_dim = df_dim\n",
    "#     self.image_shape = image_shape\n",
    "\n",
    "#     self.z_ = nn.Linear(self.z_dim, self.gf_dim * 7 * 4 * 4, bias=True)\n",
    "#     self.z_.bias.data.fill_(0)\n",
    "#     self.BN_ = nn.BatchNorm2d(self.gf_dim * 7)\n",
    "\n",
    "#     self.CT2d_1 = deconv_layer(self.gf_dim * 8, self.gf_dim * 4, k_size = (5,5), pad = (2,2))\n",
    "    \n",
    "#     self.CT2d_2 = deconv_layer(self.gf_dim * 5, self.gf_dim * 2)\n",
    "\n",
    "#     self.half = max(self.gf_dim // 2, 1) \n",
    "#     self.CT2d_3 = deconv_layer(self.gf_dim * 2 + self.half, self.gf_dim * 1)\n",
    "\n",
    "#     self.quarter = max(self.gf_dim // 4, 1)\n",
    "#     self.CT2d_4 = deconv_layer(self.gf_dim * 1 + self.quarter, self.gf_dim * 1)\n",
    "\n",
    "#     self.eighth = max(self.gf_dim // 8, 1)\n",
    "#     self.CT2d_5 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "\n",
    "#     # sixteenth = max(self.gf_dim // 16, 1)\n",
    "#     self.CT2d_6 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "\n",
    "#     # sixteenth = max(self.gf_dim // 16, 1)\n",
    "#     self.CT2d_7 = deconv_layer(self.gf_dim * 1 + self.eighth, 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "#   def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "#     h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "#     h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "#     output = deconv_layer(h_input)\n",
    "#     assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "#             \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "#             \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "#     return output\n",
    "  \n",
    "#   def forward_z(self, z):\n",
    "#     self.bs = z.shape[0]\n",
    "    \n",
    "#     h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "#     assert h0.shape[2:] == (4, 4), \"Non-expected shape, it shoud be (4,4)\"\n",
    "\n",
    "#     h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "#     h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "#     h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "#     h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "#     h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "#     h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "#     h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "#     ksi = 10.0\n",
    "#     output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "#     # this coeff scales the output to be appropriate for images that are \n",
    "#     # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "#     # interval)\n",
    "#     return output_coeff * torch.tanh(h7)\n",
    "\n",
    "#   def forward(self, inputs):\n",
    "#     self.bs = inputs.shape[0]\n",
    "#     z = inputs.new_empty([self.bs, self.z_dim]).uniform_(-1,1).cuda()\n",
    "# #     p, n = self.make_triplet_samples(z, 0.1, 0.1, 2.)\n",
    "    \n",
    "#     z_out = self.forward_z(z)\n",
    "# #     p_out = self.forward_z(p)\n",
    "# #     n_out = self.forward_z(n)\n",
    "    \n",
    "# #     return z_out, p_out, n_out, inputs\n",
    "#     return z_out, None, None, inputs\n",
    "  \n",
    "#   def forward_single_z(self, z):\n",
    "#     return self.forward_z(z[None]).squeeze()\n",
    "           \n",
    "  \n",
    "#   def make_triplet_samples(self, z, margin, r2, r3):\n",
    "#     positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "#     negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "#     return positive_sample, negative_sample\n",
    "\n",
    "#   def random_vector_surface(self, shape, r = 1.):\n",
    "#     mat = torch.randn(size=shape).cuda()\n",
    "#     norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "#     return (mat/norm) * r\n",
    "\n",
    "  \n",
    "#   def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "#     fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "#     fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "#     fraction.unsqueeze_(-1)\n",
    "#     return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "#   def make_z(self, in_shape):\n",
    "#     return torch.empty(in_shape).cuda().uniform_(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gen(nn.Module):\n",
    "  def __init__(self, z_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "    super(Gen, self).__init__()\n",
    "\n",
    "    self.bs = None\n",
    "    self.z_dim = z_dim\n",
    "    self.gf_dim = gf_dim\n",
    "    self.y_dim = y_dim\n",
    "    self.df_dim = df_dim\n",
    "    self.image_shape = image_shape\n",
    "\n",
    "    self.z_ = nn.Linear(self.z_dim, self.gf_dim * 7 * 4 * 4, bias=True)\n",
    "    self.z_.bias.data.fill_(0)\n",
    "    self.BN_ = nn.BatchNorm2d(self.gf_dim * 7)\n",
    "\n",
    "    self.CT2d_1 = deconv_layer(self.gf_dim * 8, self.gf_dim * 4, k_size = (5,5), pad = (2,2))\n",
    "    \n",
    "    self.CT2d_2 = deconv_layer(self.gf_dim * 5, self.gf_dim * 2)\n",
    "\n",
    "    self.half = max(self.gf_dim // 2, 1) \n",
    "    self.CT2d_3 = deconv_layer(self.gf_dim * 2 + self.half, self.gf_dim * 1)\n",
    "\n",
    "    self.quarter = max(self.gf_dim // 4, 1)\n",
    "    self.CT2d_4 = deconv_layer(self.gf_dim * 1 + self.quarter, self.gf_dim * 1)\n",
    "\n",
    "    self.eighth = max(self.gf_dim // 8, 1)\n",
    "    self.CT2d_5 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "\n",
    "    # sixteenth = max(self.gf_dim // 16, 1)\n",
    "    self.CT2d_6 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "\n",
    "    # sixteenth = max(self.gf_dim // 16, 1)\n",
    "    self.CT2d_7 = deconv_layer(self.gf_dim * 1 + self.eighth, 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "  def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "    h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "    h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "    output = deconv_layer(h_input)\n",
    "    assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "            \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "            \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "    return output\n",
    "  \n",
    "  def forward_z(self, z):\n",
    "    self.bs = z.shape[0]\n",
    "    \n",
    "    h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "    assert h0.shape[2:] == (4, 4), \"Non-expected shape, it shoud be (4,4)\"\n",
    "\n",
    "    h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "    h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "    h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "    h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "    h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "    h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "    h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "    ksi = 10.0\n",
    "    output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "    # this coeff scales the output to be appropriate for images that are \n",
    "    # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "    # interval)\n",
    "    return output_coeff * torch.tanh(h7)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    self.bs = inputs.shape[0]\n",
    "\n",
    "    benign_preds_onehot = arch(inputs)\n",
    "    benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "    \n",
    "    z = torch.zeros([self.bs, 1000]).cuda()\n",
    "    for i in range(self.bs):\n",
    "      random_label = self.randint(0,1000, exclude = benign_preds[i].item())\n",
    "      z[i][random_label] = 1.\n",
    "    \n",
    "    z_out = self.forward_z(z)\n",
    "    \n",
    "    return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "  \n",
    "  @staticmethod\n",
    "  def randint(low, high, exclude):\n",
    "    temp = np.random.randint(low, high - 1)\n",
    "    if temp == exclude:\n",
    "      temp = temp + 1\n",
    "    return temp\n",
    "  \n",
    "  def forward_single_z(self, z):\n",
    "    return self.forward_z(z[None]).squeeze()\n",
    "           \n",
    "  \n",
    "  def make_triplet_samples(self, z, margin, r2, r3):\n",
    "    positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "    negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "    return positive_sample, negative_sample\n",
    "\n",
    "  def random_vector_surface(self, shape, r = 1.):\n",
    "    mat = torch.randn(size=shape).cuda()\n",
    "    norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "    return (mat/norm) * r\n",
    "\n",
    "  \n",
    "  def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "    fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "    fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "    fraction.unsqueeze_(-1)\n",
    "    return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "  def make_z(self, in_shape):\n",
    "    return torch.empty(in_shape).cuda().uniform_(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorify(x):\n",
    "  return x if isinstance(x, torch.Tensor) else torch.tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NkfbLWEQqRA_"
   },
   "outputs": [],
   "source": [
    "def js_distance(x1, x2):\n",
    "  m = 0.5 * (x1 + x2)\n",
    "  return 0.5 * (F.kl_div(x1, m) + F.kl_div(x2, m))\n",
    "\n",
    "def kl_distance(x1, x2):\n",
    "  inp = torch.log(x1)\n",
    "  target = x2\n",
    "  return F.kl_div(inp, target, reduction='batchmean')\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  x1 = tensorify(x1)\n",
    "  x2 = tensorify(x2)\n",
    "  x1 = x1 / torch.sum(x1)\n",
    "  x2 = x2 / torch.sum(x2)\n",
    "  return kl_distance(x1[None], x2[None])\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  if not isinstance(x1, torch.Tensor): x1 = torch.tensor(x1)\n",
    "  if not isinstance(x2, torch.Tensor): x2 = torch.tensor(x2)\n",
    "  x1 = x1 * 100. / torch.sum(x1)\n",
    "  x2 = x2 * 100. / torch.sum(x2)\n",
    "#   print_big_vector(x1, 0.1)\n",
    "#   print_big_vector(x2, 0.1)\n",
    "  return torch.norm(x1 - x2, 2)\n",
    "\n",
    "def distance_from_uniform(x):\n",
    "  return distrib_distance(x, [1.] * len(x))\n",
    "\n",
    "def wasserstein_distance(x1, x2):\n",
    "  return torch.mean(x1 - x2)\n",
    "\n",
    "def l1_distance(x1, x2):\n",
    "  return F.l1_loss(x1, x2)\n",
    "\n",
    "def l2_distance(x1, x2):\n",
    "  return F.mse_loss(x1 * 10, x2 * 10)\n",
    "\n",
    "def mse_loss(x1, x2):\n",
    "  return F.mse_loss(x1, x2)\n",
    "\n",
    "def cos_distance(x1, x2, dim = 1):\n",
    "  return -1 * torch.mean(F.cosine_similarity(x1, x2, dim=dim))\n",
    "\n",
    "triplet_call_cnt = 0\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, distance_func, margin):\n",
    "  # max distance when using l1_distance is 2\n",
    "  # max distacne when using l2-distance is sqrt(2)\n",
    "#   print(\"anchor: \", anchor.min(), anchor.max())\n",
    "  ap_dist = distance_func(anchor, positive)\n",
    "  an_dist = distance_func(anchor, negative)\n",
    "\n",
    "  global triplet_call_cnt\n",
    "  triplet_call_cnt += 1\n",
    "  if triplet_call_cnt % 10 in [0,1] : #and anchor.shape[1] == 1000:\n",
    "#     print(\"a: \", end=\"\"); print_big_vector(anchor[0])\n",
    "#     print(\"p: \", end=\"\"); print_big_vector(positive[0])\n",
    "#     print(\"n: \", end=\"\"); print_big_vector(negative[0])\n",
    "#     print(\"ap_dist: {}, an_dist: {}\".format(ap_dist, an_dist))\n",
    "    print(\"func:{}, ap_dist: {}, an_dist: {}\".format(distance_func.__name__, ap_dist, an_dist))\n",
    "    \n",
    "  return torch.mean(F.relu(ap_dist - an_dist + margin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsFgfiN8EV7z"
   },
   "outputs": [],
   "source": [
    "def diversity_loss(input, target):\n",
    "#   return -1 * torch.mean(torch.pow(f_x_a-f_x_s,2))\n",
    "  if input.shape[0] != batch_size:\n",
    "    print(\"input shape: \", input.shape)\n",
    "    print(\"target shape: \", target.shape, \"\\n\\n\")\n",
    "  return torch.mean(F.cosine_similarity(\n",
    "    input.view([batch_size, -1]),\n",
    "    target.view([batch_size, -1]), \n",
    "  ))\n",
    "\n",
    "# def fool_loss(input, target):\n",
    "#     true_class = torch.argmax(target, dim=1).view(-1,1).long()\n",
    "#     return -1 * torch.log(torch.mean(1 - input.gather(1, true_class)))\n",
    "\n",
    "def fool_loss_old(input, target, trash):\n",
    "  print(\"fool_loss:\")\n",
    "  true_class = torch.argmax(target, dim=1).view(-1,1).long()\n",
    "  print(true_class)\n",
    "  print(\"input: \", input.shape)\n",
    "  a = input.gather(1, true_class)\n",
    "  print(a)\n",
    "  print(1 - a)\n",
    "  print(torch.mean(1 - a))\n",
    "  print(torch.log(torch.mean(1-a)))\n",
    "  print(\"\\n\\n\")\n",
    "  # this is wrong! first log should be taken, THEN mean.\n",
    "  return -1 * torch.log(torch.mean(1 - input.gather(1, true_class)))\n",
    "\n",
    "fool_loss_count = 0\n",
    "\n",
    "def fool_loss(model_output, target_labels):\n",
    "  target_labels = target_labels.view(-1, 1).long().cuda()\n",
    "  target_probabilities = model_output.gather(1, target_labels)\n",
    "  epsilon = 1e-10\n",
    "  # highest possible fool_loss is - log(1e-10) == 23\n",
    "  result = torch.mean(-1 * torch.log(target_probabilities + epsilon))\n",
    "  \n",
    "  global fool_loss_count\n",
    "  fool_loss_count += 1\n",
    "  if fool_loss_count % 20 == 0:\n",
    "    print(\"target probs {}, loss: {}: \".format(target_probabilities, result))\n",
    "  \n",
    "  return result\n",
    "\n",
    "\n",
    "def validation(gen_output, target):\n",
    "  perturbations, _, _, clean_images, _, _ = gen_output\n",
    "  perturbed_images = clean_images + perturbations\n",
    "  benign_preds = torch.argmax(arch(clean_images), 1)\n",
    "  adversary_preds = torch.argmax(arch(perturbed_images), 1)\n",
    "  return (benign_preds != adversary_preds).float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FVegHeYovws"
   },
   "outputs": [],
   "source": [
    "# class FeatureLoss(nn.Module):\n",
    "#     def __name__(self):\n",
    "#       return \"feature_loss\"\n",
    "  \n",
    "#     def __init__(self, dis, layers, layer_weights):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # define generator here \n",
    "#         self.dis = dis\n",
    "#         self.diversity_layers = layers\n",
    "#         self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "#         self.weights = layer_weights\n",
    "#         self.metric_names = [\"fool_loss\"] + [f\"div_loss_{i}\" for i in range(len(layers))] + ['triplet_conv'] + ['triplet_raw'] #+ ['triplet_loss_2']# Maybe Gram\n",
    "#         self.triplet_weight = 10.\n",
    "#         self.triplet_weight_noise = 5.\n",
    "#         self.triplet_weight_sm = 5.\n",
    "        \n",
    "#         self.triplet_hooks = hook_outputs([arch.m.features[4]], detach=False)\n",
    "    \n",
    "#     def make_features(self, x, clone=False):\n",
    "#         y = self.dis(x)\n",
    "#         return y, [(o.clone() if clone else o) for o in self.hooks.stored], [(o.clone() if clone else o) for o in self.triplet_hooks.stored]\n",
    "    \n",
    "#     #use the raw, and the conv triplet losses\n",
    "#     def forward(self, inp, target):\n",
    "#         sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "#         X_A = X_B + sigma_B\n",
    "#         X_A_pos = X_B + sigma_pos\n",
    "#         X_A_neg = X_B + sigma_neg\n",
    "        \n",
    "#         X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "        \n",
    "#         B_Y, _, _ = self.make_features(X_B)\n",
    "#         A_Y, A_feat, anchor_hook = self.make_features(X_A)\n",
    "#         _, S_feat, _ = self.make_features(X_S)\n",
    "#         pos_softmax, _, pos_hook = self.make_features(X_A_pos)\n",
    "#         neg_softmax, _, neg_hook = self.make_features(X_A_neg)\n",
    "        \n",
    "#         fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "      \n",
    "#         raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "#         weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "        \n",
    "# #         flatten = lambda x: x.view(x.shape[0], x.shape[1], -1)\n",
    "# #         raw_triplet_loss = triplet_loss(\n",
    "# #           flatten(anchor_hook[0]), flatten(pos_hook[0]), flatten(neg_hook[0]), \n",
    "# #           partial(cos_distance, dim=2), 1.4\n",
    "# #         )\n",
    "#         raw_triplet_losses = [triplet_loss(\n",
    "#           anchor_hook[0], pos_hook[0], neg_hook[0], mse_loss, 2.\n",
    "#         )] + [triplet_loss(\n",
    "#           sigma_B, sigma_pos, sigma_neg, l2_distance, 10.\n",
    "#         )]\n",
    "#         weighted_triplet_losses = [(5. * t_loss) for t_loss in raw_triplet_losses]\n",
    "    \n",
    "#         self.losses = [fooling_loss] + weighted_diversity_losses + weighted_triplet_losses\n",
    "#         self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses + weighted_triplet_losses))\n",
    "\n",
    "#         return sum(self.losses)\n",
    "\n",
    "# #     #use two types of triplet losses\n",
    "# #     def forward(self, inp, target):\n",
    "# #       sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "# #       X_A = X_B + sigma_B\n",
    "# #       X_A_pos = X_B + sigma_pos\n",
    "# #       X_A_neg = X_B + sigma_neg \n",
    "\n",
    "# #       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "# #       B_Y, _ = self.make_features(X_B)\n",
    "# #       A_Y, A_feat = self.make_features(X_A)\n",
    "# #       _, S_feat = self.make_features(X_S)\n",
    "# #       pos_softmax, _ = self.make_features(X_A_pos)\n",
    "# #       neg_softmax, _ = self.make_features(X_A_neg)\n",
    "\n",
    "# #       fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "# #       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "# #       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "      \n",
    "# #       raw_triplet_loss_sm = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "# #       weighted_triplet_loss_sm = raw_triplet_loss_sm * self.triplet_weight_sm\n",
    "      \n",
    "# #       raw_triplet_loss_noise = triplet_loss(sigma_B, sigma_pos, sigma_neg, l2_distance, 5.)\n",
    "# #       weighted_triplet_loss_noise = raw_triplet_loss_noise * self.triplet_weight_noise\n",
    "\n",
    "# #       self.losses = [fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss_sm, weighted_triplet_loss_noise] \n",
    "# #       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses + [weighted_triplet_loss_sm, weighted_triplet_loss_noise]))\n",
    "\n",
    "# #       return sum(self.losses)\n",
    "\n",
    "# #     # just fooling and diversity\n",
    "# #     def forward(self, inp, target):\n",
    "# #       sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "# #       X_A = X_B + sigma_B\n",
    "\n",
    "# #       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "# #       B_Y, _ = self.make_features(X_B)\n",
    "# #       A_Y, A_feat = self.make_features(X_A)\n",
    "# #       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "# #       fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "# #       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "# #       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "# #       self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "# #       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "# #       return sum(self.losses)  \n",
    "  \n",
    "#     def add_perturbation_shuffled(self, inp, perturbation):\n",
    "# #         j = torch.randperm(inp.shape[0])\n",
    "#         j = derangement(inp.shape[0])\n",
    "#         return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureLoss(nn.Module):\n",
    "    def __name__(self):\n",
    "      return \"feature_loss\"\n",
    "  \n",
    "    def __init__(self, dis, layers, layer_weights):\n",
    "        super().__init__()\n",
    "        \n",
    "        # define generator here \n",
    "        self.dis = dis\n",
    "        self.diversity_layers = layers\n",
    "        self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "        self.weights = layer_weights\n",
    "        self.metric_names = [\"fool_loss\"] #+ [f\"div_loss_{i}\" for i in range(len(layers))] #maybe Gram\n",
    "        \n",
    "        self.triplet_hooks = hook_outputs([arch.m.features[4]], detach=False)\n",
    "    \n",
    "    def make_features(self, x, clone=False):\n",
    "        y = self.dis(x)\n",
    "        return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "  \n",
    "    def forward(self, inp, target):\n",
    "      sigma_B, _, _, X_B, B_Y, z = inp\n",
    "\n",
    "      X_A = X_B + sigma_B\n",
    "      X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "      A_Y, A_feat = self.make_features(X_A)\n",
    "      _, S_feat = self.make_features(X_S)\n",
    "\n",
    "      chosen_labels = z.argmax(dim=1)\n",
    "      fooling_loss =  fool_loss(A_Y, chosen_labels)\n",
    "\n",
    "#       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "#       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "#       self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "#       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "      self.losses = [fooling_loss]\n",
    "      self.metrics = dict(zip(self.metric_names, [fooling_loss]))\n",
    "\n",
    "      return sum(self.losses)\n",
    "  \n",
    "    def add_perturbation_shuffled(self, inp, perturbation):\n",
    "#         j = torch.randperm(inp.shape[0])\n",
    "        j = derangement(inp.shape[0])\n",
    "        return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qd9gXUy_ovww"
   },
   "outputs": [],
   "source": [
    "feat_loss = FeatureLoss(arch, layers, layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfZKdYD2MSdi"
   },
   "outputs": [],
   "source": [
    "env.save_filename = 'vgg16_26'\n",
    "\n",
    "if Path(env.get_csv_path() + '.csv').exists(): raise FileExistsError(\"csv_path already exists\")\n",
    "if Path(env.get_models_path()).exists(): raise FileExistsError(\"models_path already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9J20CBLS8S9"
   },
   "outputs": [],
   "source": [
    "learn = None; gc.collect()\n",
    "csv_logger = partial(ImmediateCSVLogger, filename= env.temp_csv_path + '/' + env.save_filename)\n",
    "# learn = Learner(data, Gen(z_dim=10), loss_func = feat_loss, metrics=[validation], callback_fns=LossMetrics, opt_func = optim.SGD)\n",
    "# learn = Learner(data, Gen(z_dim=z_dim), loss_func = feat_loss, metrics=[validation], callback_fns=[LossMetrics, DiversityWeightsScheduler])\n",
    "learn = Learner(data, Gen(z_dim=z_dim), loss_func = feat_loss, metrics=[validation], callback_fns=[LossMetrics, csv_logger])\n",
    "# load_starting_point(learn, model_name, z_dim)\n",
    "# random_seed(42, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wk9E0AUm9rmn"
   },
   "outputs": [],
   "source": [
    "# learn.lr_find(1e-6, 1000)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wOZYzOHDEdB"
   },
   "outputs": [],
   "source": [
    "# !cp \"/content/gdrive/My Drive/DL/models/vgg16_12-last.pth\"  \"/content/\"\n",
    "# learn.load('/content/vgg16_12-last')\n",
    "\n",
    "# learn.load('/root/Derakhshani/adversarial/models/vgg16_10/vgg16_10_29')\n",
    "# learn.load('/root/Derakhshani/adversarial/models/vgg16_12-last')\n",
    "# learn.load('/root/Derakhshani/adversarial/models/resnet50-11_39')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 783
    },
    "colab_type": "code",
    "id": "LA1ffVbbEwQS",
    "outputId": "cb14c6fd-158b-4deb-c931-792b0fd7b3a2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2' class='' max='40', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      5.00% [2/40 08:51<2:48:27]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8.875056</td>\n",
       "      <td>9.114141</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>9.114142</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.931685</td>\n",
       "      <td>8.955181</td>\n",
       "      <td>0.874000</td>\n",
       "      <td>8.955181</td>\n",
       "      <td>04:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='850' class='' max='1125', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      75.56% [850/1125 03:02<00:58 8.5610]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 14.321199417114258: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 14.145423889160156: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 13.532535552978516: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 13.229262351989746: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.15],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.678140640258789: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 12.61186408996582: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 13.736177444458008: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 15.085898399353027: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 14.464235305786133: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 13.514619827270508: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 12.22598648071289: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 13.141674041748047: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.63896369934082: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.15],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.285297393798828: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 12.086355209350586: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.87930679321289: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 16.068126678466797: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.507015228271484: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 12.917906761169434: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 12.437744140625: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 12.151098251342773: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.53429126739502: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.02],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.586721420288086: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 12.099544525146484: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.650415420532227: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.04975700378418: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.708330154418945: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.516364097595215: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.239595413208008: \n",
      "target probs tensor([[    0.01],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.081628799438477: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.520586967468262: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.418218612670898: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.863018989562988: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.06],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.587871551513672: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.351470947265625: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.306182861328125: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.01],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.112030982971191: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.01],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.015119552612305: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.773290634155273: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.00566291809082: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.314306259155273: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.93706226348877: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.383441925048828: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.261958122253418: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.873247146606445: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.941341400146484: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.910009384155273: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.01],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.1747465133667: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.864353656768799: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.02],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.895130157470703: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.01],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.01],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.134016036987305: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.573369979858398: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.898838996887207: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.8729248046875: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.853273868560791: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.436752319335938: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0'), loss: 9.860919952392578: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0'), loss: 8.940740585327148: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0'), loss: 9.261320114135742: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0'), loss: 9.928600311279297: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0'), loss: 11.153566360473633: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.02]], device='cuda:0'), loss: 8.311981201171875: \n",
      "Better model found at epoch 0 with validation value: 0.859000027179718.\n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.49024772644043: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.918081283569336: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.956117630004883: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.04],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.970820426940918: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.014440536499023: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.815582275390625: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.028743743896484: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.768083572387695: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.601371765136719: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.653726577758789: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.304415702819824: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.01],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.648721694946289: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.057201385498047: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.089326858520508: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.05143928527832: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.01],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.31960391998291: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.437749862670898: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.456478118896484: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.208627700805664: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.521917343139648: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.761171817779541: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.85842514038086: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.124860763549805: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.116693496704102: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.700471878051758: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.114377975463867: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.14],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.997739315032959: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.205000877380371: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.345405578613281: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.151163101196289: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.685538291931152: \n",
      "target probs tensor([[0.01],\n",
      "        [0.00],\n",
      "        [0.00],\n",
      "        [0.00],\n",
      "        [0.00],\n",
      "        [0.00],\n",
      "        [0.02],\n",
      "        [0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.4766669273376465: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.003293991088867: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.82685661315918: \n",
      "target probs tensor([[0.00],\n",
      "        [0.00],\n",
      "        [0.00],\n",
      "        [0.00],\n",
      "        [0.00],\n",
      "        [0.00],\n",
      "        [0.00],\n",
      "        [0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.62581729888916: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.93893814086914: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.426600456237793: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.804550170898438: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.10953140258789: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.245576858520508: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[    0.00],\n",
      "        [    0.01],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.7410249710083: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.2778902053833: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.316314697265625: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.189708709716797: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.12445068359375: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.349672317504883: \n",
      "target probs tensor([[    0.02],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.517618179321289: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.37321662902832: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.784121990203857: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.18821907043457: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.953700065612793: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.997831344604492: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.01],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.467733383178711: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.957947731018066: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.01],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.302483558654785: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.752046585083008: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0'), loss: 8.240572929382324: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0'), loss: 8.660443305969238: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0'), loss: 8.865730285644531: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0'), loss: 10.357427597045898: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.01],\n",
      "        [    0.00]], device='cuda:0'), loss: 8.202646255493164: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0'), loss: 8.768539428710938: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0'), loss: 8.624314308166504: \n",
      "Better model found at epoch 1 with validation value: 0.8740000128746033.\n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.376202583312988: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.867374897003174: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.07],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.974285125732422: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.02],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.097450256347656: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.430804252624512: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.274881362915039: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.04],\n",
      "        [    0.01],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.167731285095215: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.098106384277344: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.281740188598633: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.156543731689453: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.075221061706543: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.02],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.097191333770752: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.938297271728516: \n",
      "target probs tensor([[0.00],\n",
      "        [0.00],\n",
      "        [0.00],\n",
      "        [0.00],\n",
      "        [0.00],\n",
      "        [0.00],\n",
      "        [0.00],\n",
      "        [0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.870891571044922: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.467182159423828: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.095741271972656: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.01],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.289938926696777: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 11.009500503540039: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.01],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.906370162963867: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.01],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.852688789367676: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.190485954284668: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.975447654724121: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.169992446899414: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.757637023925781: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.203399658203125: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.331820487976074: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.612549304962158: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.01],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.315349578857422: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.286674499511719: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.95454216003418: \n",
      "target probs tensor([[    0.02],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.114753723144531: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.648494720458984: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.14],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.68911361694336: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.01],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.452537536621094: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.918889999389648: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.746919631958008: \n",
      "target probs tensor([[    0.01],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.9901580810546875: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.01],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.727104187011719: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.942283630371094: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 10.085697174072266: \n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.38253402709961: \n",
      "target probs tensor([[0.00],\n",
      "        [0.00],\n",
      "        [0.02],\n",
      "        [0.00],\n",
      "        [0.00],\n",
      "        [0.01],\n",
      "        [0.00],\n",
      "        [0.00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.7836174964904785: \n"
     ]
    }
   ],
   "source": [
    "if mode == \"sanity_check\":\n",
    "  print(\"\\n\\n\\nWARNING: you are training on a sanity_check dataset.\\n\\n\\n\\n\")\n",
    "if len(learn.callback_fns) == 1:\n",
    "  print(\"\\n\\n\\nWARNING: you are not using the DiversityWeightsScheduler callback.\\n\\n\\n\")\n",
    "\n",
    "saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=env.save_filename + \"-best\")\n",
    "saver_every_epoch = SaveModelCallback(learn, every='epoch', name=env.save_filename)\n",
    "\n",
    "learn.fit(40, lr=5e-03, wd = 0., callbacks=[saver_best, saver_every_epoch])\n",
    "\n",
    "# learn.fit_one_cycle(8, max_lr=5e-01) #mohammad's setting that got 77 validation start on resnet with diversity loss on AdaptiveAvgPool2d\n",
    "# learn.fit_one_cycle(5, max_lr=2e-2) #used for vgg-19-bn\n",
    "# learn.fit_one_cycle(5, max_lr=3e-3) # used for resnet50\n",
    "\n",
    "shutil.copyfile(env.temp_csv_path + '/' + env.save_filename + \".csv\", env.get_csv_path() + '.csv')\n",
    "shutil.copytree(env.data_path/\"models\", env.get_models_path())\n",
    "shutil.rmtree(env.data_path/\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MO2fZ-hSSUzJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# z1 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "# z2 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "z1 = torch.tensor([0.8, -0.5] * 5).cuda()\n",
    "z2 = torch.tensor([-1.] * 10).cuda()\n",
    "print(\"z1: \", z1)\n",
    "print(\"z2: \", z2)\n",
    "print(\"distance: \", torch.norm(z1-z2,p=2))\n",
    "model = learn.model.eval()\n",
    "\n",
    "z_s = interpolate(z1, z2, 0.1)\n",
    "print(len(z_s))\n",
    "\n",
    "for i,z in enumerate(z_s):\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n",
    "  #img.save('./pics/' + str(i) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DGuGN7B7V0Xt"
   },
   "outputs": [],
   "source": [
    "def generate_perturbations(learn, n_perturbations):\n",
    "  initial_training_mode = learn.model.training\n",
    "  \n",
    "  model = learn.model.eval()\n",
    "  input_img = (learn.data.valid_ds[0][0].data)[None].cuda()\n",
    "  perturbations = []\n",
    "  for i in range(n_perturbations):\n",
    "    perturbation = model(input_img)[0].squeeze()\n",
    "    perturbations.append(perturbation)\n",
    "    \n",
    "  learn.model.train(initial_training_mode)  \n",
    "  return perturbations\n",
    "\n",
    "def compute_prediction_histogram(learn, perturbation, verbose=False):\n",
    "  pred_hist = [0] * 1000\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 and verbose: print (\"at batch no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbation[None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      pred_hist[pred] += 1\n",
    "  return pred_hist\n",
    "\n",
    "\n",
    "def compute_mean_prediction_histogram(learn, perturbations):\n",
    "  pred_histogram = torch.tensor([0] * 1000).detach_()\n",
    "  for j, perturbation in enumerate(perturbations):\n",
    "    pred_histogram_j = torch.tensor(compute_prediction_histogram(learn, perturbation, True)).detach_()\n",
    "    pred_histogram += pred_histogram_j\n",
    "    print(\"finished creating histogram for the {}th perturbation\".format(j))\n",
    "  \n",
    "  pred_histogram = pred_histogram.float() / len(perturbations)\n",
    "  return pred_histogram.tolist()\n",
    "\n",
    "\n",
    "def diversity(learn, n_perturbations, percentage):\n",
    "  pred_histogram = compute_mean_prediction_histogram(\n",
    "      learn, generate_perturbations(learn, n_perturbations)\n",
    "  )\n",
    "  print(\"finished creating the prediction histogram\")\n",
    "  pred_histogram_sum = np.sum(pred_histogram)\n",
    "\n",
    "  indexed_pred_histogram = [(i, hist_element) for i,hist_element in  \n",
    "                            enumerate(pred_histogram)]\n",
    "\n",
    "  indexed_pred_histogram.sort(key=lambda x: x[1], reverse = True)\n",
    "\n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  top_classes = []\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = indexed_pred_histogram[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / pred_histogram_sum) * 100.\n",
    "    top_classes.append(hist_elem[0])\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, indexed_pred_histogram, top_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Q8VUc3YH4vj5",
    "outputId": "62536287-c66a-4521-cfb5-8fa16cfada5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 100\n",
      "finished creating histogram for the 5th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "finished creating histogram for the 6th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "finished creating histogram for the 7th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "finished creating histogram for the 8th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "finished creating histogram for the 9th perturbation\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(456,\n",
       " [(828, 29.80),\n",
       "  (411, 29.20),\n",
       "  (794, 27.70),\n",
       "  (805, 22.80),\n",
       "  (870, 13.30),\n",
       "  (652, 12.80),\n",
       "  (186, 11.50),\n",
       "  (555, 11.00),\n",
       "  (39, 10.90),\n",
       "  (193, 10.30),\n",
       "  (230, 9.50),\n",
       "  (955, 8.90),\n",
       "  (865, 8.70),\n",
       "  (489, 8.60),\n",
       "  (422, 8.50),\n",
       "  (787, 8.00),\n",
       "  (538, 7.90),\n",
       "  (219, 7.70),\n",
       "  (661, 7.60),\n",
       "  (87, 7.50),\n",
       "  (311, 7.50),\n",
       "  (443, 6.90),\n",
       "  (586, 6.90),\n",
       "  (752, 6.90),\n",
       "  (670, 6.70),\n",
       "  (614, 6.60),\n",
       "  (751, 6.30),\n",
       "  (647, 6.00),\n",
       "  (61, 5.90),\n",
       "  (259, 5.70),\n",
       "  (545, 5.70),\n",
       "  (866, 5.70),\n",
       "  (971, 5.60),\n",
       "  (124, 5.50),\n",
       "  (861, 5.50),\n",
       "  (904, 5.50),\n",
       "  (420, 5.40),\n",
       "  (757, 5.20),\n",
       "  (156, 4.90),\n",
       "  (163, 4.90),\n",
       "  (819, 4.80),\n",
       "  (621, 4.70),\n",
       "  (430, 4.40),\n",
       "  (632, 4.40),\n",
       "  (142, 4.30),\n",
       "  (201, 4.30),\n",
       "  (319, 4.30),\n",
       "  (581, 4.30),\n",
       "  (635, 4.20),\n",
       "  (414, 4.00),\n",
       "  (850, 4.00),\n",
       "  (84, 3.80),\n",
       "  (192, 3.80),\n",
       "  (609, 3.80),\n",
       "  (160, 3.70),\n",
       "  (915, 3.70),\n",
       "  (791, 3.60),\n",
       "  (140, 3.50),\n",
       "  (151, 3.50),\n",
       "  (611, 3.40),\n",
       "  (781, 3.40),\n",
       "  (830, 3.40),\n",
       "  (48, 3.30),\n",
       "  (567, 3.30),\n",
       "  (207, 3.20),\n",
       "  (260, 3.20),\n",
       "  (727, 3.20),\n",
       "  (62, 3.10),\n",
       "  (264, 3.10),\n",
       "  (706, 3.10),\n",
       "  (12, 3.00),\n",
       "  (226, 3.00),\n",
       "  (369, 3.00),\n",
       "  (560, 3.00),\n",
       "  (665, 3.00),\n",
       "  (847, 2.90),\n",
       "  (580, 2.80),\n",
       "  (582, 2.80),\n",
       "  (588, 2.70),\n",
       "  (612, 2.70),\n",
       "  (685, 2.70),\n",
       "  (849, 2.70),\n",
       "  (292, 2.60),\n",
       "  (431, 2.60),\n",
       "  (491, 2.60),\n",
       "  (575, 2.60),\n",
       "  (788, 2.60),\n",
       "  (213, 2.50),\n",
       "  (271, 2.50),\n",
       "  (274, 2.50),\n",
       "  (413, 2.50),\n",
       "  (465, 2.50),\n",
       "  (868, 2.50),\n",
       "  (291, 2.40),\n",
       "  (410, 2.40),\n",
       "  (864, 2.40),\n",
       "  (981, 2.40),\n",
       "  (407, 2.30),\n",
       "  (409, 2.30),\n",
       "  (424, 2.30),\n",
       "  (97, 2.20),\n",
       "  (176, 2.20),\n",
       "  (381, 2.20),\n",
       "  (404, 2.20),\n",
       "  (556, 2.20),\n",
       "  (565, 2.20),\n",
       "  (892, 2.20),\n",
       "  (7, 2.10),\n",
       "  (205, 2.10),\n",
       "  (353, 2.10),\n",
       "  (691, 2.10),\n",
       "  (880, 2.10),\n",
       "  (40, 2.00),\n",
       "  (173, 2.00),\n",
       "  (343, 2.00),\n",
       "  (734, 2.00),\n",
       "  (762, 2.00),\n",
       "  (779, 2.00),\n",
       "  (355, 1.90),\n",
       "  (384, 1.90),\n",
       "  (464, 1.90),\n",
       "  (698, 1.90),\n",
       "  (729, 1.90),\n",
       "  (731, 1.90),\n",
       "  (790, 1.90),\n",
       "  (807, 1.90),\n",
       "  (144, 1.80),\n",
       "  (172, 1.80),\n",
       "  (349, 1.80),\n",
       "  (570, 1.80),\n",
       "  (668, 1.80),\n",
       "  (716, 1.80),\n",
       "  (738, 1.80),\n",
       "  (740, 1.80),\n",
       "  (831, 1.80),\n",
       "  (872, 1.80),\n",
       "  (189, 1.70),\n",
       "  (331, 1.70),\n",
       "  (341, 1.70),\n",
       "  (398, 1.70),\n",
       "  (417, 1.70),\n",
       "  (496, 1.70),\n",
       "  (515, 1.70),\n",
       "  (569, 1.70),\n",
       "  (641, 1.70),\n",
       "  (656, 1.70),\n",
       "  (721, 1.70),\n",
       "  (8, 1.60),\n",
       "  (86, 1.60),\n",
       "  (93, 1.60),\n",
       "  (129, 1.60),\n",
       "  (134, 1.60),\n",
       "  (345, 1.60),\n",
       "  (910, 1.60),\n",
       "  (546, 1.50),\n",
       "  (620, 1.50),\n",
       "  (743, 1.50),\n",
       "  (755, 1.50),\n",
       "  (822, 1.50),\n",
       "  (848, 1.50),\n",
       "  (858, 1.50),\n",
       "  (923, 1.50),\n",
       "  (992, 1.50),\n",
       "  (118, 1.40),\n",
       "  (132, 1.40),\n",
       "  (154, 1.40),\n",
       "  (203, 1.40),\n",
       "  (235, 1.40),\n",
       "  (258, 1.40),\n",
       "  (273, 1.40),\n",
       "  (281, 1.40),\n",
       "  (295, 1.40),\n",
       "  (300, 1.40),\n",
       "  (328, 1.40),\n",
       "  (375, 1.40),\n",
       "  (534, 1.40),\n",
       "  (603, 1.40),\n",
       "  (816, 1.40),\n",
       "  (920, 1.40),\n",
       "  (953, 1.40),\n",
       "  (15, 1.30),\n",
       "  (21, 1.30),\n",
       "  (46, 1.30),\n",
       "  (138, 1.30),\n",
       "  (253, 1.30),\n",
       "  (276, 1.30),\n",
       "  (327, 1.30),\n",
       "  (406, 1.30),\n",
       "  (423, 1.30),\n",
       "  (502, 1.30),\n",
       "  (624, 1.30),\n",
       "  (707, 1.30),\n",
       "  (750, 1.30),\n",
       "  (764, 1.30),\n",
       "  (770, 1.30),\n",
       "  (778, 1.30),\n",
       "  (837, 1.30),\n",
       "  (863, 1.30),\n",
       "  (897, 1.30),\n",
       "  (197, 1.20),\n",
       "  (242, 1.20),\n",
       "  (293, 1.20),\n",
       "  (309, 1.20),\n",
       "  (314, 1.20),\n",
       "  (315, 1.20),\n",
       "  (363, 1.20),\n",
       "  (393, 1.20),\n",
       "  (396, 1.20),\n",
       "  (401, 1.20),\n",
       "  (408, 1.20),\n",
       "  (428, 1.20),\n",
       "  (476, 1.20),\n",
       "  (518, 1.20),\n",
       "  (566, 1.20),\n",
       "  (599, 1.20),\n",
       "  (639, 1.20),\n",
       "  (679, 1.20),\n",
       "  (724, 1.20),\n",
       "  (783, 1.20),\n",
       "  (813, 1.20),\n",
       "  (889, 1.20),\n",
       "  (982, 1.20),\n",
       "  (987, 1.20),\n",
       "  (33, 1.10),\n",
       "  (83, 1.10),\n",
       "  (91, 1.10),\n",
       "  (121, 1.10),\n",
       "  (164, 1.10),\n",
       "  (171, 1.10),\n",
       "  (332, 1.10),\n",
       "  (334, 1.10),\n",
       "  (395, 1.10),\n",
       "  (448, 1.10),\n",
       "  (468, 1.10),\n",
       "  (477, 1.10),\n",
       "  (528, 1.10),\n",
       "  (637, 1.10),\n",
       "  (697, 1.10),\n",
       "  (723, 1.10),\n",
       "  (758, 1.10),\n",
       "  (803, 1.10),\n",
       "  (823, 1.10),\n",
       "  (860, 1.10),\n",
       "  (886, 1.10),\n",
       "  (890, 1.10),\n",
       "  (926, 1.10),\n",
       "  (937, 1.10),\n",
       "  (984, 1.10),\n",
       "  (25, 1.00),\n",
       "  (37, 1.00),\n",
       "  (45, 1.00),\n",
       "  (51, 1.00),\n",
       "  (52, 1.00),\n",
       "  (57, 1.00),\n",
       "  (66, 1.00),\n",
       "  (68, 1.00),\n",
       "  (72, 1.00),\n",
       "  (96, 1.00),\n",
       "  (101, 1.00),\n",
       "  (102, 1.00),\n",
       "  (120, 1.00),\n",
       "  (123, 1.00),\n",
       "  (128, 1.00),\n",
       "  (161, 1.00),\n",
       "  (170, 1.00),\n",
       "  (183, 1.00),\n",
       "  (198, 1.00),\n",
       "  (199, 1.00),\n",
       "  (218, 1.00),\n",
       "  (228, 1.00),\n",
       "  (255, 1.00),\n",
       "  (265, 1.00),\n",
       "  (301, 1.00),\n",
       "  (316, 1.00),\n",
       "  (321, 1.00),\n",
       "  (330, 1.00),\n",
       "  (336, 1.00),\n",
       "  (342, 1.00),\n",
       "  (347, 1.00),\n",
       "  (352, 1.00),\n",
       "  (376, 1.00),\n",
       "  (389, 1.00),\n",
       "  (399, 1.00),\n",
       "  (426, 1.00),\n",
       "  (429, 1.00),\n",
       "  (467, 1.00),\n",
       "  (498, 1.00),\n",
       "  (507, 1.00),\n",
       "  (509, 1.00),\n",
       "  (533, 1.00),\n",
       "  (571, 1.00),\n",
       "  (574, 1.00),\n",
       "  (625, 1.00),\n",
       "  (645, 1.00),\n",
       "  (658, 1.00),\n",
       "  (690, 1.00),\n",
       "  (694, 1.00),\n",
       "  (741, 1.00),\n",
       "  (748, 1.00),\n",
       "  (753, 1.00),\n",
       "  (754, 1.00),\n",
       "  (776, 1.00),\n",
       "  (852, 1.00),\n",
       "  (871, 1.00),\n",
       "  (875, 1.00),\n",
       "  (907, 1.00),\n",
       "  (917, 1.00),\n",
       "  (918, 1.00),\n",
       "  (939, 1.00),\n",
       "  (944, 1.00),\n",
       "  (24, 0.90),\n",
       "  (44, 0.90),\n",
       "  (47, 0.90),\n",
       "  (58, 0.90),\n",
       "  (252, 0.90),\n",
       "  (289, 0.90),\n",
       "  (378, 0.90),\n",
       "  (454, 0.90),\n",
       "  (474, 0.90),\n",
       "  (479, 0.90),\n",
       "  (487, 0.90),\n",
       "  (517, 0.90),\n",
       "  (537, 0.90),\n",
       "  (547, 0.90),\n",
       "  (572, 0.90),\n",
       "  (576, 0.90),\n",
       "  (640, 0.90),\n",
       "  (646, 0.90),\n",
       "  (684, 0.90),\n",
       "  (746, 0.90),\n",
       "  (756, 0.90),\n",
       "  (873, 0.90),\n",
       "  (932, 0.90),\n",
       "  (973, 0.90),\n",
       "  (76, 0.80),\n",
       "  (115, 0.80),\n",
       "  (131, 0.80),\n",
       "  (141, 0.80),\n",
       "  (185, 0.80),\n",
       "  (214, 0.80),\n",
       "  (272, 0.80),\n",
       "  (346, 0.80),\n",
       "  (360, 0.80),\n",
       "  (386, 0.80),\n",
       "  (447, 0.80),\n",
       "  (456, 0.80),\n",
       "  (483, 0.80),\n",
       "  (492, 0.80),\n",
       "  (511, 0.80),\n",
       "  (543, 0.80),\n",
       "  (558, 0.80),\n",
       "  (568, 0.80),\n",
       "  (585, 0.80),\n",
       "  (786, 0.80),\n",
       "  (812, 0.80),\n",
       "  (843, 0.80),\n",
       "  (38, 0.70),\n",
       "  (55, 0.70),\n",
       "  (202, 0.70),\n",
       "  (212, 0.70),\n",
       "  (286, 0.70),\n",
       "  (288, 0.70),\n",
       "  (307, 0.70),\n",
       "  (340, 0.70),\n",
       "  (416, 0.70),\n",
       "  (508, 0.70),\n",
       "  (526, 0.70),\n",
       "  (548, 0.70),\n",
       "  (550, 0.70),\n",
       "  (554, 0.70),\n",
       "  (561, 0.70),\n",
       "  (589, 0.70),\n",
       "  (598, 0.70),\n",
       "  (703, 0.70),\n",
       "  (730, 0.70),\n",
       "  (1, 0.60),\n",
       "  (31, 0.60),\n",
       "  (50, 0.60),\n",
       "  (75, 0.60),\n",
       "  (94, 0.60),\n",
       "  (113, 0.60),\n",
       "  (116, 0.60),\n",
       "  (133, 0.60),\n",
       "  (195, 0.60),\n",
       "  (233, 0.60),\n",
       "  (236, 0.60),\n",
       "  (238, 0.60),\n",
       "  (275, 0.60),\n",
       "  (290, 0.60),\n",
       "  (294, 0.60),\n",
       "  (303, 0.60),\n",
       "  (310, 0.60),\n",
       "  (361, 0.60),\n",
       "  (391, 0.60),\n",
       "  (412, 0.60),\n",
       "  (497, 0.60),\n",
       "  (506, 0.60),\n",
       "  (514, 0.60),\n",
       "  (523, 0.60),\n",
       "  (595, 0.60),\n",
       "  (636, 0.60),\n",
       "  (725, 0.60),\n",
       "  (760, 0.60),\n",
       "  (763, 0.60),\n",
       "  (767, 0.60),\n",
       "  (777, 0.60),\n",
       "  (836, 0.60),\n",
       "  (845, 0.60),\n",
       "  (867, 0.60),\n",
       "  (906, 0.60),\n",
       "  (0, 0.50),\n",
       "  (5, 0.50),\n",
       "  (30, 0.50),\n",
       "  (42, 0.50),\n",
       "  (99, 0.50),\n",
       "  (109, 0.50),\n",
       "  (155, 0.50),\n",
       "  (157, 0.50),\n",
       "  (168, 0.50),\n",
       "  (182, 0.50),\n",
       "  (194, 0.50),\n",
       "  (211, 0.50),\n",
       "  (227, 0.50),\n",
       "  (249, 0.50),\n",
       "  (251, 0.50),\n",
       "  (263, 0.50),\n",
       "  (282, 0.50),\n",
       "  (312, 0.50),\n",
       "  (313, 0.50),\n",
       "  (320, 0.50),\n",
       "  (354, 0.50),\n",
       "  (382, 0.50),\n",
       "  (383, 0.50),\n",
       "  (402, 0.50),\n",
       "  (480, 0.50),\n",
       "  (488, 0.50),\n",
       "  (490, 0.50),\n",
       "  (584, 0.50),\n",
       "  (607, 0.50),\n",
       "  (617, 0.50),\n",
       "  (660, 0.50),\n",
       "  (704, 0.50),\n",
       "  (739, 0.50),\n",
       "  (782, 0.50),\n",
       "  (801, 0.50),\n",
       "  (808, 0.50),\n",
       "  (815, 0.50),\n",
       "  (820, 0.50),\n",
       "  (834, 0.50),\n",
       "  (895, 0.50),\n",
       "  (912, 0.50),\n",
       "  (28, 0.40),\n",
       "  (56, 0.40),\n",
       "  (71, 0.40),\n",
       "  (74, 0.40),\n",
       "  (90, 0.40),\n",
       "  (177, 0.40),\n",
       "  (178, 0.40),\n",
       "  (191, 0.40),\n",
       "  (196, 0.40),\n",
       "  (204, 0.40),\n",
       "  (231, 0.40),\n",
       "  (254, 0.40),\n",
       "  (270, 0.40),\n",
       "  (368, 0.40),\n",
       "  (374, 0.40),\n",
       "  (392, 0.40),\n",
       "  (394, 0.40),\n",
       "  (397, 0.40),\n",
       "  (427, 0.40),\n",
       "  (436, 0.40),\n",
       "  (444, 0.40),\n",
       "  (457, 0.40),\n",
       "  (463, 0.40),\n",
       "  (471, 0.40),\n",
       "  (494, 0.40),\n",
       "  (516, 0.40),\n",
       "  (520, 0.40),\n",
       "  (522, 0.40),\n",
       "  (535, 0.40),\n",
       "  (541, 0.40),\n",
       "  (552, 0.40),\n",
       "  (557, 0.40),\n",
       "  (579, 0.40),\n",
       "  (597, 0.40),\n",
       "  (667, 0.40),\n",
       "  (688, 0.40),\n",
       "  (709, 0.40),\n",
       "  (719, 0.40),\n",
       "  (772, 0.40),\n",
       "  (784, 0.40),\n",
       "  (879, 0.40),\n",
       "  (996, 0.40),\n",
       "  (997, 0.40),\n",
       "  (10, 0.30),\n",
       "  (16, 0.30),\n",
       "  (17, 0.30),\n",
       "  (23, 0.30),\n",
       "  (89, 0.30),\n",
       "  (92, 0.30),\n",
       "  (119, 0.30),\n",
       "  (122, 0.30),\n",
       "  (135, 0.30),\n",
       "  (184, 0.30),\n",
       "  (188, 0.30),\n",
       "  (217, 0.30),\n",
       "  (221, 0.30),\n",
       "  (232, 0.30),\n",
       "  (239, 0.30),\n",
       "  (246, 0.30),\n",
       "  (250, 0.30),\n",
       "  (284, 0.30),\n",
       "  (285, 0.30),\n",
       "  (308, 0.30),\n",
       "  (318, 0.30),\n",
       "  (325, 0.30),\n",
       "  (365, 0.30),\n",
       "  (425, 0.30),\n",
       "  (451, 0.30),\n",
       "  (459, 0.30),\n",
       "  (484, 0.30),\n",
       "  (486, 0.30),\n",
       "  (512, 0.30),\n",
       "  (513, 0.30),\n",
       "  (562, 0.30),\n",
       "  (591, 0.30),\n",
       "  (602, 0.30),\n",
       "  (608, 0.30),\n",
       "  (629, 0.30),\n",
       "  (651, 0.30),\n",
       "  (676, 0.30),\n",
       "  (687, 0.30),\n",
       "  (692, 0.30),\n",
       "  (722, 0.30),\n",
       "  (737, 0.30),\n",
       "  (745, 0.30),\n",
       "  (800, 0.30),\n",
       "  (802, 0.30),\n",
       "  (851, 0.30),\n",
       "  (884, 0.30),\n",
       "  (898, 0.30),\n",
       "  (909, 0.30),\n",
       "  (966, 0.30),\n",
       "  (11, 0.20),\n",
       "  (22, 0.20),\n",
       "  (41, 0.20),\n",
       "  (82, 0.20),\n",
       "  (88, 0.20),\n",
       "  (98, 0.20),\n",
       "  (107, 0.20),\n",
       "  (110, 0.20),\n",
       "  (153, 0.20),\n",
       "  (162, 0.20),\n",
       "  (169, 0.20),\n",
       "  (187, 0.20),\n",
       "  (215, 0.20),\n",
       "  (237, 0.20),\n",
       "  (241, 0.20),\n",
       "  (256, 0.20),\n",
       "  (287, 0.20),\n",
       "  (302, 0.20),\n",
       "  (333, 0.20),\n",
       "  (348, 0.20),\n",
       "  (405, 0.20),\n",
       "  (415, 0.20),\n",
       "  (437, 0.20),\n",
       "  (439, 0.20),\n",
       "  (442, 0.20),\n",
       "  (461, 0.20),\n",
       "  (472, 0.20),\n",
       "  (499, 0.20),\n",
       "  (531, 0.20),\n",
       "  (539, 0.20),\n",
       "  (577, 0.20),\n",
       "  (587, 0.20),\n",
       "  (592, 0.20),\n",
       "  (601, 0.20),\n",
       "  (618, 0.20),\n",
       "  (619, 0.20),\n",
       "  (627, 0.20),\n",
       "  (642, 0.20),\n",
       "  (674, 0.20),\n",
       "  (693, 0.20),\n",
       "  (735, 0.20),\n",
       "  (759, 0.20),\n",
       "  (761, 0.20),\n",
       "  (765, 0.20),\n",
       "  (796, 0.20),\n",
       "  (840, 0.20),\n",
       "  (854, 0.20),\n",
       "  (869, 0.20),\n",
       "  (934, 0.20),\n",
       "  (951, 0.20),\n",
       "  (956, 0.20),\n",
       "  (963, 0.20),\n",
       "  (985, 0.20),\n",
       "  (990, 0.20),\n",
       "  (998, 0.20),\n",
       "  (999, 0.20),\n",
       "  (9, 0.10),\n",
       "  (64, 0.10),\n",
       "  (77, 0.10),\n",
       "  (78, 0.10),\n",
       "  (85, 0.10),\n",
       "  (106, 0.10),\n",
       "  (112, 0.10),\n",
       "  (127, 0.10),\n",
       "  (146, 0.10),\n",
       "  (180, 0.10),\n",
       "  (181, 0.10),\n",
       "  (200, 0.10),\n",
       "  (209, 0.10),\n",
       "  (222, 0.10),\n",
       "  (224, 0.10),\n",
       "  (234, 0.10),\n",
       "  (244, 0.10),\n",
       "  (245, 0.10),\n",
       "  (247, 0.10),\n",
       "  (257, 0.10),\n",
       "  (266, 0.10),\n",
       "  (269, 0.10),\n",
       "  (280, 0.10),\n",
       "  (283, 0.10),\n",
       "  (306, 0.10),\n",
       "  (338, 0.10),\n",
       "  (344, 0.10),\n",
       "  (364, 0.10),\n",
       "  (367, 0.10),\n",
       "  (377, 0.10),\n",
       "  (388, 0.10),\n",
       "  (418, 0.10),\n",
       "  (432, 0.10),\n",
       "  (440, 0.10),\n",
       "  (446, 0.10),\n",
       "  (450, 0.10),\n",
       "  (455, 0.10),\n",
       "  (485, 0.10),\n",
       "  (495, 0.10),\n",
       "  (504, 0.10),\n",
       "  (505, 0.10),\n",
       "  (521, 0.10),\n",
       "  (524, 0.10),\n",
       "  (529, 0.10),\n",
       "  (530, 0.10),\n",
       "  (532, 0.10),\n",
       "  (536, 0.10),\n",
       "  (544, 0.10),\n",
       "  (551, 0.10),\n",
       "  (553, 0.10),\n",
       "  (593, 0.10),\n",
       "  (594, 0.10),\n",
       "  (605, 0.10),\n",
       "  (638, 0.10),\n",
       "  (650, 0.10),\n",
       "  (683, 0.10),\n",
       "  (700, 0.10),\n",
       "  (701, 0.10),\n",
       "  (712, 0.10),\n",
       "  (720, 0.10),\n",
       "  (732, 0.10),\n",
       "  (774, 0.10),\n",
       "  (795, 0.10),\n",
       "  (797, 0.10),\n",
       "  (817, 0.10),\n",
       "  (826, 0.10),\n",
       "  (833, 0.10),\n",
       "  (835, 0.10),\n",
       "  (842, 0.10),\n",
       "  (846, 0.10),\n",
       "  (856, 0.10),\n",
       "  (857, 0.10),\n",
       "  (859, 0.10),\n",
       "  (874, 0.10),\n",
       "  (877, 0.10),\n",
       "  (891, 0.10),\n",
       "  (896, 0.10),\n",
       "  (899, 0.10),\n",
       "  (921, 0.10),\n",
       "  (929, 0.10),\n",
       "  (943, 0.10),\n",
       "  (950, 0.10),\n",
       "  (954, 0.10),\n",
       "  (986, 0.10),\n",
       "  (995, 0.10),\n",
       "  (2, 0.00),\n",
       "  (3, 0.00),\n",
       "  (4, 0.00),\n",
       "  (6, 0.00),\n",
       "  (13, 0.00),\n",
       "  (14, 0.00),\n",
       "  (18, 0.00),\n",
       "  (19, 0.00),\n",
       "  (20, 0.00),\n",
       "  (26, 0.00),\n",
       "  (27, 0.00),\n",
       "  (29, 0.00),\n",
       "  (32, 0.00),\n",
       "  (34, 0.00),\n",
       "  (35, 0.00),\n",
       "  (36, 0.00),\n",
       "  (43, 0.00),\n",
       "  (49, 0.00),\n",
       "  (53, 0.00),\n",
       "  (54, 0.00),\n",
       "  (59, 0.00),\n",
       "  (60, 0.00),\n",
       "  (63, 0.00),\n",
       "  (65, 0.00),\n",
       "  (67, 0.00),\n",
       "  (69, 0.00),\n",
       "  (70, 0.00),\n",
       "  (73, 0.00),\n",
       "  (79, 0.00),\n",
       "  (80, 0.00),\n",
       "  (81, 0.00),\n",
       "  (95, 0.00),\n",
       "  (100, 0.00),\n",
       "  (103, 0.00),\n",
       "  (104, 0.00),\n",
       "  (105, 0.00),\n",
       "  (108, 0.00),\n",
       "  (111, 0.00),\n",
       "  (114, 0.00),\n",
       "  (117, 0.00),\n",
       "  (125, 0.00),\n",
       "  (126, 0.00),\n",
       "  (130, 0.00),\n",
       "  (136, 0.00),\n",
       "  (137, 0.00),\n",
       "  (139, 0.00),\n",
       "  (143, 0.00),\n",
       "  (145, 0.00),\n",
       "  (147, 0.00),\n",
       "  (148, 0.00),\n",
       "  (149, 0.00),\n",
       "  (150, 0.00),\n",
       "  (152, 0.00),\n",
       "  (158, 0.00),\n",
       "  (159, 0.00),\n",
       "  (165, 0.00),\n",
       "  (166, 0.00),\n",
       "  (167, 0.00),\n",
       "  (174, 0.00),\n",
       "  (175, 0.00),\n",
       "  (179, 0.00),\n",
       "  (190, 0.00),\n",
       "  (206, 0.00),\n",
       "  (208, 0.00),\n",
       "  (210, 0.00),\n",
       "  (216, 0.00),\n",
       "  (220, 0.00),\n",
       "  (223, 0.00),\n",
       "  (225, 0.00),\n",
       "  (229, 0.00),\n",
       "  (240, 0.00),\n",
       "  (243, 0.00),\n",
       "  (248, 0.00),\n",
       "  (261, 0.00),\n",
       "  (262, 0.00),\n",
       "  (267, 0.00),\n",
       "  (268, 0.00),\n",
       "  (277, 0.00),\n",
       "  (278, 0.00),\n",
       "  (279, 0.00),\n",
       "  (296, 0.00),\n",
       "  (297, 0.00),\n",
       "  (298, 0.00),\n",
       "  (299, 0.00),\n",
       "  (304, 0.00),\n",
       "  (305, 0.00),\n",
       "  (317, 0.00),\n",
       "  (322, 0.00),\n",
       "  (323, 0.00),\n",
       "  (324, 0.00),\n",
       "  (326, 0.00),\n",
       "  (329, 0.00),\n",
       "  (335, 0.00),\n",
       "  (337, 0.00),\n",
       "  (339, 0.00),\n",
       "  (350, 0.00),\n",
       "  (351, 0.00),\n",
       "  (356, 0.00),\n",
       "  (357, 0.00),\n",
       "  (358, 0.00),\n",
       "  (359, 0.00),\n",
       "  (362, 0.00),\n",
       "  (366, 0.00),\n",
       "  (370, 0.00),\n",
       "  (371, 0.00),\n",
       "  (372, 0.00),\n",
       "  (373, 0.00),\n",
       "  (379, 0.00),\n",
       "  (380, 0.00),\n",
       "  (385, 0.00),\n",
       "  (387, 0.00),\n",
       "  (390, 0.00),\n",
       "  (400, 0.00),\n",
       "  (403, 0.00),\n",
       "  (419, 0.00),\n",
       "  (421, 0.00),\n",
       "  (433, 0.00),\n",
       "  (434, 0.00),\n",
       "  (435, 0.00),\n",
       "  (438, 0.00),\n",
       "  (441, 0.00),\n",
       "  (445, 0.00),\n",
       "  (449, 0.00),\n",
       "  (452, 0.00),\n",
       "  (453, 0.00),\n",
       "  (458, 0.00),\n",
       "  (460, 0.00),\n",
       "  (462, 0.00),\n",
       "  (466, 0.00),\n",
       "  (469, 0.00),\n",
       "  (470, 0.00),\n",
       "  (473, 0.00),\n",
       "  (475, 0.00),\n",
       "  (478, 0.00),\n",
       "  (481, 0.00),\n",
       "  (482, 0.00),\n",
       "  (493, 0.00),\n",
       "  (500, 0.00),\n",
       "  (501, 0.00),\n",
       "  (503, 0.00),\n",
       "  (510, 0.00),\n",
       "  (519, 0.00),\n",
       "  (525, 0.00),\n",
       "  (527, 0.00),\n",
       "  (540, 0.00),\n",
       "  (542, 0.00),\n",
       "  (549, 0.00),\n",
       "  (559, 0.00),\n",
       "  (563, 0.00),\n",
       "  (564, 0.00),\n",
       "  (573, 0.00),\n",
       "  (578, 0.00),\n",
       "  (583, 0.00),\n",
       "  (590, 0.00),\n",
       "  (596, 0.00),\n",
       "  (600, 0.00),\n",
       "  (604, 0.00),\n",
       "  (606, 0.00),\n",
       "  (610, 0.00),\n",
       "  (613, 0.00),\n",
       "  (615, 0.00),\n",
       "  (616, 0.00),\n",
       "  (622, 0.00),\n",
       "  (623, 0.00),\n",
       "  (626, 0.00),\n",
       "  (628, 0.00),\n",
       "  (630, 0.00),\n",
       "  (631, 0.00),\n",
       "  (633, 0.00),\n",
       "  (634, 0.00),\n",
       "  (643, 0.00),\n",
       "  (644, 0.00),\n",
       "  (648, 0.00),\n",
       "  (649, 0.00),\n",
       "  (653, 0.00),\n",
       "  (654, 0.00),\n",
       "  (655, 0.00),\n",
       "  (657, 0.00),\n",
       "  (659, 0.00),\n",
       "  (662, 0.00),\n",
       "  (663, 0.00),\n",
       "  (664, 0.00),\n",
       "  (666, 0.00),\n",
       "  (669, 0.00),\n",
       "  (671, 0.00),\n",
       "  (672, 0.00),\n",
       "  (673, 0.00),\n",
       "  (675, 0.00),\n",
       "  (677, 0.00),\n",
       "  (678, 0.00),\n",
       "  (680, 0.00),\n",
       "  (681, 0.00),\n",
       "  (682, 0.00),\n",
       "  (686, 0.00),\n",
       "  (689, 0.00),\n",
       "  (695, 0.00),\n",
       "  (696, 0.00),\n",
       "  (699, 0.00),\n",
       "  (702, 0.00),\n",
       "  (705, 0.00),\n",
       "  (708, 0.00),\n",
       "  (710, 0.00),\n",
       "  (711, 0.00),\n",
       "  (713, 0.00),\n",
       "  (714, 0.00),\n",
       "  (715, 0.00),\n",
       "  (717, 0.00),\n",
       "  (718, 0.00),\n",
       "  (726, 0.00),\n",
       "  (728, 0.00),\n",
       "  (733, 0.00),\n",
       "  (736, 0.00),\n",
       "  (742, 0.00),\n",
       "  (744, 0.00),\n",
       "  (747, 0.00),\n",
       "  (749, 0.00),\n",
       "  (766, 0.00),\n",
       "  (768, 0.00),\n",
       "  (769, 0.00),\n",
       "  (771, 0.00),\n",
       "  (773, 0.00),\n",
       "  (775, 0.00),\n",
       "  (780, 0.00),\n",
       "  (785, 0.00),\n",
       "  (789, 0.00),\n",
       "  (792, 0.00),\n",
       "  (793, 0.00),\n",
       "  (798, 0.00),\n",
       "  (799, 0.00),\n",
       "  (804, 0.00),\n",
       "  (806, 0.00),\n",
       "  (809, 0.00),\n",
       "  (810, 0.00),\n",
       "  (811, 0.00),\n",
       "  (814, 0.00),\n",
       "  (818, 0.00),\n",
       "  (821, 0.00),\n",
       "  (824, 0.00),\n",
       "  (825, 0.00),\n",
       "  (827, 0.00),\n",
       "  (829, 0.00),\n",
       "  (832, 0.00),\n",
       "  (838, 0.00),\n",
       "  (839, 0.00),\n",
       "  (841, 0.00),\n",
       "  (844, 0.00),\n",
       "  (853, 0.00),\n",
       "  (855, 0.00),\n",
       "  (862, 0.00),\n",
       "  (876, 0.00),\n",
       "  (878, 0.00),\n",
       "  (881, 0.00),\n",
       "  (882, 0.00),\n",
       "  (883, 0.00),\n",
       "  (885, 0.00),\n",
       "  (887, 0.00),\n",
       "  (888, 0.00),\n",
       "  (893, 0.00),\n",
       "  (894, 0.00),\n",
       "  (900, 0.00),\n",
       "  (901, 0.00),\n",
       "  (902, 0.00),\n",
       "  (903, 0.00),\n",
       "  (905, 0.00),\n",
       "  (908, 0.00),\n",
       "  (911, 0.00),\n",
       "  (913, 0.00),\n",
       "  (914, 0.00),\n",
       "  (916, 0.00),\n",
       "  (919, 0.00),\n",
       "  (922, 0.00),\n",
       "  (924, 0.00),\n",
       "  (925, 0.00),\n",
       "  (927, 0.00),\n",
       "  (928, 0.00),\n",
       "  (930, 0.00),\n",
       "  (931, 0.00),\n",
       "  (933, 0.00),\n",
       "  (935, 0.00),\n",
       "  (936, 0.00),\n",
       "  (938, 0.00),\n",
       "  (940, 0.00),\n",
       "  (941, 0.00),\n",
       "  (942, 0.00),\n",
       "  (945, 0.00),\n",
       "  (946, 0.00),\n",
       "  (947, 0.00),\n",
       "  (948, 0.00),\n",
       "  (949, 0.00),\n",
       "  (952, 0.00),\n",
       "  (957, 0.00),\n",
       "  (958, 0.00),\n",
       "  (959, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (962, 0.00),\n",
       "  (964, 0.00),\n",
       "  (965, 0.00),\n",
       "  (967, 0.00),\n",
       "  (968, 0.00),\n",
       "  (969, 0.00),\n",
       "  (970, 0.00),\n",
       "  (972, 0.00),\n",
       "  (974, 0.00),\n",
       "  (975, 0.00),\n",
       "  (976, 0.00),\n",
       "  (977, 0.00),\n",
       "  (978, 0.00),\n",
       "  (979, 0.00),\n",
       "  (980, 0.00),\n",
       "  (983, 0.00),\n",
       "  (988, 0.00),\n",
       "  (989, 0.00),\n",
       "  (991, 0.00),\n",
       "  (993, 0.00),\n",
       "  (994, 0.00)],\n",
       " [828,\n",
       "  411,\n",
       "  794,\n",
       "  805,\n",
       "  870,\n",
       "  652,\n",
       "  186,\n",
       "  555,\n",
       "  39,\n",
       "  193,\n",
       "  230,\n",
       "  955,\n",
       "  865,\n",
       "  489,\n",
       "  422,\n",
       "  787,\n",
       "  538,\n",
       "  219,\n",
       "  661,\n",
       "  87,\n",
       "  311,\n",
       "  443,\n",
       "  586,\n",
       "  752,\n",
       "  670,\n",
       "  614,\n",
       "  751,\n",
       "  647,\n",
       "  61,\n",
       "  259,\n",
       "  545,\n",
       "  866,\n",
       "  971,\n",
       "  124,\n",
       "  861,\n",
       "  904,\n",
       "  420,\n",
       "  757,\n",
       "  156,\n",
       "  163,\n",
       "  819,\n",
       "  621,\n",
       "  430,\n",
       "  632,\n",
       "  142,\n",
       "  201,\n",
       "  319,\n",
       "  581,\n",
       "  635,\n",
       "  414,\n",
       "  850,\n",
       "  84,\n",
       "  192,\n",
       "  609,\n",
       "  160,\n",
       "  915,\n",
       "  791,\n",
       "  140,\n",
       "  151,\n",
       "  611,\n",
       "  781,\n",
       "  830,\n",
       "  48,\n",
       "  567,\n",
       "  207,\n",
       "  260,\n",
       "  727,\n",
       "  62,\n",
       "  264,\n",
       "  706,\n",
       "  12,\n",
       "  226,\n",
       "  369,\n",
       "  560,\n",
       "  665,\n",
       "  847,\n",
       "  580,\n",
       "  582,\n",
       "  588,\n",
       "  612,\n",
       "  685,\n",
       "  849,\n",
       "  292,\n",
       "  431,\n",
       "  491,\n",
       "  575,\n",
       "  788,\n",
       "  213,\n",
       "  271,\n",
       "  274,\n",
       "  413,\n",
       "  465,\n",
       "  868,\n",
       "  291,\n",
       "  410,\n",
       "  864,\n",
       "  981,\n",
       "  407,\n",
       "  409,\n",
       "  424,\n",
       "  97,\n",
       "  176,\n",
       "  381,\n",
       "  404,\n",
       "  556,\n",
       "  565,\n",
       "  892,\n",
       "  7,\n",
       "  205,\n",
       "  353,\n",
       "  691,\n",
       "  880,\n",
       "  40,\n",
       "  173,\n",
       "  343,\n",
       "  734,\n",
       "  762,\n",
       "  779,\n",
       "  355,\n",
       "  384,\n",
       "  464,\n",
       "  698,\n",
       "  729,\n",
       "  731,\n",
       "  790,\n",
       "  807,\n",
       "  144,\n",
       "  172,\n",
       "  349,\n",
       "  570,\n",
       "  668,\n",
       "  716,\n",
       "  738,\n",
       "  740,\n",
       "  831,\n",
       "  872,\n",
       "  189,\n",
       "  331,\n",
       "  341,\n",
       "  398,\n",
       "  417,\n",
       "  496,\n",
       "  515,\n",
       "  569,\n",
       "  641,\n",
       "  656,\n",
       "  721,\n",
       "  8,\n",
       "  86,\n",
       "  93,\n",
       "  129,\n",
       "  134,\n",
       "  345,\n",
       "  910,\n",
       "  546,\n",
       "  620,\n",
       "  743,\n",
       "  755,\n",
       "  822,\n",
       "  848,\n",
       "  858,\n",
       "  923,\n",
       "  992,\n",
       "  118,\n",
       "  132,\n",
       "  154,\n",
       "  203,\n",
       "  235,\n",
       "  258,\n",
       "  273,\n",
       "  281,\n",
       "  295,\n",
       "  300,\n",
       "  328,\n",
       "  375,\n",
       "  534,\n",
       "  603,\n",
       "  816,\n",
       "  920,\n",
       "  953,\n",
       "  15,\n",
       "  21,\n",
       "  46,\n",
       "  138,\n",
       "  253,\n",
       "  276,\n",
       "  327,\n",
       "  406,\n",
       "  423,\n",
       "  502,\n",
       "  624,\n",
       "  707,\n",
       "  750,\n",
       "  764,\n",
       "  770,\n",
       "  778,\n",
       "  837,\n",
       "  863,\n",
       "  897,\n",
       "  197,\n",
       "  242,\n",
       "  293,\n",
       "  309,\n",
       "  314,\n",
       "  315,\n",
       "  363,\n",
       "  393,\n",
       "  396,\n",
       "  401,\n",
       "  408,\n",
       "  428,\n",
       "  476,\n",
       "  518,\n",
       "  566,\n",
       "  599,\n",
       "  639,\n",
       "  679,\n",
       "  724,\n",
       "  783,\n",
       "  813,\n",
       "  889,\n",
       "  982,\n",
       "  987,\n",
       "  33,\n",
       "  83,\n",
       "  91,\n",
       "  121,\n",
       "  164,\n",
       "  171,\n",
       "  332,\n",
       "  334,\n",
       "  395,\n",
       "  448,\n",
       "  468,\n",
       "  477,\n",
       "  528,\n",
       "  637,\n",
       "  697,\n",
       "  723,\n",
       "  758,\n",
       "  803,\n",
       "  823,\n",
       "  860,\n",
       "  886,\n",
       "  890,\n",
       "  926,\n",
       "  937,\n",
       "  984,\n",
       "  25,\n",
       "  37,\n",
       "  45,\n",
       "  51,\n",
       "  52,\n",
       "  57,\n",
       "  66,\n",
       "  68,\n",
       "  72,\n",
       "  96,\n",
       "  101,\n",
       "  102,\n",
       "  120,\n",
       "  123,\n",
       "  128,\n",
       "  161,\n",
       "  170,\n",
       "  183,\n",
       "  198,\n",
       "  199,\n",
       "  218,\n",
       "  228,\n",
       "  255,\n",
       "  265,\n",
       "  301,\n",
       "  316,\n",
       "  321,\n",
       "  330,\n",
       "  336,\n",
       "  342,\n",
       "  347,\n",
       "  352,\n",
       "  376,\n",
       "  389,\n",
       "  399,\n",
       "  426,\n",
       "  429,\n",
       "  467,\n",
       "  498,\n",
       "  507,\n",
       "  509,\n",
       "  533,\n",
       "  571,\n",
       "  574,\n",
       "  625,\n",
       "  645,\n",
       "  658,\n",
       "  690,\n",
       "  694,\n",
       "  741,\n",
       "  748,\n",
       "  753,\n",
       "  754,\n",
       "  776,\n",
       "  852,\n",
       "  871,\n",
       "  875,\n",
       "  907,\n",
       "  917,\n",
       "  918,\n",
       "  939,\n",
       "  944,\n",
       "  24,\n",
       "  44,\n",
       "  47,\n",
       "  58,\n",
       "  252,\n",
       "  289,\n",
       "  378,\n",
       "  454,\n",
       "  474,\n",
       "  479,\n",
       "  487,\n",
       "  517,\n",
       "  537,\n",
       "  547,\n",
       "  572,\n",
       "  576,\n",
       "  640,\n",
       "  646,\n",
       "  684,\n",
       "  746,\n",
       "  756,\n",
       "  873,\n",
       "  932,\n",
       "  973,\n",
       "  76,\n",
       "  115,\n",
       "  131,\n",
       "  141,\n",
       "  185,\n",
       "  214,\n",
       "  272,\n",
       "  346,\n",
       "  360,\n",
       "  386,\n",
       "  447,\n",
       "  456,\n",
       "  483,\n",
       "  492,\n",
       "  511,\n",
       "  543,\n",
       "  558,\n",
       "  568,\n",
       "  585,\n",
       "  786,\n",
       "  812,\n",
       "  843,\n",
       "  38,\n",
       "  55,\n",
       "  202,\n",
       "  212,\n",
       "  286,\n",
       "  288,\n",
       "  307,\n",
       "  340,\n",
       "  416,\n",
       "  508,\n",
       "  526,\n",
       "  548,\n",
       "  550,\n",
       "  554,\n",
       "  561,\n",
       "  589,\n",
       "  598,\n",
       "  703,\n",
       "  730,\n",
       "  1,\n",
       "  31,\n",
       "  50,\n",
       "  75,\n",
       "  94,\n",
       "  113,\n",
       "  116,\n",
       "  133,\n",
       "  195,\n",
       "  233,\n",
       "  236,\n",
       "  238,\n",
       "  275,\n",
       "  290,\n",
       "  294,\n",
       "  303,\n",
       "  310,\n",
       "  361,\n",
       "  391,\n",
       "  412,\n",
       "  497,\n",
       "  506,\n",
       "  514,\n",
       "  523,\n",
       "  595,\n",
       "  636,\n",
       "  725,\n",
       "  760,\n",
       "  763,\n",
       "  767,\n",
       "  777,\n",
       "  836,\n",
       "  845,\n",
       "  867,\n",
       "  906,\n",
       "  0,\n",
       "  5,\n",
       "  30,\n",
       "  42,\n",
       "  99,\n",
       "  109,\n",
       "  155,\n",
       "  157,\n",
       "  168,\n",
       "  182,\n",
       "  194,\n",
       "  211,\n",
       "  227,\n",
       "  249,\n",
       "  251,\n",
       "  263,\n",
       "  282,\n",
       "  312,\n",
       "  313,\n",
       "  320,\n",
       "  354,\n",
       "  382,\n",
       "  383,\n",
       "  402,\n",
       "  480,\n",
       "  488,\n",
       "  490,\n",
       "  584,\n",
       "  607,\n",
       "  617,\n",
       "  660,\n",
       "  704,\n",
       "  739,\n",
       "  782,\n",
       "  801,\n",
       "  808,\n",
       "  815,\n",
       "  820,\n",
       "  834,\n",
       "  895,\n",
       "  912,\n",
       "  28,\n",
       "  56,\n",
       "  71,\n",
       "  74,\n",
       "  90])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%precision 2\n",
    "n, hist, tk = diversity(learn, 10, 95)\n",
    "n, hist, tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rNhaIflDMKtq",
    "outputId": "0a18884c-0e92-4f1b-9c1e-b6295fbea01f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdd5e9eca58>]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd8HOWdP/DPV5K7jasMxgW5QSjBFGFayBFDCC2UEAhwIcCRkOQXLuQud5xpgRAC5I5QkhCCCaYGQwLGOC6AGxhckXuRu2W5yJJsyapW2d3n98fOSrOzMzszu7Nt9Hm/Xn5ZOzs788zMM9955ikzopQCERHlvrxMJ4CIiLzBgE5E5BMM6EREPsGATkTkEwzoREQ+wYBOROQTDOhERD7BgE5E5BMM6EREPlGQzpUNGTJEFRUVpXOVREQ5b9WqVYeUUoV286U1oBcVFaGkpCSdqyQiynkissfJfKxyISLyCQZ0IiKfYEAnIvIJ24AuIj1FZKWIrBORTSLya236aBFZISLbReRdEeme+uQSEZEVJyX0VgCTlFITAJwB4HIROQ/A7wA8q5QaD6AWwF2pSyYREdmxDegqrFH72E37pwBMAvCeNv11ANelJIVEROSIozp0EckXkbUAqgDMA7ATwBGlVECbZR+A4alJIhEROeEooCulgkqpMwCMADARwMlms5n9VkTuFpESESmprq5OPKVE1CW0tAfxj5K94Osx3XPVy0UpdQTApwDOAzBARCIDk0YAOGDxmylKqWKlVHFhoe1AJyLq4p6Ztw3//d56zC+tynRSco6TXi6FIjJA+7sXgEsBlAJYBOC72my3A/gwVYkkoq6juqEVANDQ0p7hlOQeJ0P/hwF4XUTyEb4A/F0pNUtENgN4R0QeB7AGwCspTCcREdmwDehKqfUAzjSZvgvh+nQizzW2BjBnQwVuPHsERCTTySHKCWl9OBeRU7+asRHT1+zHmCF9UFw0KNPJIcoJHPpPWalKq0c92h7McEqIcgcDOhGRTzCgExH5BAM6ZSVlPk6NiOJgQCci8gkGdMpKAnZVJHKLAZ2IyCcY0ImIfIIBnYjSYu3eI/h0Kx+4lUocKUpEaXHdC0sAAGVPXZXhlPgXS+iUldhtkcg9BnQiIp9gQCci8gkGdCIin2BAp6zEgUVE7jGgU1ZioyiRewzoREQ+wYBOROQTDOhERD7BgE5E5BMM6JTV2NuFyDkGdMpq7O1C5BwDOhGRT9gGdBEZKSKLRKRURDaJyL3a9EdFZL+IrNX+XZn65BIRkRUnj88NAPilUmq1iPQDsEpE5mnfPauUejp1ySMiIqdsA7pSqgJAhfZ3g4iUAhie6oQREZE7rurQRaQIwJkAVmiT7hGR9SIyVUQGepw2IiJywXFAF5G+AN4H8AulVD2AFwGMBXAGwiX431v87m4RKRGRkurqag+STF0Juy0SOecooItIN4SD+d+UUtMBQClVqZQKKqVCAF4GMNHst0qpKUqpYqVUcWFhoVfpJiIiAye9XATAKwBKlVLP6KYP0812PYCN3iePiIicctLL5UIAtwHYICJrtWkPALhFRM4AoACUAfhxSlJIXRoHFhE556SXyxeAaUXmHO+TQ0REieJIUcpqbBQlco4BnYjIJxjQiYh8ggGdiMgnGNCJiHyCAZ2ykmJvRSLXGNCJiHyCAZ2ykrC3IpFrDOhERD7BgE5EaTdvcyWKJs9G2aGmTCfFVxjQKSuxUdTfZq47AABYt+9IhlPiLwzoREQ+wYBOROQTDOhERD7BgE5Zid0WidxjQCci8gkGdCIin2BAp6zEbotE7jGgExH5BAM6EZFPMKATEfkEAzoRkU8woFNWY390IucY0CmrsbcLkXMM6EREPmEb0EVkpIgsEpFSEdkkIvdq0weJyDwR2a79PzD1ySUiIitOSugBAL9USp0M4DwAPxORUwBMBrBAKTUewALtMxERZYhtQFdKVSilVmt/NwAoBTAcwLUAXtdmex3AdalKJHVdbBQlcs5VHbqIFAE4E8AKAMcqpSqAcNAHMNTiN3eLSImIlFRXVyeXWiIisuQ4oItIXwDvA/iFUqre6e+UUlOUUsVKqeLCwsJE0khERA44Cugi0g3hYP43pdR0bXKliAzTvh8GoCo1SaSujN0WiZxz0stFALwCoFQp9Yzuq5kAbtf+vh3Ah94nj4iInCpwMM+FAG4DsEFE1mrTHgDwFIC/i8hdAMoB3JiaJBIRkRO2AV0p9QUAq74Gl3ibHCIiShRHilJWY7dFIucY0ImIfIIBnYjIJxjQiYh8ggGdiMgnGNCJiHyCAZ2IyCcY0ImIfIIBnYjIJxjQiYh8ggGdshKfskjkHgM6EZFPMKATEfkEAzplNVa9EDnHgE5ZiU9ZJHKPAZ2ymgKL6EROMaATEfkEAzplJdadE7nHgE5ZjYGdyDkGdCIin2BAp6zGAjqRcwzolJXYbZHIPQZ0ykqsOydyjwGdsppiZCdyzDagi8hUEakSkY26aY+KyH4RWav9uzK1ySQiIjtOSuivAbjcZPqzSqkztH9zvE0WURjL50TO2QZ0pdRiADVpSAsRESUhmTr0e0RkvVYlM9BqJhG5W0RKRKSkuro6idUREVE8iQb0FwGMBXAGgAoAv7eaUSk1RSlVrJQqLiwsTHB11GWxzoXIsYQCulKqUikVVEqFALwMYKK3ySIiIrcSCugiMkz38XoAG63mJUoGH59L5FyB3QwiMg3AxQCGiMg+AI8AuFhEzkD4hrgMwI9TmEYiInLANqArpW4xmfxKCtJCFIPjioic40hRIiKfYEAnIvIJBnTKaqxyIXKOAZ2IyCcY0CmrsYBO5BwDOhHlvCfnlqJo8uxMJyPjGNCJKOe99NmuTCchKzCgU1bjCy6InGNAJ8phf1q4HSt38+nWFGY7UpQok1g+j+/pT7YBAMqeuirDKaFswBI6EZFPMKBTVuJTFikRXb3NhQGdsloXPz+JXGFAp6wkkEwngXJQVy8AMKBTluviZyi50tVzCwM6EWWlRErbrEOnrBAMqS6fGc1wl5AbXT27MKBnibEPzMGP31yV6WQQZY1EgnNXLwAwoGeRTzZXZjoJWYPdFv0rlXeiXT3fMKBTVuvapye5xRK6DwWCIQSCoUwng5KQa90Wl+86jP98dy3bQRxwuou4L93zZUCf9PvPMO7BuZlOBnkgV87pW19ejulr9iOUI+n1q1zJL6niy4BeXtOc6SRQF8VSpT2neyihRtEuXknny4BOuS9XT8zcTLV/dPXrqW1AF5GpIlIlIht10waJyDwR2a79PzC1yew6WgNBtAVY/x+RK4FdJFznH+rqESXDuvred1JCfw3A5YZpkwEsUEqNB7BA+0weOOmhj3DBUwsznQxKUDrjea5W7zhON0eKumYb0JVSiwEYX4lyLYDXtb9fB3Cdx+nq0g41tmY6CVmji5+f5FJXzy6J1qEfq5SqAADt/6HeJYkod6WzyiVXL3apTHau7hOvpLxRVETuFpESESmprq5O9eqIMiqtVS7pW1VGJNR+4vedYiPRgF4pIsMAQPu/ympGpdQUpVSxUqq4sLAwwdVRV5Vr52c605ur9cWpTHauNKKnSqIBfSaA27W/bwfwoTfJITc+3nQQt72yItPJIKBjXGtaq1zStqbMSOzxud6nI5cU2M0gItMAXAxgiIjsA/AIgKcA/F1E7gJQDuDGVCaSzHWFpzPmWik0vb1c0reuVPF6G3ywS5JiG9CVUrdYfHWJx2khyn1prUPPzfCVynTnWgHAaxwp6gNdPRNnA21cEXu5uBQvuCc29L9rY0D3AT4QKvMiwZWHwl5KG0VTfACmfrEb97y9OrUrSQIDug+whJ490nks/HDY422D0+2bsWa/N4lx4LFZmzFrfUXa1ucWA7oP+OC8thQ5qQ83tqLdw2fcv760DD98/UvPltdZ5eLZIm3lah26l440t+EX767t+NzV9wkDehZItlTnh5JaPIFgCGc/Ph//8956z5b5yMxNmF9qOXwiYekMKH447sluQ9B4BfXBPkkGA3oWSDZT+/kJfwoKQW37svlWtwNHinrGycUxT6LfbOX3fWKHAT0LdPVM6FQ2305HXpmX1iqXHL2Q65Pt9Rbk6C7xjG0/dEo9VrnEl23b19QaQHswhAG9u8d8l9Yql7StKTn3vrMGx/Xv6fp3To67cZZsvuinAwN6Fkg2C/q6yiULN+1rv1uI2uZ2lD11Vcx3HCka68O1B6I+64Nu8oUZZfic1OJyHqtcsoCbTHj71JV4dt626N97nJ5sk20XrNrmdsvv0ppWw6r21jTnbDVMotgmGo0B3eDN5XtQ1dCS1nW6uU38bFs1nl+wPfr3KTyJF22pwtq9R1K2fDtK6Qbt5MDZmqk0lpTV4KL/XYR/lOzLTAJccFqH7mRXxpbQcyCTpBADuk754WY8PGMjfvpWekeCJZsHU5mF73ztS1z3wpIUrsFe1z5FrekLAturGgEAq8trM5WcjIgpoXfxzMKArtOmDVypbWpL63qTDug+f6d0tlW5mMrws1xyYReZiltEt9+onMgbacSAbkbsZ/FSsi3zfm7ZV8itYMU3FtnzMt3GgJ5LeSUVGNCzQNIldJ9n4ki9qNlmbtxfh9lZNODI7lC8uawMB44c9WZdGTzwe2ua8ebyPUkvJ+nCTEyjqM9PBhsM6FEykxnYbdGaUiruBevqP36Bn2XR0+/iHYtDja14+MNNuH3qSk/WZbYmEaAtEMLemmZP1mHlu39ZiodnbERLe9D1b51eiJzMxRJ6NAZ0E2mucUm+L65H6chWubR98Q5lSGvBO3LUutvjzupGTFtZnvC6lAImT1+Pi/53ERpbA46Wk4jK+lZPluP13Wku5ZVU4MAinUxd3ZNdrR9LJfptipTCcqNLmnUanaT+6j98gaPtQdwycZSDNZkvcfG2agDA0bYg+vZI7SmeyrtDJ4uOLaHnQh5JHZbQs0Dy3Rb9m4lzpVG08yXRyS3nqJsqDJN1SZpvLxPZXmXxtxfrz4GsklIM6CYk3WcFG0XjyqVSl5OkOsldTrY5G/ZKpttvOPQ/GqtcdDKVF7xu6fcVlR2Byymv7paUil/aLj/cjLdWJN/LJFmJjIFw2n/eyUUt9g4hl3KL97pcCf0rD8/Fa0t2x50n/Y2iSf7eh5lYH8xS+w5Kbxbe8cYijwZ52aXqX19ZjimLd+nmz0weyHQJnb1conWpgK6UQkt7CI/+c3OmkxIl2ZMiXj3msp2HMWv9AesZspyC6mwUTcXyk1joloP1eEvri935kug4jaIu1mWXJ+qPpq4HixsJ5d2oZ7kk14gcE9Ddp8ZXulSVi10Djtu8WVF3FN3y8zCkb4/EEwUverlYL+GWl5cDAK4+/fgk15I5qTxJk1n25c99DgD4/nkndC4vXrdF7UsnTTRelHzTUWpP5ws9jJRS2LS/3jDN/nebDtTh5OOOQV6eN/fid74aHlfw6p0TPVleMpIqoYtImYhsEJG1IlLiVaJSxe4kcXPCAcD5Ty5E8ePzk00WR4qa0G9TSVlNytbjVZVBJM84CeiOXtzgQbLSkS8y+XKWGWv34773o98za3cRW7f3CK76wxd48bOdia/YYNHWaizaWu3Z8pLhRQn9G0qpQx4sJ+WcBvR0Y6OoNaWAe99Z2/F3Kpbv6fLSVOXiqBdMGvLFjupG9Oqej349uzn+jdP8bpf+rQcbHa8zIvLYhQ376lz/Nhd0sTp0Z99L+p/OleTPfRzRU8zri3j8XhveLCe+zrybjgLKrS+vwI1/WZbw771OYboKN9nalTbZgK4AfCIiq0Tkbi8S5CXjTs9UCb20oh4t7UHsqm5EncnbbjhS1FoubFoopHQvibZOcXrq0Dt/l647zi0HG1zNH5WsOGm0S71ZQSZd50Im2w7iSbbK5UKl1AERGQpgnohsUUot1s+gBfq7AWDUKPvhzF4y9uW1OwipOEhHmttwxfOf4+rTh2HW+gqMHNQLn983KSadycjSvJWUdI3t8iLoBXXLiLc0N+uybcB3sIxcutArpfCHBTtw/ZnDXfzIbFJ6NjpcWEx3B2d7SZXQlVIHtP+rAHwAIKaZVyk1RSlVrJQqLiwsTGZ17tNn+Ox1o6gTzW3hodyr9oTfJLO3JvbRqclmwkz3BU6F9N06J7+MoC76xm8Udb7MORsqUDR5dgIvW+nMvNmaLZTJ3xV1LXh2/jbc8VpyT6JMW75Jz2pcSzigi0gfEekX+RvAZQA2epUwL8QMC7YZ9JGpejGnq7VKX7aeuF5I9bZ5sXj9BTVeHnKTv17WBg3trHbf8GeWrmwXSWtLW+ezbLK1nhrI3n2bTAn9WABfiMg6ACsBzFZKfeRNsrxhLBHZl9CTX2cwpLBqj7tudmarDS8n+v2Q1ulLfeZqDQQz+rLoiMf+uRnjH5wTMz3Rk9+LE1N/XKyWdqS5DVsrw3XNThrdgx13i4L2YAhFk2dj6he77VcUla7sDDp6ySTR7Ke5dGeXCgkHdKXULqXUBO3fqUqp33qZMC8YqzJsA7oHEf0vn+3EDS8uw/Jdhx3/xmy9z8/fhhteXBoVRK2CVjoaaB7752Zc98IS7DnclPqV6RiP4dQlu9Ee9K4xLF1VLtf8aQnueXuN42UGgp3Vf82t4VLrY7M2o6LO/G1HZs9HydaGu1SWvLt6jy9fd1s05pt0NIpu1Vr8K+tbklrO5orY5VilLx2lhY37w/12a0166aSS0/7CiZZGvQguoZDqfJaLxfLKXb5BqKM9xzD9P99d53gZ2VxlEaF0dyJOHGpsxY6qhqjfRi/Pu7TFk613P74O6EZ2Gdxt5nLCyYE3m8UsCVbLclIqCWZrcc3GO1/uNZ0e2yU1seV7UkJX9iV0t0IWebE9GG4IireajlGr3iTFc/HSZZfmi363CJc+E+5IZ7av07XNWRrP/R3QjQHQ7qS/9a8rkl5nTM8arSG2os66xJ7syDknmSvg1WMAs4Sx2iXRW21P6tD1VS4O0mF2sW4NBFE0eXbHZy1uQ+AujdHddLM06uhEUui0tG33AhC3dyW7DzVZVmPFXY/rX6SHrx/OFVvlYn0YvKg/D68zumSVcAnd5DurYOFkHYmU0LP5lr09GEL3gs7ySMJ16B6kJapRNMEFNrREPz1R34U26GKh+lmz9RputjmJpNW0UdTlMr7x9KcAgLKnrnL1u2y9WPo7oBs+xzsIAd1ZmUyFy6z1FR1/3z99g6MX/pqlqrOk1fltMnXogYQCummCskJbIIQ+uodcJnqCeT6wyKPzPHIBFkh0t0gXy8h00Kmsb0FdnBdiA/oGXBX1GQB2HYrfAG9a5WKzyR13BD59fpLvq1z+tHA7VpeHu//FOwipqGN2/vZ2Z+tO5gQNmvQMsZOleRZAZ11yRMKHz4ONNKty+duKPVhQWpn0MhVUVOlVdQQ+84Rn03X33CcW4LJnF9vPCPO7EKfnT7Q05dosPTl8HdCVAp7+ZBu+8+elAOxK6J1njRcnhZtFxMsbv/toa0fdqtXAKCeBfvH26pggaCfTJbx4WgPGgJ5oCT35tARDKuYl0Q9+sBF3ve78idLG9EcCXEiZBzsnyXayT+qOtncUeIzTiybPxtsrEgmq8elLx/Hq0BNats1ipOP/5E7ybD03cjKgv7Boh7N+3jF16NazBhIowcbj5qIQL2/s1t126k+Emes630LkJG/d+85a/N/HW50nCt5l2rZACPdP34CqJLty6hkvTom82xLwpt+yPuA+N3+baYC0Y6xDjtwxBkPKVfvO2yvKcagx/LgAJz/70Rsl+M6fl6LNcIGMNBS+sazM8bqT4bKsAcD82KWtl0ua1uNWTgb0//t4K26estx2PmPPDqd16OkXu25jCSIUUlEn6M+ndQ5ScZry7ZVJPBWvY5r7/bRwSyWmrSzHo//c5Pq3VtqMAT3BU8yLa5Z+n6wpP9JxR+iGsRQe0lWtmNWhG9NtthlOLsiRfv7G/ZlSUY3IkTuRziomK4/Psn91ZPpGimZnSM/JgO6UMUjHOwj6OnRvqlycL8TsWmJMQyCkLE9Qp5kr3lxKqdhn3+g+dlYpuM/Ike3z8hxoDxgDYGLL8aRRNORu7EJFXUtMqdvYxqEvoTt9+JeRft4pi3fi061VMfPka69hM5bQ0x2vnLRh/VX/6AN4V+BIRLYO6/B1QDdm0ngHwW39slnGSbTro5M8WHa4qWMUasx6Ha5WKWDl7hrM2VCBo23R/Xl/9EYJJvz6E8NyYxec0K1xCjJ/WzA6/WZpXb/viO2I3SfmlCadlmAo9mIY8faK8qj+5RH7j0T3fY4toXf+n/igqc4fPjFnC+549cuYeSIBvTUQv3+31XITSpfpMl0uw3KQXXpk6yMGfN1t0XgbGa80FlVCd1C6NjvJAgmW8p1kjvi9BZxlrrZACDe9FH67zHfPHoGnb5zQ8d380tjSm9lSk+kN5Gaf2JWcjQOLzOa/5k9LMLhPd6x6+JuWy5mz4aDzRFmIl9a3V+5xtAyr/RqyqnIxHB3TAoaDQ9Ut37yEHu9YpaJ0GrmgOQ3sIZXY0H+vkm61npnrDqD+aHvUi8PTKedK6G5KBzFd2+KULs3q0JftPGz6hiHA/CTWT3MSu1btqcGhxlbHQ/+tON0l+v3h5LGs5iV096eEMfjUNrVh5W7zJ1LWNrXhwqcW4suy+A2LxkZsY1ID2rYedv08cfeCIWVZ5eJ0dG+8gO5kn5vdHTqpTsqTSAndeZWLl118jf3QnbJKg1nhaMvBepQfbo77O7eskvvzaWvw0IzMPUU8pwL65gP1eOCDDY7nj61yidcoGj1vU2sAt7y8HD9+y7zrmVnGiKrrtElbIBjCDS8uw3UvLEm6SsJpHtXfsXTLsz/0Zj1HrEYthrRqB6UUHp25qeNhXnqRO59//esK3PTSMtOL8w+mroypjjBjPF7GRdkNaPFSSCnLC7jVsTXuR6tAE4zTdhK1HofrfnjGRpRW1Hd8LrCoQzc+411/rMzS46agFa/u2/hVvAud+cLDD5L79T83dSzz8uc+x9f/b1H837mUrVUuORXQ73l7NaatNH9Ykxk3DT36Ep9I5zMjtlWal2TNlqUv5duVBMY9OBcAsK/2qGnmcNOo6vRk0u+Pgnzz5evTbXoXYrJddc3tGPPAHLy6pAy1ze14bWkZrv7jF7r0Rc+/WQsoZqX0DSYXAjPGEroxrbXNqS+Z69dttT+tjkzAYXWgUoY7S6sLarwgqfvNm8v34EdvlGDVnlpU1rcgz6IOPbLOLQcbMPr+Obh/emdByiwJo++fg/qWzotoSZmzdwJE3kdq1TZjPIcjAiHrkHrTS8vw6pIyNLXFtgt41Z7DRlEP9O6R72r+lvbE6tDDvw1nhkgJJmZ+m2D32tIyy3VVN7RGfU42kzn9ub6EXpDfeej1wUV/AsWrQw/pel/srQ3fzv718134yVurOuY9aHwgmWFXfs/Q9dTN7bBdl9S6o+Fno/Tpbp9nzNYbctH/OxgCuuWbn0r60rCesX3HqttsMKRi3lkaboSNTa9RZJJxXfl5ghteXIpvPbe4I3+3BkIorajH/dM3IBAMod2wf9/5ci+KJs/GnsNNlndp+nw9L84oWX0ofn/1Pi2t5su0CuhWeeXTbdUdy7KrFk0Guy16YECv7q7mb24zf+CRGX2AUAr4YPV+ANYnqj5Dzd1Qga0HG6Lefm5825BeJADG5WEDYkRUCV13odI3MOpLaqZ16Nq0Sb//FOc/uQAAOkrjB+paokrdkWVFltIxSs9k2z7aeBBjH4h9E5GV2C6p0d9H1l1gcfz0xj4wJ6aEev2fl+Ckh+c6SkswpCzziRVjo67TRtH1++q09EYHut+a9NaJ/M64rsixP9Lc3rF/WgMh/PD1EkxbWY5xD87FT3UXZr373lvvqPussReVHauLZ2vQfDkhk4saAExZvKtjervJxSCymvZgCB9vSrxBPEvjeW71cunX011ym9uMXdus59Xfwm/YX9dx659vUkIPhVRUA+NP/7baVbrqDfW7yWaOQDBcmsyzuJuI0Kc5P08QCIaQJxIV6JvagujdPfwkQ6sql0AwhLLD9hcls2qjtkAIgtjS/yeb3Z1cdlUukYBndvzMNLUG0aOgszS/zuLFGmbdW+NVuVjRLycQDFl2mw0pdyNF9SK/itd9N1/M69Ar66PvIiNW7K5BlcV3+k0wnntR6TLZnFbtx9UNrVEvA7EqoTe2BvDmcvMeRJHFm931RPLJgi1VWLCls2fXjqoG/GHBDjx944Sop3i62Qa9YEjhwJGj+N1HW/D0jRPQs5u72oVE5VQJPc9QtKtpasOcDRWWJ4OxJ0e82ySrW16zE/X7r6xA8ePz7ZJrydhgZ16H7twPpq7Er2ZGt6wfamzF59uro6bpT44Vuw5j3INz8eCMDVG35Bc+tRAnPjQXs9dXmNa5BJXCt/+0xFG6Inc9kf3+8aaDOPGhuaYXVn0wdSJm6L+xhK5VtzmM59he2YA1Dobsj38wttTe1BqwrJqzoi89jntwLm592Xzk86It1Qn3zOgsoUfvq1bdM8Xz88x7ucTzx4XbTadvOViPpTsO4VBjq+1zy43u1PWR19/dlh9u7rjru3Dc4I7pczd2PtU0RqSqybBNs9dXWF4cL31mMWauOxD3zlr/nV2jaEt7EA9/uBGz1ldgyY5Dcef1Uk6V0I236mf9Zh4A4N8njcMvLzspXHrUHbA/LtwRNX/cErrFl2a9QZbudP6+UDPGErrZqvWP4XXireXlGNqvJ0YP6YNvTzge33tpGXZWRz9+VJ/B67Xnb09buRf3TBofs7x/rNqLnt06tz3ybtNgSEXVCxsb96LWFwzhoRkbOtZr9i7QYEjhL5/tdP1kPWOQO9oehFKd3QcjVSjtwXAPjU+3VWNBaSV+c+1ppl0MI/X5xudiv72iHLeeOwqBYCim7lgkfCGpbW5zXeXSppXK/+e99QCs8+a7JXtx0YlDXC07QimFd1aWx9Tj64O31UjReKyee3TvO2sBAGML++CEwX3iLkN/QXtyrvXgrshLZ8qeugrd9e0+cU7myIWsNRDC67q2rJ+9vRoXjY+/L/UXv9ZAEN3z8zryi/5k1SxsAAAPZklEQVT5UXYl9Mdnb8anW6vjz5QCORXQrW6f/6addM/N2453S8x7wawpr8UHa/Z1fH5m3jacfcJA9OtZgLNGDbQMTFsrG/DRxgpcftqw5DdAY3y/5NoEHuZk5pl52wAA355wfEwwB8wDKmBe11jT1Ia734ytRy2tiB6tOs6kxBpxtC2It5bHD9RzN1a4fmgYALQbTugrnv8cj117Kn5wfhGAzqBVd7QdLy3ehWkry7HncDO+VzwKXx3R33K5by4rQ2G/zgetP/DBBtx67ihc8sxn2GOoZjqmZzfUHW1HbXO764AeCCqs33cE09fst513l8mxdCIUAiZPj+3mq++bn8hI0c0WDb0RO6ubTPNfhALwka7++qXPdjlarz6IW1X76Of7zp+XdBRcnPwOCN/tbvnN5ahvacfE3y7ApScfizsuKMKJx/Z19TYoN73xvJRTAd1Y5RJR09SGBz/YiIVbYkc7RlxveGDSHxZ03jZufuxbpl2cIn7y1mrseuJK5OWJJ282Wr4rukvXo/+0f+iQF6wewDTfpEdC3dF25OdJTEn4L5/tdLy+yFP/4rnn7TWm0yeM6G9Zjw2Y3xm8vaIcC7dU4X9vOD2qxDljzf6OYPzQhxvx8g/Otlzuwx+aP0DMGMyBzpO6pqnN9fN/2oMh1DQ56yu/1+ULpiOc5NSOXi7tzkvou21ePGEnkR4ibYFQVBrj9SKLMAZzwNmrGN/9cm9Hm8780krML63EqEG9cfPEkR3zRLZAvy1W+yWdz6jPqTr0eBkhmRGBp/zqY9tHu0bqBB+ZmfwTA/X9dbPB47Njb3nbAyFXt+FmfvSG8+eBx7A5C8xu+7ccbMCnW6sx8YkFUdUK+t5H6/YewcTfLnCVFKvSa+S1cVMW78JhBxcvvbZgCDVN8UuLEY2tsYHJCSe9n0q0euG2YMh1oH3mpgn2M5lw2wMGCFevrnTYtz0eJ+0Rj8zchCU7oqtVy2uaoxr5O0e4ds7zqw/NR4ims0dMTgT08sPN+LfXvoypqtAzqzZww64U1NQaQHVDq2XLuhsVR7x7LriZFpcNUmYOxHmpdTrYFWrsHnfsxT6IOOmhj2znMY5uHdK3B35z7amW8+8+1IT/ed/ZqGfj+0adchOg3ZTQIwr79cDPJ41z/btai8dpxJPoRc3IqtrRCX0Zw/jYX+PfXq3TraQCuohcLiJbRWSHiEz2KlFGd7y2Egu3VGF1+RHLeezq9ezstwmyry4twzm/Tbxni16qnz090SadRYN7p3T9Xhg+sBeuOO04y+/XlNdi/T7r/GAcvJVu/XsV4KwTBlp+/9x8854iZhoSvKNzUzu40OTRunZ6dsuPW1Vp5d53zKvZ0sFJlYsTkV0bFdAtFj11yW5sOuBsBHSyEg7oIpIP4AUAVwA4BcAtInKKVwnTe+iqk1Ox2CiRN7RYSez9hplhVneod+eFo2OmXTR+CP7rshNN538sTkkzVbrn5+En/zLW8vtPNlfimjjdJ53UsfZKYd/g6oZW9O3hTRNVvLaEeNy8bWjdXuuLo5Xu+XkxPbacqMjg3V9za+J3bk26uwSlwl1d75ja2d3SqoS+cncNrv7jF1G/T5VkSugTAexQSu1SSrUBeAfAtd4kK9rpIwakYrFRNh2IX8I/ksBtYqJ2P3mlJ8sZMyS261hBnmDc0L4x088cOcC0L/hPLx6LayYc70l64jF2Fzz1+GNiBmMMH9ALSyZPct3n28qF4+J3YRs+oFfCy65vCaCPRwE9UcbGdztuq9kUgLEmeSmbNSQRVPXdoFftqcU3n12MZbqujPHaLJQCvvSgDcBOMgF9OAB935x92jTPDejVLe73TkcDOtGvZwGW3T/J0bz3fMNd/eHA3p3bcerxx0R9t/z+Szr+FhF856zhGFsYvy9vPJeePBS/NilZjx7SJ6bkOOW2s/HzS8ZH9TuPuOtro9G7u7PAVBynisGtf7twNE46rh+e/V5nw9t9l5+E4QN6YXBfd4+AMHPruaPiDjq644Ii1yOT9RfQPIFnJfRMcHr38qOLxuCl287G988bleIUhekHF+l9ft838JfvW/de8prZ6xTjPfL59zdOwCnDjrH83ivJ5Diz0yHmEiUidwO4GwBGjUrsoBfk5+E/Lj0Rb63Yg8euORX9e3XDh2sPIC9PMLhPd5xdNBD3vbce4wr7orymGT+8aDQuGj8En2yuxBkjBmDJzkOorG/FBWMHY8baA1izpxYNrQEUnzAQXxnWD9eeMRyvLy3DreeOwgVjw6W2aT86D3trmrGtsgE7qhvR3BpE354FCCmFngX5+OnFY9EtPw8VdS0Y0rc7rjp9GN5Ytgcb99fh1OP74/3V+3DhuMGoPxrAb68/DW8s24M7LijC1CW7cf6YwRhT2Ad//3If+vQowM0TR+K4/j3x2p3ndIwifeamMwAAfy/Zi/veW497LxmPsUP7YntlA9buPYKN++tw2/lFKD5hIL5+YiF2VDXgyTlbcNYJA9EaCOHq04fhxGP7YfeTV+KJOaU4YXAfrN17BDecNQInHdcPN58zEiMG9ooaVPQvJw7FtWfUYuTA3uhRkIe+PQswpG+4T/bkK76CYf174sO1B7DpQB2u/Oow3HDWCNz00jKMKeyD935yAXp2y8cLi3agb48C7KxuxMG6FvTr2Q0De3dDSAGV9S2448IizFp3AGedMBB/XrQTg/t276gCmnpHMf5Rsg/fO2dkx2MMrj9zBHZVN2Hl7hpc+dXwWIDfXHsaPlizH4caWzG0X08c178nCvIFa8qP4BsnDcX2qgZMX70f/z5pHEorGtDcFsDe2mZ8dXh/1B8N4Ltnj8B1Zw7HwboWjD+2L374tTH45T/WoaGlHX+9/Ry8sGgH7pk0DocaWjFnQwVOHzEAD87YgFfvmIhxQ/siFFL4j7+vRUFeHh6++mS8+OlO/Ne3TkJrIITn529DayCEc4oGoWe3fEy+4isYMbAXyg41oXf3AkwcPQjbqxowqE8PlFbUo7k1gKBSeHtFOW47vwib9tdha2UDnr5xAuZtrsSGfXVYWVaDHgV5uPmckXj0mlMxb3MlFm2tBqDQs1s+1u+rw6o9tfj6iYXYWdWIMYV9sL2yEQfrW9A9Pw9jCvvgovFDUF7TjMtPOw5H20KYs6ECwZBCnx4FONoewPJdNRjarweOtgfxzE0TcMqw/vj+Kyvw/y4ei0+3VqN/r264qXgk7n1nDfr37obCvj1w+vD+yMsTfOvU4/D18YXYsK8Ou6qb0NAawDUTjkdx0UAM7dcD6/fV4VBjKxpbA7j4pKFYsasGXz9xCJ5fsL3jLuirw/tjZ3UjAkGFzRX1aGwJ4Lyxg/Hvk8bh6U+2obSiHjcVj8B/XXYSFm6pwoMfbMS0u8/DZ1urUFw0CCMH9cbIQb3x8g+K8d/vrcNpx/fHFzsO4SvH9cOP/2UMlu44jFvOHYXXlpRh+a7DOHPUAOw53Iz9tUcxZmhfDOnTHV+W1aCxNdDR9tCzWx4G9u6O4QN6YegxPVDd0IpvnnIsapraUV7ThPmlVcgXwYnH9cO6vUdwxWnHIS9P0B4I4dsTjscTc0pR29yG31x7Gm44e0RCsc8tSfSpYSJyPoBHlVLf0j7fDwBKqSetflNcXKxKSpLoykZE1AWJyCqlVLHdfMlUuXwJYLyIjBaR7gBuBjAzieUREVESEq5yUUoFROQeAB8DyAcwVSmV/KgbIiJKSFKtNkqpOQCcP8SaiIhSJidGihIRkT0GdCIin2BAJyLyCQZ0IiKfYEAnIvKJhAcWJbQykWoAiT5/dgiA9L2cLztwm7sGbnPXkMw2n6CUKrSbKa0BPRkiUuJkpJSfcJu7Bm5z15CObWaVCxGRTzCgExH5RC4F9CmZTkAGcJu7Bm5z15Dybc6ZOnQiIoovl0roREQUR04E9HS9jDqdRGSkiCwSkVIR2SQi92rTB4nIPBHZrv0/UJsuIvIHbR+sF5GzMrsFiRORfBFZIyKztM+jRWSFts3vao9jhoj00D7v0L4vymS6EyUiA0TkPRHZoh3v8/1+nEXkP7R8vVFEpolIT78dZxGZKiJVIrJRN831cRWR27X5t4vI7cmkKesDejpfRp1mAQC/VEqdDOA8AD/TtmsygAVKqfEAFmifgfD2j9f+3Q3gxfQn2TP3AijVff4dgGe1ba4FcJc2/S4AtUqpcQCe1ebLRc8D+Egp9RUAExDedt8eZxEZDuDnAIqVUqch/Hjtm+G/4/wagMsN01wdVxEZBOARAOci/J7mRyIXgYQopbL6H4DzAXys+3w/gPszna4UbOeHAL4JYCuAYdq0YQC2an+/BOAW3fwd8+XSPwAjtIw+CcAshF9leAhAgfF4I/ys/fO1vwu0+STT2+Bye48BsNuYbj8fZ3S+b3iQdtxmAfiWH48zgCIAGxM9rgBuAfCSbnrUfG7/ZX0JHWl8GXWmaLeYZwJYAeBYpVQFAGj/D9Vm88t+eA7AfQBC2ufBAI4opSKvY9dvV8c2a9/XafPnkjEAqgG8qlUz/VVE+sDHx1kptR/A0wDKAVQgfNxWwd/HOcLtcfX0eOdCQHf0MupcJSJ9AbwP4BdKqfp4s5pMy6n9ICJXA6hSSq3STzaZVTn4LlcUADgLwItKqTMBNKHzNtxMzm+zVmVwLYDRAI4H0AfhKgcjPx1nO1bb6Om250JA3wdgpO7zCAAHMpQWT4lIN4SD+d+UUtO1yZUiMkz7fhiAKm26H/bDhQCuEZEyAO8gXO3yHIABIhJ5e5Z+uzq2Wfu+P4CadCbYA/sA7FNKrdA+v4dwgPfzcb4UwG6lVLVSqh3AdAAXwN/HOcLtcfX0eOdCQPfly6hFRAC8AqBUKfWM7quZACIt3bcjXLcemf4DrbX8PAB1kVu7XKGUul8pNUIpVYTwcVyolPpXAIsAfFebzbjNkX3xXW3+nCq5KaUOAtgrIidpky4BsBk+Ps4IV7WcJyK9tXwe2WbfHmcdt8f1YwCXichA7c7mMm1aYjLdqOCw4eFKANsA7ATwYKbT49E2fQ3hW6v1ANZq/65EuO5wAYDt2v+DtPkF4d4+OwFsQLgHQca3I4ntvxjALO3vMQBWAtgB4B8AemjTe2qfd2jfj8l0uhPc1jMAlGjHegaAgX4/zgB+DWALgI0A3gTQw2/HGcA0hNsI2hEuad+VyHEF8G/atu8AcGcyaeJIUSIin8iFKhciInKAAZ2IyCcY0ImIfIIBnYjIJxjQiYh8ggGdiMgnGNCJiHyCAZ2IyCf+P7XqE091iS0nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_hist = sorted(hist, key=lambda x: x[0], reverse = False)\n",
    "values = [elem[1] for elem in sorted_hist]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.3277)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.34)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_from_uniform(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fCvFG0VMKts"
   },
   "outputs": [],
   "source": [
    "def make_triplet_samples(z, margin, r2, r3):\n",
    "  positive_sample = z + random_vector_volume(z.shape, 0, margin).cuda() \n",
    "  negative_sample = z + random_vector_volume(z.shape, r2, r3).cuda()\n",
    "  return positive_sample, negative_sample\n",
    "\n",
    "def random_vector_surface(shape, r = 1.):\n",
    "  mat = torch.randn(size=shape).cuda()\n",
    "  norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "  return (mat/norm) * r\n",
    "\n",
    "def random_vector_volume(shape, inner_r, outer_r):\n",
    "  fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "  fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "  fraction.unsqueeze_(-1)\n",
    "  return random_vector_surface(shape, 1) * fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PizmBkGqMKtu"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_frequent(x):\n",
    "  return Counter(x).most_common(1)[0]\n",
    "\n",
    "def preds_around(center, radius, n_preds, model, dummy_img):\n",
    "  z_s = random_vector_volume([n_preds, 10], radius, radius + 0.01) + center[None]\n",
    "  noises = model.forward_z(z_s)\n",
    "  perturbed_imgs = noises + dummy_img \n",
    "  return torch.argmax(arch(perturbed_imgs), 1)\n",
    "  \n",
    "def most_freq_pred_around(center, radius, n_preds, model, dummy_img):\n",
    "  preds = preds_around(center, radius, n_preds, model, dummy_img)\n",
    "  most_freq = most_frequent(preds.tolist())\n",
    "  return (class_index_to_label(most_freq[0]), most_freq[1]/n_preds)\n",
    "\n",
    "def investigate_neighborhood(z, step, model, dummy_img):\n",
    "  with torch.no_grad():\n",
    "    result = []\n",
    "    for radius in np.arange(0.1, 6., step):\n",
    "#       print(\"creating {} more preds\".format(int(10 + 5 * (radius ** 2))))\n",
    "      most_freq_pred = most_freq_pred_around(z, radius, int(10 + 5 * (radius ** 2)), model, dummy_img)\n",
    "      result.append((radius, most_freq_pred))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-btRW4qMKtw",
    "outputId": "90e81f6a-1b9e-45a4-ae82-bda370319bd9"
   },
   "outputs": [],
   "source": [
    "#experiment 1\n",
    "\n",
    "z = torch.tensor([0.5] * 10).cuda()\n",
    "# z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "# z_s = z[None]\n",
    "\n",
    "model = learn.model.eval()\n",
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "  \n",
    "for i in range(6):\n",
    "  z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "  print(\"investigation for: \", z)\n",
    "  for elem in investigate_neighborhood(z, 0.5, model, x_img):\n",
    "    print(elem)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bp6YOnipMKtz"
   },
   "outputs": [],
   "source": [
    "#experiment 2\n",
    "import itertools\n",
    "z_s = [torch.tensor(t).cuda() for t in itertools.product( *([[-0.33, 0.33]] * 10) )]\n",
    "model = learn.model.eval()\n",
    "noises = []\n",
    "with torch.no_grad():\n",
    "  for z in z_s:\n",
    "    noises.append(model.forward_single_z(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "55lErWDyMKt1",
    "outputId": "93d8fb71-3fd5-44a5-d3ec-8013e13f17ba"
   },
   "outputs": [],
   "source": [
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "\n",
    "preds = []\n",
    "for noise in noises:\n",
    "  perturbed_img = x_img + noise\n",
    "  preds.append(torch.argmax(arch(perturbed_img[None]), 1)[0].item())\n",
    "\n",
    "from collections import Counter\n",
    "result = [(class_index_to_label(index), count) for index, count in Counter(preds).most_common(5)]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4WSg-wBFMKt5"
   },
   "outputs": [],
   "source": [
    "#experiment 3\n",
    "import itertools\n",
    "dimension_values = [[-0.33, 0.33]] * 10\n",
    "# dimension_values[0] = [0.]\n",
    "# dimension_values[3] = [0.]\n",
    "# dimension_values[6] = [0.]\n",
    "# dimension_values[9] = [0.]\n",
    "pruned_z_s = [torch.tensor(t).cuda() for t in itertools.product(*dimension_values)]\n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yK3mAyw_MKt7",
    "outputId": "2d8e9ac6-52ea-406d-f79c-dfdb56eddc8d"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for z, noise in zip(pruned_z_s, pruned_noises):\n",
    "  hist = compute_prediction_histogram(learn, noise)\n",
    "  indexed_hist = [(i, val) for i, val in enumerate(hist)]\n",
    "  sorted_hist = sorted(indexed_hist, key=lambda x: x[1], reverse=True)\n",
    "  labeled_hist = [(class_index_to_label(i), count) for i, count in sorted_hist]\n",
    "  results.append((z, labeled_hist[:6]))\n",
    "  print(\"new result:\")\n",
    "  print(z)\n",
    "  print(labeled_hist[:6])\n",
    "  print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise in pruned_noises[0:500]:\n",
    "  img = noise_to_image(noise)\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider web\n",
    "z_values = [\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33],\n",
    "  [-0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33,  0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33,  0.33,  0.33],\n",
    "  [-0.33,  0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_values = [\n",
    "  # window screen\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuAVZzmKMKt9",
    "outputId": "7e6643e0-ce55-438b-e0ae-79bde3ee4cef"
   },
   "outputs": [],
   "source": [
    "#vgg-16_12 most repeated labels:\n",
    "l = [(611, 215.0),\n",
    "  (474, 194.1),\n",
    "  (398, 120.3),\n",
    "  (721, 79.6),\n",
    "  (741, 73.5),\n",
    "  (510, 62.5)]\n",
    "\n",
    "[(class_index_to_label(index), count) for index, count in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAgk-YyWc3rG"
   },
   "outputs": [],
   "source": [
    "# learn.recorder.plot_losses()\n",
    "# learn.recorder.plot_lr()\n",
    "# learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTHG4Bt7VDYp"
   },
   "outputs": [],
   "source": [
    "fooling_rates = []\n",
    "model = learn.model.eval()\n",
    "learn.metrics = [validation_single_perturbation]\n",
    "for i in range(10):\n",
    "  global_perturbations = model(torch.rand(1, 3, 224, 244).cuda())[0]\n",
    "  nag_util.global_perturbations = global_perturbations\n",
    "  fooling_rates.append(learn.validate()[1].cpu().item())\n",
    "  print(\"%d : %f\"%(i, fooling_rates[-1]))\n",
    "\n",
    "mean = np.mean(fooling_rates)\n",
    "stddev = np.std(fooling_rates)\n",
    "print(mean, stddev); print(fooling_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "OFCjzI7UaY3C",
    "outputId": "740185b4-dd54-46f4-b0af-79ee452568e1"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[200][0]\n",
    "x = normalize(x_img.data.cuda())\n",
    "z = torch.tensor([-0.33,  0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33], dtype=torch.float32).cuda()\n",
    "# z = torch.empty(z_dim).uniform_(-1,1).cuda()\n",
    "p = model.forward_single_z(z).detach()\n",
    "\n",
    "p_x = x + p\n",
    "# print(\"img range, noise range\")\n",
    "# print_range(x); print_range(p)\n",
    "adv_label = class_index_to_label(arch(p_x[None]).argmax(1).item())\n",
    "print_big_vector(arch(p_x[None])[0])\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0., 1.])\n",
    "p_img = Image(p)\n",
    "x_img.show()\n",
    "p_img.show()\n",
    "p_x_img.show()\n",
    "\n",
    "\n",
    "# print_range(p)\n",
    "# print_range(denormalize(x))\n",
    "# print_range(p_x)\n",
    "\n",
    "benign_label = class_index_to_label(arch(x[None]).argmax(1).item())\n",
    "\n",
    "print_big_vector(arch(x[None])[0])\n",
    "print(benign_label, adv_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzwsI2P1ZANz"
   },
   "outputs": [],
   "source": [
    "z1 = torch.tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p1 = model.forward_single_z(z1)\n",
    "\n",
    "z2 = torch.tensor([1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p2 = model.forward_single_z(z2)\n",
    "\n",
    "z3 = torch.tensor([1, 1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p3 = model.forward_single_z(z3)\n",
    "\n",
    "l2_distance(p1, p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eroI82OKSnAL"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[4][0]\n",
    "x = x_img.data[None].cuda()\n",
    "p = model(x)[0].squeeze().detach() \n",
    "x = x.squeeze()\n",
    "x = normalize(x)\n",
    "\n",
    "p_x = x + p\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0.,1.])\n",
    "p_img = Image(p)\n",
    "# x_img.show()\n",
    "p_img.show()\n",
    "# p_x_img.show()\n",
    "\n",
    "print_range(p)\n",
    "print_range(x)\n",
    "print_range(p_x)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NAG-tripletLossExperiment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
