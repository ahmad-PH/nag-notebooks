{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqeZpz16do4y"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def run_shell_command(cmd):\n",
    "  p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "  print(str(p.communicate()[0], 'utf-8'))\n",
    "  \n",
    "def detect_env():\n",
    "    import os\n",
    "    if 'content' in os.listdir('/'):\n",
    "      return \"colab\"\n",
    "    else:\n",
    "      return \"IBM\"\n",
    "    \n",
    "def create_env():\n",
    "  if detect_env() == \"IBM\":\n",
    "    return IBMEnv()\n",
    "  elif detect_env() == \"colab\":\n",
    "    return ColabEnv()\n",
    "\n",
    "\n",
    "class Env:\n",
    "  def get_nag_util_files(self):\n",
    "      import os\n",
    "      \n",
    "      print(\"\\ngetting git files ...\")\n",
    "      if os.path.isdir(self.python_files_path):\n",
    "        os.chdir(self.python_files_path)\n",
    "        run_shell_command('git pull')\n",
    "        os.chdir(self.root_folder)\n",
    "      else:\n",
    "        run_shell_command('git clone https://github.com/ahmad-PH/nag-public.git')\n",
    "      print(\"done.\")\n",
    "  \n",
    "\n",
    "class IBMEnv(Env):\n",
    "    def __init__(self):\n",
    "      self.root_folder = \"/root/Derakhshani/adversarial\"\n",
    "      self.temp_csv_path = self.root_folder + \"/temp\"\n",
    "      self.python_files_path = self.root_folder + \"/nag-public\"\n",
    "      self.python_files_dir = \"NAG-11May-beforeDenoiser\"\n",
    "      \n",
    "      import sys\n",
    "      sys.path.append('./nag/nag_util')\n",
    "      \n",
    "    def get_csv_path(self):\n",
    "      return self.root_folder + \"/textual_notes/CSVs/\" + self.save_filename\n",
    "    \n",
    "    def get_models_path(self):\n",
    "      return self.root_folder + \"/models/\" + self.save_filename\n",
    "      \n",
    "    def setup(self):\n",
    "      self.get_nag_util_files()\n",
    "#       defaults.device = torch.device('cuda:0')\n",
    "      import os;\n",
    "      os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "      os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "      \n",
    "    def load_dataset(self, compressed_name, unpacked_name):\n",
    "      pass\n",
    "\n",
    "    def load_test_dataset(self, root_folder):\n",
    "      pass\n",
    "    \n",
    "    def set_data_path(self, path):\n",
    "      self.data_path = Path(self.root_folder + '/datasets/' + path)\n",
    "    \n",
    "        \n",
    "class ColabEnv(Env):\n",
    "    def __init__(self):\n",
    "      self.root_folder = '/content'\n",
    "      self.temp_csv_path = self.root_folder\n",
    "      self.python_files_path = self.root_folder + '/nag-public'\n",
    "      self.python_files_dir = \"NAG-11May-beforeDenoiser\"\n",
    "      self.torchvision_upgraded = False\n",
    "      \n",
    "    def get_csv_path(self):\n",
    "      return self.root_folder + '/gdrive/My Drive/DL/textual_notes/CSVs/' + self.save_filename\n",
    "    \n",
    "    def get_models_path(self):\n",
    "      return self.root_folder + \"/gdrive/My Drive/DL/models/\" + self.save_filename\n",
    "        \n",
    "    def setup(self):\n",
    "        # ######################################################\n",
    "        # # TODO remove this once torchvision 0.3 is present by\n",
    "        # # default in Colab\n",
    "        # ######################################################\n",
    "        global torchvision_upgraded\n",
    "        try:\n",
    "            torchvision_upgraded\n",
    "        except NameError:\n",
    "          !pip uninstall -y torchvision\n",
    "          !pip install https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl\n",
    "          torchvision_upgraded = True\n",
    "        else:\n",
    "          print(\"torchvision already upgraded\")\n",
    "          \n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/gdrive')\n",
    "        \n",
    "        self.get_nag_util_files()\n",
    "        \n",
    "    def load_dataset(self, compressed_name, unpacked_name):\n",
    "      if compressed_name not in os.listdir('.'):\n",
    "        print(compressed_name + ' not found, getting it from drive')\n",
    "        shutil.copyfile(\"/content/gdrive/My Drive/DL/{}.tar.gz\".format(compressed_name), \"./{}.tar.gz\".format(compressed_name))\n",
    "\n",
    "        gunzip_arg = \"./{}.tar.gz\".format(compressed_name)\n",
    "        !gunzip -f $gunzip_arg\n",
    "\n",
    "        tar_arg = \"./{}.tar\".format(compressed_name)\n",
    "        !tar -xvf $tar_arg > /dev/null\n",
    "\n",
    "        os.rename(unpacked_name, compressed_name)\n",
    "\n",
    "    #     ls_arg = \"./{}/train/n01440764\".format(compressed_name)\n",
    "    #     !ls $ls_arg\n",
    "\n",
    "        !rm $tar_arg\n",
    "\n",
    "        print(\"done\") \n",
    "      else:\n",
    "        print(compressed_name + \" found\")\n",
    "        \n",
    "    def load_test_dataset(self, root_folder):\n",
    "      test_folder = root_folder + '/test/'\n",
    "      if 'test' not in os.listdir(root_folder):\n",
    "        print('getting test dataset from drive')\n",
    "        os.mkdir(test_folder)\n",
    "        for i in range(1,11):\n",
    "          shutil.copy(\"/content/gdrive/My Drive/DL/full_test_folder/{}.zip\".format(i), test_folder)\n",
    "          shutil.unpack_archive(test_folder + \"/{}.zip\".format(i), test_folder)\n",
    "          os.remove(test_folder + \"/{}.zip\".format(i))\n",
    "          print(\"done with the {}th fragment\".format(i))\n",
    "      else:\n",
    "        print('test dataset found.')\n",
    "        \n",
    "    def set_data_path(self, path):\n",
    "      self.data_path = Path('./' + path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "YyZUYSjBi9K9",
    "outputId": "a88472cd-6bbe-474d-c505-1c5791d6de13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "getting git files ...\n",
      "Already up-to-date.\n",
      "\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "env = create_env()\n",
    "env.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Ev7jcRKoARg"
   },
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.imports import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys; import os; import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3_1aE41PZAMw"
   },
   "outputs": [],
   "source": [
    "sys.path.append(env.python_files_path + '/' + env.python_files_dir)\n",
    "\n",
    "from nag_util import *\n",
    "import nag_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tltucTv2ep9-"
   },
   "outputs": [],
   "source": [
    "# mode = \"sanity_check\"\n",
    "mode = \"normal\"\n",
    "# mode = \"div_metric_calc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "SO1h55obXzOv",
    "outputId": "32b4001a-9389-4ae2-bd0c-c09953991995"
   },
   "outputs": [],
   "source": [
    "if mode == \"normal\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "elif mode == \"sanity_check\":\n",
    "  env.load_dataset('dataset_sanity_check_small', 'dataset_sanity_check_small')  \n",
    "  env.set_data_path('dataset_sanity_check_small')\n",
    "elif mode == \"div_metric_calc\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "  env.load_test_dataset(str(env.data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "koaQZmjMom7w"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "gpu_flag = True\n",
    "nag_util.batch_size = batch_size; nag_util.gpu_flag = gpu_flag;\n",
    "# nag_util.set_globals(gpu_flag, batch_size)\n",
    "tfms = get_transforms(do_flip=False, max_rotate=0)\n",
    "data = (ImageList.from_folder(env.data_path)\n",
    "        .split_by_folder(valid=('test' if mode == 'div_metric_calc' else 'valid'))\n",
    "        .label_from_folder()\n",
    "        .transform(tfms, size=224)\n",
    "        .databunch(bs=batch_size, num_workers=1)\n",
    "        .normalize(imagenet_stats))\n",
    "\n",
    "# data.show_batch(rows=2, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDBkRV8yovwV"
   },
   "outputs": [],
   "source": [
    "# model = models.resnet50\n",
    "model = models.vgg16_bn\n",
    "# model = torchvision.models.googlenet\n",
    "model_name = model.__name__\n",
    "z_dim = 10\n",
    "\n",
    "class SoftmaxWrapper(nn.Module):\n",
    "  def __init__(self, m):\n",
    "    super().__init__()\n",
    "    self.m = m\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "  def forward(self, inp):\n",
    "    out = self.m(inp)\n",
    "    return self.softmax(out)\n",
    "  \n",
    "arch = SoftmaxWrapper(model(pretrained=True).cuda().eval())\n",
    "nag_util.arch = arch\n",
    "requires_grad(arch, False)\n",
    "\n",
    "# vgg:\n",
    "# layers = []\n",
    "# blocks = [i-1 for i,o in enumerate(children(arch.features)) if isinstance(o, nn.MaxPool2d)]\n",
    "# layers = [arch.features[i] for i in blocks]\n",
    "# layer_weights = [1] * len(layers)\n",
    "\n",
    "# resnet:\n",
    "# layers = [\n",
    "#   arch.layer2[0].downsample,\n",
    "#   arch.layer3[0].downsample,\n",
    "#   arch.layer4[0].downsample\n",
    "# ]\n",
    "layers = [\n",
    "    arch.softmax\n",
    "]\n",
    "\n",
    "layer_weights = [1.] * len(layers)\n",
    "\n",
    "# layers = []\n",
    "# last_layer = None\n",
    "# for o in children(arch):\n",
    "#   if isinstance(o, nn.AdaptiveAvgPool2d):\n",
    "#     layers.append(last_layer)\n",
    "#   last_layer = o\n",
    "    \n",
    "# # layers = [arch.fc]\n",
    "\n",
    "# layer_weights = [1] * len(layers)\n",
    "\n",
    "# inception:\n",
    "# layers = [\n",
    "#     arch.Conv2d_1a_3x3,\n",
    "#     arch.Mixed_6e,\n",
    "#     arch.Mixed_7a,\n",
    "#     arch.fc    \n",
    "# ]\n",
    "# layer_weights = [1.0/4.0] * len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QdqhsfBNWx66"
   },
   "outputs": [],
   "source": [
    "class Gen(nn.Module):\n",
    "  def __init__(self, z_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "    super(Gen, self).__init__()\n",
    "\n",
    "    self.bs = None\n",
    "    self.z_dim = z_dim\n",
    "    self.gf_dim = gf_dim\n",
    "    self.y_dim = y_dim\n",
    "    self.df_dim = df_dim\n",
    "    self.image_shape = image_shape\n",
    "\n",
    "    self.z_ = nn.Linear(self.z_dim, self.gf_dim * 7 * 4 * 4, bias=True)\n",
    "    self.z_.bias.data.fill_(0)\n",
    "    self.BN_ = nn.BatchNorm2d(self.gf_dim * 7)\n",
    "\n",
    "    self.CT2d_1 = deconv_layer(self.gf_dim * 8, \n",
    "                             self.gf_dim * 4,\n",
    "                              k_size = (5,5), s = (2,2), pad = (2,2))\n",
    "    self.CT2d_2 = deconv_layer(self.gf_dim * 5, self.gf_dim * 2)\n",
    "\n",
    "    self.half = self.gf_dim // 2\n",
    "    if self.half == 0:\n",
    "      self.half == 1\n",
    "    self.CT2d_3 = deconv_layer(self.gf_dim * 2 + self.half, self.gf_dim * 1)\n",
    "\n",
    "    self.quarter = self.gf_dim // 4\n",
    "    if self.quarter == 0:\n",
    "      self.quarter == 1\n",
    "    self.CT2d_4 = deconv_layer(self.gf_dim * 1 + self.quarter, self.gf_dim * 1)\n",
    "\n",
    "    self.eighth = self.gf_dim // 8\n",
    "    if self.eighth == 0:\n",
    "      self.eighth == 1\n",
    "    self.CT2d_5 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "\n",
    "    # sixteenth = self.gf_dim // 16\n",
    "    # if half == 0:\n",
    "      # half == 1\n",
    "    self.CT2d_6 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "\n",
    "    # sixteenth = self.gf_dim // 16\n",
    "    # if half == 0:\n",
    "      # half == 1\n",
    "    self.CT2d_7 = deconv_layer(self.gf_dim * 1 + self.eighth, 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "\n",
    "  def forward_z(self, z):\n",
    "    self.bs = z.shape[0]\n",
    "      \n",
    "    # define generator here\n",
    "    # input: bs * 100\n",
    "    # Linear (z_dim, gf_dim * 7 * 4 * 4), bias = (True, init with zero), \n",
    "    # Reshape (bs, gf_dim * 7 * 4 * 4) -> (bs, gf_dim * 7, 4 , 4)\n",
    "    # Virtual Batch Norm = VBN\n",
    "    # ReLU\n",
    "    # h0 <- relu output\n",
    "    h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "    assert h0.shape[2:] == (4, 4), \"Non-expected shape, it shoud be (4,4)\"\n",
    "\n",
    "    # h0z = self.make_z([bs, gf_dim, 4, 4])\n",
    "    # h0 = torch.cat([h0, h0z], dim=1)\n",
    "    # h1 = deconv(gf_dim * 8, gf_dim * 4, kernel = (5, 5), stride = (2,2), padding = (2,2), bias = (True, 0))\n",
    "    # h1 = ReLU(VBN(h1))\n",
    "    h0z = self.make_z([self.bs, self.gf_dim, 4, 4])\n",
    "    h0 = torch.cat([h0, h0z], dim=1)\n",
    "    h1 = self.CT2d_1(h0)\n",
    "    assert h1.shape[2:] == (7, 7), \"Non-expected shape, it shoud be (7,7)\"\n",
    "\n",
    "    # h1z = self.make_z([bs, gf_dim, 7, 7])\n",
    "    # h1 = torch.cat([h1, h1z], dim=1)\n",
    "    # h2 = deconv(gf_dim * 5, gf_dim * 2, kernel = (5, 5), stride = (2,2), padding = (2,2), bias = (True, 0))\n",
    "    # h2 = ReLU(VBN(h2))\n",
    "    # assert output size (14,14)\n",
    "    h1z = self.make_z([self.bs, self.gf_dim, 7, 7])\n",
    "    h1 = torch.cat([h1, h1z], dim=1)\n",
    "    h2 = self.CT2d_2(h1)\n",
    "    assert h2.shape[2:] == (14,14), \"Non-expected shape, it shoud be (14,14)\"\n",
    "\n",
    "    # h2z = self.make_z([bs, half, 14, 14])\n",
    "    # h2 = torch.cat([h2, h2z], dim=1)\n",
    "    # h3 = deconv(gf_dim  2 + half, gf_dim  1, kernel = (5, 5), stride = (2,2), padding = (2,2), bias = (True, 0))\n",
    "    # h3 = ReLU(VBN(h3))\n",
    "    h2z = self.make_z([self.bs, self.half, 14, 14])\n",
    "    h2 = torch.cat([h2, h2z], dim=1)\n",
    "    h3 = self.CT2d_3(h2)\n",
    "    assert h3.shape[2:] == (28,28), \"Non-expected shape, it shoud be (28,28)\"\n",
    "\n",
    "    # h3z = self.make_z([bs, quarter, 28, 28])\n",
    "    # h3 = torch.cat([h3, h3z], dim=1)\n",
    "    # h4 = deconv(gf_dim * 1 + quarter, gf_dim * 1, kernel = (5, 5), stride = (2,2), padding = (2,2), bias = (True, 0))\n",
    "    # h4 = ReLU(VBN(h4))\n",
    "    h3z = self.make_z([self.bs, self.quarter, 28, 28])\n",
    "    h3 = torch.cat([h3, h3z], dim=1)\n",
    "    h4 = self.CT2d_4(h3)\n",
    "    assert h4.shape[2:] == (56,56), \"Non-expected shape, it shoud be (56,56)\"\n",
    "\n",
    "    # h4z = self.make_z([bs, self.eighth, 56, 56])\n",
    "    # h4 = torch.cat([h4, h4z], dim=1)\n",
    "    # h5 = deconv(gf_dim * 1 + eighth, gf_dim * 1, kernel = (5, 5), stride = (2,2), padding = (2,2), bias = (True, 0))\n",
    "    # h5 = ReLU(VBN(h5))\n",
    "\n",
    "    h4z = self.make_z([self.bs, self.eighth, 56, 56])\n",
    "    h4 = torch.cat([h4, h4z], dim=1)\n",
    "    h5 = self.CT2d_5(h4)\n",
    "    assert h5.shape[2:] == (112,112), \"Non-expected shape, it shoud be (112,112)\"\n",
    "\n",
    "    # h5z = self.make_z([bs, eighth, 112, 112])\n",
    "    # h5 = torch.cat([h5, h5z], dim=1)\n",
    "    # h6 = deconv(gf_dim * 1 + eighth, gf_dim * 1, kernel = (5, 5), stride = (2,2), padding = (2,2), bias = (True, 0))\n",
    "    # h6 = ReLU(VBN(h5))\n",
    "    h5z = self.make_z([self.bs, self.eighth, 112, 112])\n",
    "    h5 = torch.cat([h5, h5z], dim=1)\n",
    "    h6 = self.CT2d_6(h5)\n",
    "    assert h6.shape[2:] == (224,224), \"Non-expected shape, it shoud be (224,224)\"\n",
    "\n",
    "    # h6z = self.make_z([bs, eighth, 224, 224])\n",
    "    # h6 = torch.cat([h6, h6z], dim=1)\n",
    "    # h7 = deconv(gf_dim * 1 + eighth, 3, kernel = (5, 5), stride = (2,2), padding = (2,2), bias = (True, 0))\n",
    "    # h7 = ReLU(VBN(h7))\n",
    "    h6z = self.make_z([self.bs, self.eighth, 224, 224])\n",
    "    h6 = torch.cat([h6, h6z], dim=1)\n",
    "    h7 = self.CT2d_7(h6)\n",
    "    assert h7.shape[2:] == (224,224), \"Non-expected shape, it shoud be (448,448)\"\n",
    "\n",
    "    # out = 10*tanh(h7)\n",
    "\n",
    "    #     return 10 *F.tanh(h7)\n",
    "    ksi = 10.0\n",
    "    output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "    # this coeff scales the output to be appropriate for images that are \n",
    "    # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "    # interval)\n",
    "    return output_coeff * torch.tanh(h7)\n",
    "    # return 0.15 * torch.tanh(h7)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    self.bs = inputs.shape[0]\n",
    "    z = inputs.new_empty([self.bs, self.z_dim]).uniform_(-1,1).cuda()\n",
    "    p, n = self.make_triplet_samples(z, 0.1, 0.1, 2.)\n",
    "    \n",
    "    z_out = self.forward_z(z)\n",
    "    p_out = self.forward_z(p)\n",
    "    n_out = self.forward_z(n)\n",
    "    \n",
    "    return z_out, p_out, n_out, inputs\n",
    "  \n",
    "  def forward_single_z(self, z):\n",
    "    return self.forward_z(z[None]).squeeze()\n",
    "           \n",
    "  \n",
    "  def make_triplet_samples(self, z, margin, r2, r3):\n",
    "    positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "    negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "#     negative_sample = z + self.random_vector_volume(z.shape, margin, margin * scale).cuda()\n",
    "    return positive_sample, negative_sample\n",
    "\n",
    "  def random_vector_surface(self, shape, r = 1.):\n",
    "    mat = torch.randn(size=shape).cuda()\n",
    "    norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "    return (mat/norm) * r\n",
    "\n",
    "#   def random_vector_volume(shape, inner_r = 0, outer_r):\n",
    "#     d = torch.zeros(shape[0]).uniform_()   ** (1/int(np.prod(shape[0])))\n",
    "#     d.unsqueeze_(-1)\n",
    "#     return random_vector_surface(shape, outer_r) * d\n",
    "  \n",
    "  def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "#     d = torch.zeros(shape[0]).uniform_(0, outer_r - inner_r).cuda()\n",
    "    fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "    fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "    fraction.unsqueeze_(-1)\n",
    "#     return self.random_vector_surface(shape, 1) * d + inner_r\n",
    "    return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "  def make_z(self, in_shape):\n",
    "    return torch.empty(in_shape).cuda().uniform_(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wULy7qXNYeVv"
   },
   "outputs": [],
   "source": [
    "def load_starting_point(learn, name, z_dim):\n",
    "  if detect_env() != \"colab\":\n",
    "    raise NotImplementedError(\"load_starting_point not implemented for non-colab environments yet.\")\n",
    "  import os\n",
    "  identity_token = name + '-zdim' + str(z_dim)\n",
    "  address = '/content/gdrive/My Drive/DL/model_starting_points/' + identity_token\n",
    "  starting_point_exists = os.path.isfile(address + '.pth')\n",
    "  if not starting_point_exists:\n",
    "    print(\"\\n\\nno starting point found for model:\" + identity_token + \". creating one from the current learner.\\n\\n\")\n",
    "    learn.save(address)\n",
    "  learn.load(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h8KJQaKHZANA"
   },
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=2, sci_mode=False, threshold=5000)\n",
    "\n",
    "def print_softmax_tensor(x):\n",
    "  print(\"[\", end=\"\")\n",
    "  for i, x_i in enumerate(x.data):\n",
    "    if abs(x_i) > 0.01:\n",
    "      print(\"{}: {:.2f}\".format(i, x_i.item()), end=(\", \" if (i < x.shape[0]-1) else \"\"))\n",
    "  print(\"]\")\n",
    "  \n",
    "# print_softmax_tensor(torch.tensor([0.01, 2.5, 5.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NkfbLWEQqRA_"
   },
   "outputs": [],
   "source": [
    "def js_distance(x1, x2):\n",
    "  m = 0.5 * (x1 + x2)\n",
    "  return 0.5 * (F.kl_div(x1, m) + F.kl_div(x2, m))\n",
    "\n",
    "def kl_distance(x1, x2):\n",
    "  inp = torch.log(x1)\n",
    "  target = x2\n",
    "  return F.kl_div(inp, target, reduction='batchmean')\n",
    "\n",
    "def wasserstein_distance(x1, x2):\n",
    "  pass\n",
    "\n",
    "def l1_distance(x1, x2):\n",
    "  return F.l1_loss(x1, x2)\n",
    "\n",
    "def l2_distance(x1, x2):\n",
    "  return F.mse_loss(x1 * 10, x2 * 10)\n",
    "\n",
    "def cos_distance(x1, x2):\n",
    "    return -1 * torch.mean(F.cosine_similarity(x1, x2))\n",
    "\n",
    "triplet_call_cnt = 0\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, distance_func, margin):\n",
    "  # max distance when using l1_distance is 2\n",
    "  # max distacne when using l2-distance is sqrt(2)\n",
    "#   print(\"anchor: \", anchor.min(), anchor.max())\n",
    "  ap_dist = distance_func(anchor, positive)\n",
    "  an_dist = distance_func(anchor, negative)\n",
    "\n",
    "  global triplet_call_cnt\n",
    "  triplet_call_cnt += 1\n",
    "  if triplet_call_cnt % 10 == 0:\n",
    "    print(\"a: \", end=\"\"); print_softmax_tensor(anchor[0])\n",
    "    print(\"p: \", end=\"\"); print_softmax_tensor(positive[0])\n",
    "    print(\"n: \", end=\"\"); print_softmax_tensor(negative[0])\n",
    "    print(\"ap_dist: {}, an_dist: {}\".format(ap_dist, an_dist))\n",
    "    \n",
    "  return torch.mean(F.relu(ap_dist - an_dist + margin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsFgfiN8EV7z"
   },
   "outputs": [],
   "source": [
    "def diversity_loss(input, target):\n",
    "#   return -1 * torch.mean(torch.pow(f_x_a-f_x_s,2))\n",
    "  if input.shape[0] != batch_size:\n",
    "    print(\"input shape: \", input.shape)\n",
    "    print(\"target shape: \", target.shape, \"\\n\\n\")\n",
    "  return torch.mean(F.cosine_similarity(\n",
    "    input.view([batch_size, -1]),\n",
    "    target.view([batch_size, -1]), \n",
    "  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eqJsujkVZANH"
   },
   "outputs": [],
   "source": [
    "# z1 = torch.tensor([[1., 0.]])\n",
    "# z2 = torch.tensor([[-1., 0]])\n",
    "# cos_sim(z1,z2)\n",
    "\n",
    "# z1 = torch.tensor([[0.5, 0.5]])\n",
    "# z2 = torch.tensor([[0.55, 0.45]])\n",
    "# F.kl_div(z1, z2, reduction='batchmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FVegHeYovws"
   },
   "outputs": [],
   "source": [
    "class FeatureLoss(nn.Module):\n",
    "    def __name__(self):\n",
    "      return \"feature_loss\"\n",
    "  \n",
    "    def __init__(self, dis, layers, layer_weights):\n",
    "        super().__init__()\n",
    "        \n",
    "        # define generator here \n",
    "        self.dis = dis\n",
    "        self.diversity_layers = layers\n",
    "        self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "        self.weights = layer_weights\n",
    "        # TEMPORARY: remove div_loss\n",
    "#         self.metric_names = [\"fool_loss\"] + [f\"div_loss_{i}\" for i in range(len(layers))] + ['triplet_loss']# Maybe Gram\n",
    "        self.metric_names = [\"fool_loss\"] + ['triplet_loss']# Maybe Gram\n",
    "        self.triplet_weight = 10.\n",
    "    \n",
    "    def make_features(self, x, clone=False):\n",
    "        y = self.dis(x)\n",
    "        return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "    \n",
    "    def forward(self, inp, target):\n",
    "        sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "        X_A = self.add_perturbation(X_B, sigma_B) \n",
    "        X_A_pos = self.add_perturbation(X_B, sigma_pos)\n",
    "        X_A_neg = self.add_perturbation(X_B, sigma_neg) \n",
    "        \n",
    "#         X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "        \n",
    "        B_Y, _ = self.make_features(X_B)\n",
    "        A_Y, A_feat = self.make_features(X_A)\n",
    "#         _, S_feat = self.make_features(X_S)\n",
    "        pos_softmax, _ = self.make_features(X_A_pos)\n",
    "        neg_softmax, _ = self.make_features(X_A_neg)\n",
    "        \n",
    "        fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "      \n",
    "#         raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "#         weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "        raw_triplet_loss = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "        weighted_triplet_loss = raw_triplet_loss * self.triplet_weight\n",
    "    \n",
    "#         self.losses = [fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss]\n",
    "#         self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses + [weighted_triplet_loss]))\n",
    "        \n",
    "        self.losses = [fooling_loss] + [weighted_triplet_loss]\n",
    "        self.metrics = dict(zip(self.metric_names, [fooling_loss] + [weighted_triplet_loss]))\n",
    "\n",
    "        return sum(self.losses)\n",
    "\n",
    "#     def forward(self, inp, target):\n",
    "#       sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "#       X_A = self.add_perturbation(X_B, sigma_B) \n",
    "\n",
    "#       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "#       B_Y, _ = self.make_features(X_B)\n",
    "#       A_Y, A_feat = self.make_features(X_A)\n",
    "#       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "#       fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "#       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "#       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "#       raw_triplet_loss = triplet_loss(sigma_B, sigma_pos, sigma_neg, l2_distance, 5.)\n",
    "#       weighted_triplet_loss = raw_triplet_loss * self.triplet_weight\n",
    "\n",
    "#       self.losses = [fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss]\n",
    "#       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses + [weighted_triplet_loss]))\n",
    "\n",
    "#       return sum(self.losses)\n",
    "  \n",
    "  \n",
    "    def add_perturbation(self, inp, perturbation):\n",
    "        return inp.add(perturbation)\n",
    "  \n",
    "    def add_perturbation_shuffled(self, inp, perturbation):\n",
    "#         j = torch.randperm(inp.shape[0])\n",
    "        j = derangement(inp.shape[0])\n",
    "        return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qd9gXUy_ovww"
   },
   "outputs": [],
   "source": [
    "feat_loss = FeatureLoss(arch, layers, layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfZKdYD2MSdi"
   },
   "outputs": [],
   "source": [
    "env.save_filename = 'vgg16_14'\n",
    "\n",
    "if Path(env.get_csv_path() + '.csv').exists(): raise FileExistsError(\"csv_path already exists\")\n",
    "if Path(env.get_models_path()).exists(): raise FileExistsError(\"models_path already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9J20CBLS8S9"
   },
   "outputs": [],
   "source": [
    "learn = None; gc.collect()\n",
    "csv_logger = partial(ImmediateCSVLogger, filename= env.temp_csv_path + '/' + env.save_filename)\n",
    "# learn = Learner(data, Gen(z_dim=10), loss_func = feat_loss, metrics=[validation], callback_fns=LossMetrics, opt_func = optim.SGD)\n",
    "# learn = Learner(data, Gen(z_dim=z_dim), loss_func = feat_loss, metrics=[validation], callback_fns=[LossMetrics, DiversityWeightsScheduler])\n",
    "learn = Learner(data, Gen(z_dim=z_dim), loss_func = feat_loss, metrics=[validation], callback_fns=[LossMetrics, csv_logger])\n",
    "# load_starting_point(learn, model_name, z_dim)\n",
    "# random_seed(42, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wk9E0AUm9rmn"
   },
   "outputs": [],
   "source": [
    "# learn.lr_find(1e-6, 1000)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wOZYzOHDEdB"
   },
   "outputs": [],
   "source": [
    "# !cp \"/content/gdrive/My Drive/DL/models/vgg16_12-last.pth\"  \"/content/\"\n",
    "# learn.load('/content/vgg16_12-last')\n",
    "\n",
    "# learn.load('/root/Derakhshani/adversarial/models/vgg16_13/vgg16_13_14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "LA1ffVbbEwQS",
    "outputId": "682ac419-ee54-497a-b89f-496f789cf368",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='15', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/15 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>triplet_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='1125', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [497: 0.23, 498: 0.17, 598: 0.08, 687: 0.01, 762: 0.01, 800: 0.35, 851: 0.01, 854: 0.02, 884: 0.01, ]\n",
      "p: [497: 0.22, 498: 0.22, 598: 0.08, 687: 0.01, 800: 0.30, 851: 0.02, 854: 0.02, ]\n",
      "n: [497: 0.26, 498: 0.38, 579: 0.01, 598: 0.05, 687: 0.02, 762: 0.01, 800: 0.10, 854: 0.05, 884: 0.01, ]\n",
      "ap_dist: -0.9979276657104492, an_dist: -0.9657758474349976\n",
      "a: [563: 0.03, 618: 0.19, 623: 0.50, 749: 0.03, 777: 0.01, 784: 0.02, 828: 0.19, ]\n",
      "p: [563: 0.02, 618: 0.18, 623: 0.57, 749: 0.02, 777: 0.01, 784: 0.02, 828: 0.15, ]\n",
      "n: [618: 0.34, 623: 0.61, 828: 0.03, ]\n",
      "ap_dist: -0.9978432655334473, an_dist: -0.9857300519943237\n",
      "a: [22: 1.00, ]\n",
      "p: [22: 1.00, ]\n",
      "n: [22: 0.98, ]\n",
      "ap_dist: -0.999677300453186, an_dist: -0.9958362579345703\n",
      "a: [556: 0.70, 588: 0.05, 794: 0.01, 828: 0.11, 904: 0.01, ]\n",
      "p: [556: 0.76, 588: 0.04, 794: 0.01, 828: 0.09, 904: 0.02, ]\n",
      "n: [556: 0.65, 588: 0.06, 790: 0.01, 828: 0.13, 904: 0.01, ]\n",
      "ap_dist: -0.9970750212669373, an_dist: -0.9819612503051758\n",
      "a: [318: 0.06, 319: 0.11, 320: 0.05, 322: 0.01, 324: 0.56, 325: 0.14, 326: 0.01, ]\n",
      "p: [318: 0.07, 319: 0.15, 320: 0.06, 322: 0.01, 324: 0.48, 325: 0.14, 326: 0.01, ]\n",
      "n: [318: 0.07, 319: 0.13, 320: 0.05, 322: 0.01, 324: 0.53, 325: 0.15, 326: 0.01, ]\n",
      "ap_dist: -0.9983329176902771, an_dist: -0.99442058801651\n",
      "a: [340: 1.00, ]\n",
      "p: [340: 1.00, ]\n",
      "n: [340: 1.00, ]\n",
      "ap_dist: -0.9984670877456665, an_dist: -0.9911985993385315\n",
      "a: [151: 0.15, 154: 0.02, 155: 0.01, 156: 0.02, 157: 0.07, 259: 0.01, 263: 0.62, 264: 0.02, ]\n",
      "p: [151: 0.14, 154: 0.01, 155: 0.01, 156: 0.03, 157: 0.09, 259: 0.01, 263: 0.61, 264: 0.03, ]\n",
      "n: [151: 0.18, 154: 0.02, 155: 0.01, 156: 0.03, 157: 0.07, 259: 0.01, 263: 0.57, 264: 0.02, ]\n",
      "ap_dist: -0.9950646162033081, an_dist: -0.8472118377685547\n",
      "a: [419: 0.02, 455: 0.03, 584: 0.10, 641: 0.01, 701: 0.02, 714: 0.68, 746: 0.04, 767: 0.01, ]\n",
      "p: [417: 0.02, 419: 0.02, 429: 0.01, 455: 0.04, 584: 0.09, 641: 0.01, 701: 0.03, 714: 0.63, 722: 0.01, 746: 0.05, 767: 0.01, ]\n",
      "n: [78: 0.04, 301: 0.08, 417: 0.08, 419: 0.02, 429: 0.04, 455: 0.07, 574: 0.02, 584: 0.05, 641: 0.07, 684: 0.01, 701: 0.14, 714: 0.15, 722: 0.02, 746: 0.08, 768: 0.03, ]\n",
      "ap_dist: -0.9970299601554871, an_dist: -0.93638014793396\n",
      "a: [52: 0.02, 54: 0.02, 58: 0.04, 60: 0.26, 61: 0.04, 62: 0.39, 63: 0.03, 66: 0.07, 68: 0.06, ]\n",
      "p: [52: 0.02, 54: 0.01, 58: 0.04, 60: 0.26, 61: 0.04, 62: 0.39, 63: 0.03, 66: 0.07, 68: 0.06, 111: 0.01, 114: 0.01, ]\n",
      "n: [52: 0.02, 54: 0.02, 58: 0.03, 60: 0.26, 61: 0.04, 62: 0.40, 63: 0.03, 66: 0.07, 68: 0.07, ]\n",
      "ap_dist: -0.9872642755508423, an_dist: -0.9350067377090454\n",
      "a: [162: 0.10, 207: 0.03, 208: 0.01, 215: 0.78, 218: 0.02, ]\n",
      "p: [162: 0.17, 166: 0.01, 207: 0.03, 208: 0.02, 215: 0.71, 218: 0.01, ]\n",
      "n: [162: 0.15, 166: 0.01, 207: 0.05, 208: 0.02, 215: 0.69, 218: 0.01, ]\n",
      "ap_dist: -0.9920276403427124, an_dist: -0.9673541784286499\n",
      "a: [158: 0.07, 164: 0.03, 227: 0.85, ]\n",
      "p: [158: 0.07, 164: 0.04, 227: 0.84, ]\n",
      "n: [158: 0.15, 227: 0.79, 264: 0.02, ]\n",
      "ap_dist: -0.9973157644271851, an_dist: -0.6806830167770386\n",
      "a: [411: 0.07, 430: 0.02, 443: 0.08, 478: 0.01, 487: 0.01, 549: 0.07, 591: 0.04, 610: 0.03, 619: 0.01, 636: 0.03, 692: 0.05, 700: 0.02, 721: 0.02, 728: 0.01, 746: 0.01, 747: 0.02, 752: 0.02, 768: 0.07, 907: 0.02, 921: 0.02, 922: 0.01, 966: 0.03, 999: 0.03]\n",
      "p: [411: 0.04, 443: 0.04, 478: 0.02, 549: 0.08, 591: 0.05, 610: 0.02, 619: 0.02, 636: 0.02, 692: 0.05, 700: 0.03, 721: 0.02, 747: 0.03, 768: 0.06, 907: 0.05, 921: 0.02, 922: 0.03, 966: 0.09, 999: 0.03]\n",
      "n: [411: 0.02, 440: 0.02, 443: 0.05, 549: 0.05, 591: 0.04, 619: 0.02, 636: 0.01, 692: 0.04, 700: 0.06, 711: 0.01, 737: 0.01, 898: 0.02, 907: 0.20, 922: 0.02, 966: 0.24, 999: 0.04]\n",
      "ap_dist: -0.9803005456924438, an_dist: -0.9164563417434692\n",
      "a: [411: 0.01, 423: 0.05, 424: 0.14, 457: 0.01, 465: 0.02, 498: 0.03, 515: 0.02, 530: 0.02, 598: 0.01, 632: 0.03, 651: 0.02, 664: 0.01, 669: 0.02, 782: 0.02, 794: 0.02, 796: 0.01, 800: 0.02, 851: 0.05, ]\n",
      "p: [411: 0.01, 423: 0.04, 424: 0.13, 457: 0.01, 465: 0.02, 498: 0.03, 515: 0.01, 530: 0.02, 598: 0.01, 632: 0.03, 651: 0.02, 664: 0.01, 669: 0.02, 745: 0.01, 782: 0.02, 794: 0.02, 796: 0.01, 800: 0.02, 851: 0.04, 904: 0.01, ]\n",
      "n: [411: 0.02, 423: 0.06, 424: 0.20, 457: 0.02, 465: 0.01, 515: 0.02, 601: 0.01, 617: 0.02, 620: 0.01, 632: 0.01, 651: 0.01, 654: 0.01, 669: 0.01, 691: 0.01, 732: 0.01, 782: 0.01, 796: 0.01, 800: 0.04, 851: 0.04, ]\n",
      "ap_dist: -0.9957672357559204, an_dist: -0.9598211050033569\n",
      "a: [212: 0.25, 222: 0.13, 251: 0.52, 257: 0.07, ]\n",
      "p: [212: 0.12, 222: 0.21, 251: 0.50, 257: 0.14, ]\n",
      "n: [212: 0.22, 222: 0.05, 251: 0.69, 257: 0.03, ]\n",
      "ap_dist: -0.9931430816650391, an_dist: -0.979142963886261\n",
      "a: [479: 0.01, 511: 0.07, 581: 0.14, 661: 0.71, 717: 0.03, ]\n",
      "p: [479: 0.01, 511: 0.05, 581: 0.11, 661: 0.78, 717: 0.02, ]\n",
      "n: [511: 0.05, 581: 0.11, 661: 0.80, 717: 0.02, ]\n",
      "ap_dist: -0.9987675547599792, an_dist: -0.9401378035545349\n",
      "a: [334: 1.00, ]\n",
      "p: [334: 1.00, ]\n",
      "n: [334: 1.00, ]\n",
      "ap_dist: -0.9856680035591125, an_dist: -0.9744707942008972\n",
      "a: [539: 0.86, 632: 0.01, 651: 0.01, 848: 0.01, 904: 0.02, ]\n",
      "p: [539: 0.85, 556: 0.01, 632: 0.01, 651: 0.02, 848: 0.01, 904: 0.01, ]\n",
      "n: [539: 0.99, ]\n",
      "ap_dist: -0.9995584487915039, an_dist: -0.9905335903167725\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-4f3489c3f6e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# pr = cProfile.Profile()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# pr.enable()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msaver_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver_every_epoch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# pr.disable()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m def get_preds(model:nn.Module, dl:DataLoader, pbar:Optional[PBar]=None, cb_handler:Optional[CallbackHandler]=None,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if mode == \"sanity_check\":\n",
    "  print(\"\\n\\n\\nWARNING: you are training on a sanity_check dataset.\\n\\n\\n\\n\")\n",
    "if len(learn.callback_fns) == 1:\n",
    "  print(\"\\n\\n\\nWARNING: you are not using the DiversityWeightsScheduler callback.\\n\\n\\n\")\n",
    "\n",
    "    \n",
    "saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=env.save_filename + \"-best\")\n",
    "saver_every_epoch = SaveModelCallback(learn, every='epoch', name=env.save_filename)\n",
    "\n",
    "# import cProfile\n",
    "\n",
    "# pr = cProfile.Profile()\n",
    "# pr.enable()\n",
    "learn.fit(15, lr=5e-03, callbacks=[saver_best, saver_every_epoch])\n",
    "# pr.disable()\n",
    "\n",
    "# learn.fit(30, lr=5e-03, wd=0.005, callbacks=[saver_best, saver_every_epoch])\n",
    "# learn.fit_one_cycle(20, max_lr=5e-1, callbacks=[saver_callback])\n",
    "# learn.fit_one_cycle(8, max_lr=5e-01) #mohammad's setting that got 77 validation start on resnet with diversity loss on AdaptiveAvgPool2d\n",
    "# learn.fit_one_cycle(5, max_lr=2e-2) #used for vgg-19-bn\n",
    "# learn.fit_one_cycle(5, max_lr=3e-3) # used for resnet50\n",
    "\n",
    "shutil.copyfile(env.temp_csv_path + '/' + env.save_filename + \".csv\", env.get_csv_path() + '.csv')\n",
    "shutil.copytree(env.data_path/\"models\", env.get_models_path())\n",
    "\n",
    "# pr.print_stats()\n",
    "\n",
    "# shutil.copyfile(\"/content/dataset/models/\" + save_filename + \"-best.pth\", \"/content/gdrive/My Drive/DL/models/\" + save_filename + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-BmJ8cESVIay"
   },
   "outputs": [],
   "source": [
    "!cp \"/content/gdrive/My Drive/DL/models/resnet50-dir/resnet50-dir-best.pth\" \"/content/resnet50-best.pth\"\n",
    "learn.load(\"/content/resnet50-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "obTWhste2pZo"
   },
   "outputs": [],
   "source": [
    "learn.fit(1, lr = 0., wd=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zUz0oXLbVVB0"
   },
   "outputs": [],
   "source": [
    "learn.validate(metrics=[feat_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MO2fZ-hSSUzJ"
   },
   "outputs": [],
   "source": [
    "z1 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "z2 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "# print(\"z1: \", z1)\n",
    "# print(\"z2: \", z2)\n",
    "print(\"distance: \", torch.norm(z1-z2,p=2))\n",
    "model = learn.model.eval()\n",
    "\n",
    "z_s = interpolate(z1, z2, 0.15)\n",
    "print(len(z_s))\n",
    "for i,z in enumerate(z_s):\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n",
    "  img.save('./pics/' + str(i) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DGuGN7B7V0Xt"
   },
   "outputs": [],
   "source": [
    "def generate_perturbations(learn, n_perturbations):\n",
    "  initial_training_mode = learn.model.training\n",
    "  \n",
    "  model = learn.model.eval()\n",
    "  input_img = (learn.data.valid_ds[0][0].data)[None].cuda()\n",
    "  perturbations = []\n",
    "  for i in range(n_perturbations):\n",
    "    perturbation = model(input_img)[0].squeeze()\n",
    "    perturbations.append(perturbation)\n",
    "    \n",
    "  learn.model.train(initial_training_mode)  \n",
    "  return perturbations\n",
    "  \n",
    "  \n",
    "# def compute_mean_prediction_histogram(learn, perturbations):\n",
    "#   pred_histogram = [0] * 1000\n",
    "#   for j, perturbation in enumerate(perturbations):\n",
    "#     for i in range(len(learn.data.valid_ds)):\n",
    "#       img = learn.data.valid_ds[i][0].data[None].cuda()\n",
    "#       perturbed_img = img + perturbation\n",
    "#       pred = torch.argmax(arch(perturbed_img).squeeze())\n",
    "#       pred_histogram[pred]+= 1./len(perturbations)\n",
    "#     print(\"finished creating histogram for the %dth perturbation\"%j)\n",
    "#   return pred_histogram\n",
    "\n",
    "  \n",
    "def compute_mean_prediction_histogram(learn, perturbations):\n",
    "  pred_histogram = [0] * 1000\n",
    "  for j, perturbation in enumerate(perturbations):\n",
    "    batch_no = -1\n",
    "    for batch, _ in learn.data.valid_dl:\n",
    "      batch_no += 1\n",
    "      if batch_no % 100 == 0 : print(\"at batch no {}\".format(batch_no))\n",
    "      perturbed_batch = batch + perturbation[None]\n",
    "      preds = arch(perturbed_batch).argmax(1)\n",
    "      for pred in preds:\n",
    "        pred_histogram[pred]+= 1. / len(perturbations)\n",
    "    print(\"finished creating histogram for the %dth perturbation\"%j)\n",
    "\n",
    "  pred_histogram = np.asarray(np.array(pred_histogram) / len(perturbations))\n",
    "\n",
    "  return pred_histogram\n",
    "\n",
    "\n",
    "def diversity(learn, n_perturbations, percentage):\n",
    "  pred_histogram = compute_mean_prediction_histogram(\n",
    "      learn, generate_perturbations(learn, n_perturbations)\n",
    "  )\n",
    "  print(\"finished creating the prediction histogram\")\n",
    "  pred_histogram_sum = np.sum(pred_histogram)\n",
    "  \n",
    "  indexed_pred_histogram = [(i, hist_element) for i,hist_element in  \n",
    "                            enumerate(pred_histogram)]\n",
    "  \n",
    "  indexed_pred_histogram.sort(key=lambda x: x[1], reverse = True)\n",
    "  \n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  top_classes = []\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = indexed_pred_histogram[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / pred_histogram_sum) * 100.\n",
    "    top_classes.append(hist_elem[0])\n",
    "    n_used_classes += 1\n",
    "  \n",
    "  #top_classes is a useful piece of info that is currently unused\n",
    "  return n_used_classes, indexed_pred_histogram, top_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 978
    },
    "colab_type": "code",
    "id": "Q8VUc3YH4vj5",
    "outputId": "aff7c648-2f10-4938-c9cd-378f6c8d0c91",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "at batch no 3200\n",
      "at batch no 3300\n",
      "at batch no 3400\n",
      "at batch no 3500\n",
      "at batch no 3600\n",
      "at batch no 3700\n",
      "at batch no 3800\n",
      "at batch no 3900\n",
      "at batch no 4400\n",
      "at batch no 4500\n",
      "at batch no 4600\n",
      "at batch no 4700\n",
      "at batch no 4800\n",
      "at batch no 4900\n",
      "at batch no 5000\n",
      "at batch no 5100\n",
      "at batch no 5200\n",
      "at batch no 5300\n",
      "at batch no 5400\n",
      "at batch no 5500\n",
      "at batch no 5600\n",
      "at batch no 5700\n",
      "at batch no 5800\n",
      "at batch no 5900\n",
      "at batch no 6000\n",
      "at batch no 6100\n",
      "at batch no 6200\n",
      "finished creating histogram for the 0th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "at batch no 3200\n",
      "at batch no 3300\n",
      "at batch no 3400\n",
      "at batch no 3500\n",
      "at batch no 3600\n",
      "at batch no 3700\n",
      "at batch no 3800\n",
      "at batch no 3900\n",
      "at batch no 4000\n",
      "at batch no 4100\n",
      "at batch no 4200\n",
      "at batch no 4300\n",
      "at batch no 4400\n",
      "at batch no 4500\n",
      "at batch no 4600\n",
      "at batch no 4700\n",
      "at batch no 4800\n",
      "at batch no 4900\n",
      "at batch no 5000\n",
      "at batch no 5100\n",
      "at batch no 5200\n",
      "at batch no 5300\n",
      "at batch no 5400\n",
      "at batch no 5500\n",
      "at batch no 5600\n",
      "at batch no 5700\n",
      "at batch no 5800\n",
      "at batch no 5900\n",
      "at batch no 6000\n",
      "at batch no 6100\n",
      "at batch no 6200\n",
      "finished creating histogram for the 1th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "at batch no 3200\n",
      "at batch no 3300\n",
      "at batch no 3400\n",
      "at batch no 3500\n",
      "at batch no 3600\n",
      "at batch no 3700\n",
      "at batch no 3800\n",
      "at batch no 3900\n",
      "at batch no 4000\n",
      "at batch no 4100\n",
      "at batch no 4200\n",
      "at batch no 4300\n",
      "at batch no 4400\n",
      "at batch no 4500\n",
      "at batch no 4600\n",
      "at batch no 4700\n",
      "at batch no 4800\n",
      "at batch no 4900\n",
      "at batch no 5000\n",
      "at batch no 5100\n",
      "at batch no 5200\n",
      "at batch no 5300\n",
      "at batch no 5400\n",
      "at batch no 5500\n",
      "at batch no 5600\n",
      "at batch no 5700\n",
      "at batch no 5800\n",
      "at batch no 5900\n",
      "at batch no 6000\n",
      "at batch no 6100\n",
      "at batch no 6200\n",
      "finished creating histogram for the 2th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "at batch no 3200\n",
      "at batch no 3300\n",
      "at batch no 3400\n",
      "at batch no 3500\n",
      "at batch no 3600\n",
      "at batch no 3700\n",
      "at batch no 3800\n",
      "at batch no 3900\n",
      "at batch no 4000\n",
      "at batch no 4100\n",
      "at batch no 4200\n",
      "at batch no 4300\n",
      "at batch no 4400\n",
      "at batch no 4500\n",
      "at batch no 4600\n",
      "at batch no 4700\n",
      "at batch no 4800\n",
      "at batch no 4900\n",
      "at batch no 5000\n",
      "at batch no 5100\n",
      "at batch no 5200\n",
      "at batch no 5300\n",
      "at batch no 5400\n",
      "at batch no 5500\n",
      "at batch no 5600\n",
      "at batch no 5700\n",
      "at batch no 5800\n",
      "at batch no 5900\n",
      "at batch no 6000\n",
      "at batch no 6100\n",
      "at batch no 6200\n",
      "finished creating histogram for the 3th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "at batch no 3200\n",
      "at batch no 3300\n",
      "at batch no 3400\n",
      "at batch no 3500\n",
      "at batch no 3600\n",
      "at batch no 3700\n",
      "at batch no 3800\n",
      "at batch no 3900\n",
      "at batch no 4000\n",
      "at batch no 4100\n",
      "at batch no 4200\n",
      "at batch no 4300\n",
      "at batch no 4400\n",
      "at batch no 4500\n",
      "at batch no 4600\n",
      "at batch no 4700\n",
      "at batch no 4800\n",
      "at batch no 4900\n",
      "at batch no 5000\n",
      "at batch no 5100\n",
      "at batch no 5200\n",
      "at batch no 5300\n",
      "at batch no 5400\n",
      "at batch no 5500\n",
      "at batch no 5600\n",
      "at batch no 5700\n",
      "at batch no 5800\n",
      "at batch no 5900\n",
      "at batch no 6000\n",
      "at batch no 6100\n",
      "at batch no 6200\n",
      "finished creating histogram for the 4th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "at batch no 3200\n",
      "at batch no 3300\n",
      "at batch no 3400\n",
      "at batch no 3500\n",
      "at batch no 3600\n",
      "at batch no 3700\n",
      "at batch no 3800\n",
      "at batch no 3900\n",
      "at batch no 4000\n",
      "at batch no 4100\n",
      "at batch no 4200\n",
      "at batch no 4300\n",
      "at batch no 4400\n",
      "at batch no 4500\n",
      "at batch no 4600\n",
      "at batch no 4700\n",
      "at batch no 4800\n",
      "at batch no 4900\n",
      "at batch no 5000\n",
      "at batch no 5100\n",
      "at batch no 5200\n",
      "at batch no 5300\n",
      "at batch no 5400\n",
      "at batch no 5500\n",
      "at batch no 5600\n",
      "at batch no 5700\n",
      "at batch no 5800\n",
      "at batch no 5900\n",
      "at batch no 6000\n",
      "at batch no 6100\n",
      "at batch no 6200\n",
      "finished creating histogram for the 5th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "at batch no 3200\n",
      "at batch no 3300\n",
      "at batch no 3400\n",
      "at batch no 3500\n",
      "at batch no 3600\n",
      "at batch no 3700\n",
      "at batch no 3800\n",
      "at batch no 3900\n",
      "at batch no 4000\n",
      "at batch no 4100\n",
      "at batch no 4200\n",
      "at batch no 4300\n",
      "at batch no 4400\n",
      "at batch no 4500\n",
      "at batch no 4600\n",
      "at batch no 4700\n",
      "at batch no 4800\n",
      "at batch no 4900\n",
      "at batch no 5000\n",
      "at batch no 5100\n",
      "at batch no 5200\n",
      "at batch no 5300\n",
      "at batch no 5400\n",
      "at batch no 5500\n",
      "at batch no 5600\n",
      "at batch no 5700\n",
      "at batch no 5800\n",
      "at batch no 5900\n",
      "at batch no 6000\n",
      "at batch no 6100\n",
      "at batch no 6200\n",
      "finished creating histogram for the 6th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "at batch no 3200\n",
      "at batch no 3300\n",
      "at batch no 3400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 3500\n",
      "at batch no 3600\n",
      "at batch no 3700\n",
      "at batch no 3800\n",
      "at batch no 3900\n",
      "at batch no 4000\n",
      "at batch no 4100\n",
      "at batch no 4200\n",
      "at batch no 4300\n",
      "at batch no 4400\n",
      "at batch no 4500\n",
      "at batch no 4600\n"
     ]
    }
   ],
   "source": [
    "n, hist, tk = diversity(learn, 10, 95)\n",
    "n, hist, tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(267,\n",
       " [(509, 1109.7700000022842),\n",
       "  (794, 481.94000000000017),\n",
       "  (815, 423.4899999997875),\n",
       "  (553, 266.649999999867),\n",
       "  (769, 260.6199999998725),\n",
       "  (918, 243.4699999998881),\n",
       "  (839, 233.83999999989686),\n",
       "  (611, 140.37999999998186),\n",
       "  (898, 131.68999999998977),\n",
       "  (860, 131.1099999999903),\n",
       "  (886, 125.96999999999498),\n",
       "  (594, 92.6200000000142),\n",
       "  (737, 67.90000000000859),\n",
       "  (458, 44.620000000003294),\n",
       "  (510, 39.330000000002094),\n",
       "  (582, 35.36000000000119),\n",
       "  (116, 32.000000000000426),\n",
       "  (863, 31.300000000000267),\n",
       "  (854, 25.109999999999),\n",
       "  (489, 21.339999999999215),\n",
       "  (580, 17.76999999999942),\n",
       "  (904, 15.799999999999532),\n",
       "  (824, 14.839999999999586),\n",
       "  (987, 14.089999999999629),\n",
       "  (750, 13.849999999999643),\n",
       "  (806, 13.569999999999657),\n",
       "  (454, 12.569999999999713),\n",
       "  (50, 12.469999999999718),\n",
       "  (741, 12.369999999999724),\n",
       "  (791, 11.949999999999749),\n",
       "  (637, 11.109999999999797),\n",
       "  (507, 11.099999999999797),\n",
       "  (565, 10.079999999999854),\n",
       "  (858, 9.879999999999866),\n",
       "  (519, 9.779999999999871),\n",
       "  (493, 9.2999999999999),\n",
       "  (624, 8.459999999999948),\n",
       "  (455, 8.439999999999948),\n",
       "  (109, 8.40999999999995),\n",
       "  (888, 8.05999999999997),\n",
       "  (474, 7.539999999999999),\n",
       "  (58, 7.35000000000001),\n",
       "  (49, 6.770000000000043),\n",
       "  (679, 6.47000000000006),\n",
       "  (84, 6.370000000000063),\n",
       "  (695, 6.150000000000061),\n",
       "  (671, 5.8900000000000565),\n",
       "  (60, 5.810000000000056),\n",
       "  (835, 5.730000000000055),\n",
       "  (798, 5.480000000000051),\n",
       "  (506, 5.330000000000049),\n",
       "  (490, 5.3100000000000485),\n",
       "  (971, 5.260000000000048),\n",
       "  (420, 5.160000000000046),\n",
       "  (749, 5.110000000000046),\n",
       "  (440, 5.100000000000046),\n",
       "  (646, 5.080000000000045),\n",
       "  (260, 4.990000000000044),\n",
       "  (546, 4.860000000000042),\n",
       "  (585, 4.850000000000042),\n",
       "  (778, 4.790000000000041),\n",
       "  (878, 4.74000000000004),\n",
       "  (363, 4.68000000000004),\n",
       "  (508, 4.350000000000035),\n",
       "  (327, 4.290000000000034),\n",
       "  (124, 4.240000000000033),\n",
       "  (61, 4.220000000000033),\n",
       "  (409, 4.160000000000032),\n",
       "  (905, 4.090000000000031),\n",
       "  (790, 4.060000000000031),\n",
       "  (761, 4.02000000000003),\n",
       "  (82, 4.01000000000003),\n",
       "  (748, 3.940000000000029),\n",
       "  (640, 3.9000000000000283),\n",
       "  (25, 3.890000000000028),\n",
       "  (692, 3.8800000000000283),\n",
       "  (917, 3.870000000000028),\n",
       "  (562, 3.8500000000000276),\n",
       "  (721, 3.8200000000000274),\n",
       "  (825, 3.810000000000027),\n",
       "  (865, 3.7900000000000267),\n",
       "  (39, 3.7700000000000267),\n",
       "  (401, 3.730000000000026),\n",
       "  (497, 3.7100000000000257),\n",
       "  (427, 3.6700000000000252),\n",
       "  (767, 3.580000000000024),\n",
       "  (37, 3.5600000000000236),\n",
       "  (340, 3.5600000000000236),\n",
       "  (274, 3.510000000000023),\n",
       "  (468, 3.4800000000000226),\n",
       "  (453, 3.4700000000000224),\n",
       "  (922, 3.460000000000022),\n",
       "  (48, 3.4400000000000217),\n",
       "  (44, 3.430000000000022),\n",
       "  (781, 3.4200000000000217),\n",
       "  (467, 3.4000000000000212),\n",
       "  (884, 3.390000000000021),\n",
       "  (687, 3.2900000000000196),\n",
       "  (955, 3.2700000000000196),\n",
       "  (682, 3.240000000000019),\n",
       "  (292, 3.220000000000019),\n",
       "  (885, 3.190000000000018),\n",
       "  (953, 3.1500000000000177),\n",
       "  (992, 3.1500000000000177),\n",
       "  (385, 3.110000000000017),\n",
       "  (67, 3.0700000000000167),\n",
       "  (716, 3.0700000000000167),\n",
       "  (348, 3.0000000000000155),\n",
       "  (555, 2.9100000000000144),\n",
       "  (581, 2.900000000000014),\n",
       "  (289, 2.890000000000014),\n",
       "  (135, 2.880000000000014),\n",
       "  (426, 2.870000000000014),\n",
       "  (533, 2.8600000000000136),\n",
       "  (561, 2.820000000000013),\n",
       "  (398, 2.810000000000013),\n",
       "  (102, 2.7200000000000117),\n",
       "  (213, 2.690000000000011),\n",
       "  (652, 2.6800000000000113),\n",
       "  (568, 2.660000000000011),\n",
       "  (491, 2.6400000000000103),\n",
       "  (328, 2.5900000000000096),\n",
       "  (645, 2.58000000000001),\n",
       "  (56, 2.5700000000000096),\n",
       "  (316, 2.520000000000009),\n",
       "  (907, 2.5100000000000087),\n",
       "  (45, 2.4500000000000077),\n",
       "  (406, 2.4400000000000075),\n",
       "  (599, 2.4400000000000075),\n",
       "  (365, 2.4300000000000077),\n",
       "  (487, 2.380000000000007),\n",
       "  (138, 2.340000000000006),\n",
       "  (293, 2.340000000000006),\n",
       "  (65, 2.3300000000000063),\n",
       "  (936, 2.3300000000000063),\n",
       "  (998, 2.3300000000000063),\n",
       "  (703, 2.310000000000006),\n",
       "  (488, 2.2800000000000056),\n",
       "  (621, 2.2700000000000053),\n",
       "  (821, 2.2700000000000053),\n",
       "  (733, 2.260000000000005),\n",
       "  (410, 2.250000000000005),\n",
       "  (288, 2.2200000000000046),\n",
       "  (788, 2.2100000000000044),\n",
       "  (218, 2.200000000000004),\n",
       "  (783, 2.180000000000004),\n",
       "  (718, 2.170000000000004),\n",
       "  (202, 2.1500000000000035),\n",
       "  (866, 2.1500000000000035),\n",
       "  (906, 2.120000000000003),\n",
       "  (701, 2.110000000000003),\n",
       "  (323, 2.1000000000000028),\n",
       "  (492, 2.1000000000000028),\n",
       "  (608, 2.1000000000000028),\n",
       "  (57, 2.0800000000000027),\n",
       "  (417, 2.0700000000000025),\n",
       "  (314, 2.0600000000000023),\n",
       "  (779, 2.040000000000002),\n",
       "  (894, 2.040000000000002),\n",
       "  (425, 2.020000000000002),\n",
       "  (133, 2.0000000000000013),\n",
       "  (90, 1.9900000000000013),\n",
       "  (483, 1.970000000000001),\n",
       "  (383, 1.9600000000000009),\n",
       "  (643, 1.9300000000000004),\n",
       "  (141, 1.9200000000000004),\n",
       "  (184, 1.9100000000000001),\n",
       "  (538, 1.9100000000000001),\n",
       "  (361, 1.9),\n",
       "  (79, 1.89),\n",
       "  (355, 1.89),\n",
       "  (873, 1.8499999999999992),\n",
       "  (219, 1.8399999999999992),\n",
       "  (616, 1.8399999999999992),\n",
       "  (799, 1.8399999999999992),\n",
       "  (973, 1.8399999999999992),\n",
       "  (201, 1.829999999999999),\n",
       "  (829, 1.829999999999999),\n",
       "  (256, 1.819999999999999),\n",
       "  (688, 1.8099999999999987),\n",
       "  (826, 1.7999999999999985),\n",
       "  (24, 1.7799999999999983),\n",
       "  (214, 1.7799999999999983),\n",
       "  (545, 1.7799999999999983),\n",
       "  (55, 1.759999999999998),\n",
       "  (698, 1.759999999999998),\n",
       "  (850, 1.759999999999998),\n",
       "  (275, 1.7499999999999978),\n",
       "  (46, 1.7199999999999975),\n",
       "  (631, 1.7199999999999975),\n",
       "  (113, 1.699999999999997),\n",
       "  (887, 1.689999999999997),\n",
       "  (282, 1.6799999999999968),\n",
       "  (352, 1.6599999999999966),\n",
       "  (984, 1.6599999999999966),\n",
       "  (76, 1.6499999999999964),\n",
       "  (8, 1.6299999999999961),\n",
       "  (264, 1.6299999999999961),\n",
       "  (664, 1.6099999999999959),\n",
       "  (334, 1.5999999999999959),\n",
       "  (880, 1.5999999999999959),\n",
       "  (912, 1.579999999999996),\n",
       "  (226, 1.569999999999996),\n",
       "  (277, 1.569999999999996),\n",
       "  (462, 1.549999999999996),\n",
       "  (754, 1.549999999999996),\n",
       "  (343, 1.539999999999996),\n",
       "  (217, 1.5299999999999963),\n",
       "  (787, 1.5299999999999963),\n",
       "  (243, 1.5099999999999962),\n",
       "  (319, 1.4999999999999962),\n",
       "  (177, 1.4899999999999962),\n",
       "  (290, 1.4899999999999962),\n",
       "  (300, 1.4799999999999964),\n",
       "  (318, 1.4699999999999964),\n",
       "  (911, 1.4599999999999964),\n",
       "  (91, 1.4399999999999964),\n",
       "  (191, 1.4399999999999964),\n",
       "  (331, 1.4399999999999964),\n",
       "  (625, 1.4299999999999966),\n",
       "  (963, 1.4299999999999966),\n",
       "  (54, 1.4099999999999966),\n",
       "  (720, 1.4099999999999966),\n",
       "  (770, 1.3999999999999966),\n",
       "  (192, 1.3899999999999966),\n",
       "  (337, 1.3699999999999968),\n",
       "  (472, 1.3699999999999968),\n",
       "  (932, 1.3699999999999968),\n",
       "  (30, 1.3499999999999968),\n",
       "  (396, 1.3399999999999967),\n",
       "  (99, 1.329999999999997),\n",
       "  (100, 1.329999999999997),\n",
       "  (730, 1.329999999999997),\n",
       "  (879, 1.329999999999997),\n",
       "  (704, 1.319999999999997),\n",
       "  (62, 1.309999999999997),\n",
       "  (251, 1.299999999999997),\n",
       "  (772, 1.299999999999997),\n",
       "  (234, 1.2799999999999971),\n",
       "  (981, 1.2799999999999971),\n",
       "  (216, 1.2699999999999971),\n",
       "  (247, 1.2699999999999971),\n",
       "  (108, 1.2599999999999971),\n",
       "  (588, 1.2599999999999971),\n",
       "  (342, 1.2499999999999971),\n",
       "  (828, 1.2499999999999971),\n",
       "  (114, 1.2299999999999973),\n",
       "  (946, 1.2299999999999973),\n",
       "  (286, 1.2199999999999973),\n",
       "  (502, 1.2099999999999973),\n",
       "  (547, 1.2099999999999973),\n",
       "  (635, 1.2099999999999973),\n",
       "  (735, 1.2099999999999973),\n",
       "  (182, 1.1999999999999973),\n",
       "  (272, 1.1999999999999973),\n",
       "  (386, 1.1899999999999973),\n",
       "  (614, 1.1899999999999973),\n",
       "  (71, 1.1799999999999975),\n",
       "  (110, 1.1799999999999975),\n",
       "  (123, 1.1599999999999975),\n",
       "  (77, 1.1499999999999975),\n",
       "  (982, 1.1499999999999975),\n",
       "  (163, 1.1399999999999975),\n",
       "  (407, 1.1399999999999975),\n",
       "  (539, 1.1399999999999975),\n",
       "  (566, 1.1399999999999975),\n",
       "  (892, 1.1399999999999975),\n",
       "  (240, 1.1299999999999977),\n",
       "  (321, 1.1199999999999977),\n",
       "  (665, 1.1199999999999977),\n",
       "  (868, 1.1199999999999977),\n",
       "  (41, 1.1099999999999977),\n",
       "  (311, 1.1099999999999977),\n",
       "  (387, 1.1099999999999977),\n",
       "  (203, 1.0999999999999976),\n",
       "  (476, 1.0999999999999976),\n",
       "  (336, 1.0899999999999976),\n",
       "  (443, 1.0899999999999976),\n",
       "  (27, 1.0699999999999978),\n",
       "  (161, 1.0699999999999978),\n",
       "  (303, 1.0699999999999978),\n",
       "  (347, 1.0699999999999978),\n",
       "  (753, 1.0699999999999978),\n",
       "  (136, 1.0599999999999978),\n",
       "  (661, 1.0599999999999978),\n",
       "  (212, 1.0499999999999978),\n",
       "  (157, 1.0399999999999978),\n",
       "  (411, 1.0399999999999978),\n",
       "  (415, 1.0399999999999978),\n",
       "  (985, 1.0399999999999978),\n",
       "  (101, 1.029999999999998),\n",
       "  (738, 1.029999999999998),\n",
       "  (853, 1.029999999999998),\n",
       "  (131, 1.019999999999998),\n",
       "  (271, 1.019999999999998),\n",
       "  (7, 0.999999999999998),\n",
       "  (261, 0.999999999999998),\n",
       "  (381, 0.999999999999998),\n",
       "  (457, 0.999999999999998),\n",
       "  (199, 0.9899999999999981),\n",
       "  (498, 0.9899999999999981),\n",
       "  (514, 0.9899999999999981),\n",
       "  (752, 0.9899999999999981),\n",
       "  (800, 0.9899999999999981),\n",
       "  (949, 0.9899999999999981),\n",
       "  (85, 0.9799999999999981),\n",
       "  (221, 0.9799999999999981),\n",
       "  (299, 0.9799999999999981),\n",
       "  (127, 0.9699999999999982),\n",
       "  (522, 0.9699999999999982),\n",
       "  (843, 0.9699999999999982),\n",
       "  (9, 0.9599999999999982),\n",
       "  (142, 0.9599999999999982),\n",
       "  (364, 0.9499999999999982),\n",
       "  (805, 0.9499999999999982),\n",
       "  (496, 0.9399999999999983),\n",
       "  (280, 0.9299999999999983),\n",
       "  (230, 0.9199999999999984),\n",
       "  (242, 0.9199999999999984),\n",
       "  (128, 0.9099999999999984),\n",
       "  (766, 0.9099999999999984),\n",
       "  (154, 0.8999999999999984),\n",
       "  (66, 0.8899999999999985),\n",
       "  (130, 0.8899999999999985),\n",
       "  (134, 0.8899999999999985),\n",
       "  (185, 0.8799999999999985),\n",
       "  (197, 0.8799999999999985),\n",
       "  (254, 0.8799999999999985),\n",
       "  (362, 0.8799999999999985),\n",
       "  (724, 0.8799999999999985),\n",
       "  (36, 0.8699999999999986),\n",
       "  (172, 0.8699999999999986),\n",
       "  (979, 0.8699999999999986),\n",
       "  (164, 0.8599999999999985),\n",
       "  (322, 0.8599999999999985),\n",
       "  (591, 0.8599999999999985),\n",
       "  (944, 0.8599999999999985),\n",
       "  (647, 0.8499999999999985),\n",
       "  (697, 0.8499999999999985),\n",
       "  (269, 0.8399999999999986),\n",
       "  (609, 0.8399999999999986),\n",
       "  (70, 0.8299999999999986),\n",
       "  (235, 0.8299999999999986),\n",
       "  (797, 0.8299999999999986),\n",
       "  (874, 0.8299999999999986),\n",
       "  (775, 0.8199999999999987),\n",
       "  (193, 0.8099999999999987),\n",
       "  (558, 0.8099999999999987),\n",
       "  (249, 0.7999999999999987),\n",
       "  (267, 0.7999999999999987),\n",
       "  (52, 0.7899999999999988),\n",
       "  (278, 0.7899999999999988),\n",
       "  (696, 0.7899999999999988),\n",
       "  (694, 0.7799999999999988),\n",
       "  (129, 0.7699999999999989),\n",
       "  (228, 0.7699999999999989),\n",
       "  (390, 0.7699999999999989),\n",
       "  (587, 0.7699999999999989),\n",
       "  (642, 0.7699999999999989),\n",
       "  (758, 0.7699999999999989),\n",
       "  (808, 0.7699999999999989),\n",
       "  (200, 0.7499999999999989),\n",
       "  (206, 0.7499999999999989),\n",
       "  (471, 0.7499999999999989),\n",
       "  (832, 0.7499999999999989),\n",
       "  (947, 0.7499999999999989),\n",
       "  (368, 0.739999999999999),\n",
       "  (446, 0.739999999999999),\n",
       "  (707, 0.739999999999999),\n",
       "  (877, 0.729999999999999),\n",
       "  (51, 0.7199999999999991),\n",
       "  (444, 0.7199999999999991),\n",
       "  (910, 0.7199999999999991),\n",
       "  (954, 0.7199999999999991),\n",
       "  (69, 0.7099999999999991),\n",
       "  (196, 0.7099999999999991),\n",
       "  (751, 0.7099999999999991),\n",
       "  (870, 0.7099999999999991),\n",
       "  (921, 0.6999999999999991),\n",
       "  (991, 0.6999999999999991),\n",
       "  (0, 0.6899999999999992),\n",
       "  (18, 0.6899999999999992),\n",
       "  (615, 0.6899999999999992),\n",
       "  (43, 0.6799999999999992),\n",
       "  (132, 0.6799999999999992),\n",
       "  (232, 0.6799999999999992),\n",
       "  (330, 0.6799999999999992),\n",
       "  (535, 0.6799999999999992),\n",
       "  (693, 0.6799999999999992),\n",
       "  (97, 0.6699999999999993),\n",
       "  (98, 0.6699999999999993),\n",
       "  (119, 0.6699999999999993),\n",
       "  (170, 0.6699999999999993),\n",
       "  (734, 0.6699999999999993),\n",
       "  (795, 0.6699999999999993),\n",
       "  (225, 0.6599999999999993),\n",
       "  (658, 0.6599999999999993),\n",
       "  (156, 0.6499999999999992),\n",
       "  (273, 0.6499999999999992),\n",
       "  (393, 0.6499999999999992),\n",
       "  (239, 0.6399999999999993),\n",
       "  (743, 0.6399999999999993),\n",
       "  (937, 0.6399999999999993),\n",
       "  (938, 0.6399999999999993),\n",
       "  (958, 0.6399999999999993),\n",
       "  (155, 0.6299999999999993),\n",
       "  (248, 0.6299999999999993),\n",
       "  (339, 0.6299999999999993),\n",
       "  (727, 0.6299999999999993),\n",
       "  (957, 0.6299999999999993),\n",
       "  (424, 0.6199999999999994),\n",
       "  (28, 0.6099999999999994),\n",
       "  (42, 0.6099999999999994),\n",
       "  (307, 0.6099999999999994),\n",
       "  (563, 0.6099999999999994),\n",
       "  (139, 0.5999999999999994),\n",
       "  (186, 0.5999999999999994),\n",
       "  (189, 0.5999999999999994),\n",
       "  (233, 0.5999999999999994),\n",
       "  (276, 0.5999999999999994),\n",
       "  (329, 0.5999999999999994),\n",
       "  (676, 0.5999999999999994),\n",
       "  (255, 0.5899999999999995),\n",
       "  (418, 0.5899999999999995),\n",
       "  (531, 0.5899999999999995),\n",
       "  (759, 0.5899999999999995),\n",
       "  (986, 0.5899999999999995),\n",
       "  (433, 0.5799999999999995),\n",
       "  (438, 0.5799999999999995),\n",
       "  (536, 0.5799999999999995),\n",
       "  (552, 0.5799999999999995),\n",
       "  (997, 0.5799999999999995),\n",
       "  (121, 0.5699999999999996),\n",
       "  (181, 0.5699999999999996),\n",
       "  (857, 0.5699999999999996),\n",
       "  (15, 0.5599999999999996),\n",
       "  (281, 0.5599999999999996),\n",
       "  (81, 0.5499999999999996),\n",
       "  (376, 0.5499999999999996),\n",
       "  (479, 0.5499999999999996),\n",
       "  (572, 0.5499999999999996),\n",
       "  (699, 0.5499999999999996),\n",
       "  (353, 0.5399999999999997),\n",
       "  (358, 0.5399999999999997),\n",
       "  (412, 0.5399999999999997),\n",
       "  (428, 0.5399999999999997),\n",
       "  (861, 0.5399999999999997),\n",
       "  (939, 0.5399999999999997),\n",
       "  (173, 0.5299999999999997),\n",
       "  (210, 0.5299999999999997),\n",
       "  (237, 0.5299999999999997),\n",
       "  (304, 0.5299999999999997),\n",
       "  (349, 0.5299999999999997),\n",
       "  (612, 0.5299999999999997),\n",
       "  (291, 0.5199999999999998),\n",
       "  (294, 0.5199999999999998),\n",
       "  (549, 0.5199999999999998),\n",
       "  (822, 0.5199999999999998),\n",
       "  (920, 0.5199999999999998),\n",
       "  (211, 0.5099999999999998),\n",
       "  (312, 0.5099999999999998),\n",
       "  (890, 0.5099999999999998),\n",
       "  (919, 0.5099999999999998),\n",
       "  (994, 0.5099999999999998),\n",
       "  (231, 0.49999999999999983),\n",
       "  (391, 0.49999999999999983),\n",
       "  (72, 0.4899999999999999),\n",
       "  (144, 0.4899999999999999),\n",
       "  (253, 0.4899999999999999),\n",
       "  (595, 0.4899999999999999),\n",
       "  (820, 0.4899999999999999),\n",
       "  (151, 0.47999999999999987),\n",
       "  (198, 0.47999999999999987),\n",
       "  (845, 0.47999999999999987),\n",
       "  (852, 0.47999999999999987),\n",
       "  (996, 0.47999999999999987),\n",
       "  (601, 0.4699999999999999),\n",
       "  (768, 0.4699999999999999),\n",
       "  (777, 0.4699999999999999),\n",
       "  (35, 0.45999999999999996),\n",
       "  (220, 0.45999999999999996),\n",
       "  (350, 0.45999999999999996),\n",
       "  (83, 0.45),\n",
       "  (96, 0.45),\n",
       "  (259, 0.45),\n",
       "  (284, 0.45),\n",
       "  (356, 0.45),\n",
       "  (367, 0.45),\n",
       "  (375, 0.45),\n",
       "  (528, 0.45),\n",
       "  (537, 0.45),\n",
       "  (988, 0.45),\n",
       "  (215, 0.44000000000000006),\n",
       "  (298, 0.44000000000000006),\n",
       "  (317, 0.44000000000000006),\n",
       "  (366, 0.44000000000000006),\n",
       "  (449, 0.44000000000000006),\n",
       "  (725, 0.44000000000000006),\n",
       "  (756, 0.44000000000000006),\n",
       "  (816, 0.44000000000000006),\n",
       "  (893, 0.44000000000000006),\n",
       "  (956, 0.44000000000000006),\n",
       "  (88, 0.43000000000000005),\n",
       "  (287, 0.43000000000000005),\n",
       "  (309, 0.43000000000000005),\n",
       "  (439, 0.43000000000000005),\n",
       "  (554, 0.43000000000000005),\n",
       "  (903, 0.43000000000000005),\n",
       "  (993, 0.43000000000000005),\n",
       "  (23, 0.4200000000000001),\n",
       "  (93, 0.4200000000000001),\n",
       "  (174, 0.4200000000000001),\n",
       "  (305, 0.4200000000000001),\n",
       "  (461, 0.4200000000000001),\n",
       "  (517, 0.4200000000000001),\n",
       "  (807, 0.4200000000000001),\n",
       "  (819, 0.4200000000000001),\n",
       "  (94, 0.41000000000000014),\n",
       "  (126, 0.41000000000000014),\n",
       "  (159, 0.41000000000000014),\n",
       "  (175, 0.41000000000000014),\n",
       "  (80, 0.4000000000000002),\n",
       "  (354, 0.4000000000000002),\n",
       "  (382, 0.4000000000000002),\n",
       "  (456, 0.4000000000000002),\n",
       "  (597, 0.4000000000000002),\n",
       "  (705, 0.4000000000000002),\n",
       "  (709, 0.4000000000000002),\n",
       "  (990, 0.4000000000000002),\n",
       "  (224, 0.39000000000000024),\n",
       "  (578, 0.39000000000000024),\n",
       "  (619, 0.39000000000000024),\n",
       "  (823, 0.39000000000000024),\n",
       "  (187, 0.3800000000000002),\n",
       "  (209, 0.3800000000000002),\n",
       "  (569, 0.3800000000000002),\n",
       "  (793, 0.3800000000000002),\n",
       "  (63, 0.3700000000000002),\n",
       "  (64, 0.3700000000000002),\n",
       "  (86, 0.3700000000000002),\n",
       "  (104, 0.3700000000000002),\n",
       "  (295, 0.3700000000000002),\n",
       "  (344, 0.3700000000000002),\n",
       "  (388, 0.3700000000000002),\n",
       "  (592, 0.3700000000000002),\n",
       "  (847, 0.3700000000000002),\n",
       "  (864, 0.3700000000000002),\n",
       "  (118, 0.3600000000000002),\n",
       "  (195, 0.3600000000000002),\n",
       "  (279, 0.3600000000000002),\n",
       "  (301, 0.3600000000000002),\n",
       "  (310, 0.3600000000000002),\n",
       "  (495, 0.3600000000000002),\n",
       "  (915, 0.3600000000000002),\n",
       "  (47, 0.3500000000000002),\n",
       "  (168, 0.3500000000000002),\n",
       "  (204, 0.3500000000000002),\n",
       "  (238, 0.3500000000000002),\n",
       "  (345, 0.3500000000000002),\n",
       "  (357, 0.3500000000000002),\n",
       "  (451, 0.3500000000000002),\n",
       "  (830, 0.3500000000000002),\n",
       "  (872, 0.3500000000000002),\n",
       "  (207, 0.3400000000000002),\n",
       "  (236, 0.3400000000000002),\n",
       "  (257, 0.3400000000000002),\n",
       "  (313, 0.3400000000000002),\n",
       "  (346, 0.3400000000000002),\n",
       "  (672, 0.3400000000000002),\n",
       "  (677, 0.3400000000000002),\n",
       "  (723, 0.3400000000000002),\n",
       "  (763, 0.3400000000000002),\n",
       "  (188, 0.3300000000000002),\n",
       "  (250, 0.3300000000000002),\n",
       "  (370, 0.3300000000000002),\n",
       "  (389, 0.3300000000000002),\n",
       "  (641, 0.3300000000000002),\n",
       "  (712, 0.3300000000000002),\n",
       "  (869, 0.3300000000000002),\n",
       "  (891, 0.3300000000000002),\n",
       "  (916, 0.3300000000000002),\n",
       "  (952, 0.3300000000000002),\n",
       "  (975, 0.3300000000000002),\n",
       "  (245, 0.3200000000000002),\n",
       "  (377, 0.3200000000000002),\n",
       "  (464, 0.3200000000000002),\n",
       "  (556, 0.3200000000000002),\n",
       "  (560, 0.3200000000000002),\n",
       "  (670, 0.3200000000000002),\n",
       "  (500, 0.31000000000000016),\n",
       "  (586, 0.31000000000000016),\n",
       "  (629, 0.31000000000000016),\n",
       "  (757, 0.31000000000000016),\n",
       "  (776, 0.31000000000000016),\n",
       "  (125, 0.30000000000000016),\n",
       "  (227, 0.30000000000000016),\n",
       "  (351, 0.30000000000000016),\n",
       "  (668, 0.30000000000000016),\n",
       "  (855, 0.30000000000000016),\n",
       "  (112, 0.29000000000000015),\n",
       "  (285, 0.29000000000000015),\n",
       "  (308, 0.29000000000000015),\n",
       "  (373, 0.29000000000000015),\n",
       "  (600, 0.29000000000000015),\n",
       "  (711, 0.29000000000000015),\n",
       "  (801, 0.29000000000000015),\n",
       "  (943, 0.29000000000000015),\n",
       "  (169, 0.28000000000000014),\n",
       "  (270, 0.28000000000000014),\n",
       "  (463, 0.28000000000000014),\n",
       "  (520, 0.28000000000000014),\n",
       "  (576, 0.28000000000000014),\n",
       "  (746, 0.28000000000000014),\n",
       "  (760, 0.28000000000000014),\n",
       "  (989, 0.28000000000000014),\n",
       "  (371, 0.27000000000000013),\n",
       "  (400, 0.27000000000000013),\n",
       "  (518, 0.27000000000000013),\n",
       "  (542, 0.27000000000000013),\n",
       "  (717, 0.27000000000000013),\n",
       "  (881, 0.27000000000000013),\n",
       "  (931, 0.27000000000000013),\n",
       "  (20, 0.2600000000000001),\n",
       "  (111, 0.2600000000000001),\n",
       "  (325, 0.2600000000000001),\n",
       "  (571, 0.2600000000000001),\n",
       "  (755, 0.2600000000000001),\n",
       "  (848, 0.2600000000000001),\n",
       "  (867, 0.2600000000000001),\n",
       "  (883, 0.2600000000000001),\n",
       "  (74, 0.2500000000000001),\n",
       "  (117, 0.2500000000000001),\n",
       "  (183, 0.2500000000000001),\n",
       "  (208, 0.2500000000000001),\n",
       "  (421, 0.2500000000000001),\n",
       "  (690, 0.2500000000000001),\n",
       "  (728, 0.2500000000000001),\n",
       "  (950, 0.2500000000000001),\n",
       "  (951, 0.2500000000000001),\n",
       "  (995, 0.2500000000000001),\n",
       "  (32, 0.24000000000000007),\n",
       "  (106, 0.24000000000000007),\n",
       "  (477, 0.24000000000000007),\n",
       "  (481, 0.24000000000000007),\n",
       "  (515, 0.24000000000000007),\n",
       "  (603, 0.24000000000000007),\n",
       "  (837, 0.24000000000000007),\n",
       "  (900, 0.24000000000000007),\n",
       "  (68, 0.23000000000000007),\n",
       "  (162, 0.23000000000000007),\n",
       "  (166, 0.23000000000000007),\n",
       "  (223, 0.23000000000000007),\n",
       "  (246, 0.23000000000000007),\n",
       "  (460, 0.23000000000000007),\n",
       "  (530, 0.23000000000000007),\n",
       "  (534, 0.23000000000000007),\n",
       "  (564, 0.23000000000000007),\n",
       "  (856, 0.23000000000000007),\n",
       "  (930, 0.23000000000000007),\n",
       "  (14, 0.22000000000000006),\n",
       "  (53, 0.22000000000000006),\n",
       "  (89, 0.22000000000000006),\n",
       "  (222, 0.22000000000000006),\n",
       "  (263, 0.22000000000000006),\n",
       "  (429, 0.22000000000000006),\n",
       "  (452, 0.22000000000000006),\n",
       "  (540, 0.22000000000000006),\n",
       "  (544, 0.22000000000000006),\n",
       "  (627, 0.22000000000000006),\n",
       "  (945, 0.22000000000000006),\n",
       "  (10, 0.21000000000000005),\n",
       "  (178, 0.21000000000000005),\n",
       "  (205, 0.21000000000000005),\n",
       "  (335, 0.21000000000000005),\n",
       "  (369, 0.21000000000000005),\n",
       "  (379, 0.21000000000000005),\n",
       "  (404, 0.21000000000000005),\n",
       "  (447, 0.21000000000000005),\n",
       "  (448, 0.21000000000000005),\n",
       "  (501, 0.21000000000000005),\n",
       "  (632, 0.21000000000000005),\n",
       "  (636, 0.21000000000000005),\n",
       "  (706, 0.21000000000000005),\n",
       "  (789, 0.21000000000000005),\n",
       "  (962, 0.21000000000000005),\n",
       "  (966, 0.21000000000000005),\n",
       "  (972, 0.21000000000000005),\n",
       "  (34, 0.20000000000000004),\n",
       "  (122, 0.20000000000000004),\n",
       "  (262, 0.20000000000000004),\n",
       "  (302, 0.20000000000000004),\n",
       "  (575, 0.20000000000000004),\n",
       "  (669, 0.20000000000000004),\n",
       "  (762, 0.20000000000000004),\n",
       "  (764, 0.20000000000000004),\n",
       "  (5, 0.19000000000000006),\n",
       "  (17, 0.19000000000000006),\n",
       "  (140, 0.19000000000000006),\n",
       "  (160, 0.19000000000000006),\n",
       "  (360, 0.19000000000000006),\n",
       "  (378, 0.19000000000000006),\n",
       "  (413, 0.19000000000000006),\n",
       "  (543, 0.19000000000000006),\n",
       "  (570, 0.19000000000000006),\n",
       "  (579, 0.19000000000000006),\n",
       "  (584, 0.19000000000000006),\n",
       "  (602, 0.19000000000000006),\n",
       "  (607, 0.19000000000000006),\n",
       "  (683, 0.19000000000000006),\n",
       "  (740, 0.19000000000000006),\n",
       "  (817, 0.19000000000000006),\n",
       "  (889, 0.19000000000000006),\n",
       "  (999, 0.19000000000000006),\n",
       "  (78, 0.18000000000000005),\n",
       "  (105, 0.18000000000000005),\n",
       "  (145, 0.18000000000000005),\n",
       "  (395, 0.18000000000000005),\n",
       "  (577, 0.18000000000000005),\n",
       "  (667, 0.18000000000000005),\n",
       "  (784, 0.18000000000000005),\n",
       "  (792, 0.18000000000000005),\n",
       "  (840, 0.18000000000000005),\n",
       "  (12, 0.17000000000000004),\n",
       "  (40, 0.17000000000000004),\n",
       "  (802, 0.17000000000000004),\n",
       "  (812, 0.17000000000000004),\n",
       "  (834, 0.17000000000000004),\n",
       "  (875, 0.17000000000000004),\n",
       "  (31, 0.16000000000000003),\n",
       "  (115, 0.16000000000000003),\n",
       "  (143, 0.16000000000000003),\n",
       "  (244, 0.16000000000000003),\n",
       "  (408, 0.16000000000000003),\n",
       "  (441, 0.16000000000000003),\n",
       "  (626, 0.16000000000000003),\n",
       "  (644, 0.16000000000000003),\n",
       "  (656, 0.16000000000000003),\n",
       "  (684, 0.16000000000000003),\n",
       "  (731, 0.16000000000000003),\n",
       "  (796, 0.16000000000000003),\n",
       "  (959, 0.16000000000000003),\n",
       "  (229, 0.15000000000000002),\n",
       "  (241, 0.15000000000000002),\n",
       "  (252, 0.15000000000000002),\n",
       "  (372, 0.15000000000000002),\n",
       "  (397, 0.15000000000000002),\n",
       "  (414, 0.15000000000000002),\n",
       "  (494, 0.15000000000000002),\n",
       "  (137, 0.14),\n",
       "  (147, 0.14),\n",
       "  (466, 0.14),\n",
       "  (714, 0.14),\n",
       "  (22, 0.13),\n",
       "  (120, 0.13),\n",
       "  (148, 0.13),\n",
       "  (165, 0.13),\n",
       "  (306, 0.13),\n",
       "  (436, 0.13),\n",
       "  (541, 0.13),\n",
       "  (630, 0.13),\n",
       "  (765, 0.13),\n",
       "  (941, 0.13),\n",
       "  (75, 0.12),\n",
       "  (87, 0.12),\n",
       "  (92, 0.12),\n",
       "  (194, 0.12),\n",
       "  (265, 0.12),\n",
       "  (380, 0.12),\n",
       "  (392, 0.12),\n",
       "  (432, 0.12),\n",
       "  (511, 0.12),\n",
       "  (521, 0.12),\n",
       "  (654, 0.12),\n",
       "  (803, 0.12),\n",
       "  (882, 0.12),\n",
       "  (897, 0.12),\n",
       "  (927, 0.12),\n",
       "  (33, 0.10999999999999999),\n",
       "  (59, 0.10999999999999999),\n",
       "  (107, 0.10999999999999999),\n",
       "  (341, 0.10999999999999999),\n",
       "  (524, 0.10999999999999999),\n",
       "  (574, 0.10999999999999999),\n",
       "  (596, 0.10999999999999999),\n",
       "  (744, 0.10999999999999999),\n",
       "  (842, 0.10999999999999999),\n",
       "  (934, 0.10999999999999999),\n",
       "  (38, 0.09999999999999999),\n",
       "  (434, 0.09999999999999999),\n",
       "  (604, 0.09999999999999999),\n",
       "  (618, 0.09999999999999999),\n",
       "  (634, 0.09999999999999999),\n",
       "  (862, 0.09999999999999999),\n",
       "  (926, 0.09999999999999999),\n",
       "  (21, 0.09),\n",
       "  (266, 0.09),\n",
       "  (332, 0.09),\n",
       "  (359, 0.09),\n",
       "  (374, 0.09),\n",
       "  (573, 0.09),\n",
       "  (620, 0.09),\n",
       "  (685, 0.09),\n",
       "  (771, 0.09),\n",
       "  (814, 0.09),\n",
       "  (833, 0.09),\n",
       "  (1, 0.07999999999999999),\n",
       "  (167, 0.07999999999999999),\n",
       "  (171, 0.07999999999999999),\n",
       "  (384, 0.07999999999999999),\n",
       "  (445, 0.07999999999999999),\n",
       "  (475, 0.07999999999999999),\n",
       "  (557, 0.07999999999999999),\n",
       "  (593, 0.07999999999999999),\n",
       "  (648, 0.07999999999999999),\n",
       "  (681, 0.07999999999999999),\n",
       "  (780, 0.07999999999999999),\n",
       "  (849, 0.07999999999999999),\n",
       "  (908, 0.07999999999999999),\n",
       "  (983, 0.07999999999999999),\n",
       "  (16, 0.06999999999999999),\n",
       "  (179, 0.06999999999999999),\n",
       "  (402, 0.06999999999999999),\n",
       "  (499, 0.06999999999999999),\n",
       "  (513, 0.06999999999999999),\n",
       "  (649, 0.06999999999999999),\n",
       "  (708, 0.06999999999999999),\n",
       "  (844, 0.06999999999999999),\n",
       "  (871, 0.06999999999999999),\n",
       "  (176, 0.06),\n",
       "  (320, 0.06),\n",
       "  (333, 0.06),\n",
       "  (450, 0.06),\n",
       "  (478, 0.06),\n",
       "  (503, 0.06),\n",
       "  (512, 0.06),\n",
       "  (548, 0.06),\n",
       "  (623, 0.06),\n",
       "  (653, 0.06),\n",
       "  (689, 0.06),\n",
       "  (811, 0.06),\n",
       "  (6, 0.05),\n",
       "  (146, 0.05),\n",
       "  (158, 0.05),\n",
       "  (180, 0.05),\n",
       "  (297, 0.05),\n",
       "  (324, 0.05),\n",
       "  (399, 0.05),\n",
       "  (419, 0.05),\n",
       "  (422, 0.05),\n",
       "  (430, 0.05),\n",
       "  (442, 0.05),\n",
       "  (628, 0.05),\n",
       "  (715, 0.05),\n",
       "  (722, 0.05),\n",
       "  (836, 0.05),\n",
       "  (913, 0.05),\n",
       "  (964, 0.05),\n",
       "  (13, 0.04),\n",
       "  (19, 0.04),\n",
       "  (153, 0.04),\n",
       "  (268, 0.04),\n",
       "  (283, 0.04),\n",
       "  (431, 0.04),\n",
       "  (470, 0.04),\n",
       "  (473, 0.04),\n",
       "  (486, 0.04),\n",
       "  (622, 0.04),\n",
       "  (638, 0.04),\n",
       "  (655, 0.04),\n",
       "  (659, 0.04),\n",
       "  (660, 0.04),\n",
       "  (686, 0.04),\n",
       "  (719, 0.04),\n",
       "  (813, 0.04),\n",
       "  (961, 0.04),\n",
       "  (11, 0.030000000000000006),\n",
       "  (26, 0.030000000000000006),\n",
       "  (95, 0.030000000000000006),\n",
       "  (152, 0.030000000000000006),\n",
       "  (315, 0.030000000000000006),\n",
       "  (326, 0.030000000000000006),\n",
       "  (416, 0.030000000000000006),\n",
       "  (459, 0.030000000000000006),\n",
       "  (469, 0.030000000000000006),\n",
       "  (567, 0.030000000000000006),\n",
       "  (583, 0.030000000000000006),\n",
       "  (613, 0.030000000000000006),\n",
       "  (639, 0.030000000000000006),\n",
       "  (729, 0.030000000000000006),\n",
       "  (773, 0.030000000000000006),\n",
       "  (774, 0.030000000000000006),\n",
       "  (782, 0.030000000000000006),\n",
       "  (851, 0.030000000000000006),\n",
       "  (859, 0.030000000000000006),\n",
       "  (924, 0.030000000000000006),\n",
       "  (968, 0.030000000000000006),\n",
       "  (2, 0.02),\n",
       "  (73, 0.02),\n",
       "  (150, 0.02),\n",
       "  (338, 0.02),\n",
       "  (523, 0.02),\n",
       "  (598, 0.02),\n",
       "  (663, 0.02),\n",
       "  (674, 0.02),\n",
       "  (675, 0.02),\n",
       "  (678, 0.02),\n",
       "  (710, 0.02),\n",
       "  (726, 0.02),\n",
       "  (739, 0.02),\n",
       "  (810, 0.02),\n",
       "  (831, 0.02),\n",
       "  (838, 0.02),\n",
       "  (876, 0.02),\n",
       "  (909, 0.02),\n",
       "  (4, 0.01),\n",
       "  (190, 0.01),\n",
       "  (258, 0.01),\n",
       "  (394, 0.01),\n",
       "  (525, 0.01),\n",
       "  (526, 0.01),\n",
       "  (527, 0.01),\n",
       "  (532, 0.01),\n",
       "  (551, 0.01),\n",
       "  (559, 0.01),\n",
       "  (589, 0.01),\n",
       "  (617, 0.01),\n",
       "  (633, 0.01),\n",
       "  (662, 0.01),\n",
       "  (673, 0.01),\n",
       "  (691, 0.01),\n",
       "  (700, 0.01),\n",
       "  (702, 0.01),\n",
       "  (804, 0.01),\n",
       "  (809, 0.01),\n",
       "  (818, 0.01),\n",
       "  (827, 0.01),\n",
       "  (902, 0.01),\n",
       "  (925, 0.01),\n",
       "  (933, 0.01),\n",
       "  (942, 0.01),\n",
       "  (3, 0.0),\n",
       "  (29, 0.0),\n",
       "  (103, 0.0),\n",
       "  (149, 0.0),\n",
       "  (296, 0.0),\n",
       "  (403, 0.0),\n",
       "  (405, 0.0),\n",
       "  (423, 0.0),\n",
       "  (435, 0.0),\n",
       "  (437, 0.0),\n",
       "  (465, 0.0),\n",
       "  (480, 0.0),\n",
       "  (482, 0.0),\n",
       "  (484, 0.0),\n",
       "  (485, 0.0),\n",
       "  (504, 0.0),\n",
       "  (505, 0.0),\n",
       "  (516, 0.0),\n",
       "  (529, 0.0),\n",
       "  (550, 0.0),\n",
       "  (590, 0.0),\n",
       "  (605, 0.0),\n",
       "  (606, 0.0),\n",
       "  (610, 0.0),\n",
       "  (650, 0.0),\n",
       "  (651, 0.0),\n",
       "  (657, 0.0),\n",
       "  (666, 0.0),\n",
       "  (680, 0.0),\n",
       "  (713, 0.0),\n",
       "  (732, 0.0),\n",
       "  (736, 0.0),\n",
       "  (742, 0.0),\n",
       "  (745, 0.0),\n",
       "  (747, 0.0),\n",
       "  (785, 0.0),\n",
       "  (786, 0.0),\n",
       "  (841, 0.0),\n",
       "  (846, 0.0),\n",
       "  (895, 0.0),\n",
       "  (896, 0.0),\n",
       "  (899, 0.0),\n",
       "  (901, 0.0),\n",
       "  (914, 0.0),\n",
       "  (923, 0.0),\n",
       "  (928, 0.0),\n",
       "  (929, 0.0),\n",
       "  (935, 0.0),\n",
       "  (940, 0.0),\n",
       "  (948, 0.0),\n",
       "  (960, 0.0),\n",
       "  (965, 0.0),\n",
       "  (967, 0.0),\n",
       "  (969, 0.0),\n",
       "  (970, 0.0),\n",
       "  (974, 0.0),\n",
       "  (976, 0.0),\n",
       "  (977, 0.0),\n",
       "  (978, 0.0),\n",
       "  (980, 0.0)],\n",
       " [509,\n",
       "  794,\n",
       "  815,\n",
       "  553,\n",
       "  769,\n",
       "  918,\n",
       "  839,\n",
       "  611,\n",
       "  898,\n",
       "  860,\n",
       "  886,\n",
       "  594,\n",
       "  737,\n",
       "  458,\n",
       "  510,\n",
       "  582,\n",
       "  116,\n",
       "  863,\n",
       "  854,\n",
       "  489,\n",
       "  580,\n",
       "  904,\n",
       "  824,\n",
       "  987,\n",
       "  750,\n",
       "  806,\n",
       "  454,\n",
       "  50,\n",
       "  741,\n",
       "  791,\n",
       "  637,\n",
       "  507,\n",
       "  565,\n",
       "  858,\n",
       "  519,\n",
       "  493,\n",
       "  624,\n",
       "  455,\n",
       "  109,\n",
       "  888,\n",
       "  474,\n",
       "  58,\n",
       "  49,\n",
       "  679,\n",
       "  84,\n",
       "  695,\n",
       "  671,\n",
       "  60,\n",
       "  835,\n",
       "  798,\n",
       "  506,\n",
       "  490,\n",
       "  971,\n",
       "  420,\n",
       "  749,\n",
       "  440,\n",
       "  646,\n",
       "  260,\n",
       "  546,\n",
       "  585,\n",
       "  778,\n",
       "  878,\n",
       "  363,\n",
       "  508,\n",
       "  327,\n",
       "  124,\n",
       "  61,\n",
       "  409,\n",
       "  905,\n",
       "  790,\n",
       "  761,\n",
       "  82,\n",
       "  748,\n",
       "  640,\n",
       "  25,\n",
       "  692,\n",
       "  917,\n",
       "  562,\n",
       "  721,\n",
       "  825,\n",
       "  865,\n",
       "  39,\n",
       "  401,\n",
       "  497,\n",
       "  427,\n",
       "  767,\n",
       "  37,\n",
       "  340,\n",
       "  274,\n",
       "  468,\n",
       "  453,\n",
       "  922,\n",
       "  48,\n",
       "  44,\n",
       "  781,\n",
       "  467,\n",
       "  884,\n",
       "  687,\n",
       "  955,\n",
       "  682,\n",
       "  292,\n",
       "  885,\n",
       "  953,\n",
       "  992,\n",
       "  385,\n",
       "  67,\n",
       "  716,\n",
       "  348,\n",
       "  555,\n",
       "  581,\n",
       "  289,\n",
       "  135,\n",
       "  426,\n",
       "  533,\n",
       "  561,\n",
       "  398,\n",
       "  102,\n",
       "  213,\n",
       "  652,\n",
       "  568,\n",
       "  491,\n",
       "  328,\n",
       "  645,\n",
       "  56,\n",
       "  316,\n",
       "  907,\n",
       "  45,\n",
       "  406,\n",
       "  599,\n",
       "  365,\n",
       "  487,\n",
       "  138,\n",
       "  293,\n",
       "  65,\n",
       "  936,\n",
       "  998,\n",
       "  703,\n",
       "  488,\n",
       "  621,\n",
       "  821,\n",
       "  733,\n",
       "  410,\n",
       "  288,\n",
       "  788,\n",
       "  218,\n",
       "  783,\n",
       "  718,\n",
       "  202,\n",
       "  866,\n",
       "  906,\n",
       "  701,\n",
       "  323,\n",
       "  492,\n",
       "  608,\n",
       "  57,\n",
       "  417,\n",
       "  314,\n",
       "  779,\n",
       "  894,\n",
       "  425,\n",
       "  133,\n",
       "  90,\n",
       "  483,\n",
       "  383,\n",
       "  643,\n",
       "  141,\n",
       "  184,\n",
       "  538,\n",
       "  361,\n",
       "  79,\n",
       "  355,\n",
       "  873,\n",
       "  219,\n",
       "  616,\n",
       "  799,\n",
       "  973,\n",
       "  201,\n",
       "  829,\n",
       "  256,\n",
       "  688,\n",
       "  826,\n",
       "  24,\n",
       "  214,\n",
       "  545,\n",
       "  55,\n",
       "  698,\n",
       "  850,\n",
       "  275,\n",
       "  46,\n",
       "  631,\n",
       "  113,\n",
       "  887,\n",
       "  282,\n",
       "  352,\n",
       "  984,\n",
       "  76,\n",
       "  8,\n",
       "  264,\n",
       "  664,\n",
       "  334,\n",
       "  880,\n",
       "  912,\n",
       "  226,\n",
       "  277,\n",
       "  462,\n",
       "  754,\n",
       "  343,\n",
       "  217,\n",
       "  787,\n",
       "  243,\n",
       "  319,\n",
       "  177,\n",
       "  290,\n",
       "  300,\n",
       "  318,\n",
       "  911,\n",
       "  91,\n",
       "  191,\n",
       "  331,\n",
       "  625,\n",
       "  963,\n",
       "  54,\n",
       "  720,\n",
       "  770,\n",
       "  192,\n",
       "  337,\n",
       "  472,\n",
       "  932,\n",
       "  30,\n",
       "  396,\n",
       "  99,\n",
       "  100,\n",
       "  730,\n",
       "  879,\n",
       "  704,\n",
       "  62,\n",
       "  251,\n",
       "  772,\n",
       "  234,\n",
       "  981,\n",
       "  216,\n",
       "  247,\n",
       "  108,\n",
       "  588,\n",
       "  342,\n",
       "  828,\n",
       "  114,\n",
       "  946,\n",
       "  286,\n",
       "  502,\n",
       "  547,\n",
       "  635,\n",
       "  735,\n",
       "  182,\n",
       "  272,\n",
       "  386,\n",
       "  614,\n",
       "  71,\n",
       "  110,\n",
       "  123,\n",
       "  77,\n",
       "  982,\n",
       "  163,\n",
       "  407,\n",
       "  539,\n",
       "  566,\n",
       "  892])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, hist, tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (9000 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "n02454379,n02454379,n02454379,n02454379,n02454379\n",
       "Path: /root/Derakhshani/adversarial/datasets/dataset;\n",
       "\n",
       "Valid: LabelList (50000 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "n02454379,n02454379,n02454379,n02454379,n02454379\n",
       "Path: /root/Derakhshani/adversarial/datasets/dataset;\n",
       "\n",
       "Test: None, model=Gen(\n",
       "  (z_): Linear(in_features=10, out_features=7168, bias=True)\n",
       "  (BN_): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (CT2d_1): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (BN2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_2): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(320, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_3): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(160, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_4): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(80, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_5): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_6): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_7): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (BN2d): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FeatureLoss(\n",
       "  (dis): SoftmaxWrapper(\n",
       "    (m): VGG(\n",
       "      (features): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace)\n",
       "        (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (9): ReLU(inplace)\n",
       "        (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (12): ReLU(inplace)\n",
       "        (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (16): ReLU(inplace)\n",
       "        (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (19): ReLU(inplace)\n",
       "        (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (22): ReLU(inplace)\n",
       "        (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (26): ReLU(inplace)\n",
       "        (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (29): ReLU(inplace)\n",
       "        (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (32): ReLU(inplace)\n",
       "        (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (36): ReLU(inplace)\n",
       "        (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (39): ReLU(inplace)\n",
       "        (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (42): ReLU(inplace)\n",
       "        (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "      (classifier): Sequential(\n",
       "        (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.5)\n",
       "        (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (4): ReLU(inplace)\n",
       "        (5): Dropout(p=0.5)\n",
       "        (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (softmax): Softmax()\n",
       "  )\n",
       "), metrics=[<function validation at 0x7f5a98632950>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/Derakhshani/adversarial/datasets/dataset'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), <class 'fastai.callbacks.loss_metrics.LossMetrics'>, functools.partial(<class 'nag_util.ImmediateCSVLogger'>, filename='/root/Derakhshani/adversarial/temp/vgg16_14')], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): Linear(in_features=10, out_features=7168, bias=True)\n",
       "  (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): ConvTranspose2d(320, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ConvTranspose2d(160, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (8): ConvTranspose2d(80, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (12): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ConvTranspose2d(72, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (15): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")], add_time=True, silent=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAgk-YyWc3rG"
   },
   "outputs": [],
   "source": [
    "# learn.recorder.plot_losses()\n",
    "# learn.recorder.plot_lr()\n",
    "# learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTHG4Bt7VDYp"
   },
   "outputs": [],
   "source": [
    "fooling_rates = []\n",
    "model = learn.model.eval()\n",
    "learn.metrics = [validation_single_perturbation]\n",
    "for i in range(10):\n",
    "  global_perturbations = model(torch.rand(1, 3, 224, 244).cuda())[0]\n",
    "  nag_util.global_perturbations = global_perturbations\n",
    "  fooling_rates.append(learn.validate()[1].cpu().item())\n",
    "  print(\"%d : %f\"%(i, fooling_rates[-1]))\n",
    "\n",
    "mean = np.mean(fooling_rates)\n",
    "stddev = np.std(fooling_rates)\n",
    "print(mean, stddev); print(fooling_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OFCjzI7UaY3C"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[4][0]\n",
    "x = x_img.data.cuda()\n",
    "z = torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.float32).cuda()\n",
    "# z = torch.empty(z_dim).uniform_(-1,1).cuda()\n",
    "p = model.forward_single_z(z).detach()\n",
    "x = normalize(x)\n",
    "\n",
    "p_x = x + p\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0., 1.])\n",
    "p_img = Image(p)\n",
    "# x_img.show()\n",
    "p_img.show()\n",
    "# p_x_img.show()\n",
    "\n",
    "# print_range(p)\n",
    "# print_range(x)\n",
    "# print_range(p_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzwsI2P1ZANz"
   },
   "outputs": [],
   "source": [
    "z1 = torch.tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p1 = model.forward_single_z(z1)\n",
    "\n",
    "z2 = torch.tensor([1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p2 = model.forward_single_z(z2)\n",
    "\n",
    "z3 = torch.tensor([1, 1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p3 = model.forward_single_z(z3)\n",
    "\n",
    "l2_distance(p1, p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eroI82OKSnAL"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[4][0]\n",
    "x = x_img.data[None].cuda()\n",
    "p = model(x)[0].squeeze().detach() \n",
    "x = x.squeeze()\n",
    "x = normalize(x)\n",
    "\n",
    "p_x = x + p\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0.,1.])\n",
    "p_img = Image(p)\n",
    "# x_img.show()\n",
    "p_img.show()\n",
    "# p_x_img.show()\n",
    "\n",
    "print_range(p)\n",
    "print_range(x)\n",
    "print_range(p_x)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NAG-tripletLossExperiment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
