{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ahmad-PH/nag-notebooks/blob/master/NAG_tripletLossExperiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqeZpz16do4y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up-to-date.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os; import subprocess\n",
    "\n",
    "def detect_env():\n",
    "    return 'colab' if 'content' in os.listdir('/') else 'IBM'\n",
    "  \n",
    "def run_shell_command(cmd):\n",
    "  p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "  print(str(p.communicate()[0], 'utf-8'))\n",
    "  \n",
    "if detect_env() == 'colab': root_folder = '/content'\n",
    "elif detect_env() == 'IBM' : root_folder = '/root/Derakhshani/adversarial'\n",
    "python_files_path = root_folder + '/nag-public'\n",
    "\n",
    "if os.path.isdir(python_files_path):\n",
    "  initial_dir = os.getcwd()\n",
    "  os.chdir(python_files_path)\n",
    "  run_shell_command('git pull')\n",
    "  os.chdir(initial_dir)\n",
    "else:\n",
    "  os.chdir('/root/Derakhshani/adversarial')\n",
    "  run_shell_command('git clone https://github.com/ahmad-PH/nag-public.git')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(python_files_path + '/NAG-11May-beforeDenoiser')\n",
    "\n",
    "from nag_util import *\n",
    "import nag_util\n",
    "from environment import *\n",
    "from visualization import *\n",
    "\n",
    "env = create_env()\n",
    "env.setup(cuda_index = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Ev7jcRKoARg"
   },
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.imports import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import sys; import os; import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralRelu(nn.Module):\n",
    "  def __init__(self, leak=None, sub=None, maxv=None):\n",
    "    super().__init__()\n",
    "    self.leak,self.sub,self.maxv = leak,sub,maxv\n",
    "\n",
    "  def forward(self, x): \n",
    "    x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n",
    "    if self.sub is not None: x.sub_(self.sub)\n",
    "    if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "    return x\n",
    "  \n",
    "class deconv_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k_size = (4,4), s = (2,2), pad = (1,1), b = True, activation = True):\n",
    "        super(deconv_layer, self).__init__()\n",
    "\n",
    "        self.CT2d = nn.ConvTranspose2d(in_channels = in_ch,\n",
    "                                  out_channels = out_ch,\n",
    "                                  kernel_size = k_size,\n",
    "                                  stride = s, \n",
    "                                  padding = pad,\n",
    "                                  bias = b)\n",
    "        self.BN2d = nn.BatchNorm2d(out_ch)\n",
    "        \n",
    "        self.activation = activation\n",
    "        if self.activation:\n",
    "            self.relu = GeneralRelu(0, 0.2, 5)\n",
    "        \n",
    "        self.weight_init()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.activation:\n",
    "            return self.relu(self.BN2d(self.CT2d(input)))\n",
    "        else:\n",
    "            return self.BN2d(self.CT2d(input))\n",
    "\n",
    "    def weight_init(self):\n",
    "        self.CT2d.weight.data.normal_(mean = 0, std = 0.02)\n",
    "        self.CT2d.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Gen(nn.Module):\n",
    "#   def __init__(self, z_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "#     super(Gen, self).__init__()\n",
    "\n",
    "#     self.bs = None\n",
    "#     self.z_dim = z_dim\n",
    "#     self.gf_dim = gf_dim\n",
    "#     self.y_dim = y_dim\n",
    "#     self.df_dim = df_dim\n",
    "#     self.image_shape = image_shape\n",
    "\n",
    "#     self.z_ = nn.Linear(self.z_dim, self.gf_dim * 7 * 4 * 4, bias=True)\n",
    "#     self.z_.bias.data.fill_(0)\n",
    "#     self.BN_ = nn.BatchNorm2d(self.gf_dim * 7)\n",
    "\n",
    "#     self.half = max(self.gf_dim // 2, 1) \n",
    "#     self.quarter = max(self.gf_dim // 4, 1)\n",
    "#     self.eighth = max(self.gf_dim // 8, 1)\n",
    "#     # sixteenth = max(self.gf_dim // 16, 1)\n",
    "    \n",
    "#     self.CT2d_1 = deconv_layer(self.gf_dim * 8, self.gf_dim * 4, k_size = (5,5), pad = (2,2))\n",
    "#     self.CT2d_2 = deconv_layer(self.gf_dim * 5, self.gf_dim * 2)    \n",
    "#     self.CT2d_3 = deconv_layer(self.gf_dim * 2 + self.half, self.gf_dim * 1)\n",
    "#     self.CT2d_4 = deconv_layer(self.gf_dim * 1 + self.quarter, self.gf_dim * 1)\n",
    "#     self.CT2d_5 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "#     self.CT2d_6 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "#     self.CT2d_7 = deconv_layer(self.gf_dim * 1 + self.eighth, 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "#   def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "#     h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "#     h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "#     output = deconv_layer(h_input)\n",
    "#     assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "#             \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "#             \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "#     return output\n",
    "  \n",
    "#   def forward_z(self, z):\n",
    "#     self.bs = z.shape[0]\n",
    "    \n",
    "#     h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "#     assert h0.shape[2:] == (4, 4), \"Unexpected shape, it shoud be (4,4)\"\n",
    "\n",
    "#     h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "#     h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "#     h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "#     h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "#     h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "#     h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "#     h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "#     ksi = 10.0\n",
    "#     output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "#     # this coeff scales the output to be appropriate for images that are \n",
    "#     # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "#     # interval)\n",
    "#     return output_coeff * torch.tanh(h7)\n",
    "  \n",
    "# #   # blind-selection\n",
    "#   def forward(self, inputs):\n",
    "#     self.bs = inputs.shape[0]\n",
    "\n",
    "#     benign_preds_onehot = arch(inputs)\n",
    "#     benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "    \n",
    "#     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "#     for i in range(self.bs):\n",
    "#       random_label = self.randint(0,1000, exclude = benign_preds[i].item())\n",
    "#       z[i][random_label] = 1.\n",
    "    \n",
    "#     z_out = self.forward_z(z)\n",
    "    \n",
    "#     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "\n",
    "# #   #second-best selection: made validation so much worse\n",
    "# #   def forward(self, inputs):\n",
    "# #     self.bs = inputs.shape[0]\n",
    "\n",
    "# #     benign_preds_onehot = arch(inputs)\n",
    "# #     target_preds = torch.topk(benign_preds_onehot, 2, dim = 1).indices[:, 1:]\n",
    "    \n",
    "# #     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "# #     for i in range(self.bs):\n",
    "# #       z[i][target_preds[i]] = 1.\n",
    "    \n",
    "# #     z_out = self.forward_z(z)\n",
    "    \n",
    "# #     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "  \n",
    "# #    def forward(self, inputs):\n",
    "# #     self.bs = inputs.shape[0]\n",
    "\n",
    "# #     benign_preds_onehot = arch(inputs)\n",
    "# #     benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "    \n",
    "# #     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "# #     random_label = self.randint(0,1000, exclude = benign_preds.tolist())\n",
    "# #     for i in range(self.bs):\n",
    "# #       z[i][random_label] = 1.\n",
    "    \n",
    "# #     z_out = self.forward_z(z)\n",
    "    \n",
    "# #     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "  \n",
    "#   @staticmethod\n",
    "#   def randint(low, high, exclude):\n",
    "#     temp = np.random.randint(low, high - 1)\n",
    "#     if temp == exclude:\n",
    "#       temp = temp + 1\n",
    "#     return temp\n",
    "  \n",
    "#   def forward_single_z(self, z):\n",
    "#     return self.forward_z(z[None]).squeeze()\n",
    "           \n",
    "  \n",
    "#   def make_triplet_samples(self, z, margin, r2, r3):\n",
    "#     positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "#     negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "#     return positive_sample, negative_sample\n",
    "\n",
    "#   def random_vector_surface(self, shape, r = 1.):\n",
    "#     mat = torch.randn(size=shape).cuda()\n",
    "#     norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "#     return (mat/norm) * r\n",
    "\n",
    "  \n",
    "#   def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "#     fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "#     fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "#     fraction.unsqueeze_(-1)\n",
    "#     return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "#   def make_z(self, in_shape):\n",
    "#     return torch.empty(in_shape).cuda().uniform_(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-targeted Gen\n",
    "class Gen(nn.Module):\n",
    "  def __init__(self, z_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "    super(Gen, self).__init__()\n",
    "\n",
    "    self.bs = None\n",
    "    self.z_dim = z_dim\n",
    "    self.gf_dim = gf_dim\n",
    "    self.y_dim = y_dim\n",
    "    self.df_dim = df_dim\n",
    "    self.image_shape = image_shape\n",
    "\n",
    "    self.z_ = nn.Linear(self.z_dim, self.gf_dim * 7 * 4 * 4, bias=True)\n",
    "    self.z_.bias.data.fill_(0)\n",
    "    self.BN_ = nn.BatchNorm2d(self.gf_dim * 7)\n",
    "\n",
    "    self.half = max(self.gf_dim // 2, 1) \n",
    "    self.quarter = max(self.gf_dim // 4, 1)\n",
    "    self.eighth = max(self.gf_dim // 8, 1)\n",
    "    # sixteenth = max(self.gf_dim // 16, 1)\n",
    "\n",
    "    self.CT2d_1 = deconv_layer(self.gf_dim * 8, self.gf_dim * 4, k_size = (5,5), pad = (2,2))\n",
    "    self.CT2d_2 = deconv_layer(self.gf_dim * 5, self.gf_dim * 2)    \n",
    "    self.CT2d_3 = deconv_layer(self.gf_dim * 2 + self.half, self.gf_dim * 1)\n",
    "    self.CT2d_4 = deconv_layer(self.gf_dim * 1 + self.quarter, self.gf_dim * 1)\n",
    "    self.CT2d_5 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "    self.CT2d_6 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "    self.CT2d_7 = deconv_layer(self.gf_dim * 1 + self.eighth, 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "  def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "    h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "    h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "    output = deconv_layer(h_input)\n",
    "    assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "            \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "            \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "    return output\n",
    "  \n",
    "  def forward_z(self, z):\n",
    "    self.bs = z.shape[0]\n",
    "    \n",
    "    h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "    assert h0.shape[2:] == (4, 4), \"Non-expected shape, it shoud be (4,4)\"\n",
    "\n",
    "    h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "    h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "    h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "    h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "    h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "    h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "    h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "    ksi = 10.0\n",
    "    output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "    # this coeff scales the output to be appropriate for images that are \n",
    "    # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "    # interval)\n",
    "    return output_coeff * torch.tanh(h7)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    self.bs = inputs.shape[0]\n",
    "    z = inputs.new_empty([self.bs, self.z_dim]).uniform_(-1,1).cuda()\n",
    "    p, n = self.make_triplet_samples(z, 0.1, 0.1, 2.)\n",
    "    \n",
    "    z_out = self.forward_z(z)\n",
    "    p_out = self.forward_z(p)\n",
    "    n_out = self.forward_z(n)\n",
    "    \n",
    "    return z_out, p_out, n_out, inputs, z\n",
    "#     return z_out, None, None, inputs\n",
    "  \n",
    "  def forward_single_z(self, z):\n",
    "    return self.forward_z(z[None]).squeeze()\n",
    "  \n",
    "  def generate_single_noise(self):\n",
    "    z = torch.empty(self.z_dim).uniform_(-1,1).cuda()\n",
    "    return self.forward_single_z(z)\n",
    "           \n",
    "  \n",
    "  def make_triplet_samples(self, z, margin, r2, r3):\n",
    "    positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "    negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "    return positive_sample, negative_sample\n",
    "\n",
    "  def random_vector_surface(self, shape, r = 1.):\n",
    "    mat = torch.randn(size=shape).cuda()\n",
    "    norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "    return (mat/norm) * r\n",
    "\n",
    "  \n",
    "  def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "    fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "    fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "    fraction.unsqueeze_(-1)\n",
    "    return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "  def make_z(self, in_shape):\n",
    "    return torch.empty(in_shape).cuda().uniform_(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def js_distance(x1, x2):\n",
    "  m = 0.5 * (x1 + x2)\n",
    "  return 0.5 * (F.kl_div(x1, m) + F.kl_div(x2, m))\n",
    "\n",
    "def kl_distance(x1, x2):\n",
    "  inp = torch.log(x1)\n",
    "  target = x2\n",
    "  return F.kl_div(inp, target, reduction='batchmean')\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  x1 = tensorify(x1)\n",
    "  x2 = tensorify(x2)\n",
    "  x1 = x1 / torch.sum(x1)\n",
    "  x2 = x2 / torch.sum(x2)\n",
    "  return kl_distance(x1[None], x2[None])\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  if not isinstance(x1, torch.Tensor): x1 = torch.tensor(x1)\n",
    "  if not isinstance(x2, torch.Tensor): x2 = torch.tensor(x2)\n",
    "  x1 = x1 * 100. / torch.sum(x1)\n",
    "  x2 = x2 * 100. / torch.sum(x2)\n",
    "  return torch.norm(x1 - x2, 2)\n",
    "\n",
    "def distance_from_uniform(x):\n",
    "  return distrib_distance(x, [1.] * len(x))\n",
    "\n",
    "def wasserstein_distance(x1, x2):\n",
    "  return torch.mean(x1 - x2)\n",
    "\n",
    "def l1_distance(x1, x2):\n",
    "  return F.l1_loss(x1, x2)\n",
    "\n",
    "def l2_distance(x1, x2):\n",
    "  return F.mse_loss(x1 * 10, x2 * 10)\n",
    "\n",
    "def mse_loss(x1, x2):\n",
    "  return F.mse_loss(x1, x2)\n",
    "\n",
    "def cos_distance(x1, x2, dim = 1):\n",
    "  return -1 * torch.mean(F.cosine_similarity(x1, x2, dim=dim))\n",
    "\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, distance_func, margin):\n",
    "  # max distance when using l1_distance is 2\n",
    "  # max distacne when using l2-distance is sqrt(2)\n",
    "#   print(\"anchor: \", anchor.min(), anchor.max())\n",
    "  ap_dist = distance_func(anchor, positive)\n",
    "  an_dist = distance_func(anchor, negative)\n",
    "\n",
    "  triplet_loss.call_count += 1\n",
    "  if triplet_loss.call_count % 200 == 0 : #and anchor.shape[1] == 1000:\n",
    "#     print(\"a: \", end=\"\"); print_big_vector(anchor[0])\n",
    "#     print(\"p: \", end=\"\"); print_big_vector(positive[0])\n",
    "#     print(\"n: \", end=\"\"); print_big_vector(negative[0])\n",
    "    print(\"func:{}, ap_dist: {}, an_dist: {}\".format(distance_func.__name__, ap_dist, an_dist))\n",
    "    \n",
    "  return torch.mean(F.relu(ap_dist - an_dist + margin))\n",
    "\n",
    "triplet_loss.call_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def diversity_loss(embeddings, z_s):\n",
    "#   size = z_s.shape[0]\n",
    "#   result = 0\n",
    "#   for i in range(size):\n",
    "#     for j in range(i+1, size):\n",
    "# #       a = F.cosine_similarity(embeddings[i], embeddings[j], dim = 0)\n",
    "# #       b = torch.norm(z_s[i] - z_s[j], 2, dim = 0)\n",
    "# #       print('embeddings: ')\n",
    "# #       print_big_vector(embeddings[i])\n",
    "# #       print_big_vector(embeddings[j])\n",
    "# #       print(f'a: {a}, b:{b}, multiple: {a*b}')\n",
    "#       result += F.cosine_similarity(embeddings[i], embeddings[j], dim = 0) * \\\n",
    "#                 torch.norm(z_s[i] - z_s[j], 2, dim = 0)\n",
    "#   n_pairs = (size * (size - 1)) / 2\n",
    "#   mean = result / n_pairs \n",
    "# #   print(f'result {result}, n_pairs {n_pairs}, mean {mean}')\n",
    "#   return mean\n",
    "  \n",
    "\n",
    "# normalized with shuffling\n",
    "def diversity_loss(embeddings, deranged_embeddings, z_s, deranged_z_s):\n",
    "    cos_similarity = F.cosine_similarity(embeddings, deranged_embeddings)\n",
    "    z_distance = torch.norm(z_s - deranged_z_s, 2, dim = 1)\n",
    "    return torch.mean(cos_similarity * z_distance)\n",
    "  \n",
    "# # normalized with shuffling\n",
    "# def diversity_loss(embeddings, z_s):\n",
    "#     deranged_embeddings, deranged_z_s = derange(embeddings, z_s)\n",
    "#     cos_similarity = F.cosine_similarity(embeddings, deranged_embeddings)\n",
    "#     z_distance = torch.norm(z_s - deranged_z_s, dim = 1)\n",
    "#     max_possible_z_distance = 6.3246\n",
    "#     return torch.mean(cos_similarity * (z_distance/max_possible_z_distance))\n",
    "\n",
    "\n",
    "# def diversity_loss(input, target):\n",
    "# #   return -1 * torch.mean(torch.pow(f_x_a-f_x_s,2))\n",
    "#   if input.shape[0] != batch_size:\n",
    "#     print(\"input shape: \", input.shape)\n",
    "#     print(\"target shape: \", target.shape, \"\\n\\n\")\n",
    "#   return torch.mean(F.cosine_similarity(\n",
    "#     input.view([batch_size, -1]),\n",
    "#     target.view([batch_size, -1]), \n",
    "#   ))\n",
    "\n",
    "def fool_loss(input, target):\n",
    "  true_class = torch.argmax(target, dim=1).view(-1,1).long().cuda()\n",
    "  target_probabilities = input.gather(1, true_class)\n",
    "  epsilon = 1e-10\n",
    "  result =  torch.mean(-1 * torch.log(1 - target_probabilities + epsilon))\n",
    "  \n",
    "  fool_loss.call_count += 1\n",
    "  if fool_loss.call_count % 200 == 0:\n",
    "    print(\"target probs {}, loss: {}: \".format(target_probabilities, result))\n",
    "    \n",
    "  return result\n",
    "\n",
    "fool_loss.call_count = 0\n",
    "\n",
    "# def fool_loss(model_output, target_labels):\n",
    "#   target_labels = target_labels.view(-1, 1).long().cuda()\n",
    "#   target_probabilities = model_output.gather(1, target_labels)\n",
    "#   epsilon = 1e-10\n",
    "#   # highest possible fool_loss is - log(1e-10) == 23\n",
    "#   result = torch.mean(-1 * torch.log(target_probabilities + epsilon))\n",
    "  \n",
    "#   global fool_loss_count\n",
    "#   fool_loss_count += 1\n",
    "#   if fool_loss_count % 20 == 0:\n",
    "#     print(\"target probs {}, loss: {}: \".format(target_probabilities, result))\n",
    "  \n",
    "#   return result\n",
    "\n",
    "# # targeted \n",
    "# def validation(gen_output, target):\n",
    "#   perturbations, _, _, clean_images, _, _ = gen_output\n",
    "#   return validation_(perturbations, clean_images)\n",
    "\n",
    "def validation(gen_output, target):\n",
    "  perturbations, _, _, clean_images, _ = gen_output\n",
    "  return validation_(perturbations, clean_images)\n",
    "\n",
    "unfooled_histogram = np.array([0.] * 1000)\n",
    "valid_cnt = 0\n",
    "\n",
    "def validation_(perturbations, clean_images):\n",
    "  perturbed_images = clean_images + perturbations\n",
    "  benign_preds = torch.argmax(arch(clean_images), 1)\n",
    "  adversary_preds = torch.argmax(arch(perturbed_images), 1)\n",
    "\n",
    "#   print('benign, adversary, unfooled')\n",
    "#   print(benign_preds)\n",
    "#   print(adversary_preds)\n",
    "\n",
    "#   is_unfooled = (benign_preds == adversary_preds)\n",
    "#   for i , unfooled in enumerate(is_unfooled):\n",
    "#     if unfooled == 1:\n",
    "#       unfooled_histogram[benign_preds[i]] += 1\n",
    "  \n",
    "#   global valid_cnt\n",
    "#   valid_cnt += 1\n",
    "#   if valid_cnt % 10 == 0:\n",
    "#     indexed = [(i, u) for i, u in enumerate(unfooled_histogram)]\n",
    "#     summarized = list(filter(lambda x: x[1] > 0.0, indexed))\n",
    "#     percent = [(i, 100. * u / np.sum(unfooled_histogram)) for i, u in summarized]\n",
    "#     print('\\nhist: ')\n",
    "#     print(sorted(summarized, key=lambda x: x[1], reverse = True))\n",
    "#     print('\\npercent: ')\n",
    "#     print(sorted(percent, key =lambda x: x[1], reverse = True))\n",
    "#     print('\\n')\n",
    "    \n",
    "  return (benign_preds != adversary_preds).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FeatureLoss(nn.Module):\n",
    "#     def __name__(self):\n",
    "#       return \"feature_loss\"\n",
    "  \n",
    "#     def __init__(self, dis, layers, layer_weights):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # define generator here \n",
    "#         self.dis = dis\n",
    "#         self.diversity_layers = layers\n",
    "#         self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "#         self.weights = layer_weights\n",
    "#         self.metric_names = [\"fool_loss\"] #+ [f\"div_loss_{i}\" for i in range(len(layers))] #maybe Gram\n",
    "# #         self.triplet_hooks = hook_outputs([arch.m.features[4]], detach=False)\n",
    "    \n",
    "#     def make_features(self, x, clone=False):\n",
    "#         y = self.dis(x)\n",
    "#         return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "  \n",
    "#     def forward(self, inp, target):\n",
    "#       sigma_B, _, _, X_B, B_Y, z = inp\n",
    "\n",
    "#       X_A = X_B + sigma_B\n",
    "# #       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "#       A_Y, A_feat = self.make_features(X_A)\n",
    "# #       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "#       chosen_labels = z.argmax(dim=1)\n",
    "#       fooling_loss =  fool_loss(A_Y, chosen_labels)\n",
    "\n",
    "# #       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "# #       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "# #       self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "# #       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "#       self.losses = [fooling_loss]\n",
    "#       self.metrics = dict(zip(self.metric_names, [fooling_loss]))\n",
    "\n",
    "#       return sum(self.losses)\n",
    "  \n",
    "#     def add_perturbation_shuffled(self, inp, perturbation):\n",
    "# #       j = torch.randperm(inp.shape[0])\n",
    "#         j = derangement(inp.shape[0])\n",
    "#         return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derange(*args):\n",
    "  if len(args) == 0: raise ValueError('shuffle function needs atleast one argument')\n",
    "  deranged_indexes = derangement(args[0].shape[0])\n",
    "  if not all([args[0].shape[0] == arg.shape[0] for arg in args]): \n",
    "    raise ValueError('inputs to shuffle must all have the same 0th dimension')\n",
    "  return [arg[deranged_indexes] for arg in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-targeted FeatureLoss\n",
    "class FeatureLoss(nn.Module):\n",
    "    def __name__(self):\n",
    "      return \"feature_loss\"\n",
    "  \n",
    "    def __init__(self, dis, layers, layer_weights):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dis = dis\n",
    "        self.diversity_layers = layers\n",
    "        self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "        self.weights = layer_weights\n",
    "        \n",
    "#         self.metric_names = [\"fool_loss\"] + [f\"div_loss_{i}\" for i in range(len(layers))] + ['triplet_loss']# Maybe Gram\n",
    "#         self.metric_names = [\"div_loss\"] + ['triplet_loss']# Maybe Gram\n",
    "        self.metric_names = [\"fool_loss\"] + ['div_loss']# Maybe Gram\n",
    "        self.triplet_weight = 4.\n",
    "        self.div_weight = 1.\n",
    "        self.fooling_weight = 3.\n",
    "    \n",
    "    def make_features(self, x, clone=False):\n",
    "        y = self.dis(x)\n",
    "        return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "  \n",
    "    # contrastive loss\n",
    "    def forward(self, inp, target):\n",
    "        sigma_B, sigma_pos, sigma_neg, X_B, z_B = inp\n",
    "\n",
    "        deranged_perturbations, deranged_z_s = derange(sigma_B, z_B)\n",
    "        \n",
    "        X_A = X_B + sigma_B\n",
    "        X_S = X_B + deranged_perturbations\n",
    "        X_A_pos = X_B + sigma_pos\n",
    "        X_A_neg = X_B + sigma_neg\n",
    "        \n",
    "        B_Y, _ = self.make_features(X_B)\n",
    "        A_Y, A_feat = self.make_features(X_A)\n",
    "        _, S_feat = self.make_features(X_S)\n",
    "        pos_softmax, _ = self.make_features(X_A_pos)\n",
    "        neg_softmax, _ = self.make_features(X_A_neg)\n",
    "        \n",
    "        raw_fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "        weighted_fooling_loss = self.fooling_weight * raw_fooling_loss\n",
    "      \n",
    "        raw_diversity_loss = diversity_loss(A_feat[0], S_feat[0], z_B, deranged_z_s)\n",
    "        weighted_diversity_loss = raw_diversity_loss * self.div_weight\n",
    "        \n",
    "        \n",
    "        raw_triplet_loss = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "        weighted_triplet_loss = raw_triplet_loss * self.triplet_weight\n",
    "    \n",
    "#         self.losses = weighted_diversity_losses + [weighted_triplet_loss]\n",
    "#         raw_losses = raw_diversity_losses + [raw_triplet_loss]\n",
    "        \n",
    "        self.losses = [weighted_fooling_loss] + [weighted_diversity_loss] #+ [weighted_triplet_loss]\n",
    "        raw_losses = [raw_fooling_loss] + [raw_diversity_loss] #+ [raw_triplet_loss]\n",
    "        \n",
    "#         self.losses = [fooling_loss] + [weighted_triplet_loss]\n",
    "#         self.metrics = dict(zip(self.metric_names, [fooling_loss] + [raw_triplet_loss]))\n",
    "      \n",
    "        if len(self.metric_names) != len(raw_losses):\n",
    "          raise Exception(\"length of metric names unequals length of losses\")\n",
    "        \n",
    "        self.metrics = dict(zip(self.metric_names, raw_losses))\n",
    "        return sum(self.losses)\n",
    "  \n",
    "  \n",
    "  \n",
    "# #     triplet loss\n",
    "#     def forward(self, inp, target):\n",
    "#         sigma_B, sigma_pos, sigma_neg, X_B, _ = inp\n",
    "\n",
    "#         X_A = self.add_perturbation(X_B, sigma_B) \n",
    "#         X_A_pos = self.add_perturbation(X_B, sigma_pos)\n",
    "#         X_A_neg = self.add_perturbation(X_B, sigma_neg) \n",
    "#         X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "        \n",
    "# #         B_Y, _ = self.make_features(X_B)\n",
    "#         A_Y, A_feat = self.make_features(X_A)\n",
    "# #         _, S_feat = self.make_features(X_S)\n",
    "#         pos_softmax, _ = self.make_features(X_A_pos)\n",
    "#         neg_softmax, _ = self.make_features(X_A_neg)\n",
    "        \n",
    "# #         raw_fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "# #         weighted_fooling_loss = self.fooling_weight * raw_fooling_loss\n",
    "      \n",
    "#         raw_diversity_losses = [diversity_loss(a_f, s_f, sigma_B, ) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "#         weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "#         raw_triplet_loss = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "#         weighted_triplet_loss = raw_triplet_loss * self.triplet_weight\n",
    "    \n",
    "#         self.losses = weighted_diversity_losses + [weighted_triplet_loss]\n",
    "#         raw_losses = raw_diversity_losses + [raw_triplet_loss]\n",
    "        \n",
    "# #         self.losses = [weighted_fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss]\n",
    "# #         raw_losses = [raw_fooling_loss] + raw_diversity_losses + [raw_triplet_loss]\n",
    "        \n",
    "# #         self.losses = [fooling_loss] + [weighted_triplet_loss]\n",
    "# #         self.metrics = dict(zip(self.metric_names, [fooling_loss] + [raw_triplet_loss]))\n",
    "      \n",
    "#         if len(self.metric_names) != len(raw_losses):\n",
    "#           raise Exception(\"length of metric names unequals length of losses\")\n",
    "        \n",
    "#         self.metrics = dict(zip(self.metric_names, raw_losses))\n",
    "#         return sum(self.losses)\n",
    "\n",
    "\n",
    "#     #use two types of triplet losses\n",
    "#     def forward(self, inp, target):\n",
    "#       sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "#       X_A = self.add_perturbation(X_B, sigma_B) \n",
    "#       X_A_pos = self.add_perturbation(X_B, sigma_pos)\n",
    "#       X_A_neg = self.add_perturbation(X_B, sigma_neg) \n",
    "\n",
    "#       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "#       B_Y, _ = self.make_features(X_B)\n",
    "#       A_Y, A_feat = self.make_features(X_A)\n",
    "#       _, S_feat = self.make_features(X_S)\n",
    "#       pos_softmax, _ = self.make_features(X_A_pos)\n",
    "#       neg_softmax, _ = self.make_features(X_A_neg)\n",
    "\n",
    "#       fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "#       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "#       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "      \n",
    "#       raw_triplet_loss_sm = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "#       weighted_triplet_loss_sm = raw_triplet_loss_sm * self.triplet_weight_sm\n",
    "      \n",
    "#       raw_triplet_loss_noise = triplet_loss(sigma_B, sigma_pos, sigma_neg, l2_distance, 5.)\n",
    "#       weighted_triplet_loss_noise = raw_triplet_loss_noise * self.triplet_weight_noise\n",
    "\n",
    "#       self.losses = [fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss_sm, weighted_triplet_loss_noise] \n",
    "#       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses + [weighted_triplet_loss_sm, weighted_triplet_loss_noise]))\n",
    "\n",
    "#       return sum(self.losses)\n",
    "\n",
    "#     # just fooling and diversity\n",
    "#     def forward(self, inp, target):\n",
    "#       sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "#       X_A = self.add_perturbation(X_B, sigma_B) \n",
    "\n",
    "#       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "#       B_Y, _ = self.make_features(X_B)\n",
    "#       A_Y, A_feat = self.make_features(X_A)\n",
    "#       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "#       fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "#       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "#       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "#       self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "#       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "#       return sum(self.losses)\n",
    "  \n",
    " \n",
    "    def add_perturbation_shuffled(self, inp, perturbation):\n",
    "      j = derangement(inp.shape[0])\n",
    "      return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def produce_summary(root_folder, n_files):\n",
    "  def writeline(file, values, fmt_string):\n",
    "    file.write(', '.join(fmt_string.format(v) for v in values) + '\\n')\n",
    "  \n",
    "  last_rows = []\n",
    "  for i in range(n_files):\n",
    "    prefix = '/root/Derakhshani/adversarial/textual_notes/CSVs'\n",
    "    df = pd.read_csv(\"{}/{}/{}.csv\".format(prefix, root_folder, i))\n",
    "    last_rows.append(df.iloc[-1][1:-1].values.tolist())\n",
    "  \n",
    "  last_rows = np.array(last_rows)\n",
    "  \n",
    "  labels = list(df.columns[1:-1])\n",
    "  means = np.mean(last_rows, axis=0).tolist()\n",
    "  outfile = open('{}/{}/summary.txt'.format(prefix, root_folder), 'w+')\n",
    "  outfile.write('means: \\n')\n",
    "  writeline(outfile, labels, '{: >11}')\n",
    "  writeline(outfile, means, '{: >11.3}')\n",
    "  outfile.write('\\n')\n",
    "      \n",
    "  operations = []\n",
    "  for column in df.columns[1:-1]:\n",
    "    if column in ['train_loss', 'valid_loss', 'fool_loss', 'triplet_loss'] or column[:8] == 'div_loss':\n",
    "      operations.append('min')\n",
    "    elif column in ['validation', 'div_metric']:\n",
    "      operations.append('max')\n",
    "    else:\n",
    "      raise ValueError('column {} is not recognized'.format(column))\n",
    "    \n",
    "  results = []\n",
    "  indexes = []\n",
    "  \n",
    "  for i in range(len(operations)):\n",
    "    values = last_rows[:, i]\n",
    "    if operations[i] == 'max': operation = np.max\n",
    "    elif operations[i] == 'min': operation = np.min\n",
    "    result = operation(values)\n",
    "    results.append(result)\n",
    "    indexes.append(values.tolist().index(result))\n",
    "  \n",
    "  outfile.write('bests: \\n')\n",
    "  writeline(outfile, labels, '{: >11}')\n",
    "  writeline(outfile, operations, '{: >11}')\n",
    "  writeline(outfile, results, '{: >11.3}')\n",
    "  writeline(outfile, indexes, '{: >11}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils import dir_util \n",
    "\n",
    "def investigate_initial_settings(n_settings, n_epochs, lr, wd, results_dir):\n",
    "  os.mkdir(env.get_csv_dir() + results_dir)\n",
    "  os.mkdir(env.get_models_dir() + results_dir)\n",
    "  \n",
    "  for setting_ind in range(n_settings):\n",
    "    print(f\"investigation no: {setting_ind}\")\n",
    "    learn = None; gen = None; gc.collect()\n",
    "    gen = Gen(z_dim = z_dim)\n",
    "    init_cnn(gen, True)\n",
    "    div_metric = DiversityMetric(10, 95)\n",
    "    \n",
    "    tmp_csv_filename =  env.temp_csv_path + '/' + results_dir + '/' + str(setting_ind)\n",
    "    csv_logger = partial(ImmediateCSVLogger, filename=tmp_csv_filename)\n",
    "    \n",
    "    learn = Learner(data, gen, loss_func = feat_loss, metrics=[validation, div_metric], \n",
    "                    model_dir = env.get_learner_models_dir(), callback_fns=[LossMetrics, csv_logger])\n",
    "    div_metric.set_learner(learn)\n",
    "    \n",
    "    saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=model.__name__ + \"-best\")\n",
    "    saver_every_epoch = SaveModelCallback(learn, every='epoch', name=model.__name__)\n",
    "\n",
    "    learn.fit(n_epochs, lr=lr, wd = wd, callbacks=[saver_best, saver_every_epoch])\n",
    "    \n",
    "    shutil.copyfile(tmp_csv_filename + \".csv\", env.get_csv_dir() + results_dir + '/' + str(setting_ind) + '.csv')\n",
    "    \n",
    "    model_dest = env.get_models_dir() + results_dir + '/' + str(setting_ind)\n",
    "    os.mkdir(model_dest)\n",
    "    dir_util.copy_tree(env.data_path/env.get_learner_models_dir(), model_dest)\n",
    "    shutil.rmtree(env.data_path/env.get_learner_models_dir())  \n",
    "    \n",
    "  produce_summary(results_dir, n_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_perturbations(learn, n_perturbations):\n",
    "  initial_training_mode = learn.model.training\n",
    "  gen = learn.model.eval()\n",
    "  perturbations = [gen.generate_single_noise() for _ in range(n_perturbations)]\n",
    "  learn.model.train(initial_training_mode)  \n",
    "  return perturbations\n",
    "\n",
    "def compute_prediction_histogram(learn, perturbation, verbose=False):\n",
    "  pred_hist = [0] * 1000\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 and verbose: print (\"at batch no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbation[None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      pred_hist[pred] += 1\n",
    "  return pred_hist\n",
    "\n",
    "\n",
    "def compute_mean_prediction_histogram(learn, perturbations):\n",
    "  pred_hist = torch.tensor([0] * 1000).detach_()\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 5 == 0 : print(f\"at batch no {batch_no}\")\n",
    "    for j, perturbation in enumerate(perturbations):\n",
    "      perturbed_batch = batch + perturbation[None]\n",
    "      preds = arch(perturbed_batch).argmax(1)\n",
    "      for pred in preds:\n",
    "        pred_hist[pred] += 1\n",
    "  pred_hist = pred_hist.float() / len(perturbations)\n",
    "  return pred_hist.tolist()\n",
    "\n",
    "\n",
    "def classes_needed_to_reach(percentage, hist):\n",
    "  hist_sum = np.sum(hist)\n",
    "  indexed_hist = [(i, hist_element) for i,hist_element in  \n",
    "                          enumerate(hist)]\n",
    "  sorted_hist = sorted(indexed_hist, key=lambda x: x[1], reverse = True)\n",
    "  \n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = sorted_hist[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / hist_sum) * 100.\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, sorted_hist\n",
    "\n",
    "def diversity(learn, n_perturbations, percentage = 95, p = None):\n",
    "  pred_histogram = compute_mean_prediction_histogram(\n",
    "      learn, generate_perturbations(learn, n_perturbations)\n",
    "  )\n",
    "  print(\"finished creating the prediction histogram\")\n",
    "\n",
    "  return classes_needed_to_reach(95, pred_histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiversityMetric(Callback):\n",
    "  def __init__(self, n_perturbations, percentage):\n",
    "    super().__init__()\n",
    "    self.name = \"div_metric\"\n",
    "    self.average_over = 4\n",
    "    self.n_perturbations = n_perturbations\n",
    "    self.percentage = percentage\n",
    "    self.learn = None\n",
    "  \n",
    "  def set_learner(self, learn):\n",
    "    self.learn = learn\n",
    "    \n",
    "  def on_epoch_begin(self, **kwargs):\n",
    "    global learn\n",
    "    self.perturbations_list = [generate_perturbations(self.learn, self.n_perturbations) \\\n",
    "                          for _ in range(self.average_over)]\n",
    "    self.pred_hist_list = [torch.tensor([0] * 1000).detach_() for _ in range(self.average_over)]\n",
    "    \n",
    "  def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "    images = last_output[3]; assert(images.shape[1:] == (3,224, 224))\n",
    "    for perturbations, pred_hist in zip(self.perturbations_list, self.pred_hist_list):\n",
    "      for j, perturbation in enumerate(perturbations):\n",
    "        perturbed_batch = images + perturbation[None]\n",
    "        preds = arch(perturbed_batch).argmax(1)\n",
    "        for pred in preds:\n",
    "          pred_hist[pred] += 1\n",
    "  \n",
    "  def on_epoch_end(self, last_metrics, **kwargs):\n",
    "    for i in range(len(self.pred_hist_list)):\n",
    "      self.pred_hist_list[i] = (self.pred_hist_list[i].float() / self.n_perturbations).tolist()\n",
    "    \n",
    "    div_metric_list = [classes_needed_to_reach(self.percentage, pred_hist)[0] \\\n",
    "                          for pred_hist in self.pred_hist_list]\n",
    "    div_metric = np.mean(div_metric_list)\n",
    "    print('list of metrics: ', div_metric_list)\n",
    "    return add_metrics(last_metrics, div_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tltucTv2ep9-"
   },
   "outputs": [],
   "source": [
    "# mode = \"sanity_check\"\n",
    "# mode = \"normal\"\n",
    "mode = \"div_metric_calc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50\n",
    "# model = models.resnet152\n",
    "# model = models.vgg16_bn\n",
    "# model = torchvision.models.googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SO1h55obXzOv",
    "outputId": "54414cc5-84d5-4f45-ecab-87374a58dd33"
   },
   "outputs": [],
   "source": [
    "if mode == \"normal\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "elif mode == \"sanity_check\":\n",
    "  env.load_dataset('dataset_sanity_check_small', 'dataset_sanity_check_small')  \n",
    "  env.set_data_path('dataset_sanity_check_small')\n",
    "elif mode == \"div_metric_calc\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "  env.load_test_dataset(str(env.data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "koaQZmjMom7w"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "gpu_flag = True\n",
    "nag_util.batch_size = batch_size; nag_util.gpu_flag = gpu_flag;\n",
    "tfms = get_transforms(do_flip=False, max_rotate=0)\n",
    "data = (ImageList.from_folder(env.data_path)\n",
    "        .split_by_folder(valid=('test' if mode == 'div_metric_calc' else 'valid'))\n",
    "        .label_from_folder()\n",
    "        .transform(tfms, size=224)\n",
    "        .databunch(bs=batch_size, num_workers=1)\n",
    "        .normalize(imagenet_stats))\n",
    "\n",
    "# data.show_batch(rows=2, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDBkRV8yovwV"
   },
   "outputs": [],
   "source": [
    "z_dim = 10\n",
    "arch = SoftmaxWrapper(model(pretrained=True).cuda().eval())\n",
    "nag_util.arch = arch\n",
    "requires_grad(arch, False)\n",
    "\n",
    "# vgg:\n",
    "# layers = []\n",
    "# blocks = [i-1 for i,o in enumerate(children(arch.features)) if isinstance(o, nn.MaxPool2d)]\n",
    "# layers = [arch.features[i] for i in blocks]\n",
    "# layer_weights = [1] * len(layers)\n",
    "\n",
    "layers = [\n",
    "    arch.softmax\n",
    "]\n",
    "\n",
    "layer_weights = [1.] * len(layers)\n",
    "\n",
    "# inception:\n",
    "# layers = [\n",
    "#     arch.Conv2d_1a_3x3,\n",
    "#     arch.Mixed_6e,\n",
    "#     arch.Mixed_7a,\n",
    "#     arch.fc    \n",
    "# ]\n",
    "# layer_weights = [1.0/4.0] * len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qd9gXUy_ovww"
   },
   "outputs": [],
   "source": [
    "feat_loss = FeatureLoss(arch, layers, layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfZKdYD2MSdi"
   },
   "outputs": [],
   "source": [
    "env.save_filename = 'resnet50_x3' #resnet50_64\n",
    "# env.save_filename = 'resnet50_17'\n",
    "# env.save_filename = 'vgg16_32'\n",
    "\n",
    "if Path(env.get_csv_path() + '.csv').exists(): raise FileExistsError(\"csv_path already exists\")\n",
    "if Path(env.get_models_path()).exists(): raise FileExistsError(\"models_path already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9J20CBLS8S9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models_directory returned is:  models/248\n"
     ]
    }
   ],
   "source": [
    "learn = None; gen = None; gc.collect()\n",
    "csv_logger = partial(ImmediateCSVLogger, filename= env.temp_csv_path + '/' + env.save_filename)\n",
    "gen = Gen(z_dim=z_dim)\n",
    "init_cnn(gen, True)\n",
    "div_metric = DiversityMetric(10, 95)\n",
    "\n",
    "learn = Learner(data, gen, loss_func = feat_loss, model_dir = env.get_learner_models_dir(),\n",
    "                metrics=[validation, div_metric], callback_fns=[LossMetrics, csv_logger])\n",
    "\n",
    "div_metric.set_learner(learn)\n",
    "\n",
    "# learn = Learner(data, Gen(z_dim=10), loss_func = feat_loss, metrics=[validation], callback_fns=LossMetrics, opt_func = optim.SGD)\n",
    "# learn = Learner(data, Gen(z_dim=z_dim), loss_func = feat_loss, metrics=[validation], callback_fns=[LossMetrics, DiversityWeightsScheduler])\n",
    "\n",
    "# load_starting_point(learn, model.__name__, z_dim)\n",
    "# random_seed(42, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0wOZYzOHDEdB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GeneralRelu. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type FeatureLoss. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Gen. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type deconv_layer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (9000 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "n02454379,n02454379,n02454379,n02454379,n02454379\n",
       "Path: /root/Derakhshani/adversarial/datasets/dataset;\n",
       "\n",
       "Valid: LabelList (50000 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "n02454379,n02454379,n02454379,n02454379,n02454379\n",
       "Path: /root/Derakhshani/adversarial/datasets/dataset;\n",
       "\n",
       "Test: None, model=Gen(\n",
       "  (z_): Linear(in_features=10, out_features=7168, bias=True)\n",
       "  (BN_): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (CT2d_1): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (BN2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_2): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(320, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_3): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(160, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_4): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(80, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_5): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_6): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_7): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (BN2d): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FeatureLoss(\n",
       "  (dis): SoftmaxWrapper(\n",
       "    (m): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax()\n",
       "  )\n",
       "), metrics=[<function validation at 0x7f1b5808a158>, DiversityMetric\n",
       "n_perturbations: 10\n",
       "percentage: 95], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/Derakhshani/adversarial/datasets/dataset'), model_dir='models/248', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), <class 'fastai.callbacks.loss_metrics.LossMetrics'>, functools.partial(<class 'nag_util.ImmediateCSVLogger'>, filename='/root/Derakhshani/adversarial/temp/resnet50_x3')], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): Linear(in_features=10, out_features=7168, bias=True)\n",
       "  (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): GeneralRelu()\n",
       "  (5): ConvTranspose2d(320, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): GeneralRelu()\n",
       "  (8): ConvTranspose2d(160, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): GeneralRelu()\n",
       "  (11): ConvTranspose2d(80, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): GeneralRelu()\n",
       "  (14): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (16): GeneralRelu()\n",
       "  (17): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): GeneralRelu()\n",
       "  (20): ConvTranspose2d(72, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (21): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")], add_time=True, silent=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !cp \"/content/gdrive/My Drive/DL/models/vgg16_12-last.pth\"  \"/content/\"\n",
    "# learn.load('/content/vgg16_12-last')\n",
    "\n",
    "# load_filename = 'resnet50-11_39'\n",
    "# load_filename = 'resnet50_startpoint_0'\n",
    "load_filename = 'resnet50_64/resnet50_64_28'\n",
    "# load_filename = 'investigate_resnet50_7/7/resnet50_3'\n",
    "# load_filename = 'vgg16_30/vgg16_30_69'\n",
    "# load_filename = 'vgg16_12-last'\n",
    "\n",
    "learn.load('/root/Derakhshani/adversarial/models/' + load_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the selected settings are : \n",
      "\tmode: div_metric_calc \n",
      "\tnetw-under-attack: resnet50 \n",
      "\tload filename: resnet50_64/resnet50_64_28 \n",
      "      \tsave filename: resnet50_x3\n",
      "\tmetric names: ['fool_loss', 'div_loss']\n",
      "\n",
      "please MAKE SURE that the config is correct.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  load_filename\n",
    "except NameError:\n",
    "  load_filename = None\n",
    "\n",
    "print(\"the selected settings are : \")\n",
    "print('''\\tmode: {} \\n\\tnetw-under-attack: {} \\n\\tload filename: {} \n",
    "      \\tsave filename: {}\\n\\tmetric names: {}\\n'''.format(\n",
    "      mode, model.__name__, load_filename , env.save_filename, feat_loss.metric_names\n",
    "))\n",
    "print(\"please MAKE SURE that the config is correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find(1e-6, 100)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dir = 'investigate_vgg16_0'\n",
    "# investigate_initial_settings(7, 3, lr = 1e-2, wd = 0.0, results_dir = results_dir)\n",
    "# shutil.rmtree(env.get_models_dir() + results_dir)\n",
    "# shutil.rmtree(env.get_csv_dir() + results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WeightTuner(LearnerCallback):\n",
    "#   def __init__(self, learn: Learner):\n",
    "#     super().__init__(learn)\n",
    "#     self.fooling_weight = learn.loss_func.fooling_weight\n",
    "    \n",
    "#   def on_epoch_begin(self, **kwargs):\n",
    "#     fooling_rate = \n",
    "#     print(\"by how much to increase the fooling_weight? (current value: {})\".format(fooling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoolingWeightScheduler(LearnerCallback):\n",
    "  def __init__(self, learn: Learner, fooling_loss_index):\n",
    "    super().__init__(learn)\n",
    "    self.fooling_loss_index = fooling_loss_index\n",
    "    self.weights_history = []\n",
    "    self.fooling_loss_history = []\n",
    "  \n",
    "  def on_epoch_end(self, last_metrics, **kwargs):\n",
    "    # history keeping\n",
    "    self.weights_history.append((kwargs['epoch'], self.learn.loss_func.fooling_weight))\n",
    "    \n",
    "    # the actualy functionality\n",
    "    fooling_loss = last_metrics[self.fooling_loss_index]\n",
    "    self.fooling_loss_history.append(fooling_loss)\n",
    "    \n",
    "    if len(self.weights_history) < 2:\n",
    "      return\n",
    "    \n",
    "    if self.fooling_loss_history[-1] > self.fooling_loss_history[-2]:\n",
    "      self.learn.loss_func.fooling_weight += 0.3    \n",
    "      print('fooling weight increased to {} at the end of epoch {}'.format(\n",
    "        self.learn.loss_func.fooling_weight, kwargs['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='41' class='' max='80', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      51.25% [41/80 3:51:12<3:39:55]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>div_metric</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>div_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.804379</td>\n",
       "      <td>6.071239</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>807.250000</td>\n",
       "      <td>1.180983</td>\n",
       "      <td>2.528290</td>\n",
       "      <td>05:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.737497</td>\n",
       "      <td>5.651329</td>\n",
       "      <td>0.627000</td>\n",
       "      <td>467.000000</td>\n",
       "      <td>1.042578</td>\n",
       "      <td>2.523594</td>\n",
       "      <td>05:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.343431</td>\n",
       "      <td>5.581724</td>\n",
       "      <td>0.638000</td>\n",
       "      <td>397.750000</td>\n",
       "      <td>1.010541</td>\n",
       "      <td>2.550102</td>\n",
       "      <td>05:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.429296</td>\n",
       "      <td>5.528463</td>\n",
       "      <td>0.652000</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>1.004411</td>\n",
       "      <td>2.515229</td>\n",
       "      <td>05:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.179765</td>\n",
       "      <td>5.438075</td>\n",
       "      <td>0.657000</td>\n",
       "      <td>368.500000</td>\n",
       "      <td>0.974202</td>\n",
       "      <td>2.515469</td>\n",
       "      <td>05:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.234446</td>\n",
       "      <td>5.477950</td>\n",
       "      <td>0.662000</td>\n",
       "      <td>366.750000</td>\n",
       "      <td>0.964718</td>\n",
       "      <td>2.583795</td>\n",
       "      <td>05:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.065498</td>\n",
       "      <td>5.366141</td>\n",
       "      <td>0.674000</td>\n",
       "      <td>367.750000</td>\n",
       "      <td>0.937212</td>\n",
       "      <td>2.554503</td>\n",
       "      <td>05:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.993763</td>\n",
       "      <td>5.121913</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>0.868813</td>\n",
       "      <td>2.515473</td>\n",
       "      <td>05:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.517351</td>\n",
       "      <td>4.727676</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>333.500000</td>\n",
       "      <td>0.728277</td>\n",
       "      <td>2.542845</td>\n",
       "      <td>05:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.448887</td>\n",
       "      <td>4.690315</td>\n",
       "      <td>0.722000</td>\n",
       "      <td>320.750000</td>\n",
       "      <td>0.705230</td>\n",
       "      <td>2.574625</td>\n",
       "      <td>05:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.546614</td>\n",
       "      <td>4.621417</td>\n",
       "      <td>0.736000</td>\n",
       "      <td>315.750000</td>\n",
       "      <td>0.699805</td>\n",
       "      <td>2.522000</td>\n",
       "      <td>05:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.494963</td>\n",
       "      <td>4.607039</td>\n",
       "      <td>0.735000</td>\n",
       "      <td>306.750000</td>\n",
       "      <td>0.684424</td>\n",
       "      <td>2.553768</td>\n",
       "      <td>05:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.260228</td>\n",
       "      <td>4.558016</td>\n",
       "      <td>0.731000</td>\n",
       "      <td>308.500000</td>\n",
       "      <td>0.679472</td>\n",
       "      <td>2.519603</td>\n",
       "      <td>05:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.137546</td>\n",
       "      <td>4.225687</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>305.250000</td>\n",
       "      <td>0.570111</td>\n",
       "      <td>2.515353</td>\n",
       "      <td>05:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4.038159</td>\n",
       "      <td>4.037896</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>279.500000</td>\n",
       "      <td>0.495712</td>\n",
       "      <td>2.550759</td>\n",
       "      <td>05:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.699043</td>\n",
       "      <td>3.859499</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>254.750000</td>\n",
       "      <td>0.462884</td>\n",
       "      <td>2.470849</td>\n",
       "      <td>05:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.231748</td>\n",
       "      <td>3.471029</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>235.250000</td>\n",
       "      <td>0.465390</td>\n",
       "      <td>2.074860</td>\n",
       "      <td>05:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.002136</td>\n",
       "      <td>3.156529</td>\n",
       "      <td>0.807000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>0.441439</td>\n",
       "      <td>1.699780</td>\n",
       "      <td>05:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.683121</td>\n",
       "      <td>2.912395</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>240.250000</td>\n",
       "      <td>0.411413</td>\n",
       "      <td>1.554733</td>\n",
       "      <td>05:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.559156</td>\n",
       "      <td>2.750621</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>227.500000</td>\n",
       "      <td>0.360468</td>\n",
       "      <td>1.561078</td>\n",
       "      <td>05:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.504597</td>\n",
       "      <td>2.785078</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>1.532299</td>\n",
       "      <td>05:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.552112</td>\n",
       "      <td>2.852176</td>\n",
       "      <td>0.848000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>0.378175</td>\n",
       "      <td>1.490747</td>\n",
       "      <td>05:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.468014</td>\n",
       "      <td>2.690710</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>191.500000</td>\n",
       "      <td>0.347324</td>\n",
       "      <td>1.440343</td>\n",
       "      <td>05:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.306194</td>\n",
       "      <td>2.672696</td>\n",
       "      <td>0.854000</td>\n",
       "      <td>184.250000</td>\n",
       "      <td>0.335647</td>\n",
       "      <td>1.464366</td>\n",
       "      <td>05:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.286475</td>\n",
       "      <td>2.687349</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>0.333924</td>\n",
       "      <td>1.485221</td>\n",
       "      <td>05:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.631288</td>\n",
       "      <td>2.708202</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>171.750000</td>\n",
       "      <td>0.349766</td>\n",
       "      <td>1.449047</td>\n",
       "      <td>05:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.461373</td>\n",
       "      <td>2.681382</td>\n",
       "      <td>0.861000</td>\n",
       "      <td>194.750000</td>\n",
       "      <td>0.325326</td>\n",
       "      <td>1.412610</td>\n",
       "      <td>05:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.495262</td>\n",
       "      <td>2.711009</td>\n",
       "      <td>0.862000</td>\n",
       "      <td>169.250000</td>\n",
       "      <td>0.337201</td>\n",
       "      <td>1.395927</td>\n",
       "      <td>05:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.389108</td>\n",
       "      <td>2.715378</td>\n",
       "      <td>0.874000</td>\n",
       "      <td>175.250000</td>\n",
       "      <td>0.330445</td>\n",
       "      <td>1.327508</td>\n",
       "      <td>05:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.522681</td>\n",
       "      <td>2.760419</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>161.250000</td>\n",
       "      <td>0.327194</td>\n",
       "      <td>1.386206</td>\n",
       "      <td>05:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.457072</td>\n",
       "      <td>2.630656</td>\n",
       "      <td>0.874000</td>\n",
       "      <td>163.250000</td>\n",
       "      <td>0.305562</td>\n",
       "      <td>1.347296</td>\n",
       "      <td>05:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.330910</td>\n",
       "      <td>2.513175</td>\n",
       "      <td>0.882000</td>\n",
       "      <td>153.500000</td>\n",
       "      <td>0.282947</td>\n",
       "      <td>1.324798</td>\n",
       "      <td>05:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.384599</td>\n",
       "      <td>2.690951</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>147.750000</td>\n",
       "      <td>0.316411</td>\n",
       "      <td>1.362024</td>\n",
       "      <td>05:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.311977</td>\n",
       "      <td>2.640660</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>155.500000</td>\n",
       "      <td>0.311963</td>\n",
       "      <td>1.236825</td>\n",
       "      <td>05:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.261256</td>\n",
       "      <td>2.711717</td>\n",
       "      <td>0.878000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>0.290388</td>\n",
       "      <td>1.404972</td>\n",
       "      <td>05:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.393911</td>\n",
       "      <td>2.624569</td>\n",
       "      <td>0.886000</td>\n",
       "      <td>140.750000</td>\n",
       "      <td>0.271036</td>\n",
       "      <td>1.404906</td>\n",
       "      <td>05:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.330518</td>\n",
       "      <td>2.486551</td>\n",
       "      <td>0.882000</td>\n",
       "      <td>129.500000</td>\n",
       "      <td>0.276216</td>\n",
       "      <td>1.243581</td>\n",
       "      <td>05:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.260116</td>\n",
       "      <td>2.589136</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>134.750000</td>\n",
       "      <td>0.268511</td>\n",
       "      <td>1.300281</td>\n",
       "      <td>05:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.391211</td>\n",
       "      <td>2.500390</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>136.500000</td>\n",
       "      <td>0.254216</td>\n",
       "      <td>1.280154</td>\n",
       "      <td>05:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.350343</td>\n",
       "      <td>2.587438</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>136.500000</td>\n",
       "      <td>0.256190</td>\n",
       "      <td>1.357726</td>\n",
       "      <td>05:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.441919</td>\n",
       "      <td>2.712134</td>\n",
       "      <td>0.882000</td>\n",
       "      <td>136.250000</td>\n",
       "      <td>0.282537</td>\n",
       "      <td>1.271195</td>\n",
       "      <td>05:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='562', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[1.2717e-01],\n",
      "        [5.2973e-01],\n",
      "        [6.1017e-01],\n",
      "        [6.6038e-01],\n",
      "        [1.2149e-07],\n",
      "        [9.5841e-01],\n",
      "        [1.8749e-02],\n",
      "        [7.5266e-01],\n",
      "        [9.9884e-01],\n",
      "        [6.1464e-02],\n",
      "        [1.2376e-02],\n",
      "        [1.2696e-01],\n",
      "        [1.6993e-02],\n",
      "        [4.9640e-01],\n",
      "        [7.9482e-01],\n",
      "        [5.3833e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.0964627265930176: \n",
      "func:cos_distance, ap_dist: -0.9999642372131348, an_dist: -0.999814510345459\n",
      "target probs tensor([[5.6839e-01],\n",
      "        [9.2052e-01],\n",
      "        [5.1365e-05],\n",
      "        [1.8817e-04],\n",
      "        [3.5497e-01],\n",
      "        [1.1514e-02],\n",
      "        [2.1562e-01],\n",
      "        [9.6307e-01],\n",
      "        [1.6447e-01],\n",
      "        [8.8013e-06],\n",
      "        [1.0764e-03],\n",
      "        [2.6613e-03],\n",
      "        [3.5083e-02],\n",
      "        [7.5325e-01],\n",
      "        [8.5129e-01],\n",
      "        [7.0401e-03]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.6809918880462646: \n",
      "func:cos_distance, ap_dist: -0.9997128248214722, an_dist: -0.9997859597206116\n",
      "target probs tensor([[1.0552e-05],\n",
      "        [6.8788e-03],\n",
      "        [8.3930e-01],\n",
      "        [8.1333e-01],\n",
      "        [6.4257e-02],\n",
      "        [9.9848e-01],\n",
      "        [1.7871e-06],\n",
      "        [4.0593e-02],\n",
      "        [2.7113e-05],\n",
      "        [4.1386e-01],\n",
      "        [1.2504e-02],\n",
      "        [2.6542e-02],\n",
      "        [9.2022e-01],\n",
      "        [9.6161e-02],\n",
      "        [2.2370e-02],\n",
      "        [9.9462e-01]], device='cuda:1'), loss: 1.159835696220398: \n",
      "func:cos_distance, ap_dist: -0.9998024106025696, an_dist: -0.9998186826705933\n",
      "list of metrics:  [807, 807, 809, 806]\n",
      "Better model found at epoch 0 with validation value: 0.5600000023841858.\n",
      "target probs tensor([[9.8176e-01],\n",
      "        [3.8073e-01],\n",
      "        [1.6780e-01],\n",
      "        [1.3104e-05],\n",
      "        [9.8810e-01],\n",
      "        [9.7625e-01],\n",
      "        [8.2655e-04],\n",
      "        [2.1499e-01],\n",
      "        [8.8785e-01],\n",
      "        [3.5824e-03],\n",
      "        [9.9224e-01],\n",
      "        [5.4300e-05],\n",
      "        [1.8998e-04],\n",
      "        [1.0000e+00],\n",
      "        [2.4494e-01],\n",
      "        [6.1019e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 2.7738375663757324: \n",
      "func:cos_distance, ap_dist: -0.9992267489433289, an_dist: -0.9998058080673218\n",
      "target probs tensor([[2.4654e-03],\n",
      "        [2.0765e-01],\n",
      "        [1.1393e-03],\n",
      "        [1.0137e-02],\n",
      "        [4.2900e-01],\n",
      "        [4.3556e-05],\n",
      "        [2.5150e-01],\n",
      "        [5.4358e-02],\n",
      "        [9.2424e-01],\n",
      "        [1.2754e-03],\n",
      "        [4.6302e-05],\n",
      "        [4.6436e-01],\n",
      "        [5.7426e-01],\n",
      "        [1.1666e-02],\n",
      "        [1.2288e-05],\n",
      "        [2.9130e-02]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.3283487558364868: \n",
      "func:cos_distance, ap_dist: -0.9993479251861572, an_dist: -0.9996684789657593\n",
      "target probs tensor([[1.5437e-03],\n",
      "        [6.5778e-01],\n",
      "        [1.9332e-01],\n",
      "        [8.7787e-03],\n",
      "        [1.6593e-02],\n",
      "        [4.8884e-04],\n",
      "        [9.8394e-01],\n",
      "        [5.8196e-03],\n",
      "        [3.7782e-01],\n",
      "        [2.7726e-01],\n",
      "        [3.8013e-02],\n",
      "        [3.1187e-02],\n",
      "        [7.3355e-01],\n",
      "        [1.5016e-04],\n",
      "        [9.9121e-01],\n",
      "        [7.9398e-01]], device='cuda:1'), loss: 0.8723659515380859: \n",
      "func:cos_distance, ap_dist: -0.999721884727478, an_dist: -0.9995326995849609\n",
      "list of metrics:  [467, 467, 467, 467]\n",
      "Better model found at epoch 1 with validation value: 0.6269999742507935.\n",
      "target probs tensor([[0.3070],\n",
      "        [0.3431],\n",
      "        [0.0168],\n",
      "        [0.5198],\n",
      "        [0.2887],\n",
      "        [0.4791],\n",
      "        [0.3147],\n",
      "        [0.2467],\n",
      "        [0.0023],\n",
      "        [0.5902],\n",
      "        [0.0058],\n",
      "        [0.8606],\n",
      "        [0.9986],\n",
      "        [0.8913],\n",
      "        [0.0046],\n",
      "        [0.6598]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.9958311319351196: \n",
      "func:cos_distance, ap_dist: -0.999722957611084, an_dist: -0.9995659589767456\n",
      "target probs tensor([[8.5351e-05],\n",
      "        [1.0189e-02],\n",
      "        [3.9090e-03],\n",
      "        [9.9635e-01],\n",
      "        [8.1080e-06],\n",
      "        [6.4065e-02],\n",
      "        [3.3948e-04],\n",
      "        [1.1981e-03],\n",
      "        [1.9579e-03],\n",
      "        [1.3196e-02],\n",
      "        [7.4893e-02],\n",
      "        [3.6537e-01],\n",
      "        [3.5621e-01],\n",
      "        [3.3094e-03],\n",
      "        [6.1595e-01],\n",
      "        [3.6382e-02]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.48001188039779663: \n",
      "func:cos_distance, ap_dist: -0.9998395442962646, an_dist: -0.9996393918991089\n",
      "target probs tensor([[4.3528e-03],\n",
      "        [7.2030e-01],\n",
      "        [9.9898e-01],\n",
      "        [2.5366e-02],\n",
      "        [6.2721e-01],\n",
      "        [1.2779e-01],\n",
      "        [4.6741e-02],\n",
      "        [4.8082e-03],\n",
      "        [4.4906e-05],\n",
      "        [5.6939e-02],\n",
      "        [1.2120e-01],\n",
      "        [7.9824e-03],\n",
      "        [5.7974e-02],\n",
      "        [1.3756e-01],\n",
      "        [7.1842e-02],\n",
      "        [2.5132e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.6335453987121582: \n",
      "func:cos_distance, ap_dist: -0.9995152354240417, an_dist: -0.9996627569198608\n",
      "list of metrics:  [397, 398, 398, 398]\n",
      "Better model found at epoch 2 with validation value: 0.6380000114440918.\n",
      "target probs tensor([[9.0537e-01],\n",
      "        [1.8945e-02],\n",
      "        [8.6129e-02],\n",
      "        [9.7698e-01],\n",
      "        [1.3395e-01],\n",
      "        [8.5591e-04],\n",
      "        [1.9023e-03],\n",
      "        [8.6716e-01],\n",
      "        [1.9812e-02],\n",
      "        [8.1388e-02],\n",
      "        [1.9640e-05],\n",
      "        [6.5441e-03],\n",
      "        [9.9945e-01],\n",
      "        [8.2256e-01],\n",
      "        [2.2937e-02],\n",
      "        [1.5225e-04]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.11082923412323: \n",
      "func:cos_distance, ap_dist: -0.9997488856315613, an_dist: -0.9997586607933044\n",
      "target probs tensor([[8.6030e-01],\n",
      "        [1.4233e-01],\n",
      "        [8.6352e-01],\n",
      "        [9.7763e-04],\n",
      "        [8.7565e-01],\n",
      "        [3.5774e-01],\n",
      "        [4.4082e-05],\n",
      "        [8.1709e-04],\n",
      "        [9.7223e-01],\n",
      "        [6.1121e-01],\n",
      "        [3.9996e-01],\n",
      "        [4.2845e-04],\n",
      "        [7.0538e-01],\n",
      "        [9.1444e-01],\n",
      "        [1.0896e-01],\n",
      "        [3.6872e-03]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.9676308631896973: \n",
      "func:cos_distance, ap_dist: -0.9998347759246826, an_dist: -0.9998946785926819\n",
      "target probs tensor([[5.3574e-01],\n",
      "        [1.7664e-06],\n",
      "        [3.1499e-05],\n",
      "        [6.6607e-04],\n",
      "        [9.3117e-01],\n",
      "        [5.5786e-05],\n",
      "        [7.7167e-01],\n",
      "        [4.4643e-02],\n",
      "        [9.4940e-01],\n",
      "        [7.9695e-01],\n",
      "        [3.1677e-01],\n",
      "        [1.2206e-01],\n",
      "        [4.8976e-06],\n",
      "        [2.1484e-02],\n",
      "        [6.5928e-03],\n",
      "        [6.9519e-04]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.6303226947784424: \n",
      "func:cos_distance, ap_dist: -0.999852180480957, an_dist: -0.9998341202735901\n",
      "list of metrics:  [384, 384, 384, 384]\n",
      "Better model found at epoch 3 with validation value: 0.6520000100135803.\n",
      "target probs tensor([[4.5598e-01],\n",
      "        [2.2378e-01],\n",
      "        [1.1368e-02],\n",
      "        [7.9580e-01],\n",
      "        [5.9212e-01],\n",
      "        [7.1669e-01],\n",
      "        [9.2041e-04],\n",
      "        [1.0442e-04],\n",
      "        [1.0319e-01],\n",
      "        [4.3812e-02],\n",
      "        [5.6401e-01],\n",
      "        [9.9582e-01],\n",
      "        [6.2557e-04],\n",
      "        [9.9249e-01],\n",
      "        [4.2464e-03],\n",
      "        [6.7779e-03]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.9991082549095154: \n",
      "func:cos_distance, ap_dist: -0.9994495511054993, an_dist: -0.9997925758361816\n",
      "target probs tensor([[4.4399e-01],\n",
      "        [1.4898e-05],\n",
      "        [3.2442e-06],\n",
      "        [1.1094e-01],\n",
      "        [3.9294e-02],\n",
      "        [3.1626e-03],\n",
      "        [2.2646e-01],\n",
      "        [9.9991e-01],\n",
      "        [9.9828e-01],\n",
      "        [1.2937e-01],\n",
      "        [9.7327e-01],\n",
      "        [1.3403e-01],\n",
      "        [1.1047e-01],\n",
      "        [4.0957e-05],\n",
      "        [8.3047e-03],\n",
      "        [3.6944e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.320721983909607: \n",
      "func:cos_distance, ap_dist: -0.9997984766960144, an_dist: -0.9998857975006104\n",
      "target probs tensor([[1.3702e-02],\n",
      "        [1.6625e-02],\n",
      "        [2.2506e-01],\n",
      "        [1.8129e-01],\n",
      "        [9.9616e-01],\n",
      "        [1.7939e-06],\n",
      "        [1.3649e-03],\n",
      "        [1.2648e-01],\n",
      "        [2.8546e-01],\n",
      "        [1.0822e-02],\n",
      "        [3.2933e-04],\n",
      "        [1.3127e-01],\n",
      "        [3.0551e-04],\n",
      "        [9.9807e-01],\n",
      "        [9.4190e-01],\n",
      "        [1.5026e-02]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.9864634275436401: \n",
      "func:cos_distance, ap_dist: -0.999796450138092, an_dist: -0.9996099472045898\n",
      "list of metrics:  [369, 368, 369, 368]\n",
      "Better model found at epoch 4 with validation value: 0.6570000052452087.\n",
      "target probs tensor([[1.0294e-01],\n",
      "        [3.1898e-01],\n",
      "        [8.5176e-02],\n",
      "        [1.8522e-01],\n",
      "        [7.1131e-03],\n",
      "        [7.6091e-07],\n",
      "        [7.4145e-01],\n",
      "        [9.8848e-01],\n",
      "        [2.8338e-08],\n",
      "        [2.8685e-03],\n",
      "        [1.8112e-02],\n",
      "        [7.9289e-05],\n",
      "        [1.3453e-02],\n",
      "        [9.9886e-01],\n",
      "        [8.9846e-01],\n",
      "        [9.9594e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.3257668018341064: \n",
      "func:cos_distance, ap_dist: -0.9997859597206116, an_dist: -0.9995826482772827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[7.8204e-01],\n",
      "        [3.0345e-01],\n",
      "        [9.9179e-01],\n",
      "        [2.2623e-01],\n",
      "        [1.8268e-01],\n",
      "        [7.0122e-01],\n",
      "        [5.5616e-02],\n",
      "        [9.4011e-01],\n",
      "        [6.9138e-04],\n",
      "        [1.4317e-02],\n",
      "        [2.7623e-03],\n",
      "        [1.4718e-03],\n",
      "        [8.8565e-05],\n",
      "        [5.8099e-02],\n",
      "        [4.8191e-01],\n",
      "        [3.1445e-03]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.7478682398796082: \n",
      "func:cos_distance, ap_dist: -0.9997695684432983, an_dist: -0.9997047781944275\n",
      "target probs tensor([[2.5595e-01],\n",
      "        [1.1999e-03],\n",
      "        [2.4010e-03],\n",
      "        [3.0204e-02],\n",
      "        [5.2081e-04],\n",
      "        [9.6870e-01],\n",
      "        [3.6916e-01],\n",
      "        [1.9746e-01],\n",
      "        [9.9501e-01],\n",
      "        [8.6739e-01],\n",
      "        [3.5715e-04],\n",
      "        [6.1168e-01],\n",
      "        [3.6732e-01],\n",
      "        [1.1337e-05],\n",
      "        [2.3632e-03],\n",
      "        [6.0411e-02]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.8290201425552368: \n",
      "func:cos_distance, ap_dist: -0.9998266696929932, an_dist: -0.9998339414596558\n",
      "list of metrics:  [367, 366, 367, 367]\n",
      "Better model found at epoch 5 with validation value: 0.6620000004768372.\n",
      "target probs tensor([[8.9815e-04],\n",
      "        [3.4557e-04],\n",
      "        [1.1444e-02],\n",
      "        [9.9833e-01],\n",
      "        [3.4986e-06],\n",
      "        [2.2883e-03],\n",
      "        [7.3969e-01],\n",
      "        [1.0465e-01],\n",
      "        [5.6771e-01],\n",
      "        [4.2674e-02],\n",
      "        [1.7442e-03],\n",
      "        [4.9866e-01],\n",
      "        [1.7250e-04],\n",
      "        [2.5409e-04],\n",
      "        [7.5336e-01],\n",
      "        [1.9253e-06]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.6773912310600281: \n",
      "func:cos_distance, ap_dist: -0.9999188184738159, an_dist: -0.9998676776885986\n",
      "target probs tensor([[1.1102e-01],\n",
      "        [2.5109e-03],\n",
      "        [1.9055e-02],\n",
      "        [5.1039e-01],\n",
      "        [7.1543e-02],\n",
      "        [8.5585e-01],\n",
      "        [8.2226e-01],\n",
      "        [9.6502e-01],\n",
      "        [9.9746e-01],\n",
      "        [6.9665e-02],\n",
      "        [3.4971e-04],\n",
      "        [5.8563e-01],\n",
      "        [1.8903e-01],\n",
      "        [2.2389e-01],\n",
      "        [1.7312e-02],\n",
      "        [1.2066e-02]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.9603185057640076: \n",
      "func:cos_distance, ap_dist: -0.99955815076828, an_dist: -0.9997323751449585\n",
      "target probs tensor([[5.8138e-04],\n",
      "        [9.9646e-01],\n",
      "        [1.2320e-05],\n",
      "        [2.9788e-04],\n",
      "        [2.9964e-03],\n",
      "        [1.4787e-02],\n",
      "        [8.4446e-01],\n",
      "        [6.6564e-01],\n",
      "        [8.4350e-01],\n",
      "        [9.8150e-01],\n",
      "        [7.4094e-05],\n",
      "        [4.3837e-01],\n",
      "        [4.9778e-02],\n",
      "        [2.8038e-01],\n",
      "        [7.9876e-01],\n",
      "        [2.8377e-05]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.0639622211456299: \n",
      "func:cos_distance, ap_dist: -0.9995162487030029, an_dist: -0.9994492530822754\n",
      "list of metrics:  [368, 367, 368, 368]\n",
      "Better model found at epoch 6 with validation value: 0.6740000247955322.\n",
      "target probs tensor([[5.3454e-01],\n",
      "        [9.9504e-01],\n",
      "        [1.2558e-01],\n",
      "        [1.0974e-01],\n",
      "        [1.2893e-02],\n",
      "        [9.9920e-01],\n",
      "        [1.5948e-04],\n",
      "        [9.9090e-01],\n",
      "        [3.5139e-05],\n",
      "        [6.5244e-01],\n",
      "        [2.3689e-01],\n",
      "        [8.4723e-01],\n",
      "        [9.0239e-01],\n",
      "        [9.5639e-01],\n",
      "        [9.7198e-01],\n",
      "        [9.4985e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 2.087336778640747: \n",
      "func:cos_distance, ap_dist: -0.999786376953125, an_dist: -0.999724805355072\n",
      "target probs tensor([[2.0163e-01],\n",
      "        [6.7881e-01],\n",
      "        [1.8877e-06],\n",
      "        [1.1555e-02],\n",
      "        [8.9840e-03],\n",
      "        [7.2130e-01],\n",
      "        [6.6890e-01],\n",
      "        [5.0291e-01],\n",
      "        [8.5311e-01],\n",
      "        [2.6570e-07],\n",
      "        [8.1236e-04],\n",
      "        [4.1238e-04],\n",
      "        [5.1448e-02],\n",
      "        [9.1553e-05],\n",
      "        [1.6076e-03],\n",
      "        [1.7371e-02]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.403425395488739: \n",
      "func:cos_distance, ap_dist: -0.9999443292617798, an_dist: -0.9998735189437866\n",
      "target probs tensor([[4.6918e-01],\n",
      "        [6.4498e-04],\n",
      "        [9.4910e-03],\n",
      "        [1.3332e-01],\n",
      "        [8.6164e-01],\n",
      "        [8.6141e-01],\n",
      "        [4.2451e-01],\n",
      "        [4.7198e-06],\n",
      "        [3.9900e-01],\n",
      "        [3.0678e-04],\n",
      "        [7.2008e-04],\n",
      "        [6.6002e-05],\n",
      "        [9.9089e-01],\n",
      "        [2.4775e-03],\n",
      "        [4.4441e-01],\n",
      "        [1.3507e-03]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.6933136582374573: \n",
      "func:cos_distance, ap_dist: -0.9997231364250183, an_dist: -0.9997658729553223\n",
      "target probs tensor([[9.0999e-06],\n",
      "        [9.2334e-06],\n",
      "        [5.1979e-07],\n",
      "        [1.1828e-01],\n",
      "        [2.2703e-02],\n",
      "        [2.4091e-03],\n",
      "        [1.0194e-05],\n",
      "        [8.3257e-05]], device='cuda:1'), loss: 0.018920663744211197: \n",
      "func:cos_distance, ap_dist: -0.9994739890098572, an_dist: -0.9996570348739624\n",
      "list of metrics:  [352, 353, 351, 352]\n",
      "Better model found at epoch 7 with validation value: 0.6959999799728394.\n",
      "target probs tensor([[9.8355e-07],\n",
      "        [1.8856e-01],\n",
      "        [7.2792e-02],\n",
      "        [1.6653e-06],\n",
      "        [5.6045e-01],\n",
      "        [4.6988e-01],\n",
      "        [9.5866e-01],\n",
      "        [6.2703e-01],\n",
      "        [3.5973e-06],\n",
      "        [4.2598e-01],\n",
      "        [8.3954e-03],\n",
      "        [2.2477e-04],\n",
      "        [1.1232e-07],\n",
      "        [1.3178e-04],\n",
      "        [8.8511e-01],\n",
      "        [9.4696e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.7235987186431885: \n",
      "func:cos_distance, ap_dist: -0.9998316168785095, an_dist: -0.999231219291687\n",
      "target probs tensor([[4.4200e-02],\n",
      "        [1.1305e-04],\n",
      "        [1.4785e-02],\n",
      "        [5.2289e-03],\n",
      "        [1.8321e-06],\n",
      "        [2.5165e-02],\n",
      "        [9.1457e-02],\n",
      "        [1.5819e-06],\n",
      "        [1.4295e-01],\n",
      "        [1.0949e-02],\n",
      "        [5.9264e-07],\n",
      "        [6.8079e-03],\n",
      "        [6.0029e-04],\n",
      "        [4.5718e-01],\n",
      "        [6.2954e-07],\n",
      "        [7.1089e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.1382167488336563: \n",
      "func:cos_distance, ap_dist: -0.9997787475585938, an_dist: -0.9995325803756714\n",
      "target probs tensor([[1.0527e-06],\n",
      "        [4.1282e-03],\n",
      "        [7.0879e-01],\n",
      "        [4.2601e-02],\n",
      "        [4.0523e-03],\n",
      "        [9.3514e-01],\n",
      "        [3.6997e-08],\n",
      "        [6.9962e-02],\n",
      "        [3.3372e-05],\n",
      "        [2.1382e-01],\n",
      "        [7.4349e-04],\n",
      "        [8.9367e-02],\n",
      "        [2.6166e-03],\n",
      "        [2.0954e-02],\n",
      "        [2.0038e-05],\n",
      "        [9.9581e-01]], device='cuda:1'), loss: 0.6204394102096558: \n",
      "func:cos_distance, ap_dist: -0.9996760487556458, an_dist: -0.9996157288551331\n",
      "list of metrics:  [333, 334, 334, 333]\n",
      "Better model found at epoch 8 with validation value: 0.7149999737739563.\n",
      "target probs tensor([[3.5670e-01],\n",
      "        [9.9846e-01],\n",
      "        [9.0153e-01],\n",
      "        [9.6194e-01],\n",
      "        [4.2307e-01],\n",
      "        [6.1364e-02],\n",
      "        [9.9414e-01],\n",
      "        [6.7004e-01],\n",
      "        [9.6941e-02],\n",
      "        [8.7692e-01],\n",
      "        [3.4050e-07],\n",
      "        [3.7529e-05],\n",
      "        [1.1294e-01],\n",
      "        [2.5638e-03],\n",
      "        [2.4026e-02],\n",
      "        [3.6596e-04]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.356838583946228: \n",
      "func:cos_distance, ap_dist: -0.9994959831237793, an_dist: -0.9998769164085388\n",
      "target probs tensor([[1.5974e-01],\n",
      "        [5.7396e-01],\n",
      "        [2.4130e-04],\n",
      "        [7.7175e-02],\n",
      "        [2.3319e-03],\n",
      "        [3.1522e-04],\n",
      "        [7.6971e-01],\n",
      "        [3.3281e-04],\n",
      "        [9.9974e-01],\n",
      "        [1.7138e-01],\n",
      "        [4.8567e-01],\n",
      "        [4.5058e-01],\n",
      "        [4.9448e-05],\n",
      "        [2.4583e-03],\n",
      "        [1.9319e-01],\n",
      "        [4.6130e-02]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.7853000164031982: \n",
      "func:cos_distance, ap_dist: -0.9997766017913818, an_dist: -0.9997673034667969\n",
      "target probs tensor([[1.7959e-03],\n",
      "        [4.4024e-01],\n",
      "        [2.2837e-02],\n",
      "        [1.3557e-02],\n",
      "        [2.0622e-03],\n",
      "        [3.5944e-04],\n",
      "        [5.6517e-01],\n",
      "        [2.9498e-04],\n",
      "        [6.3594e-01],\n",
      "        [1.1164e-01],\n",
      "        [3.5153e-03],\n",
      "        [4.8116e-03],\n",
      "        [1.7040e-01],\n",
      "        [1.9590e-06],\n",
      "        [8.4508e-01],\n",
      "        [1.8206e-01]], device='cuda:1'), loss: 0.30275535583496094: \n",
      "func:cos_distance, ap_dist: -0.9997262954711914, an_dist: -0.999409556388855\n",
      "list of metrics:  [321, 321, 320, 321]\n",
      "Better model found at epoch 9 with validation value: 0.722000002861023.\n",
      "target probs tensor([[5.0904e-04],\n",
      "        [2.8195e-01],\n",
      "        [2.3902e-02],\n",
      "        [1.5192e-04],\n",
      "        [2.3609e-04],\n",
      "        [7.8633e-05],\n",
      "        [4.7997e-05],\n",
      "        [3.6605e-04],\n",
      "        [3.4970e-04],\n",
      "        [8.7812e-01],\n",
      "        [3.7703e-06],\n",
      "        [1.3814e-05],\n",
      "        [6.4145e-04],\n",
      "        [1.2739e-03],\n",
      "        [6.2144e-01],\n",
      "        [8.9932e-05]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.21470452845096588: \n",
      "func:cos_distance, ap_dist: -0.9995748400688171, an_dist: -0.9997473359107971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[3.1680e-07],\n",
      "        [1.3498e-01],\n",
      "        [1.1184e-04],\n",
      "        [2.9358e-03],\n",
      "        [7.1348e-04],\n",
      "        [6.5031e-01],\n",
      "        [9.5319e-01],\n",
      "        [4.1111e-02],\n",
      "        [1.6704e-02],\n",
      "        [3.6760e-05],\n",
      "        [9.2194e-01],\n",
      "        [1.3007e-03],\n",
      "        [9.9666e-01],\n",
      "        [5.0098e-01],\n",
      "        [1.7478e-02],\n",
      "        [7.8596e-02]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.8355032801628113: \n",
      "func:cos_distance, ap_dist: -0.9996906518936157, an_dist: -0.9997185468673706\n",
      "target probs tensor([[2.8735e-03],\n",
      "        [8.9554e-01],\n",
      "        [1.1794e-03],\n",
      "        [2.2462e-04],\n",
      "        [6.5550e-01],\n",
      "        [1.2281e-02],\n",
      "        [6.6984e-03],\n",
      "        [6.7502e-01],\n",
      "        [5.5677e-01],\n",
      "        [8.8647e-01],\n",
      "        [5.8174e-01],\n",
      "        [2.5809e-06],\n",
      "        [5.3544e-04],\n",
      "        [3.1343e-01],\n",
      "        [1.7421e-03],\n",
      "        [3.3373e-06]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.5444514155387878: \n",
      "func:cos_distance, ap_dist: -0.9996841549873352, an_dist: -0.9996435642242432\n",
      "list of metrics:  [316, 316, 316, 315]\n",
      "Better model found at epoch 10 with validation value: 0.7360000014305115.\n",
      "target probs tensor([[9.7516e-03],\n",
      "        [4.0974e-01],\n",
      "        [2.8269e-02],\n",
      "        [1.2485e-01],\n",
      "        [2.4645e-04],\n",
      "        [2.9928e-01],\n",
      "        [2.6640e-03],\n",
      "        [9.0019e-01],\n",
      "        [3.5375e-06],\n",
      "        [1.0075e-01],\n",
      "        [2.8381e-06],\n",
      "        [1.9468e-03],\n",
      "        [5.1309e-02],\n",
      "        [5.4032e-03],\n",
      "        [3.2039e-07],\n",
      "        [5.8312e-05]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.22052349150180817: \n",
      "func:cos_distance, ap_dist: -0.999780535697937, an_dist: -0.9998219013214111\n",
      "target probs tensor([[5.4554e-04],\n",
      "        [6.9227e-04],\n",
      "        [1.1436e-01],\n",
      "        [9.9742e-01],\n",
      "        [7.6097e-03],\n",
      "        [1.7000e-04],\n",
      "        [2.1054e-05],\n",
      "        [7.3084e-01],\n",
      "        [7.9722e-01],\n",
      "        [1.8296e-03],\n",
      "        [6.4247e-03],\n",
      "        [3.1993e-03],\n",
      "        [6.3215e-03],\n",
      "        [3.9739e-02],\n",
      "        [3.9254e-06],\n",
      "        [9.9224e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.8696651458740234: \n",
      "func:cos_distance, ap_dist: -0.9998500347137451, an_dist: -0.9998042583465576\n",
      "target probs tensor([[3.1225e-03],\n",
      "        [1.2413e-02],\n",
      "        [1.4931e-01],\n",
      "        [5.6796e-01],\n",
      "        [9.8029e-01],\n",
      "        [3.2035e-06],\n",
      "        [5.7000e-04],\n",
      "        [2.9398e-02],\n",
      "        [1.6596e-02],\n",
      "        [7.9471e-01],\n",
      "        [3.3627e-03],\n",
      "        [6.8735e-04],\n",
      "        [7.0718e-04],\n",
      "        [4.8662e-04],\n",
      "        [1.4060e-02],\n",
      "        [8.1770e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.5184522271156311: \n",
      "func:cos_distance, ap_dist: -0.9995896816253662, an_dist: -0.9995245933532715\n",
      "list of metrics:  [306, 307, 307, 307]\n",
      "target probs tensor([[3.5326e-01],\n",
      "        [2.6421e-03],\n",
      "        [4.1844e-04],\n",
      "        [1.6839e-02],\n",
      "        [2.1338e-02],\n",
      "        [7.4314e-05],\n",
      "        [8.5699e-07],\n",
      "        [4.7273e-05],\n",
      "        [1.5413e-03],\n",
      "        [1.6680e-02],\n",
      "        [1.8154e-02],\n",
      "        [3.9493e-04],\n",
      "        [9.9576e-01],\n",
      "        [3.3924e-01],\n",
      "        [4.3861e-06],\n",
      "        [4.2777e-03]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.39974820613861084: \n",
      "func:cos_distance, ap_dist: -0.9998316764831543, an_dist: -0.9996320605278015\n",
      "target probs tensor([[4.0801e-01],\n",
      "        [3.4596e-01],\n",
      "        [2.3252e-05],\n",
      "        [1.2620e-04],\n",
      "        [1.5774e-01],\n",
      "        [1.2252e-01],\n",
      "        [2.0619e-04],\n",
      "        [1.8108e-05],\n",
      "        [5.9557e-02],\n",
      "        [1.5448e-03],\n",
      "        [9.5128e-02],\n",
      "        [9.4711e-01],\n",
      "        [5.4155e-04],\n",
      "        [8.2981e-02],\n",
      "        [2.7529e-04],\n",
      "        [1.2112e-04]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.2775998115539551: \n",
      "func:cos_distance, ap_dist: -0.9997941255569458, an_dist: -0.9998027086257935\n",
      "target probs tensor([[3.9472e-01],\n",
      "        [6.4142e-03],\n",
      "        [8.2385e-01],\n",
      "        [2.3083e-01],\n",
      "        [5.1799e-01],\n",
      "        [3.7112e-06],\n",
      "        [9.9368e-03],\n",
      "        [1.4001e-01],\n",
      "        [8.1699e-07],\n",
      "        [4.8204e-03],\n",
      "        [1.2249e-01],\n",
      "        [1.6071e-02],\n",
      "        [2.6451e-03],\n",
      "        [4.2844e-01],\n",
      "        [1.5009e-01],\n",
      "        [1.5192e-06]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.2671486437320709: \n",
      "func:cos_distance, ap_dist: -0.9996985793113708, an_dist: -0.9995307922363281\n",
      "list of metrics:  [308, 308, 309, 309]\n",
      "target probs tensor([[5.9613e-06],\n",
      "        [1.2265e-01],\n",
      "        [3.6515e-09],\n",
      "        [2.4558e-02],\n",
      "        [6.7864e-03],\n",
      "        [9.9345e-01],\n",
      "        [1.7626e-01],\n",
      "        [5.0965e-03],\n",
      "        [1.2451e-01],\n",
      "        [9.4521e-01],\n",
      "        [9.7128e-02],\n",
      "        [2.6258e-01],\n",
      "        [5.2575e-04],\n",
      "        [1.5563e-05],\n",
      "        [9.6752e-01],\n",
      "        [7.9603e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.8656688332557678: \n",
      "func:cos_distance, ap_dist: -0.9995929598808289, an_dist: -0.999762237071991\n",
      "target probs tensor([[2.6954e-04],\n",
      "        [6.3972e-05],\n",
      "        [1.2835e-04],\n",
      "        [2.5268e-01],\n",
      "        [4.6143e-02],\n",
      "        [7.3974e-07],\n",
      "        [9.9878e-01],\n",
      "        [5.2906e-07],\n",
      "        [1.1182e-04],\n",
      "        [1.4761e-04],\n",
      "        [9.9983e-01],\n",
      "        [9.9602e-01],\n",
      "        [2.0171e-01],\n",
      "        [6.4709e-02],\n",
      "        [1.2611e-03],\n",
      "        [5.2404e-03]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.3477287292480469: \n",
      "func:cos_distance, ap_dist: -0.9989594221115112, an_dist: -0.9993696212768555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[7.5043e-02],\n",
      "        [2.9450e-01],\n",
      "        [8.3504e-02],\n",
      "        [6.8296e-06],\n",
      "        [2.7504e-01],\n",
      "        [8.5536e-01],\n",
      "        [1.1594e-01],\n",
      "        [1.0952e-03],\n",
      "        [8.9943e-01],\n",
      "        [2.2621e-03],\n",
      "        [9.1214e-05],\n",
      "        [1.8176e-01],\n",
      "        [2.9114e-06],\n",
      "        [1.0397e-01],\n",
      "        [1.9906e-02],\n",
      "        [1.9576e-02]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.34644073247909546: \n",
      "func:cos_distance, ap_dist: -0.9949756264686584, an_dist: -0.7458868026733398\n",
      "target probs tensor([[3.4397e-01],\n",
      "        [3.5593e-04],\n",
      "        [2.5577e-03],\n",
      "        [4.4842e-01],\n",
      "        [1.6896e-05],\n",
      "        [4.7994e-01],\n",
      "        [4.9953e-01],\n",
      "        [1.3698e-01],\n",
      "        [6.7050e-06],\n",
      "        [2.8787e-05],\n",
      "        [5.5499e-05],\n",
      "        [1.0302e-05],\n",
      "        [6.4034e-02],\n",
      "        [1.1426e-04],\n",
      "        [1.4082e-01],\n",
      "        [1.6012e-02]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.17169295251369476: \n",
      "func:cos_distance, ap_dist: -0.9981915354728699, an_dist: -0.8691593408584595\n",
      "target probs tensor([[1.9019e-03],\n",
      "        [3.8473e-01],\n",
      "        [1.3379e-02],\n",
      "        [9.1808e-04],\n",
      "        [9.5917e-03],\n",
      "        [1.1156e-06],\n",
      "        [7.8497e-03],\n",
      "        [1.1278e-04],\n",
      "        [6.9185e-01],\n",
      "        [2.5277e-03],\n",
      "        [3.2626e-03],\n",
      "        [5.7999e-03],\n",
      "        [1.2319e-04],\n",
      "        [9.7620e-06],\n",
      "        [5.5555e-02],\n",
      "        [4.6482e-05]], device='cuda:1'), loss: 0.11035944521427155: \n",
      "func:cos_distance, ap_dist: -0.9954360723495483, an_dist: -0.8182127475738525\n",
      "list of metrics:  [246, 243, 245, 250]\n",
      "Better model found at epoch 17 with validation value: 0.8069999814033508.\n",
      "target probs tensor([[3.6549e-04],\n",
      "        [3.6627e-09],\n",
      "        [8.9925e-06],\n",
      "        [7.7087e-04],\n",
      "        [1.5619e-02],\n",
      "        [4.8411e-05],\n",
      "        [6.7134e-05],\n",
      "        [2.2062e-05],\n",
      "        [2.9577e-02],\n",
      "        [1.9722e-03],\n",
      "        [5.6175e-03],\n",
      "        [1.9950e-01],\n",
      "        [9.7434e-01],\n",
      "        [4.5606e-05],\n",
      "        [1.0654e-05],\n",
      "        [1.6669e-03]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.24636752903461456: \n",
      "func:cos_distance, ap_dist: -0.9863286018371582, an_dist: -0.9097511172294617\n",
      "target probs tensor([[5.3959e-03],\n",
      "        [1.8309e-01],\n",
      "        [6.4900e-08],\n",
      "        [3.2444e-03],\n",
      "        [3.1784e-08],\n",
      "        [2.0303e-02],\n",
      "        [1.3399e-03],\n",
      "        [2.0318e-04],\n",
      "        [4.2058e-05],\n",
      "        [6.5632e-03],\n",
      "        [8.6029e-01],\n",
      "        [1.4953e-01],\n",
      "        [5.6001e-07],\n",
      "        [4.6068e-07],\n",
      "        [3.9584e-02],\n",
      "        [9.9631e-06]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.15063469111919403: \n",
      "func:cos_distance, ap_dist: -0.9618161916732788, an_dist: -0.8285032510757446\n",
      "target probs tensor([[1.2338e-03],\n",
      "        [5.0062e-07],\n",
      "        [9.1077e-04],\n",
      "        [6.1865e-05],\n",
      "        [2.9354e-05],\n",
      "        [2.9354e-01],\n",
      "        [2.7467e-03],\n",
      "        [7.8936e-03],\n",
      "        [5.5566e-01],\n",
      "        [1.0286e-06],\n",
      "        [1.2449e-02],\n",
      "        [2.1708e-01],\n",
      "        [7.7770e-05],\n",
      "        [4.1578e-05],\n",
      "        [5.4442e-01],\n",
      "        [2.4964e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.15639516711235046: \n",
      "func:cos_distance, ap_dist: -0.9959481358528137, an_dist: -0.7044570446014404\n",
      "list of metrics:  [235, 240, 240, 246]\n",
      "Better model found at epoch 18 with validation value: 0.8259999752044678.\n",
      "target probs tensor([[7.3050e-01],\n",
      "        [2.5285e-02],\n",
      "        [2.7666e-02],\n",
      "        [1.8997e-02],\n",
      "        [3.2961e-06],\n",
      "        [4.1471e-02],\n",
      "        [2.6400e-01],\n",
      "        [2.4567e-01],\n",
      "        [2.9126e-01],\n",
      "        [1.8748e-02],\n",
      "        [2.7739e-05],\n",
      "        [4.3054e-06],\n",
      "        [1.1177e-05],\n",
      "        [6.6144e-04],\n",
      "        [1.9896e-04],\n",
      "        [2.5381e-02]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.15028926730155945: \n",
      "func:cos_distance, ap_dist: -0.9853166937828064, an_dist: -0.8305608630180359\n",
      "target probs tensor([[6.9000e-06],\n",
      "        [1.2616e-05],\n",
      "        [7.3262e-01],\n",
      "        [3.2510e-09],\n",
      "        [1.8196e-02],\n",
      "        [2.5809e-01],\n",
      "        [3.0804e-03],\n",
      "        [1.2815e-01],\n",
      "        [6.4633e-03],\n",
      "        [3.5614e-06],\n",
      "        [3.8851e-02],\n",
      "        [1.2570e-03],\n",
      "        [5.8083e-06],\n",
      "        [3.0854e-06],\n",
      "        [2.6763e-04],\n",
      "        [3.7008e-04]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.11401411145925522: \n",
      "func:cos_distance, ap_dist: -0.9986374974250793, an_dist: -0.5830115079879761\n",
      "target probs tensor([[4.5739e-02],\n",
      "        [8.7343e-07],\n",
      "        [7.9693e-03],\n",
      "        [1.6746e-05],\n",
      "        [9.3735e-05],\n",
      "        [1.4383e-03],\n",
      "        [1.4483e-05],\n",
      "        [2.1562e-03],\n",
      "        [5.3807e-04],\n",
      "        [3.2717e-02],\n",
      "        [7.7505e-01],\n",
      "        [2.5480e-01],\n",
      "        [1.1587e-07],\n",
      "        [1.0812e-03],\n",
      "        [4.2265e-06],\n",
      "        [3.7771e-02]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.11987043917179108: \n",
      "func:cos_distance, ap_dist: -0.9987117052078247, an_dist: -0.7351709604263306\n",
      "list of metrics:  [229, 230, 230, 221]\n",
      "Better model found at epoch 19 with validation value: 0.8450000286102295.\n",
      "target probs tensor([[2.2071e-02],\n",
      "        [6.7453e-05],\n",
      "        [4.3340e-01],\n",
      "        [2.4242e-01],\n",
      "        [1.6179e-03],\n",
      "        [5.2153e-01],\n",
      "        [1.1672e-05],\n",
      "        [2.8072e-02],\n",
      "        [4.2470e-03],\n",
      "        [9.2297e-05],\n",
      "        [4.2405e-07],\n",
      "        [1.0636e-02],\n",
      "        [2.0397e-01],\n",
      "        [7.0629e-01],\n",
      "        [1.0839e-07],\n",
      "        [4.5789e-03]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.1942673921585083: \n",
      "func:cos_distance, ap_dist: -0.9975222945213318, an_dist: -0.8535141944885254\n",
      "target probs tensor([[9.3172e-01],\n",
      "        [5.9698e-01],\n",
      "        [1.8752e-06],\n",
      "        [5.7898e-01],\n",
      "        [5.8963e-01],\n",
      "        [3.0849e-06],\n",
      "        [3.8597e-01],\n",
      "        [2.5868e-05],\n",
      "        [7.4887e-04],\n",
      "        [3.8878e-06],\n",
      "        [2.3166e-04],\n",
      "        [3.6068e-01],\n",
      "        [1.0510e-05],\n",
      "        [1.3305e-03],\n",
      "        [2.7586e-02],\n",
      "        [1.5049e-02]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.39558279514312744: \n",
      "func:cos_distance, ap_dist: -0.9994576573371887, an_dist: -0.8413888216018677\n",
      "target probs tensor([[1.1770e-03],\n",
      "        [1.2038e-04],\n",
      "        [2.0322e-03],\n",
      "        [9.9948e-01],\n",
      "        [1.2605e-02],\n",
      "        [5.9464e-08],\n",
      "        [5.6499e-02],\n",
      "        [9.9947e-01],\n",
      "        [8.7472e-05],\n",
      "        [6.7717e-01],\n",
      "        [1.5756e-05],\n",
      "        [8.0293e-01],\n",
      "        [3.3076e-04],\n",
      "        [3.6769e-03],\n",
      "        [1.1250e-06],\n",
      "        [3.7134e-04]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.121086597442627: \n",
      "func:cos_distance, ap_dist: -0.9908066987991333, an_dist: -0.7243518829345703\n",
      "list of metrics:  [205, 192, 193, 206]\n",
      "fooling weight increased to 3.5999999999999996 at the end of epoch 20\n",
      "target probs tensor([[7.7189e-06],\n",
      "        [3.2414e-02],\n",
      "        [2.9195e-05],\n",
      "        [2.5469e-01],\n",
      "        [9.9912e-01],\n",
      "        [1.7897e-04],\n",
      "        [3.3308e-02],\n",
      "        [1.4998e-06],\n",
      "        [1.0782e-06],\n",
      "        [1.3048e-04],\n",
      "        [8.8547e-05],\n",
      "        [5.7435e-04],\n",
      "        [9.4143e-01],\n",
      "        [5.7891e-03],\n",
      "        [1.1581e-03],\n",
      "        [9.2083e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.7989523410797119: \n",
      "func:cos_distance, ap_dist: -0.9975696802139282, an_dist: -0.9061731100082397\n",
      "target probs tensor([[4.7887e-02],\n",
      "        [1.4599e-04],\n",
      "        [3.1890e-07],\n",
      "        [5.0819e-01],\n",
      "        [1.4351e-04],\n",
      "        [4.8943e-02],\n",
      "        [3.8993e-05],\n",
      "        [9.9766e-01],\n",
      "        [1.4786e-03],\n",
      "        [7.3306e-01],\n",
      "        [2.6855e-06],\n",
      "        [1.3367e-01],\n",
      "        [3.8338e-03],\n",
      "        [1.3994e-07],\n",
      "        [9.7619e-01],\n",
      "        [6.6391e-07]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.7547517418861389: \n",
      "func:cos_distance, ap_dist: -0.9953269958496094, an_dist: -0.8364704847335815\n",
      "target probs tensor([[2.0658e-01],\n",
      "        [2.6493e-03],\n",
      "        [2.9414e-04],\n",
      "        [6.1757e-04],\n",
      "        [7.8181e-02],\n",
      "        [9.9835e-04],\n",
      "        [1.7458e-05],\n",
      "        [4.7237e-06],\n",
      "        [2.2096e-03],\n",
      "        [5.2185e-07],\n",
      "        [3.3853e-04],\n",
      "        [1.4680e-01],\n",
      "        [7.1770e-06],\n",
      "        [9.3794e-06],\n",
      "        [9.7139e-01],\n",
      "        [3.6425e-03]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.2522830367088318: \n",
      "func:cos_distance, ap_dist: -0.9930937886238098, an_dist: -0.6846985220909119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[6.5066e-06],\n",
      "        [1.4476e-04],\n",
      "        [7.1659e-05],\n",
      "        [1.1267e-03],\n",
      "        [3.4752e-04],\n",
      "        [9.6027e-01],\n",
      "        [5.5931e-10],\n",
      "        [4.1170e-03],\n",
      "        [1.0439e-04],\n",
      "        [9.1881e-04],\n",
      "        [8.0315e-04],\n",
      "        [4.3313e-02],\n",
      "        [6.6047e-02],\n",
      "        [2.4993e-05],\n",
      "        [9.7044e-06],\n",
      "        [4.2957e-02]], device='cuda:1'), loss: 0.21186786890029907: \n",
      "func:cos_distance, ap_dist: -0.9791551828384399, an_dist: -0.5983177423477173\n",
      "list of metrics:  [175, 186, 180, 187]\n",
      "Better model found at epoch 24 with validation value: 0.8600000143051147.\n",
      "target probs tensor([[1.3289e-03],\n",
      "        [9.2883e-08],\n",
      "        [3.5125e-04],\n",
      "        [4.3580e-02],\n",
      "        [7.1685e-01],\n",
      "        [3.1581e-04],\n",
      "        [1.1454e-03],\n",
      "        [5.0893e-03],\n",
      "        [3.4215e-06],\n",
      "        [2.6702e-06],\n",
      "        [6.8946e-08],\n",
      "        [1.8334e-06],\n",
      "        [3.4274e-05],\n",
      "        [3.0567e-07],\n",
      "        [6.3328e-06],\n",
      "        [9.8922e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.3653232753276825: \n",
      "func:cos_distance, ap_dist: -0.9984961152076721, an_dist: -0.7382917404174805\n",
      "target probs tensor([[4.9896e-04],\n",
      "        [1.4450e-05],\n",
      "        [1.4804e-03],\n",
      "        [1.7617e-02],\n",
      "        [8.0648e-08],\n",
      "        [6.2038e-05],\n",
      "        [5.4227e-01],\n",
      "        [4.7569e-03],\n",
      "        [2.1468e-10],\n",
      "        [9.6180e-01],\n",
      "        [5.4393e-02],\n",
      "        [1.2888e-01],\n",
      "        [4.1214e-04],\n",
      "        [1.3283e-06],\n",
      "        [4.7129e-05],\n",
      "        [9.4874e-03]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.2671738862991333: \n",
      "func:cos_distance, ap_dist: -0.9969433546066284, an_dist: -0.7752703428268433\n",
      "target probs tensor([[2.8880e-04],\n",
      "        [3.4918e-03],\n",
      "        [4.6935e-03],\n",
      "        [2.9068e-04],\n",
      "        [9.1581e-03],\n",
      "        [8.1525e-08],\n",
      "        [3.4115e-03],\n",
      "        [5.5884e-05],\n",
      "        [7.6973e-01],\n",
      "        [1.5138e-03],\n",
      "        [8.6447e-03],\n",
      "        [2.2133e-03],\n",
      "        [1.9424e-04],\n",
      "        [1.3321e-05],\n",
      "        [1.5277e-02],\n",
      "        [3.6781e-07]], device='cuda:1'), loss: 0.09487451612949371: \n",
      "func:cos_distance, ap_dist: -0.997950553894043, an_dist: -0.6432207822799683\n",
      "list of metrics:  [165, 172, 180, 170]\n",
      "fooling weight increased to 3.8999999999999995 at the end of epoch 25\n",
      "target probs tensor([[6.9332e-02],\n",
      "        [4.1651e-04],\n",
      "        [9.8633e-01],\n",
      "        [7.6313e-01],\n",
      "        [1.0031e-04],\n",
      "        [9.9993e-01],\n",
      "        [2.3924e-06],\n",
      "        [3.6066e-03],\n",
      "        [2.3443e-03],\n",
      "        [9.9699e-01],\n",
      "        [2.8977e-07],\n",
      "        [3.8215e-02],\n",
      "        [1.4031e-04],\n",
      "        [2.1059e-04],\n",
      "        [1.7473e-01],\n",
      "        [4.1147e-02]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.3455536365509033: \n",
      "func:cos_distance, ap_dist: -0.938875675201416, an_dist: -0.8556616306304932\n",
      "target probs tensor([[1.2207e-04],\n",
      "        [1.3332e-11],\n",
      "        [8.0997e-06],\n",
      "        [2.2937e-04],\n",
      "        [6.1663e-03],\n",
      "        [1.2470e-05],\n",
      "        [5.8964e-02],\n",
      "        [9.9762e-01],\n",
      "        [9.8410e-01],\n",
      "        [8.9290e-04],\n",
      "        [3.0630e-06],\n",
      "        [6.3570e-03],\n",
      "        [1.2934e-05],\n",
      "        [3.4857e-06],\n",
      "        [1.2316e-07],\n",
      "        [1.0492e-04]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.6411186456680298: \n",
      "func:cos_distance, ap_dist: -0.9988722801208496, an_dist: -0.8047925233840942\n",
      "target probs tensor([[6.5469e-03],\n",
      "        [9.8813e-05],\n",
      "        [2.0322e-05],\n",
      "        [1.1005e-08],\n",
      "        [1.5954e-04],\n",
      "        [1.0300e-03],\n",
      "        [4.3768e-02],\n",
      "        [7.9799e-08],\n",
      "        [6.9275e-01],\n",
      "        [1.4390e-06],\n",
      "        [3.5174e-06],\n",
      "        [5.6367e-07],\n",
      "        [7.0764e-04],\n",
      "        [1.2991e-04],\n",
      "        [9.0143e-03],\n",
      "        [1.7578e-04]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.07767558842897415: \n",
      "func:cos_distance, ap_dist: -0.9880456924438477, an_dist: -0.7504258155822754\n",
      "list of metrics:  [192, 190, 199, 198]\n",
      "Better model found at epoch 26 with validation value: 0.8610000014305115.\n",
      "target probs tensor([[4.7921e-02],\n",
      "        [4.4478e-05],\n",
      "        [6.4364e-01],\n",
      "        [1.9223e-04],\n",
      "        [2.6877e-05],\n",
      "        [6.9159e-03],\n",
      "        [3.1100e-02],\n",
      "        [7.5201e-09],\n",
      "        [1.3235e-09],\n",
      "        [8.8689e-01],\n",
      "        [1.0613e-06],\n",
      "        [4.0866e-06],\n",
      "        [1.0295e-04],\n",
      "        [1.5833e-02],\n",
      "        [5.5584e-05],\n",
      "        [1.5524e-04]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.20721086859703064: \n",
      "func:cos_distance, ap_dist: -0.9983530044555664, an_dist: -0.6755613088607788\n",
      "target probs tensor([[1.5941e-01],\n",
      "        [4.0633e-02],\n",
      "        [1.3370e-09],\n",
      "        [7.4668e-05],\n",
      "        [6.6064e-03],\n",
      "        [4.6121e-02],\n",
      "        [1.2561e-02],\n",
      "        [5.9369e-02],\n",
      "        [1.2756e-05],\n",
      "        [3.9880e-07],\n",
      "        [1.5355e-05],\n",
      "        [8.2698e-07],\n",
      "        [6.6652e-11],\n",
      "        [3.1331e-04],\n",
      "        [2.5525e-02],\n",
      "        [1.3794e-02]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.023936426267027855: \n",
      "func:cos_distance, ap_dist: -0.9996637105941772, an_dist: -0.7507206201553345\n",
      "target probs tensor([[9.1254e-07],\n",
      "        [7.4113e-06],\n",
      "        [2.5508e-06],\n",
      "        [1.4897e-04],\n",
      "        [9.3461e-03],\n",
      "        [2.8913e-05],\n",
      "        [1.0186e-08],\n",
      "        [8.6833e-04],\n",
      "        [2.4988e-02],\n",
      "        [9.9202e-02],\n",
      "        [6.4859e-09],\n",
      "        [2.0976e-06],\n",
      "        [4.0217e-08],\n",
      "        [8.8560e-01],\n",
      "        [2.3704e-02],\n",
      "        [3.1028e-06]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.14576983451843262: \n",
      "func:cos_distance, ap_dist: -0.995033860206604, an_dist: -0.7333850860595703\n",
      "list of metrics:  [163, 168, 169, 177]\n",
      "Better model found at epoch 27 with validation value: 0.8619999885559082.\n",
      "fooling weight increased to 4.199999999999999 at the end of epoch 27\n",
      "target probs tensor([[8.3764e-07],\n",
      "        [3.2232e-01],\n",
      "        [4.6890e-05],\n",
      "        [1.9010e-07],\n",
      "        [1.2301e-06],\n",
      "        [4.3839e-05],\n",
      "        [5.0528e-03],\n",
      "        [6.7933e-04],\n",
      "        [6.9444e-07],\n",
      "        [7.8632e-04],\n",
      "        [1.6347e-04],\n",
      "        [7.2048e-06],\n",
      "        [1.2462e-06],\n",
      "        [1.2410e-01],\n",
      "        [1.3575e-05],\n",
      "        [9.8269e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.2865530848503113: \n",
      "func:cos_distance, ap_dist: -0.997116208076477, an_dist: -0.7404793500900269\n",
      "target probs tensor([[1.1280e-03],\n",
      "        [3.2342e-01],\n",
      "        [6.6413e-11],\n",
      "        [1.3112e-04],\n",
      "        [2.4984e-06],\n",
      "        [7.5981e-01],\n",
      "        [2.2454e-01],\n",
      "        [6.6423e-05],\n",
      "        [3.6484e-04],\n",
      "        [2.1359e-01],\n",
      "        [2.4845e-09],\n",
      "        [1.7087e-05],\n",
      "        [1.4167e-01],\n",
      "        [6.9559e-02],\n",
      "        [2.6846e-04],\n",
      "        [1.0495e-03]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.15871839225292206: \n",
      "func:cos_distance, ap_dist: -0.9661132097244263, an_dist: -0.680443286895752\n",
      "target probs tensor([[9.4884e-01],\n",
      "        [7.1561e-07],\n",
      "        [2.9727e-04],\n",
      "        [3.6040e-02],\n",
      "        [1.8118e-01],\n",
      "        [1.9043e-05],\n",
      "        [2.4470e-06],\n",
      "        [1.1175e-03],\n",
      "        [2.1664e-03],\n",
      "        [1.3684e-05],\n",
      "        [1.0334e-04],\n",
      "        [3.3061e-01],\n",
      "        [2.1278e-02],\n",
      "        [1.5196e-02],\n",
      "        [3.7576e-06],\n",
      "        [1.0467e-03]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.22826769948005676: \n",
      "func:cos_distance, ap_dist: -0.9703535437583923, an_dist: -0.7658171057701111\n",
      "list of metrics:  [165, 179, 170, 187]\n",
      "Better model found at epoch 28 with validation value: 0.8740000128746033.\n",
      "target probs tensor([[7.3647e-07],\n",
      "        [8.5420e-01],\n",
      "        [3.1026e-05],\n",
      "        [6.4870e-01],\n",
      "        [3.6424e-04],\n",
      "        [5.9559e-02],\n",
      "        [2.0085e-02],\n",
      "        [1.4819e-06],\n",
      "        [3.8701e-04],\n",
      "        [9.9903e-01],\n",
      "        [7.0433e-06],\n",
      "        [3.1145e-04],\n",
      "        [2.1208e-07],\n",
      "        [9.8447e-03],\n",
      "        [2.6461e-04],\n",
      "        [3.6964e-06]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.625043511390686: \n",
      "func:cos_distance, ap_dist: -0.8979586362838745, an_dist: -0.7698885202407837\n",
      "target probs tensor([[1.7708e-02],\n",
      "        [3.8715e-06],\n",
      "        [6.2447e-09],\n",
      "        [1.5269e-05],\n",
      "        [1.5090e-05],\n",
      "        [2.5085e-05],\n",
      "        [1.3694e-03],\n",
      "        [3.0789e-03],\n",
      "        [8.7700e-09],\n",
      "        [1.4069e-06],\n",
      "        [3.9751e-05],\n",
      "        [2.1645e-02],\n",
      "        [1.0703e-03],\n",
      "        [2.5617e-04],\n",
      "        [2.0061e-03],\n",
      "        [7.2742e-04]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.00302288425154984: \n",
      "func:cos_distance, ap_dist: -0.9653095602989197, an_dist: -0.7280404567718506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[1.0059e-05],\n",
      "        [8.7745e-06],\n",
      "        [1.3288e-01],\n",
      "        [3.1037e-03],\n",
      "        [2.1570e-04],\n",
      "        [3.1712e-02],\n",
      "        [1.4613e-08],\n",
      "        [9.1173e-04],\n",
      "        [2.2085e-04],\n",
      "        [4.2849e-02],\n",
      "        [2.4754e-01],\n",
      "        [8.0995e-04],\n",
      "        [7.0217e-08],\n",
      "        [7.0879e-06],\n",
      "        [1.8486e-02],\n",
      "        [5.3408e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.08066962659358978: \n",
      "func:cos_distance, ap_dist: -0.9976083040237427, an_dist: -0.723361611366272\n",
      "list of metrics:  [162, 166, 154, 163]\n",
      "target probs tensor([[6.5591e-01],\n",
      "        [2.9408e-05],\n",
      "        [8.9485e-04],\n",
      "        [1.6464e-02],\n",
      "        [1.0247e-02],\n",
      "        [2.6756e-08],\n",
      "        [9.5021e-09],\n",
      "        [3.8745e-03],\n",
      "        [1.2163e-04],\n",
      "        [5.2483e-09],\n",
      "        [9.9029e-01],\n",
      "        [2.3359e-05],\n",
      "        [8.7643e-04],\n",
      "        [5.4852e-05],\n",
      "        [3.7632e-05],\n",
      "        [7.9489e-12]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.35839712619781494: \n",
      "func:cos_distance, ap_dist: -0.9996322989463806, an_dist: -0.6217504739761353\n",
      "target probs tensor([[3.0830e-05],\n",
      "        [2.1654e-06],\n",
      "        [4.7126e-02],\n",
      "        [5.9270e-01],\n",
      "        [5.6521e-05],\n",
      "        [4.4120e-02],\n",
      "        [3.2014e-03],\n",
      "        [7.9390e-01],\n",
      "        [1.8500e-01],\n",
      "        [4.2272e-08],\n",
      "        [3.9163e-03],\n",
      "        [3.7915e-06],\n",
      "        [5.1519e-05],\n",
      "        [7.9882e-07],\n",
      "        [1.5448e-06],\n",
      "        [1.1063e-05]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.1739269495010376: \n",
      "func:cos_distance, ap_dist: -0.9978142976760864, an_dist: -0.5991721153259277\n",
      "target probs tensor([[2.7175e-02],\n",
      "        [1.0389e-02],\n",
      "        [2.2861e-08],\n",
      "        [1.1508e-07],\n",
      "        [3.1763e-06],\n",
      "        [4.2714e-06],\n",
      "        [1.1205e-05],\n",
      "        [2.6041e-01],\n",
      "        [6.4249e-05],\n",
      "        [2.4253e-05],\n",
      "        [2.9224e-05],\n",
      "        [4.0883e-05],\n",
      "        [3.5845e-01],\n",
      "        [2.9911e-04],\n",
      "        [1.4588e-03],\n",
      "        [1.7807e-02]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.05021416395902634: \n",
      "func:cos_distance, ap_dist: -0.9888500571250916, an_dist: -0.5881683826446533\n",
      "list of metrics:  [178, 155, 158, 162]\n",
      "target probs tensor([[9.2394e-01],\n",
      "        [7.3930e-03],\n",
      "        [1.0437e-08],\n",
      "        [9.9999e-01],\n",
      "        [1.8520e-03],\n",
      "        [9.9980e-01],\n",
      "        [2.3066e-07],\n",
      "        [3.4734e-09],\n",
      "        [4.3327e-07],\n",
      "        [1.8917e-03],\n",
      "        [7.4668e-01],\n",
      "        [3.7970e-05],\n",
      "        [1.1684e-02],\n",
      "        [1.2465e-08],\n",
      "        [1.6576e-04],\n",
      "        [1.9181e-02]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.4954944849014282: \n",
      "func:cos_distance, ap_dist: -0.9996457695960999, an_dist: -0.874237060546875\n",
      "target probs tensor([[2.0211e-01],\n",
      "        [1.3888e-02],\n",
      "        [2.3147e-05],\n",
      "        [1.5360e-03],\n",
      "        [1.0421e-03],\n",
      "        [9.2323e-06],\n",
      "        [1.3434e-04],\n",
      "        [1.0059e-06],\n",
      "        [9.8166e-01],\n",
      "        [1.6362e-01],\n",
      "        [6.8926e-04],\n",
      "        [6.8465e-12],\n",
      "        [7.7974e-01],\n",
      "        [9.7070e-01],\n",
      "        [5.6482e-05],\n",
      "        [2.2490e-04]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.5915111303329468: \n",
      "func:cos_distance, ap_dist: -0.9993342161178589, an_dist: -0.8285907506942749\n",
      "target probs tensor([[3.0564e-02],\n",
      "        [4.1704e-02],\n",
      "        [1.9468e-06],\n",
      "        [2.8811e-05],\n",
      "        [2.9000e-05],\n",
      "        [2.0414e-05],\n",
      "        [2.0292e-03],\n",
      "        [5.3052e-04],\n",
      "        [1.2138e-04],\n",
      "        [7.3515e-03],\n",
      "        [1.7372e-06],\n",
      "        [2.1335e-04],\n",
      "        [3.8043e-06],\n",
      "        [1.6033e-05],\n",
      "        [1.6891e-07],\n",
      "        [1.3672e-05]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.005251931492239237: \n",
      "func:cos_distance, ap_dist: -0.9931240677833557, an_dist: -0.7180085182189941\n",
      "target probs tensor([[1.0627e-07],\n",
      "        [1.6865e-06],\n",
      "        [2.2131e-07],\n",
      "        [5.8485e-03],\n",
      "        [1.1199e-03],\n",
      "        [1.5341e-04],\n",
      "        [2.0610e-06],\n",
      "        [3.5495e-07]], device='cuda:1'), loss: 0.0008930100593715906: \n",
      "func:cos_distance, ap_dist: -0.9981362819671631, an_dist: -0.6499569416046143\n",
      "list of metrics:  [154, 147, 155, 158]\n",
      "Better model found at epoch 31 with validation value: 0.8820000290870667.\n",
      "target probs tensor([[4.8757e-01],\n",
      "        [6.5697e-04],\n",
      "        [1.1481e-05],\n",
      "        [4.6848e-07],\n",
      "        [7.5504e-03],\n",
      "        [5.5741e-03],\n",
      "        [3.2494e-02],\n",
      "        [1.2858e-08],\n",
      "        [3.9970e-01],\n",
      "        [3.2700e-06],\n",
      "        [3.5602e-02],\n",
      "        [1.1195e-02],\n",
      "        [9.9749e-01],\n",
      "        [3.5083e-01],\n",
      "        [1.3400e-05],\n",
      "        [2.6368e-05]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.4807112514972687: \n",
      "func:cos_distance, ap_dist: -0.9667277336120605, an_dist: -0.8011859059333801\n",
      "target probs tensor([[1.7363e-06],\n",
      "        [1.9151e-05],\n",
      "        [6.3060e-05],\n",
      "        [3.8165e-01],\n",
      "        [1.3737e-01],\n",
      "        [3.7264e-01],\n",
      "        [1.3350e-06],\n",
      "        [8.1913e-08],\n",
      "        [3.3606e-04],\n",
      "        [4.2051e-09],\n",
      "        [2.7039e-03],\n",
      "        [6.6487e-01],\n",
      "        [1.7290e-02],\n",
      "        [1.7872e-07],\n",
      "        [1.7794e-07],\n",
      "        [4.5365e-05]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.13803407549858093: \n",
      "func:cos_distance, ap_dist: -0.9995989799499512, an_dist: -0.6948971748352051\n",
      "target probs tensor([[6.1748e-06],\n",
      "        [1.8667e-04],\n",
      "        [9.0127e-05],\n",
      "        [1.2229e-04],\n",
      "        [4.5131e-05],\n",
      "        [9.0171e-01],\n",
      "        [1.7951e-12],\n",
      "        [1.7940e-04],\n",
      "        [4.3531e-05],\n",
      "        [3.5401e-03],\n",
      "        [2.3623e-04],\n",
      "        [5.0888e-02],\n",
      "        [6.1765e-01],\n",
      "        [1.2933e-06],\n",
      "        [6.9591e-06],\n",
      "        [9.9504e-01]], device='cuda:1'), loss: 0.5402389168739319: \n",
      "func:cos_distance, ap_dist: -0.9996183514595032, an_dist: -0.76273512840271\n",
      "list of metrics:  [150, 149, 149, 143]\n",
      "fooling weight increased to 4.499999999999999 at the end of epoch 32\n",
      "target probs tensor([[1.6697e-05],\n",
      "        [3.5998e-05],\n",
      "        [2.0304e-04],\n",
      "        [1.9052e-01],\n",
      "        [1.2574e-05],\n",
      "        [2.2965e-09],\n",
      "        [2.0206e-01],\n",
      "        [2.5850e-05],\n",
      "        [2.4209e-01],\n",
      "        [9.1905e-01],\n",
      "        [3.8886e-10],\n",
      "        [1.8196e-03],\n",
      "        [1.4704e-02],\n",
      "        [1.7241e-05],\n",
      "        [7.3625e-04],\n",
      "        [3.4697e-05]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.20286624133586884: \n",
      "func:cos_distance, ap_dist: -0.9969762563705444, an_dist: -0.6001231670379639\n",
      "target probs tensor([[2.6872e-07],\n",
      "        [9.2115e-07],\n",
      "        [4.7663e-09],\n",
      "        [2.4357e-01],\n",
      "        [3.6623e-05],\n",
      "        [9.1753e-01],\n",
      "        [3.6453e-01],\n",
      "        [4.3094e-01],\n",
      "        [1.0267e-07],\n",
      "        [3.5750e-07],\n",
      "        [3.8228e-02],\n",
      "        [2.4191e-06],\n",
      "        [8.3039e-05],\n",
      "        [5.0964e-04],\n",
      "        [8.7512e-03],\n",
      "        [7.7940e-06]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.24000179767608643: \n",
      "func:cos_distance, ap_dist: -0.9961316585540771, an_dist: -0.5802936553955078\n",
      "target probs tensor([[8.5088e-05],\n",
      "        [1.7240e-01],\n",
      "        [2.8033e-04],\n",
      "        [1.1037e-03],\n",
      "        [6.0526e-03],\n",
      "        [1.6740e-05],\n",
      "        [1.5588e-03],\n",
      "        [5.1764e-06],\n",
      "        [7.8720e-01],\n",
      "        [5.6774e-04],\n",
      "        [4.4936e-03],\n",
      "        [5.8522e-04],\n",
      "        [7.7944e-05],\n",
      "        [3.7480e-05],\n",
      "        [4.4143e-03],\n",
      "        [2.0589e-10]], device='cuda:1'), loss: 0.10974577069282532: \n",
      "func:cos_distance, ap_dist: -0.9991148710250854, an_dist: -0.8650164604187012\n",
      "list of metrics:  [162, 151, 156, 153]\n",
      "target probs tensor([[5.0645e-11],\n",
      "        [2.6812e-05],\n",
      "        [5.0617e-03],\n",
      "        [4.6209e-08],\n",
      "        [1.3230e-02],\n",
      "        [1.2606e-03],\n",
      "        [7.4683e-03],\n",
      "        [9.1005e-08],\n",
      "        [3.1765e-01],\n",
      "        [1.7005e-07],\n",
      "        [2.1627e-03],\n",
      "        [9.6091e-04],\n",
      "        [6.2471e-10],\n",
      "        [1.4029e-04],\n",
      "        [2.1666e-01],\n",
      "        [2.0129e-10]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.041053030639886856: \n",
      "func:cos_distance, ap_dist: -0.9995589852333069, an_dist: -0.7239624857902527\n",
      "target probs tensor([[1.9383e-03],\n",
      "        [2.5599e-03],\n",
      "        [5.0443e-03],\n",
      "        [3.3413e-07],\n",
      "        [1.2966e-01],\n",
      "        [1.3155e-01],\n",
      "        [6.9291e-06],\n",
      "        [8.8742e-07],\n",
      "        [6.9163e-05],\n",
      "        [1.1845e-01],\n",
      "        [1.4675e-05],\n",
      "        [7.1811e-03],\n",
      "        [1.0008e-05],\n",
      "        [3.2642e-03],\n",
      "        [1.3563e-04],\n",
      "        [3.1099e-04]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.02666083350777626: \n",
      "func:cos_distance, ap_dist: -0.9700852632522583, an_dist: -0.7338523268699646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[1.8554e-02],\n",
      "        [2.5929e-06],\n",
      "        [1.0708e-08],\n",
      "        [1.8835e-02],\n",
      "        [2.3189e-08],\n",
      "        [5.5402e-05],\n",
      "        [6.6951e-02],\n",
      "        [1.2525e-07],\n",
      "        [8.1022e-05],\n",
      "        [2.4023e-04],\n",
      "        [7.2504e-05],\n",
      "        [4.9972e-01],\n",
      "        [7.5661e-01],\n",
      "        [1.8794e-03],\n",
      "        [1.3770e-01],\n",
      "        [1.2424e-04]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.14770668745040894: \n",
      "func:cos_distance, ap_dist: -0.9981852173805237, an_dist: -0.7132474184036255\n",
      "list of metrics:  [144, 151, 143, 146]\n",
      "target probs tensor([[6.0450e-07],\n",
      "        [1.3280e-01],\n",
      "        [1.4457e-01],\n",
      "        [1.1970e-03],\n",
      "        [9.9739e-07],\n",
      "        [5.2394e-01],\n",
      "        [6.8548e-07],\n",
      "        [2.3459e-03],\n",
      "        [2.1852e-07],\n",
      "        [9.8774e-06],\n",
      "        [3.0487e-05],\n",
      "        [7.4141e-05],\n",
      "        [3.3630e-02],\n",
      "        [2.9620e-02],\n",
      "        [3.4243e-02],\n",
      "        [2.9020e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.09290079772472382: \n",
      "func:cos_distance, ap_dist: -0.9994043707847595, an_dist: -0.785459041595459\n",
      "target probs tensor([[6.9660e-01],\n",
      "        [3.8378e-06],\n",
      "        [1.8357e-07],\n",
      "        [8.9731e-06],\n",
      "        [1.5645e-07],\n",
      "        [4.3335e-08],\n",
      "        [7.0811e-03],\n",
      "        [4.4215e-08],\n",
      "        [5.5375e-01],\n",
      "        [1.1018e-05],\n",
      "        [5.7639e-04],\n",
      "        [8.2805e-08],\n",
      "        [5.6742e-03],\n",
      "        [6.8810e-10],\n",
      "        [5.2633e-06],\n",
      "        [4.4585e-03]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.12609010934829712: \n",
      "func:cos_distance, ap_dist: -0.9997055530548096, an_dist: -0.5463960766792297\n",
      "target probs tensor([[2.2563e-03],\n",
      "        [1.6154e-01],\n",
      "        [3.5050e-03],\n",
      "        [1.6602e-03],\n",
      "        [1.9760e-03],\n",
      "        [9.1245e-02],\n",
      "        [2.6034e-02],\n",
      "        [2.3491e-02],\n",
      "        [3.5551e-01],\n",
      "        [6.7865e-01],\n",
      "        [3.1513e-02],\n",
      "        [9.0764e-04],\n",
      "        [2.1847e-07],\n",
      "        [2.1159e-01],\n",
      "        [4.1902e-06],\n",
      "        [2.2375e-05]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.13604003190994263: \n",
      "func:cos_distance, ap_dist: -0.9879167079925537, an_dist: -0.5155337452888489\n",
      "list of metrics:  [138, 151, 135, 139]\n",
      "Better model found at epoch 35 with validation value: 0.8859999775886536.\n",
      "target probs tensor([[6.2825e-07],\n",
      "        [9.9834e-06],\n",
      "        [3.4525e-04],\n",
      "        [6.2865e-08],\n",
      "        [5.9616e-01],\n",
      "        [1.0279e-05],\n",
      "        [6.9731e-03],\n",
      "        [3.2391e-04],\n",
      "        [5.7501e-03],\n",
      "        [1.1215e-04],\n",
      "        [5.0669e-05],\n",
      "        [3.5998e-08],\n",
      "        [1.6819e-05],\n",
      "        [5.3887e-04],\n",
      "        [4.7166e-08],\n",
      "        [1.3210e-03]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.057640012353658676: \n",
      "func:cos_distance, ap_dist: -0.9886517524719238, an_dist: -0.715575098991394\n",
      "target probs tensor([[5.3694e-02],\n",
      "        [4.8736e-07],\n",
      "        [9.0319e-04],\n",
      "        [1.5048e-02],\n",
      "        [8.4093e-08],\n",
      "        [1.0662e-06],\n",
      "        [7.9079e-06],\n",
      "        [4.4312e-06],\n",
      "        [2.3486e-05],\n",
      "        [8.3465e-01],\n",
      "        [1.6073e-05],\n",
      "        [4.8241e-09],\n",
      "        [4.4214e-01],\n",
      "        [5.5272e-06],\n",
      "        [3.0111e-05],\n",
      "        [1.6111e-02]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.15443438291549683: \n",
      "func:cos_distance, ap_dist: -0.9973077774047852, an_dist: -0.7651722431182861\n",
      "target probs tensor([[3.7694e-09],\n",
      "        [1.2960e-03],\n",
      "        [1.5586e-06],\n",
      "        [5.4625e-06],\n",
      "        [6.2314e-05],\n",
      "        [1.0863e-11],\n",
      "        [6.8163e-02],\n",
      "        [9.8163e-01],\n",
      "        [1.3581e-01],\n",
      "        [1.1036e-04],\n",
      "        [8.0671e-03],\n",
      "        [3.9898e-05],\n",
      "        [1.5579e-07],\n",
      "        [8.2258e-05],\n",
      "        [3.6569e-04],\n",
      "        [1.2493e-05]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.26397061347961426: \n",
      "func:cos_distance, ap_dist: -0.9803320169448853, an_dist: -0.71876060962677\n",
      "list of metrics:  [121, 132, 126, 139]\n",
      "fooling weight increased to 4.799999999999999 at the end of epoch 36\n",
      "target probs tensor([[8.3102e-04],\n",
      "        [7.9128e-10],\n",
      "        [8.6535e-01],\n",
      "        [1.7950e-04],\n",
      "        [4.3523e-08],\n",
      "        [3.8789e-05],\n",
      "        [5.2071e-08],\n",
      "        [1.4746e-06],\n",
      "        [3.0771e-06],\n",
      "        [4.0688e-06],\n",
      "        [7.8348e-07],\n",
      "        [3.6734e-04],\n",
      "        [8.1938e-07],\n",
      "        [6.5729e-08],\n",
      "        [4.9389e-04],\n",
      "        [1.0672e-08]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.12543930113315582: \n",
      "func:cos_distance, ap_dist: -0.9948168396949768, an_dist: -0.7828341722488403\n",
      "target probs tensor([[8.0842e-05],\n",
      "        [9.5395e-05],\n",
      "        [5.6978e-02],\n",
      "        [1.5640e-08],\n",
      "        [1.1383e-04],\n",
      "        [9.2915e-01],\n",
      "        [2.5490e-04],\n",
      "        [8.7373e-05],\n",
      "        [8.6717e-01],\n",
      "        [5.4413e-04],\n",
      "        [1.6821e-06],\n",
      "        [6.3132e-02],\n",
      "        [2.5344e-02],\n",
      "        [3.0242e-06],\n",
      "        [9.9980e-01],\n",
      "        [3.5121e-05]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.8328895568847656: \n",
      "func:cos_distance, ap_dist: -0.9836882948875427, an_dist: -0.8002955913543701\n",
      "target probs tensor([[1.0483e-05],\n",
      "        [3.2826e-03],\n",
      "        [1.0091e-02],\n",
      "        [9.9453e-01],\n",
      "        [9.0783e-02],\n",
      "        [1.8438e-06],\n",
      "        [7.4857e-03],\n",
      "        [2.9324e-05],\n",
      "        [1.0398e-06],\n",
      "        [7.9568e-01],\n",
      "        [6.4471e-01],\n",
      "        [2.2778e-04],\n",
      "        [3.3129e-02],\n",
      "        [6.6227e-02],\n",
      "        [4.4056e-03],\n",
      "        [9.9392e-06]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.5033633708953857: \n",
      "func:cos_distance, ap_dist: -0.9988870620727539, an_dist: -0.639259397983551\n",
      "list of metrics:  [142, 131, 136, 130]\n",
      "Better model found at epoch 37 with validation value: 0.8939999938011169.\n",
      "target probs tensor([[8.8894e-03],\n",
      "        [1.4353e-01],\n",
      "        [1.5466e-10],\n",
      "        [6.4340e-03],\n",
      "        [2.7114e-01],\n",
      "        [6.5859e-01],\n",
      "        [1.7299e-06],\n",
      "        [1.7222e-04],\n",
      "        [1.1601e-01],\n",
      "        [7.4909e-06],\n",
      "        [1.0225e-05],\n",
      "        [1.1872e-03],\n",
      "        [5.1683e-07],\n",
      "        [1.2907e-03],\n",
      "        [4.3585e-08],\n",
      "        [3.7414e-02]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.10783621668815613: \n",
      "func:cos_distance, ap_dist: -0.9980267286300659, an_dist: -0.6713176965713501\n",
      "target probs tensor([[1.2160e-08],\n",
      "        [4.4603e-03],\n",
      "        [1.4252e-04],\n",
      "        [2.8917e-03],\n",
      "        [3.9499e-01],\n",
      "        [7.2231e-07],\n",
      "        [1.0793e-08],\n",
      "        [1.3520e-10],\n",
      "        [2.8827e-07],\n",
      "        [6.5813e-09],\n",
      "        [4.9660e-08],\n",
      "        [7.8560e-07],\n",
      "        [1.3552e-06],\n",
      "        [1.4801e-03],\n",
      "        [9.3867e-08],\n",
      "        [3.0295e-04]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.03198838606476784: \n",
      "func:cos_distance, ap_dist: -0.9985012412071228, an_dist: -0.8765197992324829\n",
      "target probs tensor([[1.7658e-04],\n",
      "        [1.2159e-02],\n",
      "        [9.8668e-08],\n",
      "        [7.5468e-03],\n",
      "        [1.7113e-01],\n",
      "        [1.1423e-05],\n",
      "        [8.5543e-06],\n",
      "        [9.5635e-07],\n",
      "        [8.1492e-03],\n",
      "        [4.8578e-01],\n",
      "        [2.2542e-05],\n",
      "        [5.4715e-04],\n",
      "        [3.7436e-04],\n",
      "        [6.2730e-04],\n",
      "        [1.3138e-01],\n",
      "        [7.2864e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.14548279345035553: \n",
      "func:cos_distance, ap_dist: -0.9986825585365295, an_dist: -0.8394762873649597\n",
      "list of metrics:  [132, 141, 137, 136]\n",
      "target probs tensor([[7.6863e-03],\n",
      "        [5.2997e-02],\n",
      "        [2.3357e-06],\n",
      "        [9.9923e-01],\n",
      "        [1.8348e-02],\n",
      "        [3.6208e-03],\n",
      "        [1.1876e-04],\n",
      "        [5.3302e-02],\n",
      "        [2.8194e-03],\n",
      "        [2.4990e-01],\n",
      "        [1.0016e-04],\n",
      "        [5.5778e-04],\n",
      "        [3.6999e-04],\n",
      "        [3.3329e-04],\n",
      "        [3.4278e-01],\n",
      "        [3.7438e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.5309020280838013: \n",
      "func:cos_distance, ap_dist: -0.9916296601295471, an_dist: -0.7265685796737671\n",
      "target probs tensor([[5.1753e-11],\n",
      "        [1.8295e-03],\n",
      "        [6.6461e-14],\n",
      "        [3.0736e-05],\n",
      "        [1.4137e-04],\n",
      "        [2.4787e-01],\n",
      "        [9.9502e-01],\n",
      "        [1.3291e-07],\n",
      "        [2.9274e-05],\n",
      "        [3.1886e-04],\n",
      "        [1.3569e-01],\n",
      "        [1.2181e-02],\n",
      "        [5.7275e-07],\n",
      "        [3.1038e-13],\n",
      "        [2.6316e-02],\n",
      "        [9.3713e-02]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.3670309782028198: \n",
      "func:cos_distance, ap_dist: -0.9931710958480835, an_dist: -0.7709710001945496\n",
      "target probs tensor([[7.0133e-10],\n",
      "        [7.3285e-03],\n",
      "        [1.0264e-02],\n",
      "        [1.6723e-08],\n",
      "        [4.2464e-04],\n",
      "        [4.0635e-06],\n",
      "        [1.1471e-06],\n",
      "        [5.6166e-02],\n",
      "        [1.4710e-03],\n",
      "        [2.5790e-03],\n",
      "        [6.8182e-11],\n",
      "        [5.3198e-03],\n",
      "        [2.5477e-06],\n",
      "        [3.3487e-02],\n",
      "        [4.3998e-05],\n",
      "        [5.2234e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.05364105477929115: \n",
      "func:cos_distance, ap_dist: -0.964669942855835, an_dist: -0.6919647455215454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[1.3772e-06],\n",
      "        [9.3070e-07],\n",
      "        [4.9041e-06],\n",
      "        [5.0894e-03],\n",
      "        [5.1311e-04],\n",
      "        [5.6855e-04],\n",
      "        [4.7419e-08],\n",
      "        [6.0391e-07]], device='cuda:1'), loss: 0.0007740329601801932: \n",
      "func:cos_distance, ap_dist: -0.99930739402771, an_dist: -0.6594560742378235\n",
      "list of metrics:  [133, 144, 138, 131]\n",
      "fooling weight increased to 5.099999999999999 at the end of epoch 39\n",
      "target probs tensor([[6.2108e-04],\n",
      "        [8.6823e-07],\n",
      "        [1.2066e-04],\n",
      "        [4.9274e-01],\n",
      "        [8.6290e-01],\n",
      "        [6.8357e-03],\n",
      "        [2.3681e-06],\n",
      "        [2.0682e-07],\n",
      "        [1.1583e-06],\n",
      "        [7.8797e-08],\n",
      "        [9.1636e-01],\n",
      "        [2.0556e-02],\n",
      "        [1.1024e-04],\n",
      "        [2.8773e-07],\n",
      "        [9.5505e-02],\n",
      "        [6.2303e-06]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.3297390639781952: \n",
      "func:cos_distance, ap_dist: -0.9990927577018738, an_dist: -0.805485188961029\n",
      "target probs tensor([[9.5313e-08],\n",
      "        [1.9883e-05],\n",
      "        [1.0941e-07],\n",
      "        [1.2015e-06],\n",
      "        [3.7862e-05],\n",
      "        [7.2417e-04],\n",
      "        [2.4471e-08],\n",
      "        [1.4002e-05],\n",
      "        [6.0065e-05],\n",
      "        [6.5417e-06],\n",
      "        [4.6137e-09],\n",
      "        [8.7632e-01],\n",
      "        [4.1236e-07],\n",
      "        [8.3601e-01],\n",
      "        [7.6059e-06],\n",
      "        [3.0213e-06]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.24368233978748322: \n",
      "func:cos_distance, ap_dist: -0.9987026453018188, an_dist: -0.811549723148346\n",
      "target probs tensor([[2.9760e-06],\n",
      "        [1.1955e-04],\n",
      "        [8.5958e-04],\n",
      "        [5.9326e-04],\n",
      "        [8.9667e-04],\n",
      "        [9.3859e-01],\n",
      "        [1.5620e-13],\n",
      "        [3.0093e-03],\n",
      "        [1.6383e-04],\n",
      "        [3.2227e-03],\n",
      "        [6.3062e-05],\n",
      "        [6.9193e-03],\n",
      "        [4.4447e-03],\n",
      "        [3.6792e-06],\n",
      "        [1.9574e-07],\n",
      "        [9.9546e-01]], device='cuda:1'), loss: 0.5127915143966675: \n",
      "func:cos_distance, ap_dist: -0.9883061051368713, an_dist: -0.7853947877883911\n",
      "list of metrics:  [125, 135, 148, 137]\n",
      "fooling weight increased to 5.399999999999999 at the end of epoch 40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-1d0ca4603c08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# learn.fit(30, lr=1e-2, wd = 0., callbacks=[saver_best, saver_every_epoch])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msaver_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver_every_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfooling_weight_scheduler\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# # learn.fit(70, lr=1e-02, wd = 0.001, callbacks=[saver_best, saver_every_epoch])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_loss_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-52c5b60f90f6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_triplet_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mz_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mp_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-52c5b60f90f6>\u001b[0m in \u001b[0;36mforward_z\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mh5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomized_deconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meighth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m56\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCT2d_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m112\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mh6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomized_deconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meighth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m112\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCT2d_6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mh7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomized_deconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meighth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCT2d_7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mksi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-52c5b60f90f6>\u001b[0m in \u001b[0;36mrandomized_deconv_layer\u001b[0;34m(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrandomized_deconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_size_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_size_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeconv_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_output_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mh_input_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_size_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_size_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_size_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mh_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_input_z\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-52c5b60f90f6>\u001b[0m in \u001b[0;36mmake_z\u001b[0;34m(self, in_shape)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmake_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# RUN SITE\n",
    "\n",
    "if mode == \"sanity_check\":\n",
    "  print(\"\\n\\n\\nWARNING: you are training on a sanity_check dataset.\\n\\n\\n\\n\")\n",
    "\n",
    "saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=env.save_filename + \"-best\")\n",
    "saver_every_epoch = SaveModelCallback(learn, every='epoch', name=env.save_filename)\n",
    "fooling_weight_scheduler = FoolingWeightScheduler(learn, fooling_loss_index = 3)\n",
    "\n",
    "# with Hooks(gen, append_stats_normal) as hooks:\n",
    "#   learn.fit(1, lr=5e-03, wd = 0., callbacks=[saver_best, saver_every_epoch])\n",
    "  \n",
    "# learn.fit(30, lr=1e-2, wd = 0., callbacks=[saver_best, saver_every_epoch])\n",
    "learn.fit(80, lr=1e-2, wd = 0., callbacks=[saver_best, saver_every_epoch, fooling_weight_scheduler])\n",
    "\n",
    "# # learn.fit(70, lr=1e-02, wd = 0.001, callbacks=[saver_best, saver_every_epoch])\n",
    "\n",
    "# learn.fit(60, lr=1e-2, wd = 0.001, callbacks=[saver_best, saver_every_epoch])\n",
    "\n",
    "# for i in range(10):\n",
    "#   learn.fit_one_cycle(7, wd = 0.,max_lr=1., div_factor = 1000.) \n",
    "  \n",
    "# learn.fit_one_cycle(5, max_lr=2e-2) #used for vgg-19-bn\n",
    "# learn.fit_one_cycle(5, max_lr=3e-3) # used for resnet50\n",
    "\n",
    "shutil.copyfile(env.temp_csv_path + '/' + env.save_filename + \".csv\", env.get_csv_path() + '.csv')\n",
    "shutil.copytree(env.data_path/env.get_learner_models_dir(), env.get_models_path())\n",
    "shutil.rmtree(env.data_path/env.get_learner_models_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram\n",
    "fig, axes = plt.subplots(len(hooks),1, figsize=(30,12))\n",
    "for ax,h in zip(axes.flatten(), hooks):\n",
    "  ax.imshow(get_hist(h), origin='lower')\n",
    "  ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mean and std\n",
    "fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "for h in hooks:\n",
    "  ms, ss, _ = h.stats\n",
    "  ax0.plot(ms[:100])\n",
    "  ax1.plot(ss[:100])\n",
    "plt.legend(range(len(hooks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "for h in hooks:\n",
    "  ms, ss, _ = h.stats\n",
    "  ax0.plot(ms)\n",
    "  ax1.plot(ss)\n",
    "plt.legend(range(len(hooks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero precentage:\n",
    "fig,axes = plt.subplots(len(hooks),1, figsize=(30,30))\n",
    "for ax,h in zip(axes.flatten(), hooks):\n",
    "    ax.plot(get_min(h))\n",
    "    ax.set_ylim(0,1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MO2fZ-hSSUzJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# z1 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "# z2 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "z1 = torch.tensor([0.8, -0.5] * 5).cuda()\n",
    "z2 = torch.tensor([-1.] * 10).cuda()\n",
    "print(\"z1: \", z1)\n",
    "print(\"z2: \", z2)\n",
    "print(\"distance: \", torch.norm(z1-z2,p=2))\n",
    "model = learn.model.eval()\n",
    "\n",
    "z_s = interpolate(z1, z2, 0.1)\n",
    "print(len(z_s))\n",
    "\n",
    "for i,z in enumerate(z_s):\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n",
    "  #img.save('./pics/' + str(i) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea : have 200 noises (1 for each class), then start iterating the dataset, and for each image, randomly apply one noise and record the result\n",
    "def targeted_diversity(learn, n_perturbations = 200, percentage = 95):\n",
    "  model = learn.model.eval()\n",
    "\n",
    "  one_hot_conditions = [torch.empty(z_dim).uniform_(0,1).cuda().detach() for _ in range(n_perturbations)]\n",
    "#   for i in range(z_dim):\n",
    "#     one_hot_conditions[i][i] = 1.\n",
    "\n",
    "  perturbations = [model.forward_single_z(z) for z in one_hot_conditions]\n",
    "\n",
    "  hist = [0.] * z_dim\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 : print(\"at batch_no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbations[np.random.randint(0,len(perturbations))][None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      hist[pred] += 1\n",
    "\n",
    "  pred_histogram_sum = np.sum(hist)\n",
    "  indexed_pred_histogram = [(i, hist_element) for i,hist_element in  \n",
    "                            enumerate(hist)]\n",
    "\n",
    "  indexed_pred_histogram.sort(key=lambda x: x[1], reverse = True)\n",
    "\n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = indexed_pred_histogram[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / pred_histogram_sum) * 100.\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, indexed_pred_histogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 5\n",
      "at batch no 10\n",
      "at batch no 15\n",
      "at batch no 20\n",
      "at batch no 25\n",
      "at batch no 30\n",
      "at batch no 35\n",
      "at batch no 40\n",
      "at batch no 45\n",
      "at batch no 50\n",
      "at batch no 55\n",
      "at batch no 60\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(364,\n",
       " [(721, 143.20),\n",
       "  (750, 49.40),\n",
       "  (971, 48.40),\n",
       "  (794, 47.10),\n",
       "  (431, 35.10),\n",
       "  (669, 35.10),\n",
       "  (414, 30.60),\n",
       "  (588, 28.40),\n",
       "  (520, 24.90),\n",
       "  (61, 24.20),\n",
       "  (904, 18.30),\n",
       "  (411, 14.30),\n",
       "  (828, 13.50),\n",
       "  (39, 11.70),\n",
       "  (556, 11.40),\n",
       "  (581, 9.20),\n",
       "  (651, 8.30),\n",
       "  (489, 7.80),\n",
       "  (599, 6.60),\n",
       "  (84, 5.50),\n",
       "  (572, 4.80),\n",
       "  (907, 4.70),\n",
       "  (987, 4.40),\n",
       "  (401, 4.30),\n",
       "  (490, 4.30),\n",
       "  (614, 4.30),\n",
       "  (60, 4.00),\n",
       "  (955, 3.50),\n",
       "  (711, 3.30),\n",
       "  (48, 3.20),\n",
       "  (691, 3.10),\n",
       "  (709, 3.00),\n",
       "  (419, 2.90),\n",
       "  (770, 2.90),\n",
       "  (815, 2.90),\n",
       "  (864, 2.80),\n",
       "  (879, 2.80),\n",
       "  (108, 2.70),\n",
       "  (441, 2.70),\n",
       "  (632, 2.70),\n",
       "  (56, 2.50),\n",
       "  (604, 2.50),\n",
       "  (55, 2.40),\n",
       "  (464, 2.30),\n",
       "  (549, 2.30),\n",
       "  (575, 2.20),\n",
       "  (893, 2.20),\n",
       "  (973, 2.20),\n",
       "  (96, 2.10),\n",
       "  (496, 2.10),\n",
       "  (518, 2.10),\n",
       "  (412, 2.00),\n",
       "  (641, 2.00),\n",
       "  (762, 2.00),\n",
       "  (801, 2.00),\n",
       "  (872, 2.00),\n",
       "  (858, 1.90),\n",
       "  (871, 1.90),\n",
       "  (580, 1.80),\n",
       "  (621, 1.80),\n",
       "  (746, 1.80),\n",
       "  (806, 1.80),\n",
       "  (46, 1.70),\n",
       "  (151, 1.70),\n",
       "  (171, 1.70),\n",
       "  (633, 1.70),\n",
       "  (781, 1.70),\n",
       "  (790, 1.70),\n",
       "  (850, 1.70),\n",
       "  (424, 1.60),\n",
       "  (443, 1.60),\n",
       "  (453, 1.60),\n",
       "  (646, 1.60),\n",
       "  (981, 1.60),\n",
       "  (128, 1.50),\n",
       "  (360, 1.50),\n",
       "  (457, 1.50),\n",
       "  (545, 1.50),\n",
       "  (680, 1.50),\n",
       "  (722, 1.50),\n",
       "  (743, 1.50),\n",
       "  (1, 1.40),\n",
       "  (94, 1.40),\n",
       "  (230, 1.40),\n",
       "  (292, 1.40),\n",
       "  (406, 1.40),\n",
       "  (410, 1.40),\n",
       "  (440, 1.40),\n",
       "  (506, 1.40),\n",
       "  (847, 1.40),\n",
       "  (897, 1.40),\n",
       "  (67, 1.30),\n",
       "  (68, 1.30),\n",
       "  (124, 1.30),\n",
       "  (417, 1.30),\n",
       "  (538, 1.30),\n",
       "  (706, 1.30),\n",
       "  (779, 1.30),\n",
       "  (791, 1.30),\n",
       "  (826, 1.30),\n",
       "  (865, 1.30),\n",
       "  (898, 1.30),\n",
       "  (937, 1.30),\n",
       "  (953, 1.30),\n",
       "  (242, 1.20),\n",
       "  (310, 1.20),\n",
       "  (408, 1.20),\n",
       "  (582, 1.20),\n",
       "  (591, 1.20),\n",
       "  (698, 1.20),\n",
       "  (857, 1.20),\n",
       "  (896, 1.20),\n",
       "  (963, 1.20),\n",
       "  (83, 1.10),\n",
       "  (90, 1.10),\n",
       "  (92, 1.10),\n",
       "  (123, 1.10),\n",
       "  (293, 1.10),\n",
       "  (300, 1.10),\n",
       "  (316, 1.10),\n",
       "  (393, 1.10),\n",
       "  (407, 1.10),\n",
       "  (468, 1.10),\n",
       "  (612, 1.10),\n",
       "  (619, 1.10),\n",
       "  (645, 1.10),\n",
       "  (656, 1.10),\n",
       "  (738, 1.10),\n",
       "  (783, 1.10),\n",
       "  (805, 1.10),\n",
       "  (817, 1.10),\n",
       "  (906, 1.10),\n",
       "  (7, 1.00),\n",
       "  (25, 1.00),\n",
       "  (31, 1.00),\n",
       "  (33, 1.00),\n",
       "  (37, 1.00),\n",
       "  (57, 1.00),\n",
       "  (75, 1.00),\n",
       "  (88, 1.00),\n",
       "  (91, 1.00),\n",
       "  (102, 1.00),\n",
       "  (115, 1.00),\n",
       "  (120, 1.00),\n",
       "  (163, 1.00),\n",
       "  (176, 1.00),\n",
       "  (186, 1.00),\n",
       "  (195, 1.00),\n",
       "  (218, 1.00),\n",
       "  (231, 1.00),\n",
       "  (247, 1.00),\n",
       "  (260, 1.00),\n",
       "  (274, 1.00),\n",
       "  (281, 1.00),\n",
       "  (290, 1.00),\n",
       "  (306, 1.00),\n",
       "  (307, 1.00),\n",
       "  (314, 1.00),\n",
       "  (318, 1.00),\n",
       "  (334, 1.00),\n",
       "  (376, 1.00),\n",
       "  (392, 1.00),\n",
       "  (429, 1.00),\n",
       "  (474, 1.00),\n",
       "  (483, 1.00),\n",
       "  (488, 1.00),\n",
       "  (492, 1.00),\n",
       "  (498, 1.00),\n",
       "  (507, 1.00),\n",
       "  (516, 1.00),\n",
       "  (528, 1.00),\n",
       "  (530, 1.00),\n",
       "  (533, 1.00),\n",
       "  (562, 1.00),\n",
       "  (566, 1.00),\n",
       "  (602, 1.00),\n",
       "  (625, 1.00),\n",
       "  (637, 1.00),\n",
       "  (661, 1.00),\n",
       "  (684, 1.00),\n",
       "  (694, 1.00),\n",
       "  (716, 1.00),\n",
       "  (725, 1.00),\n",
       "  (734, 1.00),\n",
       "  (787, 1.00),\n",
       "  (816, 1.00),\n",
       "  (822, 1.00),\n",
       "  (824, 1.00),\n",
       "  (837, 1.00),\n",
       "  (873, 1.00),\n",
       "  (923, 1.00),\n",
       "  (934, 1.00),\n",
       "  (944, 1.00),\n",
       "  (992, 1.00),\n",
       "  (997, 1.00),\n",
       "  (40, 0.90),\n",
       "  (62, 0.90),\n",
       "  (105, 0.90),\n",
       "  (164, 0.90),\n",
       "  (301, 0.90),\n",
       "  (347, 0.90),\n",
       "  (369, 0.90),\n",
       "  (397, 0.90),\n",
       "  (425, 0.90),\n",
       "  (444, 0.90),\n",
       "  (753, 0.90),\n",
       "  (819, 0.90),\n",
       "  (868, 0.90),\n",
       "  (889, 0.90),\n",
       "  (982, 0.90),\n",
       "  (47, 0.80),\n",
       "  (72, 0.80),\n",
       "  (86, 0.80),\n",
       "  (97, 0.80),\n",
       "  (118, 0.80),\n",
       "  (254, 0.80),\n",
       "  (275, 0.80),\n",
       "  (304, 0.80),\n",
       "  (331, 0.80),\n",
       "  (463, 0.80),\n",
       "  (509, 0.80),\n",
       "  (532, 0.80),\n",
       "  (576, 0.80),\n",
       "  (586, 0.80),\n",
       "  (606, 0.80),\n",
       "  (638, 0.80),\n",
       "  (703, 0.80),\n",
       "  (732, 0.80),\n",
       "  (772, 0.80),\n",
       "  (786, 0.80),\n",
       "  (803, 0.80),\n",
       "  (829, 0.80),\n",
       "  (878, 0.80),\n",
       "  (899, 0.80),\n",
       "  (905, 0.80),\n",
       "  (915, 0.80),\n",
       "  (984, 0.80),\n",
       "  (8, 0.70),\n",
       "  (15, 0.70),\n",
       "  (38, 0.70),\n",
       "  (42, 0.70),\n",
       "  (109, 0.70),\n",
       "  (116, 0.70),\n",
       "  (144, 0.70),\n",
       "  (155, 0.70),\n",
       "  (189, 0.70),\n",
       "  (235, 0.70),\n",
       "  (313, 0.70),\n",
       "  (341, 0.70),\n",
       "  (353, 0.70),\n",
       "  (355, 0.70),\n",
       "  (387, 0.70),\n",
       "  (438, 0.70),\n",
       "  (476, 0.70),\n",
       "  (497, 0.70),\n",
       "  (508, 0.70),\n",
       "  (517, 0.70),\n",
       "  (526, 0.70),\n",
       "  (547, 0.70),\n",
       "  (552, 0.70),\n",
       "  (564, 0.70),\n",
       "  (570, 0.70),\n",
       "  (579, 0.70),\n",
       "  (620, 0.70),\n",
       "  (629, 0.70),\n",
       "  (727, 0.70),\n",
       "  (741, 0.70),\n",
       "  (752, 0.70),\n",
       "  (778, 0.70),\n",
       "  (788, 0.70),\n",
       "  (814, 0.70),\n",
       "  (925, 0.70),\n",
       "  (17, 0.60),\n",
       "  (19, 0.60),\n",
       "  (87, 0.60),\n",
       "  (289, 0.60),\n",
       "  (291, 0.60),\n",
       "  (294, 0.60),\n",
       "  (327, 0.60),\n",
       "  (375, 0.60),\n",
       "  (398, 0.60),\n",
       "  (409, 0.60),\n",
       "  (433, 0.60),\n",
       "  (445, 0.60),\n",
       "  (472, 0.60),\n",
       "  (482, 0.60),\n",
       "  (535, 0.60),\n",
       "  (544, 0.60),\n",
       "  (565, 0.60),\n",
       "  (671, 0.60),\n",
       "  (679, 0.60),\n",
       "  (696, 0.60),\n",
       "  (701, 0.60),\n",
       "  (751, 0.60),\n",
       "  (760, 0.60),\n",
       "  (796, 0.60),\n",
       "  (797, 0.60),\n",
       "  (820, 0.60),\n",
       "  (867, 0.60),\n",
       "  (892, 0.60),\n",
       "  (902, 0.60),\n",
       "  (920, 0.60),\n",
       "  (998, 0.60),\n",
       "  (0, 0.50),\n",
       "  (45, 0.50),\n",
       "  (50, 0.50),\n",
       "  (52, 0.50),\n",
       "  (107, 0.50),\n",
       "  (149, 0.50),\n",
       "  (159, 0.50),\n",
       "  (336, 0.50),\n",
       "  (342, 0.50),\n",
       "  (348, 0.50),\n",
       "  (391, 0.50),\n",
       "  (495, 0.50),\n",
       "  (515, 0.50),\n",
       "  (523, 0.50),\n",
       "  (527, 0.50),\n",
       "  (539, 0.50),\n",
       "  (555, 0.50),\n",
       "  (605, 0.50),\n",
       "  (607, 0.50),\n",
       "  (609, 0.50),\n",
       "  (626, 0.50),\n",
       "  (654, 0.50),\n",
       "  (655, 0.50),\n",
       "  (664, 0.50),\n",
       "  (674, 0.50),\n",
       "  (705, 0.50),\n",
       "  (719, 0.50),\n",
       "  (745, 0.50),\n",
       "  (754, 0.50),\n",
       "  (759, 0.50),\n",
       "  (768, 0.50),\n",
       "  (823, 0.50),\n",
       "  (853, 0.50),\n",
       "  (900, 0.50),\n",
       "  (917, 0.50),\n",
       "  (985, 0.50),\n",
       "  (9, 0.40),\n",
       "  (23, 0.40),\n",
       "  (24, 0.40),\n",
       "  (28, 0.40),\n",
       "  (63, 0.40),\n",
       "  (77, 0.40),\n",
       "  (126, 0.40),\n",
       "  (134, 0.40),\n",
       "  (140, 0.40),\n",
       "  (168, 0.40),\n",
       "  (192, 0.40),\n",
       "  (197, 0.40),\n",
       "  (205, 0.40),\n",
       "  (219, 0.40),\n",
       "  (224, 0.40),\n",
       "  (236, 0.40),\n",
       "  (249, 0.40),\n",
       "  (305, 0.40),\n",
       "  (319, 0.40),\n",
       "  (321, 0.40),\n",
       "  (332, 0.40),\n",
       "  (363, 0.40),\n",
       "  (381, 0.40),\n",
       "  (383, 0.40),\n",
       "  (388, 0.40),\n",
       "  (389, 0.40),\n",
       "  (396, 0.40),\n",
       "  (473, 0.40),\n",
       "  (481, 0.40),\n",
       "  (491, 0.40),\n",
       "  (522, 0.40),\n",
       "  (546, 0.40),\n",
       "  (574, 0.40),\n",
       "  (584, 0.40),\n",
       "  (672, 0.40),\n",
       "  (692, 0.40),\n",
       "  (697, 0.40),\n",
       "  (712, 0.40),\n",
       "  (802, 0.40),\n",
       "  (843, 0.40),\n",
       "  (854, 0.40),\n",
       "  (870, 0.40),\n",
       "  (880, 0.40),\n",
       "  (882, 0.40),\n",
       "  (890, 0.40),\n",
       "  (918, 0.40),\n",
       "  (938, 0.40),\n",
       "  (991, 0.40),\n",
       "  (41, 0.30),\n",
       "  (65, 0.30),\n",
       "  (71, 0.30),\n",
       "  (74, 0.30),\n",
       "  (113, 0.30),\n",
       "  (183, 0.30),\n",
       "  (191, 0.30),\n",
       "  (196, 0.30),\n",
       "  (202, 0.30),\n",
       "  (228, 0.30),\n",
       "  (232, 0.30),\n",
       "  (253, 0.30),\n",
       "  (303, 0.30),\n",
       "  (320, 0.30),\n",
       "  (337, 0.30),\n",
       "  (350, 0.30),\n",
       "  (358, 0.30),\n",
       "  (364, 0.30),\n",
       "  (399, 0.30),\n",
       "  (452, 0.30),\n",
       "  (454, 0.30),\n",
       "  (477, 0.30),\n",
       "  (480, 0.30),\n",
       "  (512, 0.30),\n",
       "  (537, 0.30),\n",
       "  (550, 0.30),\n",
       "  (554, 0.30),\n",
       "  (560, 0.30),\n",
       "  (593, 0.30),\n",
       "  (640, 0.30),\n",
       "  (644, 0.30),\n",
       "  (683, 0.30),\n",
       "  (707, 0.30),\n",
       "  (720, 0.30),\n",
       "  (729, 0.30),\n",
       "  (748, 0.30),\n",
       "  (757, 0.30),\n",
       "  (758, 0.30),\n",
       "  (777, 0.30),\n",
       "  (811, 0.30),\n",
       "  (831, 0.30),\n",
       "  (840, 0.30),\n",
       "  (846, 0.30),\n",
       "  (875, 0.30),\n",
       "  (883, 0.30),\n",
       "  (932, 0.30),\n",
       "  (939, 0.30),\n",
       "  (951, 0.30),\n",
       "  (957, 0.30),\n",
       "  (968, 0.30),\n",
       "  (995, 0.30),\n",
       "  (58, 0.20),\n",
       "  (82, 0.20),\n",
       "  (93, 0.20),\n",
       "  (98, 0.20),\n",
       "  (99, 0.20),\n",
       "  (100, 0.20),\n",
       "  (117, 0.20),\n",
       "  (122, 0.20),\n",
       "  (131, 0.20),\n",
       "  (138, 0.20),\n",
       "  (153, 0.20),\n",
       "  (161, 0.20),\n",
       "  (178, 0.20),\n",
       "  (188, 0.20),\n",
       "  (198, 0.20),\n",
       "  (206, 0.20),\n",
       "  (238, 0.20),\n",
       "  (283, 0.20),\n",
       "  (284, 0.20),\n",
       "  (317, 0.20),\n",
       "  (323, 0.20),\n",
       "  (326, 0.20),\n",
       "  (340, 0.20),\n",
       "  (344, 0.20),\n",
       "  (379, 0.20),\n",
       "  (395, 0.20),\n",
       "  (420, 0.20),\n",
       "  (432, 0.20),\n",
       "  (435, 0.20),\n",
       "  (436, 0.20),\n",
       "  (455, 0.20),\n",
       "  (484, 0.20),\n",
       "  (485, 0.20),\n",
       "  (487, 0.20),\n",
       "  (502, 0.20),\n",
       "  (505, 0.20),\n",
       "  (514, 0.20),\n",
       "  (534, 0.20),\n",
       "  (557, 0.20),\n",
       "  (585, 0.20),\n",
       "  (595, 0.20),\n",
       "  (618, 0.20),\n",
       "  (635, 0.20),\n",
       "  (643, 0.20),\n",
       "  (681, 0.20),\n",
       "  (700, 0.20),\n",
       "  (717, 0.20),\n",
       "  (723, 0.20),\n",
       "  (728, 0.20),\n",
       "  (736, 0.20),\n",
       "  (747, 0.20),\n",
       "  (785, 0.20),\n",
       "  (800, 0.20),\n",
       "  (808, 0.20),\n",
       "  (809, 0.20),\n",
       "  (818, 0.20),\n",
       "  (821, 0.20),\n",
       "  (825, 0.20),\n",
       "  (832, 0.20),\n",
       "  (844, 0.20),\n",
       "  (852, 0.20),\n",
       "  (876, 0.20),\n",
       "  (881, 0.20),\n",
       "  (926, 0.20),\n",
       "  (962, 0.20),\n",
       "  (966, 0.20),\n",
       "  (996, 0.20),\n",
       "  (11, 0.10),\n",
       "  (18, 0.10),\n",
       "  (21, 0.10),\n",
       "  (22, 0.10),\n",
       "  (26, 0.10),\n",
       "  (66, 0.10),\n",
       "  (70, 0.10),\n",
       "  (76, 0.10),\n",
       "  (79, 0.10),\n",
       "  (95, 0.10),\n",
       "  (111, 0.10),\n",
       "  (114, 0.10),\n",
       "  (121, 0.10),\n",
       "  (125, 0.10),\n",
       "  (129, 0.10),\n",
       "  (132, 0.10),\n",
       "  (139, 0.10),\n",
       "  (141, 0.10),\n",
       "  (142, 0.10),\n",
       "  (146, 0.10),\n",
       "  (158, 0.10),\n",
       "  (173, 0.10),\n",
       "  (179, 0.10),\n",
       "  (193, 0.10),\n",
       "  (214, 0.10),\n",
       "  (222, 0.10),\n",
       "  (229, 0.10),\n",
       "  (237, 0.10),\n",
       "  (252, 0.10),\n",
       "  (256, 0.10),\n",
       "  (268, 0.10),\n",
       "  (273, 0.10),\n",
       "  (276, 0.10),\n",
       "  (278, 0.10),\n",
       "  (282, 0.10),\n",
       "  (295, 0.10),\n",
       "  (298, 0.10),\n",
       "  (308, 0.10),\n",
       "  (309, 0.10),\n",
       "  (311, 0.10),\n",
       "  (315, 0.10),\n",
       "  (330, 0.10),\n",
       "  (343, 0.10),\n",
       "  (346, 0.10),\n",
       "  (351, 0.10),\n",
       "  (352, 0.10),\n",
       "  (356, 0.10),\n",
       "  (361, 0.10),\n",
       "  (365, 0.10),\n",
       "  (366, 0.10),\n",
       "  (377, 0.10),\n",
       "  (413, 0.10),\n",
       "  (423, 0.10),\n",
       "  (427, 0.10),\n",
       "  (428, 0.10),\n",
       "  (439, 0.10),\n",
       "  (442, 0.10),\n",
       "  (447, 0.10),\n",
       "  (448, 0.10),\n",
       "  (450, 0.10),\n",
       "  (451, 0.10),\n",
       "  (459, 0.10),\n",
       "  (475, 0.10),\n",
       "  (486, 0.10),\n",
       "  (493, 0.10),\n",
       "  (504, 0.10),\n",
       "  (521, 0.10),\n",
       "  (540, 0.10),\n",
       "  (551, 0.10),\n",
       "  (563, 0.10),\n",
       "  (569, 0.10),\n",
       "  (577, 0.10),\n",
       "  (603, 0.10),\n",
       "  (611, 0.10),\n",
       "  (613, 0.10),\n",
       "  (615, 0.10),\n",
       "  (616, 0.10),\n",
       "  (636, 0.10),\n",
       "  (639, 0.10),\n",
       "  (650, 0.10),\n",
       "  (665, 0.10),\n",
       "  (668, 0.10),\n",
       "  (673, 0.10),\n",
       "  (730, 0.10),\n",
       "  (735, 0.10),\n",
       "  (755, 0.10),\n",
       "  (756, 0.10),\n",
       "  (761, 0.10),\n",
       "  (763, 0.10),\n",
       "  (764, 0.10),\n",
       "  (765, 0.10),\n",
       "  (775, 0.10),\n",
       "  (792, 0.10),\n",
       "  (804, 0.10),\n",
       "  (813, 0.10),\n",
       "  (830, 0.10),\n",
       "  (833, 0.10),\n",
       "  (834, 0.10),\n",
       "  (836, 0.10),\n",
       "  (839, 0.10),\n",
       "  (842, 0.10),\n",
       "  (855, 0.10),\n",
       "  (856, 0.10),\n",
       "  (863, 0.10),\n",
       "  (866, 0.10),\n",
       "  (884, 0.10),\n",
       "  (885, 0.10),\n",
       "  (901, 0.10),\n",
       "  (927, 0.10),\n",
       "  (946, 0.10),\n",
       "  (950, 0.10),\n",
       "  (952, 0.10),\n",
       "  (954, 0.10),\n",
       "  (978, 0.10),\n",
       "  (983, 0.10),\n",
       "  (988, 0.10),\n",
       "  (999, 0.10),\n",
       "  (2, 0.00),\n",
       "  (3, 0.00),\n",
       "  (4, 0.00),\n",
       "  (5, 0.00),\n",
       "  (6, 0.00),\n",
       "  (10, 0.00),\n",
       "  (12, 0.00),\n",
       "  (13, 0.00),\n",
       "  (14, 0.00),\n",
       "  (16, 0.00),\n",
       "  (20, 0.00),\n",
       "  (27, 0.00),\n",
       "  (29, 0.00),\n",
       "  (30, 0.00),\n",
       "  (32, 0.00),\n",
       "  (34, 0.00),\n",
       "  (35, 0.00),\n",
       "  (36, 0.00),\n",
       "  (43, 0.00),\n",
       "  (44, 0.00),\n",
       "  (49, 0.00),\n",
       "  (51, 0.00),\n",
       "  (53, 0.00),\n",
       "  (54, 0.00),\n",
       "  (59, 0.00),\n",
       "  (64, 0.00),\n",
       "  (69, 0.00),\n",
       "  (73, 0.00),\n",
       "  (78, 0.00),\n",
       "  (80, 0.00),\n",
       "  (81, 0.00),\n",
       "  (85, 0.00),\n",
       "  (89, 0.00),\n",
       "  (101, 0.00),\n",
       "  (103, 0.00),\n",
       "  (104, 0.00),\n",
       "  (106, 0.00),\n",
       "  (110, 0.00),\n",
       "  (112, 0.00),\n",
       "  (119, 0.00),\n",
       "  (127, 0.00),\n",
       "  (130, 0.00),\n",
       "  (133, 0.00),\n",
       "  (135, 0.00),\n",
       "  (136, 0.00),\n",
       "  (137, 0.00),\n",
       "  (143, 0.00),\n",
       "  (145, 0.00),\n",
       "  (147, 0.00),\n",
       "  (148, 0.00),\n",
       "  (150, 0.00),\n",
       "  (152, 0.00),\n",
       "  (154, 0.00),\n",
       "  (156, 0.00),\n",
       "  (157, 0.00),\n",
       "  (160, 0.00),\n",
       "  (162, 0.00),\n",
       "  (165, 0.00),\n",
       "  (166, 0.00),\n",
       "  (167, 0.00),\n",
       "  (169, 0.00),\n",
       "  (170, 0.00),\n",
       "  (172, 0.00),\n",
       "  (174, 0.00),\n",
       "  (175, 0.00),\n",
       "  (177, 0.00),\n",
       "  (180, 0.00),\n",
       "  (181, 0.00),\n",
       "  (182, 0.00),\n",
       "  (184, 0.00),\n",
       "  (185, 0.00),\n",
       "  (187, 0.00),\n",
       "  (190, 0.00),\n",
       "  (194, 0.00),\n",
       "  (199, 0.00),\n",
       "  (200, 0.00),\n",
       "  (201, 0.00),\n",
       "  (203, 0.00),\n",
       "  (204, 0.00),\n",
       "  (207, 0.00),\n",
       "  (208, 0.00),\n",
       "  (209, 0.00),\n",
       "  (210, 0.00),\n",
       "  (211, 0.00),\n",
       "  (212, 0.00),\n",
       "  (213, 0.00),\n",
       "  (215, 0.00),\n",
       "  (216, 0.00),\n",
       "  (217, 0.00),\n",
       "  (220, 0.00),\n",
       "  (221, 0.00),\n",
       "  (223, 0.00),\n",
       "  (225, 0.00),\n",
       "  (226, 0.00),\n",
       "  (227, 0.00),\n",
       "  (233, 0.00),\n",
       "  (234, 0.00),\n",
       "  (239, 0.00),\n",
       "  (240, 0.00),\n",
       "  (241, 0.00),\n",
       "  (243, 0.00),\n",
       "  (244, 0.00),\n",
       "  (245, 0.00),\n",
       "  (246, 0.00),\n",
       "  (248, 0.00),\n",
       "  (250, 0.00),\n",
       "  (251, 0.00),\n",
       "  (255, 0.00),\n",
       "  (257, 0.00),\n",
       "  (258, 0.00),\n",
       "  (259, 0.00),\n",
       "  (261, 0.00),\n",
       "  (262, 0.00),\n",
       "  (263, 0.00),\n",
       "  (264, 0.00),\n",
       "  (265, 0.00),\n",
       "  (266, 0.00),\n",
       "  (267, 0.00),\n",
       "  (269, 0.00),\n",
       "  (270, 0.00),\n",
       "  (271, 0.00),\n",
       "  (272, 0.00),\n",
       "  (277, 0.00),\n",
       "  (279, 0.00),\n",
       "  (280, 0.00),\n",
       "  (285, 0.00),\n",
       "  (286, 0.00),\n",
       "  (287, 0.00),\n",
       "  (288, 0.00),\n",
       "  (296, 0.00),\n",
       "  (297, 0.00),\n",
       "  (299, 0.00),\n",
       "  (302, 0.00),\n",
       "  (312, 0.00),\n",
       "  (322, 0.00),\n",
       "  (324, 0.00),\n",
       "  (325, 0.00),\n",
       "  (328, 0.00),\n",
       "  (329, 0.00),\n",
       "  (333, 0.00),\n",
       "  (335, 0.00),\n",
       "  (338, 0.00),\n",
       "  (339, 0.00),\n",
       "  (345, 0.00),\n",
       "  (349, 0.00),\n",
       "  (354, 0.00),\n",
       "  (357, 0.00),\n",
       "  (359, 0.00),\n",
       "  (362, 0.00),\n",
       "  (367, 0.00),\n",
       "  (368, 0.00),\n",
       "  (370, 0.00),\n",
       "  (371, 0.00),\n",
       "  (372, 0.00),\n",
       "  (373, 0.00),\n",
       "  (374, 0.00),\n",
       "  (378, 0.00),\n",
       "  (380, 0.00),\n",
       "  (382, 0.00),\n",
       "  (384, 0.00),\n",
       "  (385, 0.00),\n",
       "  (386, 0.00),\n",
       "  (390, 0.00),\n",
       "  (394, 0.00),\n",
       "  (400, 0.00),\n",
       "  (402, 0.00),\n",
       "  (403, 0.00),\n",
       "  (404, 0.00),\n",
       "  (405, 0.00),\n",
       "  (415, 0.00),\n",
       "  (416, 0.00),\n",
       "  (418, 0.00),\n",
       "  (421, 0.00),\n",
       "  (422, 0.00),\n",
       "  (426, 0.00),\n",
       "  (430, 0.00),\n",
       "  (434, 0.00),\n",
       "  (437, 0.00),\n",
       "  (446, 0.00),\n",
       "  (449, 0.00),\n",
       "  (456, 0.00),\n",
       "  (458, 0.00),\n",
       "  (460, 0.00),\n",
       "  (461, 0.00),\n",
       "  (462, 0.00),\n",
       "  (465, 0.00),\n",
       "  (466, 0.00),\n",
       "  (467, 0.00),\n",
       "  (469, 0.00),\n",
       "  (470, 0.00),\n",
       "  (471, 0.00),\n",
       "  (478, 0.00),\n",
       "  (479, 0.00),\n",
       "  (494, 0.00),\n",
       "  (499, 0.00),\n",
       "  (500, 0.00),\n",
       "  (501, 0.00),\n",
       "  (503, 0.00),\n",
       "  (510, 0.00),\n",
       "  (511, 0.00),\n",
       "  (513, 0.00),\n",
       "  (519, 0.00),\n",
       "  (524, 0.00),\n",
       "  (525, 0.00),\n",
       "  (529, 0.00),\n",
       "  (531, 0.00),\n",
       "  (536, 0.00),\n",
       "  (541, 0.00),\n",
       "  (542, 0.00),\n",
       "  (543, 0.00),\n",
       "  (548, 0.00),\n",
       "  (553, 0.00),\n",
       "  (558, 0.00),\n",
       "  (559, 0.00),\n",
       "  (561, 0.00),\n",
       "  (567, 0.00),\n",
       "  (568, 0.00),\n",
       "  (571, 0.00),\n",
       "  (573, 0.00),\n",
       "  (578, 0.00),\n",
       "  (583, 0.00),\n",
       "  (587, 0.00),\n",
       "  (589, 0.00),\n",
       "  (590, 0.00),\n",
       "  (592, 0.00),\n",
       "  (594, 0.00),\n",
       "  (596, 0.00),\n",
       "  (597, 0.00),\n",
       "  (598, 0.00),\n",
       "  (600, 0.00),\n",
       "  (601, 0.00),\n",
       "  (608, 0.00),\n",
       "  (610, 0.00),\n",
       "  (617, 0.00),\n",
       "  (622, 0.00),\n",
       "  (623, 0.00),\n",
       "  (624, 0.00),\n",
       "  (627, 0.00),\n",
       "  (628, 0.00),\n",
       "  (630, 0.00),\n",
       "  (631, 0.00),\n",
       "  (634, 0.00),\n",
       "  (642, 0.00),\n",
       "  (647, 0.00),\n",
       "  (648, 0.00),\n",
       "  (649, 0.00),\n",
       "  (652, 0.00),\n",
       "  (653, 0.00),\n",
       "  (657, 0.00),\n",
       "  (658, 0.00),\n",
       "  (659, 0.00),\n",
       "  (660, 0.00),\n",
       "  (662, 0.00),\n",
       "  (663, 0.00),\n",
       "  (666, 0.00),\n",
       "  (667, 0.00),\n",
       "  (670, 0.00),\n",
       "  (675, 0.00),\n",
       "  (676, 0.00),\n",
       "  (677, 0.00),\n",
       "  (678, 0.00),\n",
       "  (682, 0.00),\n",
       "  (685, 0.00),\n",
       "  (686, 0.00),\n",
       "  (687, 0.00),\n",
       "  (688, 0.00),\n",
       "  (689, 0.00),\n",
       "  (690, 0.00),\n",
       "  (693, 0.00),\n",
       "  (695, 0.00),\n",
       "  (699, 0.00),\n",
       "  (702, 0.00),\n",
       "  (704, 0.00),\n",
       "  (708, 0.00),\n",
       "  (710, 0.00),\n",
       "  (713, 0.00),\n",
       "  (714, 0.00),\n",
       "  (715, 0.00),\n",
       "  (718, 0.00),\n",
       "  (724, 0.00),\n",
       "  (726, 0.00),\n",
       "  (731, 0.00),\n",
       "  (733, 0.00),\n",
       "  (737, 0.00),\n",
       "  (739, 0.00),\n",
       "  (740, 0.00),\n",
       "  (742, 0.00),\n",
       "  (744, 0.00),\n",
       "  (749, 0.00),\n",
       "  (766, 0.00),\n",
       "  (767, 0.00),\n",
       "  (769, 0.00),\n",
       "  (771, 0.00),\n",
       "  (773, 0.00),\n",
       "  (774, 0.00),\n",
       "  (776, 0.00),\n",
       "  (780, 0.00),\n",
       "  (782, 0.00),\n",
       "  (784, 0.00),\n",
       "  (789, 0.00),\n",
       "  (793, 0.00),\n",
       "  (795, 0.00),\n",
       "  (798, 0.00),\n",
       "  (799, 0.00),\n",
       "  (807, 0.00),\n",
       "  (810, 0.00),\n",
       "  (812, 0.00),\n",
       "  (827, 0.00),\n",
       "  (835, 0.00),\n",
       "  (838, 0.00),\n",
       "  (841, 0.00),\n",
       "  (845, 0.00),\n",
       "  (848, 0.00),\n",
       "  (849, 0.00),\n",
       "  (851, 0.00),\n",
       "  (859, 0.00),\n",
       "  (860, 0.00),\n",
       "  (861, 0.00),\n",
       "  (862, 0.00),\n",
       "  (869, 0.00),\n",
       "  (874, 0.00),\n",
       "  (877, 0.00),\n",
       "  (886, 0.00),\n",
       "  (887, 0.00),\n",
       "  (888, 0.00),\n",
       "  (891, 0.00),\n",
       "  (894, 0.00),\n",
       "  (895, 0.00),\n",
       "  (903, 0.00),\n",
       "  (908, 0.00),\n",
       "  (909, 0.00),\n",
       "  (910, 0.00),\n",
       "  (911, 0.00),\n",
       "  (912, 0.00),\n",
       "  (913, 0.00),\n",
       "  (914, 0.00),\n",
       "  (916, 0.00),\n",
       "  (919, 0.00),\n",
       "  (921, 0.00),\n",
       "  (922, 0.00),\n",
       "  (924, 0.00),\n",
       "  (928, 0.00),\n",
       "  (929, 0.00),\n",
       "  (930, 0.00),\n",
       "  (931, 0.00),\n",
       "  (933, 0.00),\n",
       "  (935, 0.00),\n",
       "  (936, 0.00),\n",
       "  (940, 0.00),\n",
       "  (941, 0.00),\n",
       "  (942, 0.00),\n",
       "  (943, 0.00),\n",
       "  (945, 0.00),\n",
       "  (947, 0.00),\n",
       "  (948, 0.00),\n",
       "  (949, 0.00),\n",
       "  (956, 0.00),\n",
       "  (958, 0.00),\n",
       "  (959, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (964, 0.00),\n",
       "  (965, 0.00),\n",
       "  (967, 0.00),\n",
       "  (969, 0.00),\n",
       "  (970, 0.00),\n",
       "  (972, 0.00),\n",
       "  (974, 0.00),\n",
       "  (975, 0.00),\n",
       "  (976, 0.00),\n",
       "  (977, 0.00),\n",
       "  (979, 0.00),\n",
       "  (980, 0.00),\n",
       "  (986, 0.00),\n",
       "  (989, 0.00),\n",
       "  (990, 0.00),\n",
       "  (993, 0.00),\n",
       "  (994, 0.00)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5f9012c6a0>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_hist = sorted(hist, key=lambda x: x[0], reverse = False)\n",
    "values = [elem[1] for elem in sorted_hist]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.8413)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 5\n",
      "at batch no 10\n",
      "at batch no 15\n",
      "at batch no 20\n",
      "at batch no 25\n",
      "at batch no 30\n",
      "at batch no 35\n",
      "at batch no 40\n",
      "at batch no 45\n",
      "at batch no 50\n",
      "at batch no 55\n",
      "at batch no 60\n",
      "at batch no 65\n",
      "at batch no 70\n",
      "at batch no 75\n",
      "at batch no 80\n",
      "at batch no 85\n",
      "at batch no 90\n",
      "at batch no 95\n",
      "at batch no 100\n",
      "at batch no 105\n",
      "at batch no 110\n",
      "at batch no 115\n",
      "at batch no 120\n",
      "at batch no 125\n",
      "at batch no 130\n",
      "at batch no 135\n",
      "at batch no 140\n",
      "at batch no 145\n",
      "at batch no 150\n",
      "at batch no 155\n",
      "at batch no 160\n",
      "at batch no 165\n",
      "at batch no 170\n",
      "at batch no 175\n",
      "at batch no 180\n",
      "at batch no 185\n",
      "at batch no 190\n",
      "at batch no 195\n",
      "at batch no 200\n",
      "at batch no 205\n",
      "at batch no 210\n",
      "at batch no 215\n",
      "at batch no 220\n",
      "at batch no 225\n",
      "at batch no 230\n",
      "at batch no 235\n",
      "at batch no 240\n",
      "at batch no 245\n",
      "at batch no 250\n",
      "at batch no 255\n",
      "at batch no 260\n",
      "at batch no 265\n",
      "at batch no 270\n",
      "at batch no 275\n",
      "at batch no 280\n",
      "at batch no 285\n",
      "at batch no 290\n",
      "at batch no 295\n",
      "at batch no 300\n",
      "at batch no 305\n",
      "at batch no 310\n",
      "at batch no 315\n",
      "at batch no 320\n",
      "at batch no 325\n",
      "at batch no 330\n",
      "at batch no 335\n",
      "at batch no 340\n",
      "at batch no 345\n",
      "at batch no 350\n",
      "at batch no 355\n",
      "at batch no 360\n",
      "at batch no 365\n",
      "at batch no 370\n",
      "at batch no 375\n",
      "at batch no 380\n",
      "at batch no 385\n",
      "at batch no 390\n",
      "at batch no 395\n",
      "at batch no 400\n",
      "at batch no 405\n",
      "at batch no 410\n",
      "at batch no 415\n",
      "at batch no 420\n",
      "at batch no 425\n",
      "at batch no 430\n",
      "at batch no 435\n",
      "at batch no 440\n",
      "at batch no 445\n",
      "at batch no 450\n",
      "at batch no 455\n",
      "at batch no 460\n",
      "at batch no 465\n",
      "at batch no 470\n",
      "at batch no 475\n",
      "at batch no 480\n",
      "at batch no 485\n",
      "at batch no 490\n",
      "at batch no 495\n",
      "at batch no 500\n",
      "at batch no 505\n",
      "at batch no 510\n",
      "at batch no 515\n",
      "at batch no 520\n",
      "at batch no 525\n",
      "at batch no 530\n",
      "at batch no 535\n",
      "at batch no 540\n",
      "at batch no 545\n",
      "at batch no 550\n",
      "at batch no 555\n",
      "at batch no 560\n",
      "at batch no 565\n",
      "at batch no 570\n",
      "at batch no 575\n",
      "at batch no 580\n",
      "at batch no 585\n",
      "at batch no 590\n",
      "at batch no 595\n",
      "at batch no 600\n",
      "at batch no 605\n",
      "at batch no 610\n",
      "at batch no 615\n",
      "at batch no 620\n",
      "at batch no 625\n",
      "at batch no 630\n",
      "at batch no 635\n",
      "at batch no 640\n",
      "at batch no 645\n",
      "at batch no 650\n",
      "at batch no 655\n",
      "at batch no 660\n",
      "at batch no 665\n",
      "at batch no 670\n",
      "at batch no 675\n",
      "at batch no 680\n",
      "at batch no 685\n",
      "at batch no 690\n",
      "at batch no 695\n",
      "at batch no 700\n",
      "at batch no 705\n",
      "at batch no 710\n",
      "at batch no 715\n",
      "at batch no 720\n",
      "at batch no 725\n",
      "at batch no 730\n",
      "at batch no 735\n",
      "at batch no 740\n",
      "at batch no 745\n",
      "at batch no 750\n",
      "at batch no 755\n",
      "at batch no 760\n",
      "at batch no 765\n",
      "at batch no 770\n",
      "at batch no 775\n",
      "at batch no 780\n",
      "at batch no 785\n",
      "at batch no 790\n",
      "at batch no 795\n",
      "at batch no 800\n",
      "at batch no 805\n",
      "at batch no 810\n",
      "at batch no 815\n",
      "at batch no 820\n",
      "at batch no 825\n",
      "at batch no 830\n",
      "at batch no 835\n",
      "at batch no 840\n",
      "at batch no 845\n",
      "at batch no 850\n",
      "at batch no 855\n",
      "at batch no 860\n",
      "at batch no 865\n",
      "at batch no 870\n",
      "at batch no 875\n",
      "at batch no 880\n",
      "at batch no 885\n",
      "at batch no 890\n",
      "at batch no 895\n",
      "at batch no 900\n",
      "at batch no 905\n",
      "at batch no 910\n",
      "at batch no 915\n",
      "at batch no 920\n",
      "at batch no 925\n",
      "at batch no 930\n",
      "at batch no 935\n",
      "at batch no 940\n",
      "at batch no 945\n",
      "at batch no 950\n",
      "at batch no 955\n",
      "at batch no 960\n",
      "at batch no 965\n",
      "at batch no 970\n",
      "at batch no 975\n",
      "at batch no 980\n",
      "at batch no 985\n",
      "at batch no 990\n",
      "at batch no 995\n",
      "at batch no 1000\n",
      "at batch no 1005\n",
      "at batch no 1010\n",
      "at batch no 1015\n",
      "at batch no 1020\n",
      "at batch no 1025\n",
      "at batch no 1030\n",
      "at batch no 1035\n",
      "at batch no 1040\n",
      "at batch no 1045\n",
      "at batch no 1050\n",
      "at batch no 1055\n",
      "at batch no 1060\n",
      "at batch no 1065\n",
      "at batch no 1070\n",
      "at batch no 1075\n",
      "at batch no 1080\n",
      "at batch no 1085\n",
      "at batch no 1090\n",
      "at batch no 1095\n",
      "at batch no 1100\n",
      "at batch no 1105\n",
      "at batch no 1110\n",
      "at batch no 1115\n",
      "at batch no 1120\n",
      "at batch no 1125\n",
      "at batch no 1130\n",
      "at batch no 1135\n",
      "at batch no 1140\n",
      "at batch no 1145\n",
      "at batch no 1150\n",
      "at batch no 1155\n",
      "at batch no 1160\n",
      "at batch no 1165\n",
      "at batch no 1170\n",
      "at batch no 1175\n",
      "at batch no 1180\n",
      "at batch no 1185\n",
      "at batch no 1190\n",
      "at batch no 1195\n",
      "at batch no 1200\n",
      "at batch no 1205\n",
      "at batch no 1210\n",
      "at batch no 1215\n",
      "at batch no 1220\n",
      "at batch no 1225\n",
      "at batch no 1230\n",
      "at batch no 1235\n",
      "at batch no 1240\n",
      "at batch no 1245\n",
      "at batch no 1250\n",
      "at batch no 1255\n",
      "at batch no 1260\n",
      "at batch no 1265\n",
      "at batch no 1270\n",
      "at batch no 1275\n",
      "at batch no 1280\n",
      "at batch no 1285\n",
      "at batch no 1290\n",
      "at batch no 1295\n",
      "at batch no 1300\n",
      "at batch no 1305\n",
      "at batch no 1310\n",
      "at batch no 1315\n",
      "at batch no 1320\n",
      "at batch no 1325\n",
      "at batch no 1330\n",
      "at batch no 1335\n",
      "at batch no 1340\n",
      "at batch no 1345\n",
      "at batch no 1350\n",
      "at batch no 1355\n",
      "at batch no 1360\n",
      "at batch no 1365\n",
      "at batch no 1370\n",
      "at batch no 1375\n",
      "at batch no 1380\n",
      "at batch no 1385\n",
      "at batch no 1390\n",
      "at batch no 1395\n",
      "at batch no 1400\n",
      "at batch no 1405\n",
      "at batch no 1410\n",
      "at batch no 1415\n",
      "at batch no 1420\n",
      "at batch no 1425\n",
      "at batch no 1430\n",
      "at batch no 1435\n",
      "at batch no 1440\n",
      "at batch no 1445\n",
      "at batch no 1450\n",
      "at batch no 1455\n",
      "at batch no 1460\n",
      "at batch no 1465\n",
      "at batch no 1470\n",
      "at batch no 1475\n",
      "at batch no 1480\n",
      "at batch no 1485\n",
      "at batch no 1490\n",
      "at batch no 1495\n",
      "at batch no 1500\n",
      "at batch no 1505\n",
      "at batch no 1510\n",
      "at batch no 1515\n",
      "at batch no 1520\n",
      "at batch no 1525\n",
      "at batch no 1530\n",
      "at batch no 1535\n",
      "at batch no 1540\n",
      "at batch no 1545\n",
      "at batch no 1550\n",
      "at batch no 1555\n",
      "at batch no 1560\n",
      "at batch no 1565\n",
      "at batch no 1570\n",
      "at batch no 1575\n",
      "at batch no 1580\n",
      "at batch no 1585\n",
      "at batch no 1590\n",
      "at batch no 1595\n",
      "at batch no 1600\n",
      "at batch no 1605\n",
      "at batch no 1610\n",
      "at batch no 1615\n",
      "at batch no 1620\n",
      "at batch no 1625\n",
      "at batch no 1630\n",
      "at batch no 1635\n",
      "at batch no 1640\n",
      "at batch no 1645\n",
      "at batch no 1650\n",
      "at batch no 1655\n",
      "at batch no 1660\n",
      "at batch no 1665\n",
      "at batch no 1670\n",
      "at batch no 1675\n",
      "at batch no 1680\n",
      "at batch no 1685\n",
      "at batch no 1690\n",
      "at batch no 1695\n",
      "at batch no 1700\n",
      "at batch no 1705\n",
      "at batch no 1710\n",
      "at batch no 1715\n",
      "at batch no 1720\n",
      "at batch no 1725\n",
      "at batch no 1730\n",
      "at batch no 1735\n",
      "at batch no 1740\n",
      "at batch no 1745\n",
      "at batch no 1750\n",
      "at batch no 1755\n",
      "at batch no 1760\n",
      "at batch no 1765\n",
      "at batch no 1770\n",
      "at batch no 1775\n",
      "at batch no 1780\n",
      "at batch no 1785\n",
      "at batch no 1790\n",
      "at batch no 1795\n",
      "at batch no 1800\n",
      "at batch no 1805\n",
      "at batch no 1810\n",
      "at batch no 1815\n",
      "at batch no 1820\n",
      "at batch no 1825\n",
      "at batch no 1830\n",
      "at batch no 1835\n",
      "at batch no 1840\n",
      "at batch no 1845\n",
      "at batch no 1850\n",
      "at batch no 1855\n",
      "at batch no 1860\n",
      "at batch no 1865\n",
      "at batch no 1870\n",
      "at batch no 1875\n",
      "at batch no 1880\n",
      "at batch no 1885\n",
      "at batch no 1890\n",
      "at batch no 1895\n",
      "at batch no 1900\n",
      "at batch no 1905\n",
      "at batch no 1910\n",
      "at batch no 1915\n",
      "at batch no 1920\n",
      "at batch no 1925\n",
      "at batch no 1930\n",
      "at batch no 1935\n",
      "at batch no 1940\n",
      "at batch no 1945\n",
      "at batch no 1950\n",
      "at batch no 1955\n",
      "at batch no 1960\n",
      "at batch no 1965\n",
      "at batch no 1970\n",
      "at batch no 1975\n",
      "at batch no 1980\n",
      "at batch no 1985\n",
      "at batch no 1990\n",
      "at batch no 1995\n",
      "at batch no 2000\n",
      "at batch no 2005\n",
      "at batch no 2010\n",
      "at batch no 2015\n",
      "at batch no 2020\n",
      "at batch no 2025\n",
      "at batch no 2030\n",
      "at batch no 2035\n",
      "at batch no 2040\n",
      "at batch no 2045\n",
      "at batch no 2050\n",
      "at batch no 2055\n",
      "at batch no 2060\n",
      "at batch no 2065\n",
      "at batch no 2070\n",
      "at batch no 2075\n",
      "at batch no 2080\n",
      "at batch no 2085\n",
      "at batch no 2090\n",
      "at batch no 2095\n",
      "at batch no 2100\n",
      "at batch no 2105\n",
      "at batch no 2110\n",
      "at batch no 2115\n",
      "at batch no 2120\n",
      "at batch no 2125\n",
      "at batch no 2130\n",
      "at batch no 2135\n",
      "at batch no 2140\n",
      "at batch no 2145\n",
      "at batch no 2150\n",
      "at batch no 2155\n",
      "at batch no 2160\n",
      "at batch no 2165\n",
      "at batch no 2170\n",
      "at batch no 2175\n",
      "at batch no 2180\n",
      "at batch no 2185\n",
      "at batch no 2190\n",
      "at batch no 2195\n",
      "at batch no 2200\n",
      "at batch no 2205\n",
      "at batch no 2210\n",
      "at batch no 2215\n",
      "at batch no 2220\n",
      "at batch no 2225\n",
      "at batch no 2230\n",
      "at batch no 2235\n",
      "at batch no 2240\n",
      "at batch no 2245\n",
      "at batch no 2250\n",
      "at batch no 2255\n",
      "at batch no 2260\n",
      "at batch no 2265\n",
      "at batch no 2270\n",
      "at batch no 2275\n",
      "at batch no 2280\n",
      "at batch no 2285\n",
      "at batch no 2290\n",
      "at batch no 2295\n",
      "at batch no 2300\n",
      "at batch no 2305\n",
      "at batch no 2310\n",
      "at batch no 2315\n",
      "at batch no 2320\n",
      "at batch no 2325\n",
      "at batch no 2330\n",
      "at batch no 2335\n",
      "at batch no 2340\n",
      "at batch no 2345\n",
      "at batch no 2350\n",
      "at batch no 2355\n",
      "at batch no 2360\n",
      "at batch no 2365\n",
      "at batch no 2370\n",
      "at batch no 2375\n",
      "at batch no 2380\n",
      "at batch no 2385\n",
      "at batch no 2390\n",
      "at batch no 2395\n",
      "at batch no 2400\n",
      "at batch no 2405\n",
      "at batch no 2410\n",
      "at batch no 2415\n",
      "at batch no 2420\n",
      "at batch no 2425\n",
      "at batch no 2430\n",
      "at batch no 2435\n",
      "at batch no 2440\n",
      "at batch no 2445\n",
      "at batch no 2450\n",
      "at batch no 2455\n",
      "at batch no 2460\n",
      "at batch no 2465\n",
      "at batch no 2470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 2475\n",
      "at batch no 2480\n",
      "at batch no 2485\n",
      "at batch no 2490\n",
      "at batch no 2495\n",
      "at batch no 2500\n",
      "at batch no 2505\n",
      "at batch no 2510\n",
      "at batch no 2515\n",
      "at batch no 2520\n",
      "at batch no 2525\n",
      "at batch no 2530\n",
      "at batch no 2535\n",
      "at batch no 2540\n",
      "at batch no 2545\n",
      "at batch no 2550\n",
      "at batch no 2555\n",
      "at batch no 2560\n",
      "at batch no 2565\n",
      "at batch no 2570\n",
      "at batch no 2575\n",
      "at batch no 2580\n",
      "at batch no 2585\n",
      "at batch no 2590\n",
      "at batch no 2595\n",
      "at batch no 2600\n",
      "at batch no 2605\n",
      "at batch no 2610\n",
      "at batch no 2615\n",
      "at batch no 2620\n",
      "at batch no 2625\n",
      "at batch no 2630\n",
      "at batch no 2635\n",
      "at batch no 2640\n",
      "at batch no 2645\n",
      "at batch no 2650\n",
      "at batch no 2655\n",
      "at batch no 2660\n",
      "at batch no 2665\n",
      "at batch no 2670\n",
      "at batch no 2675\n",
      "at batch no 2680\n",
      "at batch no 2685\n",
      "at batch no 2690\n",
      "at batch no 2695\n",
      "at batch no 2700\n",
      "at batch no 2705\n",
      "at batch no 2710\n",
      "at batch no 2715\n",
      "at batch no 2720\n",
      "at batch no 2725\n",
      "at batch no 2730\n",
      "at batch no 2735\n",
      "at batch no 2740\n",
      "at batch no 2745\n",
      "at batch no 2750\n",
      "at batch no 2755\n",
      "at batch no 2760\n",
      "at batch no 2765\n",
      "at batch no 2770\n",
      "at batch no 2775\n",
      "at batch no 2780\n",
      "at batch no 2785\n",
      "at batch no 2790\n",
      "at batch no 2795\n",
      "at batch no 2800\n",
      "at batch no 2805\n",
      "at batch no 2810\n",
      "at batch no 2815\n",
      "at batch no 2820\n",
      "at batch no 2825\n",
      "at batch no 2830\n",
      "at batch no 2835\n",
      "at batch no 2840\n",
      "at batch no 2845\n",
      "at batch no 2850\n",
      "at batch no 2855\n",
      "at batch no 2860\n",
      "at batch no 2865\n",
      "at batch no 2870\n",
      "at batch no 2875\n",
      "at batch no 2880\n",
      "at batch no 2885\n",
      "at batch no 2890\n",
      "at batch no 2895\n",
      "at batch no 2900\n",
      "at batch no 2905\n",
      "at batch no 2910\n",
      "at batch no 2915\n",
      "at batch no 2920\n",
      "at batch no 2925\n",
      "at batch no 2930\n",
      "at batch no 2935\n",
      "at batch no 2940\n",
      "at batch no 2945\n",
      "at batch no 2950\n",
      "at batch no 2955\n",
      "at batch no 2960\n",
      "at batch no 2965\n",
      "at batch no 2970\n",
      "at batch no 2975\n",
      "at batch no 2980\n",
      "at batch no 2985\n",
      "at batch no 2990\n",
      "at batch no 2995\n",
      "at batch no 3000\n",
      "at batch no 3005\n",
      "at batch no 3010\n",
      "at batch no 3015\n",
      "at batch no 3020\n",
      "at batch no 3025\n",
      "at batch no 3030\n",
      "at batch no 3035\n",
      "at batch no 3040\n",
      "at batch no 3045\n",
      "at batch no 3050\n",
      "at batch no 3055\n",
      "at batch no 3060\n",
      "at batch no 3065\n",
      "at batch no 3070\n",
      "at batch no 3075\n",
      "at batch no 3080\n",
      "at batch no 3085\n",
      "at batch no 3090\n",
      "at batch no 3095\n",
      "at batch no 3100\n",
      "at batch no 3105\n",
      "at batch no 3110\n",
      "at batch no 3115\n",
      "at batch no 3120\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(346,\n",
       " [(489, 19156.30),\n",
       "  (721, 5703.20),\n",
       "  (971, 3314.90),\n",
       "  (109, 1248.60),\n",
       "  (794, 1130.30),\n",
       "  (492, 1090.20),\n",
       "  (750, 1013.10),\n",
       "  (709, 700.20),\n",
       "  (84, 578.80),\n",
       "  (581, 393.00),\n",
       "  (414, 384.20),\n",
       "  (973, 377.40),\n",
       "  (824, 332.80),\n",
       "  (748, 284.10),\n",
       "  (805, 267.30),\n",
       "  (859, 258.20),\n",
       "  (39, 242.00),\n",
       "  (770, 230.30),\n",
       "  (828, 218.50),\n",
       "  (904, 211.20),\n",
       "  (599, 210.10),\n",
       "  (61, 203.50),\n",
       "  (556, 203.00),\n",
       "  (46, 172.00),\n",
       "  (588, 167.80),\n",
       "  (509, 166.20),\n",
       "  (893, 151.40),\n",
       "  (55, 150.90),\n",
       "  (955, 150.70),\n",
       "  (879, 144.10),\n",
       "  (580, 134.70),\n",
       "  (464, 109.10),\n",
       "  (304, 108.20),\n",
       "  (790, 107.00),\n",
       "  (815, 94.40),\n",
       "  (800, 93.20),\n",
       "  (110, 90.60),\n",
       "  (788, 89.10),\n",
       "  (515, 88.20),\n",
       "  (711, 85.40),\n",
       "  (48, 82.20),\n",
       "  (621, 80.30),\n",
       "  (611, 77.40),\n",
       "  (497, 76.00),\n",
       "  (457, 75.50),\n",
       "  (490, 73.60),\n",
       "  (735, 73.50),\n",
       "  (907, 71.30),\n",
       "  (62, 70.20),\n",
       "  (94, 67.60),\n",
       "  (806, 64.20),\n",
       "  (614, 63.30),\n",
       "  (533, 62.20),\n",
       "  (987, 57.90),\n",
       "  (406, 57.30),\n",
       "  (882, 56.80),\n",
       "  (468, 52.60),\n",
       "  (520, 51.80),\n",
       "  (854, 51.80),\n",
       "  (741, 51.30),\n",
       "  (808, 50.80),\n",
       "  (643, 50.20),\n",
       "  (953, 50.10),\n",
       "  (885, 49.10),\n",
       "  (737, 48.90),\n",
       "  (858, 47.80),\n",
       "  (992, 47.10),\n",
       "  (300, 47.00),\n",
       "  (476, 46.90),\n",
       "  (549, 45.70),\n",
       "  (898, 45.60),\n",
       "  (636, 45.30),\n",
       "  (619, 44.90),\n",
       "  (116, 44.20),\n",
       "  (401, 44.10),\n",
       "  (340, 43.80),\n",
       "  (455, 43.80),\n",
       "  (646, 43.80),\n",
       "  (411, 43.50),\n",
       "  (97, 43.30),\n",
       "  (894, 42.80),\n",
       "  (849, 42.70),\n",
       "  (981, 41.80),\n",
       "  (115, 41.70),\n",
       "  (582, 41.60),\n",
       "  (118, 41.40),\n",
       "  (878, 41.30),\n",
       "  (40, 40.50),\n",
       "  (151, 39.40),\n",
       "  (892, 39.00),\n",
       "  (754, 38.20),\n",
       "  (412, 38.00),\n",
       "  (837, 36.90),\n",
       "  (779, 36.70),\n",
       "  (60, 36.50),\n",
       "  (138, 36.40),\n",
       "  (791, 36.40),\n",
       "  (695, 35.30),\n",
       "  (323, 35.10),\n",
       "  (784, 34.70),\n",
       "  (195, 34.10),\n",
       "  (641, 33.80),\n",
       "  (518, 33.20),\n",
       "  (443, 33.10),\n",
       "  (128, 32.60),\n",
       "  (772, 32.40),\n",
       "  (915, 32.30),\n",
       "  (506, 32.20),\n",
       "  (199, 32.10),\n",
       "  (407, 32.10),\n",
       "  (440, 32.10),\n",
       "  (474, 31.90),\n",
       "  (108, 31.70),\n",
       "  (591, 31.50),\n",
       "  (67, 31.40),\n",
       "  (290, 31.40),\n",
       "  (563, 31.40),\n",
       "  (572, 31.40),\n",
       "  (25, 31.20),\n",
       "  (485, 31.10),\n",
       "  (0, 30.90),\n",
       "  (159, 30.30),\n",
       "  (645, 30.20),\n",
       "  (319, 29.90),\n",
       "  (870, 29.90),\n",
       "  (762, 29.50),\n",
       "  (65, 29.30),\n",
       "  (96, 29.30),\n",
       "  (410, 29.10),\n",
       "  (944, 29.00),\n",
       "  (575, 28.90),\n",
       "  (99, 28.60),\n",
       "  (654, 28.60),\n",
       "  (238, 28.50),\n",
       "  (545, 28.50),\n",
       "  (90, 28.30),\n",
       "  (353, 28.20),\n",
       "  (293, 28.10),\n",
       "  (135, 27.70),\n",
       "  (656, 27.60),\n",
       "  (850, 27.00),\n",
       "  (134, 26.70),\n",
       "  (88, 26.50),\n",
       "  (946, 26.50),\n",
       "  (470, 26.20),\n",
       "  (216, 25.90),\n",
       "  (768, 25.80),\n",
       "  (661, 25.70),\n",
       "  (47, 25.50),\n",
       "  (746, 25.50),\n",
       "  (242, 25.20),\n",
       "  (327, 25.10),\n",
       "  (237, 24.70),\n",
       "  (409, 24.70),\n",
       "  (7, 24.60),\n",
       "  (231, 24.30),\n",
       "  (826, 24.20),\n",
       "  (538, 24.10),\n",
       "  (275, 23.90),\n",
       "  (444, 23.90),\n",
       "  (863, 23.90),\n",
       "  (917, 23.90),\n",
       "  (241, 23.70),\n",
       "  (162, 23.60),\n",
       "  (308, 23.30),\n",
       "  (129, 23.10),\n",
       "  (251, 23.00),\n",
       "  (783, 23.00),\n",
       "  (155, 22.90),\n",
       "  (950, 22.90),\n",
       "  (282, 22.40),\n",
       "  (640, 22.20),\n",
       "  (263, 21.90),\n",
       "  (273, 21.90),\n",
       "  (916, 21.80),\n",
       "  (431, 21.50),\n",
       "  (8, 21.40),\n",
       "  (618, 21.40),\n",
       "  (698, 21.40),\n",
       "  (56, 21.30),\n",
       "  (23, 21.20),\n",
       "  (348, 21.20),\n",
       "  (679, 21.20),\n",
       "  (687, 21.00),\n",
       "  (555, 20.70),\n",
       "  (968, 20.60),\n",
       "  (562, 20.50),\n",
       "  (886, 20.50),\n",
       "  (948, 20.50),\n",
       "  (76, 20.40),\n",
       "  (363, 20.40),\n",
       "  (396, 20.40),\n",
       "  (637, 20.00),\n",
       "  (616, 19.90),\n",
       "  (82, 19.80),\n",
       "  (441, 19.70),\n",
       "  (738, 19.70),\n",
       "  (522, 19.60),\n",
       "  (703, 19.40),\n",
       "  (77, 19.10),\n",
       "  (239, 18.90),\n",
       "  (985, 18.80),\n",
       "  (50, 18.70),\n",
       "  (852, 18.60),\n",
       "  (247, 18.50),\n",
       "  (178, 18.40),\n",
       "  (576, 18.30),\n",
       "  (594, 18.30),\n",
       "  (927, 18.30),\n",
       "  (949, 18.30),\n",
       "  (74, 18.20),\n",
       "  (208, 18.00),\n",
       "  (274, 18.00),\n",
       "  (880, 17.90),\n",
       "  (365, 17.80),\n",
       "  (730, 17.80),\n",
       "  (566, 17.70),\n",
       "  (692, 17.40),\n",
       "  (260, 17.20),\n",
       "  (436, 17.20),\n",
       "  (133, 17.10),\n",
       "  (123, 17.00),\n",
       "  (670, 17.00),\n",
       "  (37, 16.90),\n",
       "  (523, 16.90),\n",
       "  (829, 16.80),\n",
       "  (9, 16.70),\n",
       "  (825, 16.70),\n",
       "  (669, 16.60),\n",
       "  (254, 16.50),\n",
       "  (419, 16.50),\n",
       "  (716, 16.40),\n",
       "  (864, 16.40),\n",
       "  (487, 16.30),\n",
       "  (982, 16.30),\n",
       "  (171, 16.20),\n",
       "  (639, 16.20),\n",
       "  (668, 16.10),\n",
       "  (717, 16.10),\n",
       "  (936, 16.10),\n",
       "  (612, 16.00),\n",
       "  (119, 15.90),\n",
       "  (136, 15.90),\n",
       "  (447, 15.80),\n",
       "  (248, 15.50),\n",
       "  (424, 15.50),\n",
       "  (801, 15.50),\n",
       "  (796, 15.40),\n",
       "  (180, 15.30),\n",
       "  (243, 15.20),\n",
       "  (249, 15.20),\n",
       "  (423, 15.20),\n",
       "  (665, 15.20),\n",
       "  (723, 15.10),\n",
       "  (234, 15.00),\n",
       "  (565, 15.00),\n",
       "  (173, 14.90),\n",
       "  (292, 14.60),\n",
       "  (211, 14.40),\n",
       "  (328, 14.40),\n",
       "  (345, 14.40),\n",
       "  (832, 14.40),\n",
       "  (57, 14.30),\n",
       "  (398, 14.30),\n",
       "  (635, 14.30),\n",
       "  (843, 14.30),\n",
       "  (429, 14.20),\n",
       "  (918, 14.10),\n",
       "  (705, 14.00),\n",
       "  (857, 14.00),\n",
       "  (124, 13.90),\n",
       "  (552, 13.90),\n",
       "  (514, 13.80),\n",
       "  (507, 13.70),\n",
       "  (642, 13.70),\n",
       "  (707, 13.60),\n",
       "  (539, 13.30),\n",
       "  (603, 13.30),\n",
       "  (264, 13.10),\n",
       "  (253, 13.00),\n",
       "  (626, 13.00),\n",
       "  (855, 13.00),\n",
       "  (496, 12.80),\n",
       "  (823, 12.80),\n",
       "  (868, 12.80),\n",
       "  (42, 12.70),\n",
       "  (24, 12.60),\n",
       "  (27, 12.60),\n",
       "  (559, 12.40),\n",
       "  (822, 12.40),\n",
       "  (354, 12.30),\n",
       "  (445, 12.30),\n",
       "  (553, 12.30),\n",
       "  (671, 12.30),\n",
       "  (765, 12.30),\n",
       "  (31, 12.20),\n",
       "  (53, 12.20),\n",
       "  (604, 12.00),\n",
       "  (866, 12.00),\n",
       "  (872, 12.00),\n",
       "  (214, 11.90),\n",
       "  (352, 11.90),\n",
       "  (453, 11.90),\n",
       "  (758, 11.90),\n",
       "  (912, 11.90),\n",
       "  (561, 11.80),\n",
       "  (700, 11.80),\n",
       "  (937, 11.80),\n",
       "  (271, 11.70),\n",
       "  (343, 11.70),\n",
       "  (479, 11.70),\n",
       "  (28, 11.60),\n",
       "  (201, 11.60),\n",
       "  (144, 11.50),\n",
       "  (227, 11.50),\n",
       "  (579, 11.50),\n",
       "  (724, 11.50),\n",
       "  (41, 11.40),\n",
       "  (301, 11.40),\n",
       "  (321, 11.40),\n",
       "  (706, 11.40),\n",
       "  (58, 11.30),\n",
       "  (200, 11.30),\n",
       "  (383, 11.30),\n",
       "  (417, 11.30),\n",
       "  (311, 11.20),\n",
       "  (451, 11.20),\n",
       "  (560, 11.20),\n",
       "  (710, 11.20),\n",
       "  (127, 11.10),\n",
       "  (390, 11.10),\n",
       "  (512, 11.10),\n",
       "  (547, 11.10),\n",
       "  (742, 11.10),\n",
       "  (998, 11.10),\n",
       "  (130, 11.00),\n",
       "  (270, 11.00),\n",
       "  (776, 11.00),\n",
       "  (215, 10.90),\n",
       "  (217, 10.90),\n",
       "  (836, 10.90),\n",
       "  (541, 10.80),\n",
       "  (954, 10.80),\n",
       "  (45, 10.70),\n",
       "  (725, 10.70),\n",
       "  (258, 10.60),\n",
       "  (288, 10.60),\n",
       "  (387, 10.50),\n",
       "  (472, 10.50),\n",
       "  (601, 10.50),\n",
       "  (658, 10.50),\n",
       "  (819, 10.50),\n",
       "  (51, 10.40),\n",
       "  (86, 10.40),\n",
       "  (719, 10.40),\n",
       "  (757, 10.40),\n",
       "  (938, 10.40),\n",
       "  (224, 10.30),\n",
       "  (775, 10.30),\n",
       "  (881, 10.30),\n",
       "  (956, 10.30),\n",
       "  (269, 10.20),\n",
       "  (157, 10.10),\n",
       "  (388, 10.10),\n",
       "  (448, 10.10),\n",
       "  (609, 10.10),\n",
       "  (627, 10.10),\n",
       "  (230, 10.00),\n",
       "  (727, 10.00),\n",
       "  (804, 10.00),\n",
       "  (887, 10.00),\n",
       "  (102, 9.90),\n",
       "  (189, 9.90),\n",
       "  (316, 9.90),\n",
       "  (605, 9.90),\n",
       "  (811, 9.90),\n",
       "  (817, 9.90),\n",
       "  (865, 9.90),\n",
       "  (997, 9.90),\n",
       "  (375, 9.80),\n",
       "  (625, 9.80),\n",
       "  (693, 9.80),\n",
       "  (92, 9.70),\n",
       "  (219, 9.70),\n",
       "  (439, 9.70),\n",
       "  (564, 9.60),\n",
       "  (734, 9.60),\n",
       "  (347, 9.50),\n",
       "  (30, 9.40),\n",
       "  (104, 9.40),\n",
       "  (85, 9.30),\n",
       "  (100, 9.30),\n",
       "  (309, 9.30),\n",
       "  (764, 9.30),\n",
       "  (72, 9.20),\n",
       "  (392, 9.20),\n",
       "  (488, 9.20),\n",
       "  (526, 9.20),\n",
       "  (534, 9.20),\n",
       "  (888, 9.20),\n",
       "  (839, 9.10),\n",
       "  (193, 9.00),\n",
       "  (235, 9.00),\n",
       "  (255, 9.00),\n",
       "  (307, 9.00),\n",
       "  (342, 9.00),\n",
       "  (425, 9.00),\n",
       "  (763, 9.00),\n",
       "  (6, 8.90),\n",
       "  (306, 8.90),\n",
       "  (792, 8.90),\n",
       "  (225, 8.80),\n",
       "  (278, 8.80),\n",
       "  (673, 8.80),\n",
       "  (257, 8.70),\n",
       "  (339, 8.70),\n",
       "  (704, 8.70),\n",
       "  (963, 8.70),\n",
       "  (79, 8.60),\n",
       "  (98, 8.60),\n",
       "  (530, 8.60),\n",
       "  (697, 8.50),\n",
       "  (113, 8.40),\n",
       "  (840, 8.40),\n",
       "  (121, 8.30),\n",
       "  (302, 8.30),\n",
       "  (454, 8.30),\n",
       "  (91, 8.20),\n",
       "  (181, 8.20),\n",
       "  (286, 8.20),\n",
       "  (393, 8.20),\n",
       "  (655, 8.20),\n",
       "  (236, 8.10),\n",
       "  (267, 8.10),\n",
       "  (607, 8.10),\n",
       "  (203, 8.00),\n",
       "  (351, 8.00),\n",
       "  (573, 8.00),\n",
       "  (221, 7.90),\n",
       "  (361, 7.90),\n",
       "  (685, 7.90),\n",
       "  (752, 7.90),\n",
       "  (766, 7.90),\n",
       "  (158, 7.80),\n",
       "  (222, 7.80),\n",
       "  (256, 7.80),\n",
       "  (696, 7.80),\n",
       "  (317, 7.70),\n",
       "  (722, 7.70),\n",
       "  (883, 7.70),\n",
       "  (131, 7.60),\n",
       "  (226, 7.60),\n",
       "  (44, 7.50),\n",
       "  (314, 7.50),\n",
       "  (355, 7.50),\n",
       "  (543, 7.50),\n",
       "  (875, 7.50),\n",
       "  (14, 7.40),\n",
       "  (676, 7.40),\n",
       "  (911, 7.40),\n",
       "  (303, 7.30),\n",
       "  (310, 7.30),\n",
       "  (330, 7.30),\n",
       "  (632, 7.30),\n",
       "  (18, 7.20),\n",
       "  (172, 7.20),\n",
       "  (245, 7.20),\n",
       "  (281, 7.20),\n",
       "  (816, 7.20),\n",
       "  (874, 7.20),\n",
       "  (889, 7.20),\n",
       "  (125, 7.10),\n",
       "  (897, 7.10),\n",
       "  (101, 7.00),\n",
       "  (890, 7.00),\n",
       "  (986, 7.00),\n",
       "  (63, 6.90),\n",
       "  (498, 6.90),\n",
       "  (337, 6.80),\n",
       "  (652, 6.80),\n",
       "  (902, 6.80),\n",
       "  (667, 6.70),\n",
       "  (732, 6.70),\n",
       "  (774, 6.70),\n",
       "  (70, 6.60),\n",
       "  (166, 6.60),\n",
       "  (584, 6.60),\n",
       "  (846, 6.60),\n",
       "  (52, 6.50),\n",
       "  (528, 6.50),\n",
       "  (529, 6.50),\n",
       "  (586, 6.50),\n",
       "  (830, 6.50),\n",
       "  (15, 6.40),\n",
       "  (250, 6.40),\n",
       "  (462, 6.40),\n",
       "  (463, 6.40),\n",
       "  (537, 6.40),\n",
       "  (602, 6.40),\n",
       "  (606, 6.40),\n",
       "  (777, 6.40),\n",
       "  (83, 6.30),\n",
       "  (694, 6.30),\n",
       "  (760, 6.30),\n",
       "  (164, 6.20),\n",
       "  (218, 6.20),\n",
       "  (294, 6.20),\n",
       "  (571, 6.20),\n",
       "  (22, 6.10),\n",
       "  (198, 6.10),\n",
       "  (624, 6.10),\n",
       "  (851, 6.10),\n",
       "  (75, 6.00),\n",
       "  (362, 6.00),\n",
       "  (491, 6.00),\n",
       "  (570, 6.00),\n",
       "  (759, 6.00),\n",
       "  (148, 5.90),\n",
       "  (185, 5.90),\n",
       "  (471, 5.90),\n",
       "  (587, 5.90),\n",
       "  (947, 5.80),\n",
       "  (160, 5.70),\n",
       "  (389, 5.70),\n",
       "  (1, 5.60),\n",
       "  (107, 5.60),\n",
       "  (244, 5.60),\n",
       "  (753, 5.60),\n",
       "  (920, 5.60),\n",
       "  (176, 5.50),\n",
       "  (197, 5.50),\n",
       "  (853, 5.50),\n",
       "  (71, 5.40),\n",
       "  (169, 5.40),\n",
       "  (276, 5.40),\n",
       "  (360, 5.40),\n",
       "  (467, 5.40),\n",
       "  (516, 5.40),\n",
       "  (577, 5.40),\n",
       "  (900, 5.40),\n",
       "  (906, 5.40),\n",
       "  (391, 5.30),\n",
       "  (420, 5.30),\n",
       "  (508, 5.30),\n",
       "  (939, 5.30),\n",
       "  (995, 5.30),\n",
       "  (212, 5.20),\n",
       "  (608, 5.20),\n",
       "  (756, 5.20),\n",
       "  (781, 5.20),\n",
       "  (787, 5.20),\n",
       "  (43, 5.10),\n",
       "  (93, 5.10),\n",
       "  (334, 5.10),\n",
       "  (397, 5.10),\n",
       "  (80, 5.00),\n",
       "  (142, 5.00),\n",
       "  (192, 5.00),\n",
       "  (284, 5.00),\n",
       "  (990, 5.00),\n",
       "  (991, 5.00),\n",
       "  (120, 4.90),\n",
       "  (163, 4.90),\n",
       "  (186, 4.90),\n",
       "  (207, 4.90),\n",
       "  (280, 4.90),\n",
       "  (318, 4.90),\n",
       "  (433, 4.90),\n",
       "  (585, 4.90),\n",
       "  (597, 4.90),\n",
       "  (382, 4.80),\n",
       "  (482, 4.80),\n",
       "  (558, 4.80),\n",
       "  (569, 4.80),\n",
       "  (782, 4.80),\n",
       "  (884, 4.80),\n",
       "  (11, 4.70),\n",
       "  (182, 4.70),\n",
       "  (358, 4.70),\n",
       "  (458, 4.70),\n",
       "  (845, 4.70),\n",
       "  (867, 4.70),\n",
       "  (336, 4.60),\n",
       "  (531, 4.60),\n",
       "  (546, 4.60),\n",
       "  (613, 4.60),\n",
       "  (620, 4.60),\n",
       "  (688, 4.60),\n",
       "  (714, 4.60),\n",
       "  (891, 4.60),\n",
       "  (984, 4.60),\n",
       "  (999, 4.60),\n",
       "  (68, 4.50),\n",
       "  (205, 4.50),\n",
       "  (291, 4.50),\n",
       "  (331, 4.50),\n",
       "  (653, 4.50),\n",
       "  (672, 4.50),\n",
       "  (786, 4.50),\n",
       "  (368, 4.40),\n",
       "  (376, 4.40),\n",
       "  (432, 4.40),\n",
       "  (503, 4.40),\n",
       "  (519, 4.40),\n",
       "  (161, 4.30),\n",
       "  (951, 4.30),\n",
       "  (126, 4.20),\n",
       "  (297, 4.20),\n",
       "  (449, 4.20),\n",
       "  (544, 4.20),\n",
       "  (660, 4.20),\n",
       "  (934, 4.20),\n",
       "  (141, 4.10),\n",
       "  (206, 4.10),\n",
       "  (593, 4.10),\n",
       "  (638, 4.10),\n",
       "  (733, 4.10),\n",
       "  (778, 4.10),\n",
       "  (931, 4.10),\n",
       "  (994, 4.10),\n",
       "  (17, 4.00),\n",
       "  (89, 4.00),\n",
       "  (132, 4.00),\n",
       "  (259, 4.00),\n",
       "  (295, 4.00),\n",
       "  (313, 4.00),\n",
       "  (344, 4.00),\n",
       "  (699, 4.00),\n",
       "  (820, 4.00),\n",
       "  (156, 3.90),\n",
       "  (210, 3.90),\n",
       "  (481, 3.90),\n",
       "  (684, 3.90),\n",
       "  (861, 3.90),\n",
       "  (921, 3.90),\n",
       "  (966, 3.90),\n",
       "  (272, 3.80),\n",
       "  (430, 3.80),\n",
       "  (535, 3.80),\n",
       "  (751, 3.80),\n",
       "  (842, 3.80),\n",
       "  (957, 3.80),\n",
       "  (117, 3.70),\n",
       "  (202, 3.70),\n",
       "  (49, 3.60),\n",
       "  (145, 3.60),\n",
       "  (188, 3.60),\n",
       "  (350, 3.60),\n",
       "  (630, 3.60),\n",
       "  (847, 3.60),\n",
       "  (989, 3.60),\n",
       "  (298, 3.50),\n",
       "  (386, 3.50),\n",
       "  (426, 3.50),\n",
       "  (532, 3.50),\n",
       "  (596, 3.50),\n",
       "  (747, 3.50),\n",
       "  (838, 3.50),\n",
       "  (952, 3.50),\n",
       "  (988, 3.50),\n",
       "  (191, 3.40),\n",
       "  (228, 3.40),\n",
       "  (359, 3.40),\n",
       "  (408, 3.40),\n",
       "  (459, 3.40),\n",
       "  (659, 3.30),\n",
       "  (683, 3.30),\n",
       "  (87, 3.20),\n",
       "  (146, 3.20),\n",
       "  (232, 3.20),\n",
       "  (329, 3.20),\n",
       "  (486, 3.20),\n",
       "  (505, 3.20),\n",
       "  (265, 3.10),\n",
       "  (325, 3.10),\n",
       "  (428, 3.10),\n",
       "  (477, 3.10),\n",
       "  (527, 3.10),\n",
       "  (945, 3.10),\n",
       "  (179, 3.00),\n",
       "  (223, 3.00),\n",
       "  (299, 3.00),\n",
       "  (320, 3.00),\n",
       "  (341, 3.00),\n",
       "  (369, 3.00),\n",
       "  (385, 3.00),\n",
       "  (466, 3.00),\n",
       "  (574, 3.00),\n",
       "  (814, 3.00),\n",
       "  (877, 3.00),\n",
       "  (66, 2.90),\n",
       "  (446, 2.90),\n",
       "  (513, 2.90),\n",
       "  (650, 2.90),\n",
       "  (663, 2.90),\n",
       "  (677, 2.90),\n",
       "  (773, 2.90),\n",
       "  (932, 2.90),\n",
       "  (36, 2.80),\n",
       "  (187, 2.80),\n",
       "  (356, 2.80),\n",
       "  (629, 2.80),\n",
       "  (761, 2.80),\n",
       "  (38, 2.70),\n",
       "  (402, 2.70),\n",
       "  (521, 2.70),\n",
       "  (731, 2.70),\n",
       "  (873, 2.70),\n",
       "  (923, 2.70),\n",
       "  (170, 2.60),\n",
       "  (213, 2.60),\n",
       "  (268, 2.60),\n",
       "  (370, 2.60),\n",
       "  (371, 2.60),\n",
       "  (456, 2.60),\n",
       "  (495, 2.60),\n",
       "  (511, 2.60),\n",
       "  (615, 2.60),\n",
       "  (701, 2.60),\n",
       "  (910, 2.60),\n",
       "  (140, 2.50),\n",
       "  (174, 2.50),\n",
       "  (177, 2.50),\n",
       "  (196, 2.50),\n",
       "  (209, 2.50),\n",
       "  (305, 2.50),\n",
       "  (367, 2.50),\n",
       "  (475, 2.50),\n",
       "  (483, 2.50),\n",
       "  (502, 2.50),\n",
       "  (517, 2.50),\n",
       "  (678, 2.50),\n",
       "  (769, 2.50),\n",
       "  (807, 2.50),\n",
       "  (821, 2.50),\n",
       "  (896, 2.50),\n",
       "  (21, 2.40),\n",
       "  (165, 2.40),\n",
       "  (175, 2.40),\n",
       "  (204, 2.40),\n",
       "  (551, 2.40),\n",
       "  (567, 2.40),\n",
       "  (578, 2.40),\n",
       "  (812, 2.40),\n",
       "  (869, 2.40),\n",
       "  (876, 2.40),\n",
       "  (183, 2.30),\n",
       "  (644, 2.30),\n",
       "  (153, 2.20),\n",
       "  (835, 2.20),\n",
       "  (862, 2.20),\n",
       "  (905, 2.20),\n",
       "  (962, 2.20),\n",
       "  (996, 2.20),\n",
       "  (10, 2.10),\n",
       "  (452, 2.10),\n",
       "  (540, 2.10),\n",
       "  (959, 2.10),\n",
       "  (33, 2.00),\n",
       "  (114, 2.00),\n",
       "  (139, 2.00),\n",
       "  (283, 2.00),\n",
       "  (377, 2.00),\n",
       "  (379, 2.00),\n",
       "  (400, 2.00),\n",
       "  (427, 2.00),\n",
       "  (493, 2.00),\n",
       "  (554, 2.00),\n",
       "  (666, 2.00),\n",
       "  (674, 2.00),\n",
       "  (925, 2.00),\n",
       "  (933, 2.00),\n",
       "  (979, 2.00),\n",
       "  (122, 1.90),\n",
       "  (279, 1.90),\n",
       "  (332, 1.90),\n",
       "  (380, 1.90),\n",
       "  (610, 1.90),\n",
       "  (691, 1.90),\n",
       "  (54, 1.80),\n",
       "  (831, 1.80),\n",
       "  (287, 1.70),\n",
       "  (749, 1.70),\n",
       "  (860, 1.70),\n",
       "  (32, 1.60),\n",
       "  (289, 1.60),\n",
       "  (346, 1.60),\n",
       "  (708, 1.60),\n",
       "  (841, 1.60),\n",
       "  (919, 1.60),\n",
       "  (95, 1.50),\n",
       "  (167, 1.50),\n",
       "  (184, 1.50),\n",
       "  (194, 1.50),\n",
       "  (220, 1.50),\n",
       "  (374, 1.50),\n",
       "  (395, 1.50),\n",
       "  (484, 1.50),\n",
       "  (542, 1.50),\n",
       "  (589, 1.50),\n",
       "  (592, 1.50),\n",
       "  (600, 1.50),\n",
       "  (744, 1.50),\n",
       "  (767, 1.50),\n",
       "  (799, 1.50),\n",
       "  (809, 1.50),\n",
       "  (969, 1.50),\n",
       "  (364, 1.40),\n",
       "  (595, 1.40),\n",
       "  (929, 1.40),\n",
       "  (338, 1.30),\n",
       "  (461, 1.30),\n",
       "  (664, 1.30),\n",
       "  (622, 1.20),\n",
       "  (922, 1.20),\n",
       "  (35, 1.10),\n",
       "  (229, 1.10),\n",
       "  (366, 1.10),\n",
       "  (372, 1.10),\n",
       "  (381, 1.10),\n",
       "  (399, 1.10),\n",
       "  (418, 1.10),\n",
       "  (930, 1.10),\n",
       "  (958, 1.10),\n",
       "  (983, 1.10),\n",
       "  (34, 1.00),\n",
       "  (69, 1.00),\n",
       "  (81, 1.00),\n",
       "  (240, 1.00),\n",
       "  (261, 1.00),\n",
       "  (262, 1.00),\n",
       "  (266, 1.00),\n",
       "  (335, 1.00),\n",
       "  (373, 1.00),\n",
       "  (378, 1.00),\n",
       "  (404, 1.00),\n",
       "  (434, 1.00),\n",
       "  (469, 1.00),\n",
       "  (510, 1.00),\n",
       "  (550, 1.00),\n",
       "  (583, 1.00),\n",
       "  (662, 1.00),\n",
       "  (690, 1.00),\n",
       "  (726, 1.00),\n",
       "  (736, 1.00),\n",
       "  (793, 1.00),\n",
       "  (797, 1.00),\n",
       "  (802, 1.00),\n",
       "  (813, 1.00),\n",
       "  (941, 1.00),\n",
       "  (12, 0.90),\n",
       "  (899, 0.90),\n",
       "  (924, 0.90),\n",
       "  (64, 0.80),\n",
       "  (322, 0.80),\n",
       "  (675, 0.80),\n",
       "  (743, 0.80),\n",
       "  (755, 0.80),\n",
       "  (798, 0.80),\n",
       "  (16, 0.70),\n",
       "  (315, 0.70),\n",
       "  (413, 0.70),\n",
       "  (623, 0.70),\n",
       "  (720, 0.70),\n",
       "  (789, 0.70),\n",
       "  (19, 0.60),\n",
       "  (154, 0.60),\n",
       "  (405, 0.60),\n",
       "  (628, 0.60),\n",
       "  (681, 0.60),\n",
       "  (715, 0.60),\n",
       "  (718, 0.60),\n",
       "  (740, 0.60),\n",
       "  (785, 0.60),\n",
       "  (795, 0.60),\n",
       "  (834, 0.60),\n",
       "  (4, 0.50),\n",
       "  (5, 0.50),\n",
       "  (73, 0.50),\n",
       "  (78, 0.50),\n",
       "  (168, 0.50),\n",
       "  (246, 0.50),\n",
       "  (252, 0.50),\n",
       "  (277, 0.50),\n",
       "  (324, 0.50),\n",
       "  (394, 0.50),\n",
       "  (421, 0.50),\n",
       "  (435, 0.50),\n",
       "  (465, 0.50),\n",
       "  (499, 0.50),\n",
       "  (633, 0.50),\n",
       "  (686, 0.50),\n",
       "  (702, 0.50),\n",
       "  (745, 0.50),\n",
       "  (803, 0.50),\n",
       "  (848, 0.50),\n",
       "  (901, 0.50),\n",
       "  (903, 0.50),\n",
       "  (909, 0.50),\n",
       "  (967, 0.50),\n",
       "  (143, 0.40),\n",
       "  (500, 0.40),\n",
       "  (524, 0.40),\n",
       "  (657, 0.40),\n",
       "  (827, 0.40),\n",
       "  (844, 0.40),\n",
       "  (943, 0.40),\n",
       "  (972, 0.40),\n",
       "  (105, 0.30),\n",
       "  (312, 0.30),\n",
       "  (478, 0.30),\n",
       "  (557, 0.30),\n",
       "  (651, 0.30),\n",
       "  (26, 0.20),\n",
       "  (190, 0.20),\n",
       "  (357, 0.20),\n",
       "  (473, 0.20),\n",
       "  (712, 0.20),\n",
       "  (780, 0.20),\n",
       "  (871, 0.20),\n",
       "  (415, 0.10),\n",
       "  (437, 0.10),\n",
       "  (631, 0.10),\n",
       "  (647, 0.10),\n",
       "  (680, 0.10),\n",
       "  (771, 0.10),\n",
       "  (913, 0.10),\n",
       "  (942, 0.10),\n",
       "  (2, 0.00),\n",
       "  (3, 0.00),\n",
       "  (13, 0.00),\n",
       "  (20, 0.00),\n",
       "  (29, 0.00),\n",
       "  (59, 0.00),\n",
       "  (103, 0.00),\n",
       "  (106, 0.00),\n",
       "  (111, 0.00),\n",
       "  (112, 0.00),\n",
       "  (137, 0.00),\n",
       "  (147, 0.00),\n",
       "  (149, 0.00),\n",
       "  (150, 0.00),\n",
       "  (152, 0.00),\n",
       "  (233, 0.00),\n",
       "  (285, 0.00),\n",
       "  (296, 0.00),\n",
       "  (326, 0.00),\n",
       "  (333, 0.00),\n",
       "  (349, 0.00),\n",
       "  (384, 0.00),\n",
       "  (403, 0.00),\n",
       "  (416, 0.00),\n",
       "  (422, 0.00),\n",
       "  (438, 0.00),\n",
       "  (442, 0.00),\n",
       "  (450, 0.00),\n",
       "  (460, 0.00),\n",
       "  (480, 0.00),\n",
       "  (494, 0.00),\n",
       "  (501, 0.00),\n",
       "  (504, 0.00),\n",
       "  (525, 0.00),\n",
       "  (536, 0.00),\n",
       "  (548, 0.00),\n",
       "  (568, 0.00),\n",
       "  (590, 0.00),\n",
       "  (598, 0.00),\n",
       "  (617, 0.00),\n",
       "  (634, 0.00),\n",
       "  (648, 0.00),\n",
       "  (649, 0.00),\n",
       "  (682, 0.00),\n",
       "  (689, 0.00),\n",
       "  (713, 0.00),\n",
       "  (728, 0.00),\n",
       "  (729, 0.00),\n",
       "  (739, 0.00),\n",
       "  (810, 0.00),\n",
       "  (818, 0.00),\n",
       "  (833, 0.00),\n",
       "  (856, 0.00),\n",
       "  (895, 0.00),\n",
       "  (908, 0.00),\n",
       "  (914, 0.00),\n",
       "  (926, 0.00),\n",
       "  (928, 0.00),\n",
       "  (935, 0.00),\n",
       "  (940, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (964, 0.00),\n",
       "  (965, 0.00),\n",
       "  (970, 0.00),\n",
       "  (974, 0.00),\n",
       "  (975, 0.00),\n",
       "  (976, 0.00),\n",
       "  (977, 0.00),\n",
       "  (978, 0.00),\n",
       "  (980, 0.00),\n",
       "  (993, 0.00)])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 5\n",
      "at batch no 10\n",
      "at batch no 15\n",
      "at batch no 20\n",
      "at batch no 25\n",
      "at batch no 30\n",
      "at batch no 35\n",
      "at batch no 40\n",
      "at batch no 45\n",
      "at batch no 50\n",
      "at batch no 55\n",
      "at batch no 60\n",
      "at batch no 65\n",
      "at batch no 70\n",
      "at batch no 75\n",
      "at batch no 80\n",
      "at batch no 85\n",
      "at batch no 90\n",
      "at batch no 95\n",
      "at batch no 100\n",
      "at batch no 105\n",
      "at batch no 110\n",
      "at batch no 115\n",
      "at batch no 120\n",
      "at batch no 125\n",
      "at batch no 130\n",
      "at batch no 135\n",
      "at batch no 140\n",
      "at batch no 145\n",
      "at batch no 150\n",
      "at batch no 155\n",
      "at batch no 160\n",
      "at batch no 165\n",
      "at batch no 170\n",
      "at batch no 175\n",
      "at batch no 180\n",
      "at batch no 185\n",
      "at batch no 190\n",
      "at batch no 195\n",
      "at batch no 200\n",
      "at batch no 205\n",
      "at batch no 210\n",
      "at batch no 215\n",
      "at batch no 220\n",
      "at batch no 225\n",
      "at batch no 230\n",
      "at batch no 235\n",
      "at batch no 240\n",
      "at batch no 245\n",
      "at batch no 250\n",
      "at batch no 255\n",
      "at batch no 260\n",
      "at batch no 265\n",
      "at batch no 270\n",
      "at batch no 275\n",
      "at batch no 280\n",
      "at batch no 285\n",
      "at batch no 290\n",
      "at batch no 295\n",
      "at batch no 300\n",
      "at batch no 305\n",
      "at batch no 310\n",
      "at batch no 315\n",
      "at batch no 320\n",
      "at batch no 325\n",
      "at batch no 330\n",
      "at batch no 335\n",
      "at batch no 340\n",
      "at batch no 345\n",
      "at batch no 350\n",
      "at batch no 355\n",
      "at batch no 360\n",
      "at batch no 365\n",
      "at batch no 370\n",
      "at batch no 375\n",
      "at batch no 380\n",
      "at batch no 385\n",
      "at batch no 390\n",
      "at batch no 395\n",
      "at batch no 400\n",
      "at batch no 405\n",
      "at batch no 410\n",
      "at batch no 415\n",
      "at batch no 420\n",
      "at batch no 425\n",
      "at batch no 430\n",
      "at batch no 435\n",
      "at batch no 440\n",
      "at batch no 445\n",
      "at batch no 450\n",
      "at batch no 455\n",
      "at batch no 460\n",
      "at batch no 465\n",
      "at batch no 470\n",
      "at batch no 475\n",
      "at batch no 480\n",
      "at batch no 485\n",
      "at batch no 490\n",
      "at batch no 495\n",
      "at batch no 500\n",
      "at batch no 505\n",
      "at batch no 510\n",
      "at batch no 515\n",
      "at batch no 520\n",
      "at batch no 525\n",
      "at batch no 530\n",
      "at batch no 535\n",
      "at batch no 540\n",
      "at batch no 545\n",
      "at batch no 550\n",
      "at batch no 555\n",
      "at batch no 560\n",
      "at batch no 565\n",
      "at batch no 570\n",
      "at batch no 575\n",
      "at batch no 580\n",
      "at batch no 585\n",
      "at batch no 590\n",
      "at batch no 595\n",
      "at batch no 600\n",
      "at batch no 605\n",
      "at batch no 610\n",
      "at batch no 615\n",
      "at batch no 620\n",
      "at batch no 625\n",
      "at batch no 630\n",
      "at batch no 635\n",
      "at batch no 640\n",
      "at batch no 645\n",
      "at batch no 650\n",
      "at batch no 655\n",
      "at batch no 660\n",
      "at batch no 665\n",
      "at batch no 670\n",
      "at batch no 675\n",
      "at batch no 680\n",
      "at batch no 685\n",
      "at batch no 690\n",
      "at batch no 695\n",
      "at batch no 700\n",
      "at batch no 705\n",
      "at batch no 710\n",
      "at batch no 715\n",
      "at batch no 720\n",
      "at batch no 725\n",
      "at batch no 730\n",
      "at batch no 735\n",
      "at batch no 740\n",
      "at batch no 745\n",
      "at batch no 750\n",
      "at batch no 755\n",
      "at batch no 760\n",
      "at batch no 765\n",
      "at batch no 770\n",
      "at batch no 775\n",
      "at batch no 780\n",
      "at batch no 785\n",
      "at batch no 790\n",
      "at batch no 795\n",
      "at batch no 800\n",
      "at batch no 805\n",
      "at batch no 810\n",
      "at batch no 815\n",
      "at batch no 820\n",
      "at batch no 825\n",
      "at batch no 830\n",
      "at batch no 835\n",
      "at batch no 840\n",
      "at batch no 845\n",
      "at batch no 850\n",
      "at batch no 855\n",
      "at batch no 860\n",
      "at batch no 865\n",
      "at batch no 870\n",
      "at batch no 875\n",
      "at batch no 880\n",
      "at batch no 885\n",
      "at batch no 890\n",
      "at batch no 895\n",
      "at batch no 900\n",
      "at batch no 905\n",
      "at batch no 910\n",
      "at batch no 915\n",
      "at batch no 920\n",
      "at batch no 925\n",
      "at batch no 930\n",
      "at batch no 935\n",
      "at batch no 940\n",
      "at batch no 945\n",
      "at batch no 950\n",
      "at batch no 955\n",
      "at batch no 960\n",
      "at batch no 965\n",
      "at batch no 970\n",
      "at batch no 975\n",
      "at batch no 980\n",
      "at batch no 985\n",
      "at batch no 990\n",
      "at batch no 995\n",
      "at batch no 1000\n",
      "at batch no 1005\n",
      "at batch no 1010\n",
      "at batch no 1015\n",
      "at batch no 1020\n",
      "at batch no 1025\n",
      "at batch no 1030\n",
      "at batch no 1035\n",
      "at batch no 1040\n",
      "at batch no 1045\n",
      "at batch no 1050\n",
      "at batch no 1055\n",
      "at batch no 1060\n",
      "at batch no 1065\n",
      "at batch no 1070\n",
      "at batch no 1075\n",
      "at batch no 1080\n",
      "at batch no 1085\n",
      "at batch no 1090\n",
      "at batch no 1095\n",
      "at batch no 1100\n",
      "at batch no 1105\n",
      "at batch no 1110\n",
      "at batch no 1115\n",
      "at batch no 1120\n",
      "at batch no 1125\n",
      "at batch no 1130\n",
      "at batch no 1135\n",
      "at batch no 1140\n",
      "at batch no 1145\n",
      "at batch no 1150\n",
      "at batch no 1155\n",
      "at batch no 1160\n",
      "at batch no 1165\n",
      "at batch no 1170\n",
      "at batch no 1175\n",
      "at batch no 1180\n",
      "at batch no 1185\n",
      "at batch no 1190\n",
      "at batch no 1195\n",
      "at batch no 1200\n",
      "at batch no 1205\n",
      "at batch no 1210\n",
      "at batch no 1215\n",
      "at batch no 1220\n",
      "at batch no 1225\n",
      "at batch no 1230\n",
      "at batch no 1235\n",
      "at batch no 1240\n",
      "at batch no 1245\n",
      "at batch no 1250\n",
      "at batch no 1255\n",
      "at batch no 1260\n",
      "at batch no 1265\n",
      "at batch no 1270\n",
      "at batch no 1275\n",
      "at batch no 1280\n",
      "at batch no 1285\n",
      "at batch no 1290\n",
      "at batch no 1295\n",
      "at batch no 1300\n",
      "at batch no 1305\n",
      "at batch no 1310\n",
      "at batch no 1315\n",
      "at batch no 1320\n",
      "at batch no 1325\n",
      "at batch no 1330\n",
      "at batch no 1335\n",
      "at batch no 1340\n",
      "at batch no 1345\n",
      "at batch no 1350\n",
      "at batch no 1355\n",
      "at batch no 1360\n",
      "at batch no 1365\n",
      "at batch no 1370\n",
      "at batch no 1375\n",
      "at batch no 1380\n",
      "at batch no 1385\n",
      "at batch no 1390\n",
      "at batch no 1395\n",
      "at batch no 1400\n",
      "at batch no 1405\n",
      "at batch no 1410\n",
      "at batch no 1415\n",
      "at batch no 1420\n",
      "at batch no 1425\n",
      "at batch no 1430\n",
      "at batch no 1435\n",
      "at batch no 1440\n",
      "at batch no 1445\n",
      "at batch no 1450\n",
      "at batch no 1455\n",
      "at batch no 1460\n",
      "at batch no 1465\n",
      "at batch no 1470\n",
      "at batch no 1475\n",
      "at batch no 1480\n",
      "at batch no 1485\n",
      "at batch no 1490\n",
      "at batch no 1495\n",
      "at batch no 1500\n",
      "at batch no 1505\n",
      "at batch no 1510\n",
      "at batch no 1515\n",
      "at batch no 1520\n",
      "at batch no 1525\n",
      "at batch no 1530\n",
      "at batch no 1535\n",
      "at batch no 1540\n",
      "at batch no 1545\n",
      "at batch no 1550\n",
      "at batch no 1555\n",
      "at batch no 1560\n",
      "at batch no 1565\n",
      "at batch no 1570\n",
      "at batch no 1575\n",
      "at batch no 1580\n",
      "at batch no 1585\n",
      "at batch no 1590\n",
      "at batch no 1595\n",
      "at batch no 1600\n",
      "at batch no 1605\n",
      "at batch no 1610\n",
      "at batch no 1615\n",
      "at batch no 1620\n",
      "at batch no 1625\n",
      "at batch no 1630\n",
      "at batch no 1635\n",
      "at batch no 1640\n",
      "at batch no 1645\n",
      "at batch no 1650\n",
      "at batch no 1655\n",
      "at batch no 1660\n",
      "at batch no 1665\n",
      "at batch no 1670\n",
      "at batch no 1675\n",
      "at batch no 1680\n",
      "at batch no 1685\n",
      "at batch no 1690\n",
      "at batch no 1695\n",
      "at batch no 1700\n",
      "at batch no 1705\n",
      "at batch no 1710\n",
      "at batch no 1715\n",
      "at batch no 1720\n",
      "at batch no 1725\n",
      "at batch no 1730\n",
      "at batch no 1735\n",
      "at batch no 1740\n",
      "at batch no 1745\n",
      "at batch no 1750\n",
      "at batch no 1755\n",
      "at batch no 1760\n",
      "at batch no 1765\n",
      "at batch no 1770\n",
      "at batch no 1775\n",
      "at batch no 1780\n",
      "at batch no 1785\n",
      "at batch no 1790\n",
      "at batch no 1795\n",
      "at batch no 1800\n",
      "at batch no 1805\n",
      "at batch no 1810\n",
      "at batch no 1815\n",
      "at batch no 1820\n",
      "at batch no 1825\n",
      "at batch no 1830\n",
      "at batch no 1835\n",
      "at batch no 1840\n",
      "at batch no 1845\n",
      "at batch no 1850\n",
      "at batch no 1855\n",
      "at batch no 1860\n",
      "at batch no 1865\n",
      "at batch no 1870\n",
      "at batch no 1875\n",
      "at batch no 1880\n",
      "at batch no 1885\n",
      "at batch no 1890\n",
      "at batch no 1895\n",
      "at batch no 1900\n",
      "at batch no 1905\n",
      "at batch no 1910\n",
      "at batch no 1915\n",
      "at batch no 1920\n",
      "at batch no 1925\n",
      "at batch no 1930\n",
      "at batch no 1935\n",
      "at batch no 1940\n",
      "at batch no 1945\n",
      "at batch no 1950\n",
      "at batch no 1955\n",
      "at batch no 1960\n",
      "at batch no 1965\n",
      "at batch no 1970\n",
      "at batch no 1975\n",
      "at batch no 1980\n",
      "at batch no 1985\n",
      "at batch no 1990\n",
      "at batch no 1995\n",
      "at batch no 2000\n",
      "at batch no 2005\n",
      "at batch no 2010\n",
      "at batch no 2015\n",
      "at batch no 2020\n",
      "at batch no 2025\n",
      "at batch no 2030\n",
      "at batch no 2035\n",
      "at batch no 2040\n",
      "at batch no 2045\n",
      "at batch no 2050\n",
      "at batch no 2055\n",
      "at batch no 2060\n",
      "at batch no 2065\n",
      "at batch no 2070\n",
      "at batch no 2075\n",
      "at batch no 2080\n",
      "at batch no 2085\n",
      "at batch no 2090\n",
      "at batch no 2095\n",
      "at batch no 2100\n",
      "at batch no 2105\n",
      "at batch no 2110\n",
      "at batch no 2115\n",
      "at batch no 2120\n",
      "at batch no 2125\n",
      "at batch no 2130\n",
      "at batch no 2135\n",
      "at batch no 2140\n",
      "at batch no 2145\n",
      "at batch no 2150\n",
      "at batch no 2155\n",
      "at batch no 2160\n",
      "at batch no 2165\n",
      "at batch no 2170\n",
      "at batch no 2175\n",
      "at batch no 2180\n",
      "at batch no 2185\n",
      "at batch no 2190\n",
      "at batch no 2195\n",
      "at batch no 2200\n",
      "at batch no 2205\n",
      "at batch no 2210\n",
      "at batch no 2215\n",
      "at batch no 2220\n",
      "at batch no 2225\n",
      "at batch no 2230\n",
      "at batch no 2235\n",
      "at batch no 2240\n",
      "at batch no 2245\n",
      "at batch no 2250\n",
      "at batch no 2255\n",
      "at batch no 2260\n",
      "at batch no 2265\n",
      "at batch no 2270\n",
      "at batch no 2275\n",
      "at batch no 2280\n",
      "at batch no 2285\n",
      "at batch no 2290\n",
      "at batch no 2295\n",
      "at batch no 2300\n",
      "at batch no 2305\n",
      "at batch no 2310\n",
      "at batch no 2315\n",
      "at batch no 2320\n",
      "at batch no 2325\n",
      "at batch no 2330\n",
      "at batch no 2335\n",
      "at batch no 2340\n",
      "at batch no 2345\n",
      "at batch no 2350\n",
      "at batch no 2355\n",
      "at batch no 2360\n",
      "at batch no 2365\n",
      "at batch no 2370\n",
      "at batch no 2375\n",
      "at batch no 2380\n",
      "at batch no 2385\n",
      "at batch no 2390\n",
      "at batch no 2395\n",
      "at batch no 2400\n",
      "at batch no 2405\n",
      "at batch no 2410\n",
      "at batch no 2415\n",
      "at batch no 2420\n",
      "at batch no 2425\n",
      "at batch no 2430\n",
      "at batch no 2435\n",
      "at batch no 2440\n",
      "at batch no 2445\n",
      "at batch no 2450\n",
      "at batch no 2455\n",
      "at batch no 2460\n",
      "at batch no 2465\n",
      "at batch no 2470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 2475\n",
      "at batch no 2480\n",
      "at batch no 2485\n",
      "at batch no 2490\n",
      "at batch no 2495\n",
      "at batch no 2500\n",
      "at batch no 2505\n",
      "at batch no 2510\n",
      "at batch no 2515\n",
      "at batch no 2520\n",
      "at batch no 2525\n",
      "at batch no 2530\n",
      "at batch no 2535\n",
      "at batch no 2540\n",
      "at batch no 2545\n",
      "at batch no 2550\n",
      "at batch no 2555\n",
      "at batch no 2560\n",
      "at batch no 2565\n",
      "at batch no 2570\n",
      "at batch no 2575\n",
      "at batch no 2580\n",
      "at batch no 2585\n",
      "at batch no 2590\n",
      "at batch no 2595\n",
      "at batch no 2600\n",
      "at batch no 2605\n",
      "at batch no 2610\n",
      "at batch no 2615\n",
      "at batch no 2620\n",
      "at batch no 2625\n",
      "at batch no 2630\n",
      "at batch no 2635\n",
      "at batch no 2640\n",
      "at batch no 2645\n",
      "at batch no 2650\n",
      "at batch no 2655\n",
      "at batch no 2660\n",
      "at batch no 2665\n",
      "at batch no 2670\n",
      "at batch no 2675\n",
      "at batch no 2680\n",
      "at batch no 2685\n",
      "at batch no 2690\n",
      "at batch no 2695\n",
      "at batch no 2700\n",
      "at batch no 2705\n",
      "at batch no 2710\n",
      "at batch no 2715\n",
      "at batch no 2720\n",
      "at batch no 2725\n",
      "at batch no 2730\n",
      "at batch no 2735\n",
      "at batch no 2740\n",
      "at batch no 2745\n",
      "at batch no 2750\n",
      "at batch no 2755\n",
      "at batch no 2760\n",
      "at batch no 2765\n",
      "at batch no 2770\n",
      "at batch no 2775\n",
      "at batch no 2780\n",
      "at batch no 2785\n",
      "at batch no 2790\n",
      "at batch no 2795\n",
      "at batch no 2800\n",
      "at batch no 2805\n",
      "at batch no 2810\n",
      "at batch no 2815\n",
      "at batch no 2820\n",
      "at batch no 2825\n",
      "at batch no 2830\n",
      "at batch no 2835\n",
      "at batch no 2840\n",
      "at batch no 2845\n",
      "at batch no 2850\n",
      "at batch no 2855\n",
      "at batch no 2860\n",
      "at batch no 2865\n",
      "at batch no 2870\n",
      "at batch no 2875\n",
      "at batch no 2880\n",
      "at batch no 2885\n",
      "at batch no 2890\n",
      "at batch no 2895\n",
      "at batch no 2900\n",
      "at batch no 2905\n",
      "at batch no 2910\n",
      "at batch no 2915\n",
      "at batch no 2920\n",
      "at batch no 2925\n",
      "at batch no 2930\n",
      "at batch no 2935\n",
      "at batch no 2940\n",
      "at batch no 2945\n",
      "at batch no 2950\n",
      "at batch no 2955\n",
      "at batch no 2960\n",
      "at batch no 2965\n",
      "at batch no 2970\n",
      "at batch no 2975\n",
      "at batch no 2980\n",
      "at batch no 2985\n",
      "at batch no 2990\n",
      "at batch no 2995\n",
      "at batch no 3000\n",
      "at batch no 3005\n",
      "at batch no 3010\n",
      "at batch no 3015\n",
      "at batch no 3020\n",
      "at batch no 3025\n",
      "at batch no 3030\n",
      "at batch no 3035\n",
      "at batch no 3040\n",
      "at batch no 3045\n",
      "at batch no 3050\n",
      "at batch no 3055\n",
      "at batch no 3060\n",
      "at batch no 3065\n",
      "at batch no 3070\n",
      "at batch no 3075\n",
      "at batch no 3080\n",
      "at batch no 3085\n",
      "at batch no 3090\n",
      "at batch no 3095\n",
      "at batch no 3100\n",
      "at batch no 3105\n",
      "at batch no 3110\n",
      "at batch no 3115\n",
      "at batch no 3120\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(334,\n",
       " [(489, 15765.10),\n",
       "  (971, 8360.70),\n",
       "  (721, 4217.50),\n",
       "  (109, 1508.00),\n",
       "  (794, 1314.80),\n",
       "  (492, 825.70),\n",
       "  (750, 801.50),\n",
       "  (709, 747.50),\n",
       "  (84, 649.80),\n",
       "  (973, 461.30),\n",
       "  (581, 367.10),\n",
       "  (414, 334.80),\n",
       "  (824, 327.00),\n",
       "  (748, 326.90),\n",
       "  (556, 255.90),\n",
       "  (828, 251.10),\n",
       "  (904, 246.60),\n",
       "  (599, 244.00),\n",
       "  (39, 228.90),\n",
       "  (805, 215.30),\n",
       "  (770, 214.50),\n",
       "  (61, 192.50),\n",
       "  (588, 180.50),\n",
       "  (893, 179.30),\n",
       "  (46, 177.10),\n",
       "  (859, 171.60),\n",
       "  (509, 168.50),\n",
       "  (955, 154.60),\n",
       "  (55, 141.40),\n",
       "  (879, 132.40),\n",
       "  (464, 130.60),\n",
       "  (304, 112.30),\n",
       "  (580, 111.00),\n",
       "  (790, 104.10),\n",
       "  (110, 103.00),\n",
       "  (815, 96.60),\n",
       "  (788, 89.00),\n",
       "  (515, 88.10),\n",
       "  (800, 86.70),\n",
       "  (711, 84.10),\n",
       "  (490, 81.00),\n",
       "  (48, 79.60),\n",
       "  (621, 78.50),\n",
       "  (611, 78.40),\n",
       "  (62, 73.20),\n",
       "  (497, 73.10),\n",
       "  (457, 72.80),\n",
       "  (907, 70.20),\n",
       "  (735, 69.90),\n",
       "  (94, 66.20),\n",
       "  (533, 65.50),\n",
       "  (406, 62.90),\n",
       "  (806, 62.50),\n",
       "  (614, 60.70),\n",
       "  (987, 58.30),\n",
       "  (854, 57.30),\n",
       "  (808, 54.20),\n",
       "  (741, 52.70),\n",
       "  (468, 51.10),\n",
       "  (520, 49.80),\n",
       "  (885, 49.50),\n",
       "  (643, 49.30),\n",
       "  (858, 49.00),\n",
       "  (898, 48.80),\n",
       "  (619, 48.70),\n",
       "  (992, 48.00),\n",
       "  (115, 47.80),\n",
       "  (300, 47.80),\n",
       "  (476, 47.30),\n",
       "  (953, 47.00),\n",
       "  (737, 46.00),\n",
       "  (894, 45.70),\n",
       "  (340, 44.80),\n",
       "  (882, 44.50),\n",
       "  (116, 44.40),\n",
       "  (646, 44.20),\n",
       "  (411, 43.20),\n",
       "  (549, 42.20),\n",
       "  (878, 41.90),\n",
       "  (118, 41.80),\n",
       "  (455, 41.80),\n",
       "  (97, 40.50),\n",
       "  (837, 40.10),\n",
       "  (151, 39.80),\n",
       "  (401, 39.80),\n",
       "  (40, 39.30),\n",
       "  (636, 38.90),\n",
       "  (784, 38.70),\n",
       "  (518, 38.00),\n",
       "  (981, 37.60),\n",
       "  (892, 37.40),\n",
       "  (582, 37.30),\n",
       "  (754, 36.90),\n",
       "  (412, 36.80),\n",
       "  (849, 35.80),\n",
       "  (695, 35.20),\n",
       "  (791, 35.20),\n",
       "  (779, 35.00),\n",
       "  (60, 34.90),\n",
       "  (323, 34.50),\n",
       "  (572, 34.10),\n",
       "  (195, 32.80),\n",
       "  (108, 32.70),\n",
       "  (0, 32.50),\n",
       "  (128, 32.30),\n",
       "  (47, 32.00),\n",
       "  (138, 32.00),\n",
       "  (440, 31.80),\n",
       "  (407, 31.50),\n",
       "  (474, 31.50),\n",
       "  (506, 31.50),\n",
       "  (591, 31.00),\n",
       "  (772, 31.00),\n",
       "  (25, 30.90),\n",
       "  (410, 30.50),\n",
       "  (641, 30.20),\n",
       "  (65, 30.10),\n",
       "  (290, 30.10),\n",
       "  (563, 29.60),\n",
       "  (575, 29.60),\n",
       "  (645, 29.40),\n",
       "  (485, 29.00),\n",
       "  (944, 28.90),\n",
       "  (870, 28.70),\n",
       "  (67, 28.30),\n",
       "  (915, 28.30),\n",
       "  (159, 27.80),\n",
       "  (199, 27.80),\n",
       "  (319, 27.50),\n",
       "  (762, 27.50),\n",
       "  (353, 27.40),\n",
       "  (443, 27.10),\n",
       "  (90, 27.00),\n",
       "  (946, 26.90),\n",
       "  (850, 26.80),\n",
       "  (238, 26.60),\n",
       "  (656, 26.40),\n",
       "  (470, 26.20),\n",
       "  (99, 26.00),\n",
       "  (293, 25.80),\n",
       "  (96, 25.60),\n",
       "  (654, 25.60),\n",
       "  (135, 25.50),\n",
       "  (327, 25.30),\n",
       "  (216, 25.10),\n",
       "  (308, 25.10),\n",
       "  (88, 24.70),\n",
       "  (783, 24.30),\n",
       "  (441, 24.20),\n",
       "  (768, 24.10),\n",
       "  (661, 23.70),\n",
       "  (7, 23.60),\n",
       "  (444, 23.60),\n",
       "  (746, 23.50),\n",
       "  (241, 23.40),\n",
       "  (545, 23.30),\n",
       "  (917, 23.30),\n",
       "  (134, 23.00),\n",
       "  (231, 23.00),\n",
       "  (162, 22.70),\n",
       "  (826, 22.70),\n",
       "  (863, 22.60),\n",
       "  (275, 22.40),\n",
       "  (129, 22.20),\n",
       "  (242, 22.20),\n",
       "  (251, 22.00),\n",
       "  (409, 22.00),\n",
       "  (679, 22.00),\n",
       "  (263, 21.80),\n",
       "  (555, 21.80),\n",
       "  (562, 21.70),\n",
       "  (916, 21.60),\n",
       "  (56, 21.50),\n",
       "  (155, 21.50),\n",
       "  (76, 21.40),\n",
       "  (538, 21.30),\n",
       "  (640, 21.30),\n",
       "  (237, 21.20),\n",
       "  (687, 20.70),\n",
       "  (273, 20.60),\n",
       "  (616, 20.50),\n",
       "  (396, 20.40),\n",
       "  (703, 20.40),\n",
       "  (968, 20.40),\n",
       "  (8, 20.30),\n",
       "  (886, 20.20),\n",
       "  (698, 20.00),\n",
       "  (950, 19.80),\n",
       "  (738, 19.70),\n",
       "  (985, 19.70),\n",
       "  (522, 19.60),\n",
       "  (637, 19.60),\n",
       "  (431, 19.50),\n",
       "  (363, 19.40),\n",
       "  (348, 19.20),\n",
       "  (282, 19.10),\n",
       "  (801, 19.10),\n",
       "  (23, 18.90),\n",
       "  (82, 18.60),\n",
       "  (74, 18.30),\n",
       "  (133, 18.30),\n",
       "  (618, 18.20),\n",
       "  (927, 18.10),\n",
       "  (669, 18.00),\n",
       "  (796, 18.00),\n",
       "  (171, 17.60),\n",
       "  (594, 17.60),\n",
       "  (692, 17.60),\n",
       "  (247, 17.50),\n",
       "  (576, 17.50),\n",
       "  (178, 17.40),\n",
       "  (566, 17.40),\n",
       "  (949, 17.40),\n",
       "  (208, 17.20),\n",
       "  (730, 17.10),\n",
       "  (880, 17.10),\n",
       "  (50, 17.00),\n",
       "  (239, 16.90),\n",
       "  (829, 16.90),\n",
       "  (123, 16.80),\n",
       "  (626, 16.80),\n",
       "  (77, 16.70),\n",
       "  (37, 16.60),\n",
       "  (523, 16.60),\n",
       "  (716, 16.40),\n",
       "  (717, 16.40),\n",
       "  (254, 16.30),\n",
       "  (825, 16.30),\n",
       "  (260, 16.20),\n",
       "  (274, 16.10),\n",
       "  (864, 16.10),\n",
       "  (982, 16.10),\n",
       "  (670, 16.00),\n",
       "  (119, 15.90),\n",
       "  (565, 15.80),\n",
       "  (852, 15.80),\n",
       "  (723, 15.70),\n",
       "  (612, 15.60),\n",
       "  (292, 15.40),\n",
       "  (365, 15.40),\n",
       "  (419, 15.30),\n",
       "  (424, 15.30),\n",
       "  (936, 15.30),\n",
       "  (9, 15.20),\n",
       "  (423, 15.10),\n",
       "  (345, 14.90),\n",
       "  (180, 14.80),\n",
       "  (436, 14.80),\n",
       "  (173, 14.70),\n",
       "  (124, 14.60),\n",
       "  (328, 14.60),\n",
       "  (447, 14.50),\n",
       "  (639, 14.50),\n",
       "  (552, 14.40),\n",
       "  (843, 14.40),\n",
       "  (948, 14.30),\n",
       "  (136, 14.10),\n",
       "  (243, 14.10),\n",
       "  (211, 14.00),\n",
       "  (487, 14.00),\n",
       "  (539, 14.00),\n",
       "  (855, 14.00),\n",
       "  (918, 14.00),\n",
       "  (249, 13.90),\n",
       "  (665, 13.90),\n",
       "  (398, 13.80),\n",
       "  (57, 13.70),\n",
       "  (234, 13.60),\n",
       "  (642, 13.50),\n",
       "  (832, 13.50),\n",
       "  (514, 13.40),\n",
       "  (553, 13.40),\n",
       "  (705, 13.20),\n",
       "  (264, 13.10),\n",
       "  (53, 13.00),\n",
       "  (603, 13.00),\n",
       "  (42, 12.90),\n",
       "  (857, 12.80),\n",
       "  (390, 12.70),\n",
       "  (706, 12.70),\n",
       "  (579, 12.60),\n",
       "  (707, 12.50),\n",
       "  (823, 12.50),\n",
       "  (248, 12.30),\n",
       "  (765, 12.30),\n",
       "  (866, 12.30),\n",
       "  (31, 12.20),\n",
       "  (58, 12.20),\n",
       "  (668, 12.20),\n",
       "  (27, 12.10),\n",
       "  (453, 12.10),\n",
       "  (507, 12.10),\n",
       "  (561, 12.10),\n",
       "  (822, 12.10),\n",
       "  (998, 12.10),\n",
       "  (868, 12.00),\n",
       "  (201, 11.90),\n",
       "  (429, 11.90),\n",
       "  (872, 11.90),\n",
       "  (354, 11.80),\n",
       "  (496, 11.70),\n",
       "  (758, 11.70),\n",
       "  (24, 11.50),\n",
       "  (635, 11.50),\n",
       "  (937, 11.50),\n",
       "  (253, 11.40),\n",
       "  (311, 11.40),\n",
       "  (776, 11.40),\n",
       "  (819, 11.40),\n",
       "  (41, 11.30),\n",
       "  (724, 11.30),\n",
       "  (301, 11.20),\n",
       "  (479, 11.20),\n",
       "  (881, 11.20),\n",
       "  (28, 11.10),\n",
       "  (343, 11.10),\n",
       "  (352, 11.10),\n",
       "  (601, 11.10),\n",
       "  (227, 11.00),\n",
       "  (271, 11.00),\n",
       "  (321, 11.00),\n",
       "  (472, 11.00),\n",
       "  (671, 11.00),\n",
       "  (912, 11.00),\n",
       "  (144, 10.90),\n",
       "  (865, 10.90),\n",
       "  (417, 10.70),\n",
       "  (448, 10.70),\n",
       "  (725, 10.70),\n",
       "  (775, 10.70),\n",
       "  (214, 10.60),\n",
       "  (309, 10.60),\n",
       "  (560, 10.60),\n",
       "  (564, 10.60),\n",
       "  (488, 10.50),\n",
       "  (541, 10.50),\n",
       "  (700, 10.50),\n",
       "  (130, 10.40),\n",
       "  (547, 10.40),\n",
       "  (200, 10.30),\n",
       "  (451, 10.30),\n",
       "  (604, 10.30),\n",
       "  (954, 10.30),\n",
       "  (45, 10.20),\n",
       "  (217, 10.20),\n",
       "  (512, 10.20),\n",
       "  (938, 10.20),\n",
       "  (997, 10.20),\n",
       "  (215, 10.10),\n",
       "  (383, 10.10),\n",
       "  (836, 10.10),\n",
       "  (559, 10.00),\n",
       "  (605, 10.00),\n",
       "  (658, 10.00),\n",
       "  (757, 10.00),\n",
       "  (439, 9.90),\n",
       "  (445, 9.90),\n",
       "  (804, 9.90),\n",
       "  (625, 9.80),\n",
       "  (86, 9.70),\n",
       "  (189, 9.70),\n",
       "  (387, 9.70),\n",
       "  (388, 9.70),\n",
       "  (526, 9.70),\n",
       "  (673, 9.70),\n",
       "  (693, 9.70),\n",
       "  (127, 9.60),\n",
       "  (258, 9.60),\n",
       "  (887, 9.50),\n",
       "  (104, 9.40),\n",
       "  (288, 9.40),\n",
       "  (764, 9.40),\n",
       "  (6, 9.30),\n",
       "  (102, 9.30),\n",
       "  (270, 9.30),\n",
       "  (224, 9.20),\n",
       "  (230, 9.20),\n",
       "  (734, 9.20),\n",
       "  (792, 9.20),\n",
       "  (956, 9.20),\n",
       "  (30, 9.10),\n",
       "  (255, 9.00),\n",
       "  (269, 9.00),\n",
       "  (710, 9.00),\n",
       "  (839, 9.00),\n",
       "  (888, 9.00),\n",
       "  (51, 8.90),\n",
       "  (157, 8.90),\n",
       "  (347, 8.90),\n",
       "  (392, 8.90),\n",
       "  (696, 8.90),\n",
       "  (742, 8.90),\n",
       "  (203, 8.80),\n",
       "  (609, 8.80),\n",
       "  (627, 8.80),\n",
       "  (219, 8.70),\n",
       "  (530, 8.70),\n",
       "  (763, 8.70),\n",
       "  (85, 8.60),\n",
       "  (92, 8.60),\n",
       "  (278, 8.60),\n",
       "  (307, 8.60),\n",
       "  (375, 8.60),\n",
       "  (425, 8.60),\n",
       "  (727, 8.60),\n",
       "  (817, 8.60),\n",
       "  (100, 8.50),\n",
       "  (316, 8.50),\n",
       "  (339, 8.50),\n",
       "  (811, 8.50),\n",
       "  (235, 8.40),\n",
       "  (722, 8.40),\n",
       "  (98, 8.20),\n",
       "  (121, 8.20),\n",
       "  (602, 8.20),\n",
       "  (697, 8.20),\n",
       "  (840, 8.20),\n",
       "  (181, 8.10),\n",
       "  (317, 8.10),\n",
       "  (889, 8.10),\n",
       "  (719, 8.00),\n",
       "  (79, 7.90),\n",
       "  (584, 7.90),\n",
       "  (704, 7.90),\n",
       "  (752, 7.90),\n",
       "  (766, 7.90),\n",
       "  (193, 7.80),\n",
       "  (302, 7.80),\n",
       "  (306, 7.80),\n",
       "  (342, 7.80),\n",
       "  (393, 7.80),\n",
       "  (875, 7.80),\n",
       "  (281, 7.70),\n",
       "  (310, 7.70),\n",
       "  (816, 7.70),\n",
       "  (113, 7.60),\n",
       "  (125, 7.60),\n",
       "  (221, 7.60),\n",
       "  (225, 7.60),\n",
       "  (236, 7.60),\n",
       "  (303, 7.60),\n",
       "  (454, 7.60),\n",
       "  (534, 7.60),\n",
       "  (158, 7.50),\n",
       "  (267, 7.50),\n",
       "  (607, 7.50),\n",
       "  (655, 7.50),\n",
       "  (830, 7.50),\n",
       "  (963, 7.50),\n",
       "  (257, 7.40),\n",
       "  (351, 7.40),\n",
       "  (14, 7.30),\n",
       "  (72, 7.30),\n",
       "  (361, 7.30),\n",
       "  (573, 7.30),\n",
       "  (44, 7.20),\n",
       "  (70, 7.20),\n",
       "  (226, 7.20),\n",
       "  (256, 7.20),\n",
       "  (286, 7.20),\n",
       "  (314, 7.20),\n",
       "  (667, 7.20),\n",
       "  (222, 7.10),\n",
       "  (330, 7.10),\n",
       "  (543, 7.10),\n",
       "  (874, 7.10),\n",
       "  (911, 7.10),\n",
       "  (1, 7.00),\n",
       "  (101, 7.00),\n",
       "  (131, 7.00),\n",
       "  (355, 7.00),\n",
       "  (498, 7.00),\n",
       "  (529, 7.00),\n",
       "  (91, 6.90),\n",
       "  (463, 6.80),\n",
       "  (245, 6.70),\n",
       "  (63, 6.60),\n",
       "  (676, 6.60),\n",
       "  (685, 6.60),\n",
       "  (777, 6.60),\n",
       "  (986, 6.60),\n",
       "  (107, 6.50),\n",
       "  (166, 6.50),\n",
       "  (172, 6.50),\n",
       "  (218, 6.50),\n",
       "  (537, 6.50),\n",
       "  (570, 6.50),\n",
       "  (624, 6.50),\n",
       "  (897, 6.50),\n",
       "  (587, 6.40),\n",
       "  (890, 6.40),\n",
       "  (250, 6.30),\n",
       "  (462, 6.30),\n",
       "  (586, 6.30),\n",
       "  (652, 6.30),\n",
       "  (760, 6.30),\n",
       "  (571, 6.20),\n",
       "  (732, 6.20),\n",
       "  (883, 6.20),\n",
       "  (337, 6.10),\n",
       "  (632, 6.10),\n",
       "  (851, 6.10),\n",
       "  (22, 6.00),\n",
       "  (694, 6.00),\n",
       "  (753, 6.00),\n",
       "  (18, 5.90),\n",
       "  (198, 5.90),\n",
       "  (294, 5.90),\n",
       "  (491, 5.90),\n",
       "  (902, 5.90),\n",
       "  (947, 5.90),\n",
       "  (52, 5.80),\n",
       "  (71, 5.80),\n",
       "  (389, 5.80),\n",
       "  (15, 5.60),\n",
       "  (148, 5.60),\n",
       "  (362, 5.60),\n",
       "  (471, 5.60),\n",
       "  (774, 5.60),\n",
       "  (853, 5.60),\n",
       "  (185, 5.50),\n",
       "  (508, 5.50),\n",
       "  (577, 5.50),\n",
       "  (75, 5.40),\n",
       "  (164, 5.40),\n",
       "  (528, 5.30),\n",
       "  (182, 5.20),\n",
       "  (360, 5.20),\n",
       "  (391, 5.20),\n",
       "  (467, 5.20),\n",
       "  (939, 5.20),\n",
       "  (995, 5.20),\n",
       "  (83, 5.10),\n",
       "  (160, 5.10),\n",
       "  (169, 5.10),\n",
       "  (432, 5.10),\n",
       "  (558, 5.10),\n",
       "  (759, 5.10),\n",
       "  (920, 5.10),\n",
       "  (176, 5.00),\n",
       "  (276, 5.00),\n",
       "  (516, 5.00),\n",
       "  (597, 5.00),\n",
       "  (608, 5.00),\n",
       "  (733, 5.00),\n",
       "  (990, 5.00),\n",
       "  (120, 4.90),\n",
       "  (163, 4.90),\n",
       "  (531, 4.90),\n",
       "  (787, 4.90),\n",
       "  (846, 4.90),\n",
       "  (906, 4.90),\n",
       "  (991, 4.90),\n",
       "  (93, 4.80),\n",
       "  (186, 4.80),\n",
       "  (197, 4.80),\n",
       "  (397, 4.80),\n",
       "  (585, 4.80),\n",
       "  (891, 4.80),\n",
       "  (142, 4.70),\n",
       "  (207, 4.70),\n",
       "  (318, 4.70),\n",
       "  (433, 4.70),\n",
       "  (756, 4.70),\n",
       "  (781, 4.70),\n",
       "  (43, 4.60),\n",
       "  (68, 4.60),\n",
       "  (244, 4.60),\n",
       "  (449, 4.60),\n",
       "  (606, 4.60),\n",
       "  (620, 4.60),\n",
       "  (688, 4.60),\n",
       "  (782, 4.60),\n",
       "  (80, 4.50),\n",
       "  (408, 4.50),\n",
       "  (458, 4.50),\n",
       "  (546, 4.50),\n",
       "  (11, 4.40),\n",
       "  (212, 4.40),\n",
       "  (284, 4.40),\n",
       "  (334, 4.40),\n",
       "  (660, 4.40),\n",
       "  (672, 4.40),\n",
       "  (126, 4.30),\n",
       "  (161, 4.30),\n",
       "  (482, 4.30),\n",
       "  (519, 4.30),\n",
       "  (714, 4.30),\n",
       "  (884, 4.30),\n",
       "  (900, 4.30),\n",
       "  (984, 4.30),\n",
       "  (141, 4.20),\n",
       "  (192, 4.20),\n",
       "  (280, 4.20),\n",
       "  (382, 4.20),\n",
       "  (430, 4.20),\n",
       "  (569, 4.20),\n",
       "  (638, 4.20),\n",
       "  (999, 4.20),\n",
       "  (291, 4.10),\n",
       "  (313, 4.10),\n",
       "  (358, 4.10),\n",
       "  (420, 4.10),\n",
       "  (477, 4.10),\n",
       "  (503, 4.10),\n",
       "  (593, 4.10),\n",
       "  (684, 4.10),\n",
       "  (699, 4.10),\n",
       "  (845, 4.10),\n",
       "  (994, 4.10),\n",
       "  (132, 4.00),\n",
       "  (331, 4.00),\n",
       "  (376, 4.00),\n",
       "  (527, 4.00),\n",
       "  (653, 4.00),\n",
       "  (786, 4.00),\n",
       "  (931, 4.00),\n",
       "  (934, 4.00),\n",
       "  (89, 3.80),\n",
       "  (156, 3.80),\n",
       "  (205, 3.80),\n",
       "  (259, 3.80),\n",
       "  (272, 3.80),\n",
       "  (368, 3.80),\n",
       "  (820, 3.80),\n",
       "  (861, 3.80),\n",
       "  (867, 3.80),\n",
       "  (36, 3.70),\n",
       "  (117, 3.70),\n",
       "  (297, 3.70),\n",
       "  (329, 3.70),\n",
       "  (336, 3.70),\n",
       "  (613, 3.70),\n",
       "  (842, 3.70),\n",
       "  (921, 3.70),\n",
       "  (206, 3.60),\n",
       "  (298, 3.60),\n",
       "  (344, 3.60),\n",
       "  (386, 3.60),\n",
       "  (426, 3.60),\n",
       "  (957, 3.60),\n",
       "  (532, 3.50),\n",
       "  (838, 3.50),\n",
       "  (951, 3.50),\n",
       "  (17, 3.40),\n",
       "  (49, 3.40),\n",
       "  (145, 3.40),\n",
       "  (202, 3.40),\n",
       "  (210, 3.40),\n",
       "  (295, 3.40),\n",
       "  (596, 3.40),\n",
       "  (988, 3.40),\n",
       "  (989, 3.40),\n",
       "  (191, 3.30),\n",
       "  (385, 3.30),\n",
       "  (544, 3.30),\n",
       "  (751, 3.30),\n",
       "  (188, 3.20),\n",
       "  (350, 3.20),\n",
       "  (428, 3.20),\n",
       "  (481, 3.20),\n",
       "  (483, 3.20),\n",
       "  (66, 3.10),\n",
       "  (320, 3.10),\n",
       "  (359, 3.10),\n",
       "  (459, 3.10),\n",
       "  (535, 3.10),\n",
       "  (778, 3.10),\n",
       "  (877, 3.10),\n",
       "  (966, 3.10),\n",
       "  (325, 3.00),\n",
       "  (513, 3.00),\n",
       "  (683, 3.00),\n",
       "  (814, 3.00),\n",
       "  (847, 3.00),\n",
       "  (38, 2.90),\n",
       "  (146, 2.90),\n",
       "  (232, 2.90),\n",
       "  (341, 2.90),\n",
       "  (486, 2.90),\n",
       "  (574, 2.90),\n",
       "  (932, 2.90),\n",
       "  (952, 2.90),\n",
       "  (179, 2.80),\n",
       "  (213, 2.80),\n",
       "  (228, 2.80),\n",
       "  (265, 2.80),\n",
       "  (446, 2.80),\n",
       "  (466, 2.80),\n",
       "  (630, 2.80),\n",
       "  (663, 2.80),\n",
       "  (677, 2.80),\n",
       "  (747, 2.80),\n",
       "  (905, 2.80),\n",
       "  (945, 2.80),\n",
       "  (511, 2.70),\n",
       "  (659, 2.70),\n",
       "  (701, 2.70),\n",
       "  (896, 2.70),\n",
       "  (87, 2.60),\n",
       "  (140, 2.60),\n",
       "  (187, 2.60),\n",
       "  (209, 2.60),\n",
       "  (299, 2.60),\n",
       "  (369, 2.60),\n",
       "  (475, 2.60),\n",
       "  (650, 2.60),\n",
       "  (731, 2.60),\n",
       "  (769, 2.60),\n",
       "  (773, 2.60),\n",
       "  (223, 2.50),\n",
       "  (371, 2.50),\n",
       "  (517, 2.50),\n",
       "  (521, 2.50),\n",
       "  (749, 2.50),\n",
       "  (821, 2.50),\n",
       "  (873, 2.50),\n",
       "  (910, 2.50),\n",
       "  (165, 2.40),\n",
       "  (174, 2.40),\n",
       "  (370, 2.40),\n",
       "  (402, 2.40),\n",
       "  (452, 2.40),\n",
       "  (456, 2.40),\n",
       "  (505, 2.40),\n",
       "  (554, 2.40),\n",
       "  (629, 2.40),\n",
       "  (644, 2.40),\n",
       "  (807, 2.40),\n",
       "  (812, 2.40),\n",
       "  (869, 2.40),\n",
       "  (196, 2.30),\n",
       "  (305, 2.30),\n",
       "  (356, 2.30),\n",
       "  (567, 2.30),\n",
       "  (600, 2.30),\n",
       "  (923, 2.30),\n",
       "  (170, 2.20),\n",
       "  (175, 2.20),\n",
       "  (268, 2.20),\n",
       "  (493, 2.20),\n",
       "  (495, 2.20),\n",
       "  (502, 2.20),\n",
       "  (540, 2.20),\n",
       "  (578, 2.20),\n",
       "  (761, 2.20),\n",
       "  (809, 2.20),\n",
       "  (959, 2.20),\n",
       "  (962, 2.20),\n",
       "  (21, 2.10),\n",
       "  (177, 2.10),\n",
       "  (367, 2.10),\n",
       "  (674, 2.10),\n",
       "  (876, 2.10),\n",
       "  (204, 2.00),\n",
       "  (379, 2.00),\n",
       "  (400, 2.00),\n",
       "  (551, 2.00),\n",
       "  (615, 2.00),\n",
       "  (678, 2.00),\n",
       "  (835, 2.00),\n",
       "  (979, 2.00),\n",
       "  (114, 1.90),\n",
       "  (139, 1.90),\n",
       "  (153, 1.90),\n",
       "  (283, 1.90),\n",
       "  (592, 1.90),\n",
       "  (862, 1.90),\n",
       "  (933, 1.90),\n",
       "  (10, 1.80),\n",
       "  (33, 1.80),\n",
       "  (194, 1.80),\n",
       "  (332, 1.80),\n",
       "  (380, 1.80),\n",
       "  (666, 1.80),\n",
       "  (831, 1.80),\n",
       "  (925, 1.80),\n",
       "  (32, 1.70),\n",
       "  (377, 1.70),\n",
       "  (427, 1.70),\n",
       "  (664, 1.70),\n",
       "  (996, 1.70),\n",
       "  (122, 1.60),\n",
       "  (183, 1.60),\n",
       "  (542, 1.60),\n",
       "  (744, 1.60),\n",
       "  (860, 1.60),\n",
       "  (899, 1.60),\n",
       "  (919, 1.60),\n",
       "  (315, 1.50),\n",
       "  (589, 1.50),\n",
       "  (610, 1.50),\n",
       "  (54, 1.40),\n",
       "  (95, 1.40),\n",
       "  (229, 1.40),\n",
       "  (289, 1.40),\n",
       "  (374, 1.40),\n",
       "  (395, 1.40),\n",
       "  (595, 1.40),\n",
       "  (691, 1.40),\n",
       "  (767, 1.40),\n",
       "  (799, 1.40),\n",
       "  (929, 1.40),\n",
       "  (287, 1.30),\n",
       "  (461, 1.30),\n",
       "  (708, 1.30),\n",
       "  (922, 1.30),\n",
       "  (69, 1.20),\n",
       "  (167, 1.20),\n",
       "  (279, 1.20),\n",
       "  (346, 1.20),\n",
       "  (418, 1.20),\n",
       "  (484, 1.20),\n",
       "  (510, 1.20),\n",
       "  (622, 1.20),\n",
       "  (841, 1.20),\n",
       "  (969, 1.20),\n",
       "  (983, 1.20),\n",
       "  (184, 1.10),\n",
       "  (372, 1.10),\n",
       "  (930, 1.10),\n",
       "  (34, 1.00),\n",
       "  (35, 1.00),\n",
       "  (220, 1.00),\n",
       "  (240, 1.00),\n",
       "  (261, 1.00),\n",
       "  (262, 1.00),\n",
       "  (335, 1.00),\n",
       "  (338, 1.00),\n",
       "  (366, 1.00),\n",
       "  (373, 1.00),\n",
       "  (399, 1.00),\n",
       "  (413, 1.00),\n",
       "  (434, 1.00),\n",
       "  (469, 1.00),\n",
       "  (550, 1.00),\n",
       "  (583, 1.00),\n",
       "  (662, 1.00),\n",
       "  (726, 1.00),\n",
       "  (736, 1.00),\n",
       "  (793, 1.00),\n",
       "  (797, 1.00),\n",
       "  (802, 1.00),\n",
       "  (941, 1.00),\n",
       "  (12, 0.90),\n",
       "  (364, 0.90),\n",
       "  (623, 0.90),\n",
       "  (675, 0.90),\n",
       "  (740, 0.90),\n",
       "  (924, 0.90),\n",
       "  (81, 0.80),\n",
       "  (266, 0.80),\n",
       "  (322, 0.80),\n",
       "  (378, 0.80),\n",
       "  (381, 0.80),\n",
       "  (690, 0.80),\n",
       "  (743, 0.80),\n",
       "  (798, 0.80),\n",
       "  (813, 0.80),\n",
       "  (848, 0.80),\n",
       "  (958, 0.80),\n",
       "  (5, 0.70),\n",
       "  (168, 0.70),\n",
       "  (404, 0.70),\n",
       "  (681, 0.70),\n",
       "  (16, 0.60),\n",
       "  (19, 0.60),\n",
       "  (73, 0.60),\n",
       "  (78, 0.60),\n",
       "  (246, 0.60),\n",
       "  (277, 0.60),\n",
       "  (394, 0.60),\n",
       "  (405, 0.60),\n",
       "  (416, 0.60),\n",
       "  (421, 0.60),\n",
       "  (628, 0.60),\n",
       "  (651, 0.60),\n",
       "  (686, 0.60),\n",
       "  (702, 0.60),\n",
       "  (715, 0.60),\n",
       "  (718, 0.60),\n",
       "  (755, 0.60),\n",
       "  (789, 0.60),\n",
       "  (795, 0.60),\n",
       "  (803, 0.60),\n",
       "  (967, 0.60),\n",
       "  (143, 0.50),\n",
       "  (657, 0.50),\n",
       "  (903, 0.50),\n",
       "  (943, 0.50),\n",
       "  (4, 0.40),\n",
       "  (26, 0.40),\n",
       "  (64, 0.40),\n",
       "  (252, 0.40),\n",
       "  (324, 0.40),\n",
       "  (435, 0.40),\n",
       "  (465, 0.40),\n",
       "  (499, 0.40),\n",
       "  (500, 0.40),\n",
       "  (524, 0.40),\n",
       "  (633, 0.40),\n",
       "  (720, 0.40),\n",
       "  (834, 0.40),\n",
       "  (844, 0.40),\n",
       "  (901, 0.40),\n",
       "  (909, 0.40),\n",
       "  (312, 0.30),\n",
       "  (473, 0.30),\n",
       "  (557, 0.30),\n",
       "  (712, 0.30),\n",
       "  (745, 0.30),\n",
       "  (785, 0.30),\n",
       "  (827, 0.30),\n",
       "  (972, 0.30),\n",
       "  (357, 0.20),\n",
       "  (422, 0.20),\n",
       "  (437, 0.20),\n",
       "  (438, 0.20),\n",
       "  (478, 0.20),\n",
       "  (59, 0.10),\n",
       "  (154, 0.10),\n",
       "  (2, 0.00),\n",
       "  (3, 0.00),\n",
       "  (13, 0.00),\n",
       "  (20, 0.00),\n",
       "  (29, 0.00),\n",
       "  (103, 0.00),\n",
       "  (105, 0.00),\n",
       "  (106, 0.00),\n",
       "  (111, 0.00),\n",
       "  (112, 0.00),\n",
       "  (137, 0.00),\n",
       "  (147, 0.00),\n",
       "  (149, 0.00),\n",
       "  (150, 0.00),\n",
       "  (152, 0.00),\n",
       "  (190, 0.00),\n",
       "  (233, 0.00),\n",
       "  (285, 0.00),\n",
       "  (296, 0.00),\n",
       "  (326, 0.00),\n",
       "  (333, 0.00),\n",
       "  (349, 0.00),\n",
       "  (384, 0.00),\n",
       "  (403, 0.00),\n",
       "  (415, 0.00),\n",
       "  (442, 0.00),\n",
       "  (450, 0.00),\n",
       "  (460, 0.00),\n",
       "  (480, 0.00),\n",
       "  (494, 0.00),\n",
       "  (501, 0.00),\n",
       "  (504, 0.00),\n",
       "  (525, 0.00),\n",
       "  (536, 0.00),\n",
       "  (548, 0.00),\n",
       "  (568, 0.00),\n",
       "  (590, 0.00),\n",
       "  (598, 0.00),\n",
       "  (617, 0.00),\n",
       "  (631, 0.00),\n",
       "  (634, 0.00),\n",
       "  (647, 0.00),\n",
       "  (648, 0.00),\n",
       "  (649, 0.00),\n",
       "  (680, 0.00),\n",
       "  (682, 0.00),\n",
       "  (689, 0.00),\n",
       "  (713, 0.00),\n",
       "  (728, 0.00),\n",
       "  (729, 0.00),\n",
       "  (739, 0.00),\n",
       "  (771, 0.00),\n",
       "  (780, 0.00),\n",
       "  (810, 0.00),\n",
       "  (818, 0.00),\n",
       "  (833, 0.00),\n",
       "  (856, 0.00),\n",
       "  (871, 0.00),\n",
       "  (895, 0.00),\n",
       "  (908, 0.00),\n",
       "  (913, 0.00),\n",
       "  (914, 0.00),\n",
       "  (926, 0.00),\n",
       "  (928, 0.00),\n",
       "  (935, 0.00),\n",
       "  (940, 0.00),\n",
       "  (942, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (964, 0.00),\n",
       "  (965, 0.00),\n",
       "  (970, 0.00),\n",
       "  (974, 0.00),\n",
       "  (975, 0.00),\n",
       "  (976, 0.00),\n",
       "  (977, 0.00),\n",
       "  (978, 0.00),\n",
       "  (980, 0.00),\n",
       "  (993, 0.00)])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 5\n",
      "at batch no 10\n",
      "at batch no 15\n",
      "at batch no 20\n",
      "at batch no 25\n",
      "at batch no 30\n",
      "at batch no 35\n",
      "at batch no 40\n",
      "at batch no 45\n",
      "at batch no 50\n",
      "at batch no 55\n",
      "at batch no 60\n",
      "at batch no 65\n",
      "at batch no 70\n",
      "at batch no 75\n",
      "at batch no 80\n",
      "at batch no 85\n",
      "at batch no 90\n",
      "at batch no 95\n",
      "at batch no 100\n",
      "at batch no 105\n",
      "at batch no 110\n",
      "at batch no 115\n",
      "at batch no 120\n",
      "at batch no 125\n",
      "at batch no 130\n",
      "at batch no 135\n",
      "at batch no 140\n",
      "at batch no 145\n",
      "at batch no 150\n",
      "at batch no 155\n",
      "at batch no 160\n",
      "at batch no 165\n",
      "at batch no 170\n",
      "at batch no 175\n",
      "at batch no 180\n",
      "at batch no 185\n",
      "at batch no 190\n",
      "at batch no 195\n",
      "at batch no 200\n",
      "at batch no 205\n",
      "at batch no 210\n",
      "at batch no 215\n",
      "at batch no 220\n",
      "at batch no 225\n",
      "at batch no 230\n",
      "at batch no 235\n",
      "at batch no 240\n",
      "at batch no 245\n",
      "at batch no 250\n",
      "at batch no 255\n",
      "at batch no 260\n",
      "at batch no 265\n",
      "at batch no 270\n",
      "at batch no 275\n",
      "at batch no 280\n",
      "at batch no 285\n",
      "at batch no 290\n",
      "at batch no 295\n",
      "at batch no 300\n",
      "at batch no 305\n",
      "at batch no 310\n",
      "at batch no 315\n",
      "at batch no 320\n",
      "at batch no 325\n",
      "at batch no 330\n",
      "at batch no 335\n",
      "at batch no 340\n",
      "at batch no 345\n",
      "at batch no 350\n",
      "at batch no 355\n",
      "at batch no 360\n",
      "at batch no 365\n",
      "at batch no 370\n",
      "at batch no 375\n",
      "at batch no 380\n",
      "at batch no 385\n",
      "at batch no 390\n",
      "at batch no 395\n",
      "at batch no 400\n",
      "at batch no 405\n",
      "at batch no 410\n",
      "at batch no 415\n",
      "at batch no 420\n",
      "at batch no 425\n",
      "at batch no 430\n",
      "at batch no 435\n",
      "at batch no 440\n",
      "at batch no 445\n",
      "at batch no 450\n",
      "at batch no 455\n",
      "at batch no 460\n",
      "at batch no 465\n",
      "at batch no 470\n",
      "at batch no 475\n",
      "at batch no 480\n",
      "at batch no 485\n",
      "at batch no 490\n",
      "at batch no 495\n",
      "at batch no 500\n",
      "at batch no 505\n",
      "at batch no 510\n",
      "at batch no 515\n",
      "at batch no 520\n",
      "at batch no 525\n",
      "at batch no 530\n",
      "at batch no 535\n",
      "at batch no 540\n",
      "at batch no 545\n",
      "at batch no 550\n",
      "at batch no 555\n",
      "at batch no 560\n",
      "at batch no 565\n",
      "at batch no 570\n",
      "at batch no 575\n",
      "at batch no 580\n",
      "at batch no 585\n",
      "at batch no 590\n",
      "at batch no 595\n",
      "at batch no 600\n",
      "at batch no 605\n",
      "at batch no 610\n",
      "at batch no 615\n",
      "at batch no 620\n",
      "at batch no 625\n",
      "at batch no 630\n",
      "at batch no 635\n",
      "at batch no 640\n",
      "at batch no 645\n",
      "at batch no 650\n",
      "at batch no 655\n",
      "at batch no 660\n",
      "at batch no 665\n",
      "at batch no 670\n",
      "at batch no 675\n",
      "at batch no 680\n",
      "at batch no 685\n",
      "at batch no 690\n",
      "at batch no 695\n",
      "at batch no 700\n",
      "at batch no 705\n",
      "at batch no 710\n",
      "at batch no 715\n",
      "at batch no 720\n",
      "at batch no 725\n",
      "at batch no 730\n",
      "at batch no 735\n",
      "at batch no 740\n",
      "at batch no 745\n",
      "at batch no 750\n",
      "at batch no 755\n",
      "at batch no 760\n",
      "at batch no 765\n",
      "at batch no 770\n",
      "at batch no 775\n",
      "at batch no 780\n",
      "at batch no 785\n",
      "at batch no 790\n",
      "at batch no 795\n",
      "at batch no 800\n",
      "at batch no 805\n",
      "at batch no 810\n",
      "at batch no 815\n",
      "at batch no 820\n",
      "at batch no 825\n",
      "at batch no 830\n",
      "at batch no 835\n",
      "at batch no 840\n",
      "at batch no 845\n",
      "at batch no 850\n",
      "at batch no 855\n",
      "at batch no 860\n",
      "at batch no 865\n",
      "at batch no 870\n",
      "at batch no 875\n",
      "at batch no 880\n",
      "at batch no 885\n",
      "at batch no 890\n",
      "at batch no 895\n",
      "at batch no 900\n",
      "at batch no 905\n",
      "at batch no 910\n",
      "at batch no 915\n",
      "at batch no 920\n",
      "at batch no 925\n",
      "at batch no 930\n",
      "at batch no 935\n",
      "at batch no 940\n",
      "at batch no 945\n",
      "at batch no 950\n",
      "at batch no 955\n",
      "at batch no 960\n",
      "at batch no 965\n",
      "at batch no 970\n",
      "at batch no 975\n",
      "at batch no 980\n",
      "at batch no 985\n",
      "at batch no 990\n",
      "at batch no 995\n",
      "at batch no 1000\n",
      "at batch no 1005\n",
      "at batch no 1010\n",
      "at batch no 1015\n",
      "at batch no 1020\n",
      "at batch no 1025\n",
      "at batch no 1030\n",
      "at batch no 1035\n",
      "at batch no 1040\n",
      "at batch no 1045\n",
      "at batch no 1050\n",
      "at batch no 1055\n",
      "at batch no 1060\n",
      "at batch no 1065\n",
      "at batch no 1070\n",
      "at batch no 1075\n",
      "at batch no 1080\n",
      "at batch no 1085\n",
      "at batch no 1090\n",
      "at batch no 1095\n",
      "at batch no 1100\n",
      "at batch no 1105\n",
      "at batch no 1110\n",
      "at batch no 1115\n",
      "at batch no 1120\n",
      "at batch no 1125\n",
      "at batch no 1130\n",
      "at batch no 1135\n",
      "at batch no 1140\n",
      "at batch no 1145\n",
      "at batch no 1150\n",
      "at batch no 1155\n",
      "at batch no 1160\n",
      "at batch no 1165\n",
      "at batch no 1170\n",
      "at batch no 1175\n",
      "at batch no 1180\n",
      "at batch no 1185\n",
      "at batch no 1190\n",
      "at batch no 1195\n",
      "at batch no 1200\n",
      "at batch no 1205\n",
      "at batch no 1210\n",
      "at batch no 1215\n",
      "at batch no 1220\n",
      "at batch no 1225\n",
      "at batch no 1230\n",
      "at batch no 1235\n",
      "at batch no 1240\n",
      "at batch no 1245\n",
      "at batch no 1250\n",
      "at batch no 1255\n",
      "at batch no 1260\n",
      "at batch no 1265\n",
      "at batch no 1270\n",
      "at batch no 1275\n",
      "at batch no 1280\n",
      "at batch no 1285\n",
      "at batch no 1290\n",
      "at batch no 1295\n",
      "at batch no 1300\n",
      "at batch no 1305\n",
      "at batch no 1310\n",
      "at batch no 1315\n",
      "at batch no 1320\n",
      "at batch no 1325\n",
      "at batch no 1330\n",
      "at batch no 1335\n",
      "at batch no 1340\n",
      "at batch no 1345\n",
      "at batch no 1350\n",
      "at batch no 1355\n",
      "at batch no 1360\n",
      "at batch no 1365\n",
      "at batch no 1370\n",
      "at batch no 1375\n",
      "at batch no 1380\n",
      "at batch no 1385\n",
      "at batch no 1390\n",
      "at batch no 1395\n",
      "at batch no 1400\n",
      "at batch no 1405\n",
      "at batch no 1410\n",
      "at batch no 1415\n",
      "at batch no 1420\n",
      "at batch no 1425\n",
      "at batch no 1430\n",
      "at batch no 1435\n",
      "at batch no 1440\n",
      "at batch no 1445\n",
      "at batch no 1450\n",
      "at batch no 1455\n",
      "at batch no 1460\n",
      "at batch no 1465\n",
      "at batch no 1470\n",
      "at batch no 1475\n",
      "at batch no 1480\n",
      "at batch no 1485\n",
      "at batch no 1490\n",
      "at batch no 1495\n",
      "at batch no 1500\n",
      "at batch no 1505\n",
      "at batch no 1510\n",
      "at batch no 1515\n",
      "at batch no 1520\n",
      "at batch no 1525\n",
      "at batch no 1530\n",
      "at batch no 1535\n",
      "at batch no 1540\n",
      "at batch no 1545\n",
      "at batch no 1550\n",
      "at batch no 1555\n",
      "at batch no 1560\n",
      "at batch no 1565\n",
      "at batch no 1570\n",
      "at batch no 1575\n",
      "at batch no 1580\n",
      "at batch no 1585\n",
      "at batch no 1590\n",
      "at batch no 1595\n",
      "at batch no 1600\n",
      "at batch no 1605\n",
      "at batch no 1610\n",
      "at batch no 1615\n",
      "at batch no 1620\n",
      "at batch no 1625\n",
      "at batch no 1630\n",
      "at batch no 1635\n",
      "at batch no 1640\n",
      "at batch no 1645\n",
      "at batch no 1650\n",
      "at batch no 1655\n",
      "at batch no 1660\n",
      "at batch no 1665\n",
      "at batch no 1670\n",
      "at batch no 1675\n",
      "at batch no 1680\n",
      "at batch no 1685\n",
      "at batch no 1690\n",
      "at batch no 1695\n",
      "at batch no 1700\n",
      "at batch no 1705\n",
      "at batch no 1710\n",
      "at batch no 1715\n",
      "at batch no 1720\n",
      "at batch no 1725\n",
      "at batch no 1730\n",
      "at batch no 1735\n",
      "at batch no 1740\n",
      "at batch no 1745\n",
      "at batch no 1750\n",
      "at batch no 1755\n",
      "at batch no 1760\n",
      "at batch no 1765\n",
      "at batch no 1770\n",
      "at batch no 1775\n",
      "at batch no 1780\n",
      "at batch no 1785\n",
      "at batch no 1790\n",
      "at batch no 1795\n",
      "at batch no 1800\n",
      "at batch no 1805\n",
      "at batch no 1810\n",
      "at batch no 1815\n",
      "at batch no 1820\n",
      "at batch no 1825\n",
      "at batch no 1830\n",
      "at batch no 1835\n",
      "at batch no 1840\n",
      "at batch no 1845\n",
      "at batch no 1850\n",
      "at batch no 1855\n",
      "at batch no 1860\n",
      "at batch no 1865\n",
      "at batch no 1870\n",
      "at batch no 1875\n",
      "at batch no 1880\n",
      "at batch no 1885\n",
      "at batch no 1890\n",
      "at batch no 1895\n",
      "at batch no 1900\n",
      "at batch no 1905\n",
      "at batch no 1910\n",
      "at batch no 1915\n",
      "at batch no 1920\n",
      "at batch no 1925\n",
      "at batch no 1930\n",
      "at batch no 1935\n",
      "at batch no 1940\n",
      "at batch no 1945\n",
      "at batch no 1950\n",
      "at batch no 1955\n",
      "at batch no 1960\n",
      "at batch no 1965\n",
      "at batch no 1970\n",
      "at batch no 1975\n",
      "at batch no 1980\n",
      "at batch no 1985\n",
      "at batch no 1990\n",
      "at batch no 1995\n",
      "at batch no 2000\n",
      "at batch no 2005\n",
      "at batch no 2010\n",
      "at batch no 2015\n",
      "at batch no 2020\n",
      "at batch no 2025\n",
      "at batch no 2030\n",
      "at batch no 2035\n",
      "at batch no 2040\n",
      "at batch no 2045\n",
      "at batch no 2050\n",
      "at batch no 2055\n",
      "at batch no 2060\n",
      "at batch no 2065\n",
      "at batch no 2070\n",
      "at batch no 2075\n",
      "at batch no 2080\n",
      "at batch no 2085\n",
      "at batch no 2090\n",
      "at batch no 2095\n",
      "at batch no 2100\n",
      "at batch no 2105\n",
      "at batch no 2110\n",
      "at batch no 2115\n",
      "at batch no 2120\n",
      "at batch no 2125\n",
      "at batch no 2130\n",
      "at batch no 2135\n",
      "at batch no 2140\n",
      "at batch no 2145\n",
      "at batch no 2150\n",
      "at batch no 2155\n",
      "at batch no 2160\n",
      "at batch no 2165\n",
      "at batch no 2170\n",
      "at batch no 2175\n",
      "at batch no 2180\n",
      "at batch no 2185\n",
      "at batch no 2190\n",
      "at batch no 2195\n",
      "at batch no 2200\n",
      "at batch no 2205\n",
      "at batch no 2210\n",
      "at batch no 2215\n",
      "at batch no 2220\n",
      "at batch no 2225\n",
      "at batch no 2230\n",
      "at batch no 2235\n",
      "at batch no 2240\n",
      "at batch no 2245\n",
      "at batch no 2250\n",
      "at batch no 2255\n",
      "at batch no 2260\n",
      "at batch no 2265\n",
      "at batch no 2270\n",
      "at batch no 2275\n",
      "at batch no 2280\n",
      "at batch no 2285\n",
      "at batch no 2290\n",
      "at batch no 2295\n",
      "at batch no 2300\n",
      "at batch no 2305\n",
      "at batch no 2310\n",
      "at batch no 2315\n",
      "at batch no 2320\n",
      "at batch no 2325\n",
      "at batch no 2330\n",
      "at batch no 2335\n",
      "at batch no 2340\n",
      "at batch no 2345\n",
      "at batch no 2350\n",
      "at batch no 2355\n",
      "at batch no 2360\n",
      "at batch no 2365\n",
      "at batch no 2370\n",
      "at batch no 2375\n",
      "at batch no 2380\n",
      "at batch no 2385\n",
      "at batch no 2390\n",
      "at batch no 2395\n",
      "at batch no 2400\n",
      "at batch no 2405\n",
      "at batch no 2410\n",
      "at batch no 2415\n",
      "at batch no 2420\n",
      "at batch no 2425\n",
      "at batch no 2430\n",
      "at batch no 2435\n",
      "at batch no 2440\n",
      "at batch no 2445\n",
      "at batch no 2450\n",
      "at batch no 2455\n",
      "at batch no 2460\n",
      "at batch no 2465\n",
      "at batch no 2470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 2475\n",
      "at batch no 2480\n",
      "at batch no 2485\n",
      "at batch no 2490\n",
      "at batch no 2495\n",
      "at batch no 2500\n",
      "at batch no 2505\n",
      "at batch no 2510\n",
      "at batch no 2515\n",
      "at batch no 2520\n",
      "at batch no 2525\n",
      "at batch no 2530\n",
      "at batch no 2535\n",
      "at batch no 2540\n",
      "at batch no 2545\n",
      "at batch no 2550\n",
      "at batch no 2555\n",
      "at batch no 2560\n",
      "at batch no 2565\n",
      "at batch no 2570\n",
      "at batch no 2575\n",
      "at batch no 2580\n",
      "at batch no 2585\n",
      "at batch no 2590\n",
      "at batch no 2595\n",
      "at batch no 2600\n",
      "at batch no 2605\n",
      "at batch no 2610\n",
      "at batch no 2615\n",
      "at batch no 2620\n",
      "at batch no 2625\n",
      "at batch no 2630\n",
      "at batch no 2635\n",
      "at batch no 2640\n",
      "at batch no 2645\n",
      "at batch no 2650\n",
      "at batch no 2655\n",
      "at batch no 2660\n",
      "at batch no 2665\n",
      "at batch no 2670\n",
      "at batch no 2675\n",
      "at batch no 2680\n",
      "at batch no 2685\n",
      "at batch no 2690\n",
      "at batch no 2695\n",
      "at batch no 2700\n",
      "at batch no 2705\n",
      "at batch no 2710\n",
      "at batch no 2715\n",
      "at batch no 2720\n",
      "at batch no 2725\n",
      "at batch no 2730\n",
      "at batch no 2735\n",
      "at batch no 2740\n",
      "at batch no 2745\n",
      "at batch no 2750\n",
      "at batch no 2755\n",
      "at batch no 2760\n",
      "at batch no 2765\n",
      "at batch no 2770\n",
      "at batch no 2775\n",
      "at batch no 2780\n",
      "at batch no 2785\n",
      "at batch no 2790\n",
      "at batch no 2795\n",
      "at batch no 2800\n",
      "at batch no 2805\n",
      "at batch no 2810\n",
      "at batch no 2815\n",
      "at batch no 2820\n",
      "at batch no 2825\n",
      "at batch no 2830\n",
      "at batch no 2835\n",
      "at batch no 2840\n",
      "at batch no 2845\n",
      "at batch no 2850\n",
      "at batch no 2855\n",
      "at batch no 2860\n",
      "at batch no 2865\n",
      "at batch no 2870\n",
      "at batch no 2875\n",
      "at batch no 2880\n",
      "at batch no 2885\n",
      "at batch no 2890\n",
      "at batch no 2895\n",
      "at batch no 2900\n",
      "at batch no 2905\n",
      "at batch no 2910\n",
      "at batch no 2915\n",
      "at batch no 2920\n",
      "at batch no 2925\n",
      "at batch no 2930\n",
      "at batch no 2935\n",
      "at batch no 2940\n",
      "at batch no 2945\n",
      "at batch no 2950\n",
      "at batch no 2955\n",
      "at batch no 2960\n",
      "at batch no 2965\n",
      "at batch no 2970\n",
      "at batch no 2975\n",
      "at batch no 2980\n",
      "at batch no 2985\n",
      "at batch no 2990\n",
      "at batch no 2995\n",
      "at batch no 3000\n",
      "at batch no 3005\n",
      "at batch no 3010\n",
      "at batch no 3015\n",
      "at batch no 3020\n",
      "at batch no 3025\n",
      "at batch no 3030\n",
      "at batch no 3035\n",
      "at batch no 3040\n",
      "at batch no 3045\n",
      "at batch no 3050\n",
      "at batch no 3055\n",
      "at batch no 3060\n",
      "at batch no 3065\n",
      "at batch no 3070\n",
      "at batch no 3075\n",
      "at batch no 3080\n",
      "at batch no 3085\n",
      "at batch no 3090\n",
      "at batch no 3095\n",
      "at batch no 3100\n",
      "at batch no 3105\n",
      "at batch no 3110\n",
      "at batch no 3115\n",
      "at batch no 3120\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(350,\n",
       " [(489, 12676.50),\n",
       "  (971, 6704.70),\n",
       "  (721, 5673.60),\n",
       "  (794, 1615.10),\n",
       "  (109, 1592.00),\n",
       "  (492, 1149.60),\n",
       "  (750, 1050.50),\n",
       "  (709, 856.50),\n",
       "  (84, 733.70),\n",
       "  (973, 471.50),\n",
       "  (581, 462.30),\n",
       "  (414, 440.40),\n",
       "  (824, 386.90),\n",
       "  (556, 381.80),\n",
       "  (748, 378.00),\n",
       "  (828, 312.40),\n",
       "  (904, 291.10),\n",
       "  (805, 283.80),\n",
       "  (770, 277.20),\n",
       "  (599, 272.60),\n",
       "  (39, 267.50),\n",
       "  (859, 237.00),\n",
       "  (588, 217.70),\n",
       "  (61, 214.80),\n",
       "  (46, 207.50),\n",
       "  (893, 196.30),\n",
       "  (509, 180.20),\n",
       "  (879, 171.50),\n",
       "  (55, 169.10),\n",
       "  (955, 168.40),\n",
       "  (464, 146.80),\n",
       "  (580, 139.70),\n",
       "  (304, 132.50),\n",
       "  (790, 127.00),\n",
       "  (815, 122.70),\n",
       "  (110, 115.40),\n",
       "  (515, 107.60),\n",
       "  (788, 107.40),\n",
       "  (711, 98.00),\n",
       "  (800, 96.60),\n",
       "  (490, 92.50),\n",
       "  (457, 91.70),\n",
       "  (48, 91.20),\n",
       "  (94, 86.80),\n",
       "  (611, 85.90),\n",
       "  (735, 84.50),\n",
       "  (62, 82.30),\n",
       "  (621, 80.90),\n",
       "  (497, 79.10),\n",
       "  (907, 77.30),\n",
       "  (806, 75.90),\n",
       "  (614, 69.70),\n",
       "  (533, 68.60),\n",
       "  (520, 65.20),\n",
       "  (854, 64.80),\n",
       "  (882, 63.40),\n",
       "  (406, 63.30),\n",
       "  (987, 61.90),\n",
       "  (898, 57.90),\n",
       "  (643, 57.80),\n",
       "  (808, 57.00),\n",
       "  (741, 56.90),\n",
       "  (549, 56.70),\n",
       "  (858, 56.30),\n",
       "  (115, 55.30),\n",
       "  (619, 54.70),\n",
       "  (468, 54.60),\n",
       "  (894, 54.50),\n",
       "  (885, 54.10),\n",
       "  (300, 52.60),\n",
       "  (476, 52.60),\n",
       "  (412, 52.40),\n",
       "  (116, 52.10),\n",
       "  (97, 49.80),\n",
       "  (953, 49.50),\n",
       "  (636, 48.90),\n",
       "  (992, 48.60),\n",
       "  (837, 48.30),\n",
       "  (411, 47.90),\n",
       "  (646, 47.10),\n",
       "  (784, 47.00),\n",
       "  (737, 46.90),\n",
       "  (401, 46.20),\n",
       "  (340, 45.80),\n",
       "  (878, 45.80),\n",
       "  (118, 45.50),\n",
       "  (849, 45.30),\n",
       "  (151, 45.20),\n",
       "  (455, 44.80),\n",
       "  (518, 43.10),\n",
       "  (892, 42.30),\n",
       "  (40, 41.80),\n",
       "  (754, 41.30),\n",
       "  (772, 40.30),\n",
       "  (791, 39.00),\n",
       "  (695, 38.10),\n",
       "  (572, 37.50),\n",
       "  (47, 37.30),\n",
       "  (60, 36.90),\n",
       "  (485, 36.80),\n",
       "  (915, 36.70),\n",
       "  (582, 36.50),\n",
       "  (195, 36.30),\n",
       "  (591, 36.30),\n",
       "  (323, 35.80),\n",
       "  (779, 35.80),\n",
       "  (641, 35.70),\n",
       "  (981, 35.60),\n",
       "  (470, 35.40),\n",
       "  (108, 34.20),\n",
       "  (443, 34.20),\n",
       "  (474, 34.10),\n",
       "  (0, 34.00),\n",
       "  (410, 34.00),\n",
       "  (563, 33.00),\n",
       "  (319, 32.60),\n",
       "  (128, 32.50),\n",
       "  (407, 32.50),\n",
       "  (440, 31.90),\n",
       "  (290, 31.60),\n",
       "  (506, 31.40),\n",
       "  (199, 31.10),\n",
       "  (25, 31.00),\n",
       "  (65, 30.70),\n",
       "  (645, 30.70),\n",
       "  (762, 30.50),\n",
       "  (575, 30.40),\n",
       "  (138, 30.00),\n",
       "  (850, 29.80),\n",
       "  (944, 29.70),\n",
       "  (308, 29.40),\n",
       "  (654, 29.40),\n",
       "  (90, 29.10),\n",
       "  (67, 29.00),\n",
       "  (159, 29.00),\n",
       "  (656, 29.00),\n",
       "  (870, 28.60),\n",
       "  (216, 28.30),\n",
       "  (946, 27.80),\n",
       "  (353, 27.30),\n",
       "  (238, 26.90),\n",
       "  (96, 26.70),\n",
       "  (327, 26.60),\n",
       "  (746, 26.50),\n",
       "  (783, 26.50),\n",
       "  (88, 26.40),\n",
       "  (7, 26.30),\n",
       "  (679, 26.10),\n",
       "  (441, 26.00),\n",
       "  (538, 25.90),\n",
       "  (263, 25.50),\n",
       "  (917, 25.40),\n",
       "  (135, 25.30),\n",
       "  (231, 25.30),\n",
       "  (237, 25.00),\n",
       "  (129, 24.90),\n",
       "  (155, 24.90),\n",
       "  (431, 24.90),\n",
       "  (669, 24.90),\n",
       "  (293, 24.80),\n",
       "  (768, 24.70),\n",
       "  (618, 24.60),\n",
       "  (863, 24.40),\n",
       "  (242, 24.30),\n",
       "  (241, 23.90),\n",
       "  (409, 23.60),\n",
       "  (251, 23.50),\n",
       "  (545, 23.30),\n",
       "  (698, 23.30),\n",
       "  (916, 23.30),\n",
       "  (162, 23.20),\n",
       "  (562, 23.20),\n",
       "  (661, 23.20),\n",
       "  (444, 23.10),\n",
       "  (396, 22.90),\n",
       "  (76, 22.80),\n",
       "  (826, 22.80),\n",
       "  (275, 22.50),\n",
       "  (687, 22.50),\n",
       "  (56, 22.30),\n",
       "  (555, 22.30),\n",
       "  (637, 22.10),\n",
       "  (8, 22.00),\n",
       "  (282, 22.00),\n",
       "  (886, 22.00),\n",
       "  (616, 21.70),\n",
       "  (801, 21.70),\n",
       "  (985, 21.50),\n",
       "  (703, 21.40),\n",
       "  (74, 21.30),\n",
       "  (796, 21.20),\n",
       "  (99, 21.00),\n",
       "  (640, 21.00),\n",
       "  (968, 20.60),\n",
       "  (738, 20.30),\n",
       "  (523, 20.10),\n",
       "  (208, 20.00),\n",
       "  (348, 20.00),\n",
       "  (363, 19.90),\n",
       "  (626, 19.90),\n",
       "  (447, 19.80),\n",
       "  (927, 19.80),\n",
       "  (576, 19.70),\n",
       "  (77, 19.40),\n",
       "  (134, 19.20),\n",
       "  (522, 19.00),\n",
       "  (171, 18.90),\n",
       "  (730, 18.90),\n",
       "  (247, 18.80),\n",
       "  (565, 18.70),\n",
       "  (273, 18.40),\n",
       "  (668, 18.40),\n",
       "  (692, 18.40),\n",
       "  (239, 18.30),\n",
       "  (345, 18.20),\n",
       "  (566, 18.20),\n",
       "  (594, 18.20),\n",
       "  (982, 18.20),\n",
       "  (133, 18.10),\n",
       "  (423, 18.00),\n",
       "  (123, 17.80),\n",
       "  (829, 17.60),\n",
       "  (864, 17.60),\n",
       "  (949, 17.60),\n",
       "  (950, 17.60),\n",
       "  (717, 17.50),\n",
       "  (880, 17.50),\n",
       "  (23, 17.40),\n",
       "  (716, 17.40),\n",
       "  (723, 17.40),\n",
       "  (82, 17.20),\n",
       "  (119, 17.10),\n",
       "  (254, 17.10),\n",
       "  (424, 17.10),\n",
       "  (124, 17.00),\n",
       "  (178, 17.00),\n",
       "  (436, 17.00),\n",
       "  (855, 17.00),\n",
       "  (948, 17.00),\n",
       "  (37, 16.90),\n",
       "  (852, 16.90),\n",
       "  (670, 16.70),\n",
       "  (832, 16.70),\n",
       "  (50, 16.60),\n",
       "  (487, 16.60),\n",
       "  (419, 16.50),\n",
       "  (553, 16.40),\n",
       "  (398, 16.30),\n",
       "  (53, 16.00),\n",
       "  (180, 16.00),\n",
       "  (328, 16.00),\n",
       "  (552, 16.00),\n",
       "  (707, 16.00),\n",
       "  (260, 15.90),\n",
       "  (539, 15.80),\n",
       "  (825, 15.80),\n",
       "  (843, 15.80),\n",
       "  (514, 15.70),\n",
       "  (612, 15.70),\n",
       "  (639, 15.70),\n",
       "  (705, 15.70),\n",
       "  (173, 15.50),\n",
       "  (274, 15.50),\n",
       "  (936, 15.40),\n",
       "  (700, 15.30),\n",
       "  (365, 15.10),\n",
       "  (31, 14.90),\n",
       "  (292, 14.90),\n",
       "  (857, 14.90),\n",
       "  (823, 14.70),\n",
       "  (710, 14.60),\n",
       "  (243, 14.40),\n",
       "  (249, 14.40),\n",
       "  (211, 14.30),\n",
       "  (234, 14.20),\n",
       "  (642, 14.20),\n",
       "  (868, 14.10),\n",
       "  (918, 13.90),\n",
       "  (136, 13.70),\n",
       "  (390, 13.70),\n",
       "  (579, 13.70),\n",
       "  (507, 13.60),\n",
       "  (57, 13.50),\n",
       "  (560, 13.40),\n",
       "  (665, 13.40),\n",
       "  (264, 13.30),\n",
       "  (706, 13.30),\n",
       "  (9, 13.20),\n",
       "  (603, 13.20),\n",
       "  (561, 13.10),\n",
       "  (673, 13.10),\n",
       "  (819, 13.10),\n",
       "  (42, 13.00),\n",
       "  (453, 13.00),\n",
       "  (27, 12.80),\n",
       "  (58, 12.80),\n",
       "  (248, 12.80),\n",
       "  (822, 12.80),\n",
       "  (765, 12.70),\n",
       "  (776, 12.70),\n",
       "  (865, 12.70),\n",
       "  (758, 12.60),\n",
       "  (201, 12.50),\n",
       "  (881, 12.50),\n",
       "  (253, 12.40),\n",
       "  (724, 12.40),\n",
       "  (866, 12.40),\n",
       "  (311, 12.30),\n",
       "  (836, 12.30),\n",
       "  (144, 12.20),\n",
       "  (601, 12.20),\n",
       "  (301, 12.10),\n",
       "  (24, 12.00),\n",
       "  (451, 12.00),\n",
       "  (937, 12.00),\n",
       "  (496, 11.90),\n",
       "  (41, 11.80),\n",
       "  (309, 11.80),\n",
       "  (448, 11.80),\n",
       "  (635, 11.80),\n",
       "  (671, 11.80),\n",
       "  (725, 11.80),\n",
       "  (742, 11.80),\n",
       "  (872, 11.80),\n",
       "  (417, 11.70),\n",
       "  (429, 11.70),\n",
       "  (354, 11.60),\n",
       "  (472, 11.60),\n",
       "  (757, 11.60),\n",
       "  (306, 11.50),\n",
       "  (512, 11.50),\n",
       "  (625, 11.50),\n",
       "  (488, 11.40),\n",
       "  (526, 11.40),\n",
       "  (604, 11.40),\n",
       "  (764, 11.40),\n",
       "  (775, 11.30),\n",
       "  (938, 11.30),\n",
       "  (727, 11.20),\n",
       "  (214, 11.10),\n",
       "  (321, 11.10),\n",
       "  (564, 11.10),\n",
       "  (887, 11.10),\n",
       "  (28, 11.00),\n",
       "  (86, 11.00),\n",
       "  (352, 11.00),\n",
       "  (541, 11.00),\n",
       "  (605, 11.00),\n",
       "  (811, 11.00),\n",
       "  (998, 11.00),\n",
       "  (130, 10.90),\n",
       "  (227, 10.90),\n",
       "  (479, 10.80),\n",
       "  (547, 10.80),\n",
       "  (559, 10.80),\n",
       "  (954, 10.70),\n",
       "  (45, 10.60),\n",
       "  (200, 10.60),\n",
       "  (271, 10.60),\n",
       "  (529, 10.60),\n",
       "  (719, 10.60),\n",
       "  (51, 10.50),\n",
       "  (189, 10.50),\n",
       "  (215, 10.40),\n",
       "  (288, 10.40),\n",
       "  (534, 10.40),\n",
       "  (697, 10.40),\n",
       "  (127, 10.30),\n",
       "  (217, 10.30),\n",
       "  (722, 10.30),\n",
       "  (734, 10.30),\n",
       "  (219, 10.20),\n",
       "  (627, 10.20),\n",
       "  (804, 10.20),\n",
       "  (997, 10.20),\n",
       "  (307, 10.10),\n",
       "  (343, 10.10),\n",
       "  (193, 10.00),\n",
       "  (203, 10.00),\n",
       "  (316, 10.00),\n",
       "  (387, 9.90),\n",
       "  (602, 9.90),\n",
       "  (912, 9.90),\n",
       "  (383, 9.80),\n",
       "  (392, 9.80),\n",
       "  (425, 9.80),\n",
       "  (693, 9.80),\n",
       "  (839, 9.80),\n",
       "  (302, 9.70),\n",
       "  (658, 9.70),\n",
       "  (30, 9.60),\n",
       "  (388, 9.60),\n",
       "  (439, 9.60),\n",
       "  (696, 9.60),\n",
       "  (92, 9.50),\n",
       "  (100, 9.50),\n",
       "  (445, 9.50),\n",
       "  (760, 9.50),\n",
       "  (255, 9.40),\n",
       "  (347, 9.40),\n",
       "  (911, 9.40),\n",
       "  (6, 9.30),\n",
       "  (222, 9.30),\n",
       "  (235, 9.30),\n",
       "  (584, 9.30),\n",
       "  (607, 9.30),\n",
       "  (609, 9.30),\n",
       "  (956, 9.30),\n",
       "  (102, 9.20),\n",
       "  (104, 9.20),\n",
       "  (752, 9.20),\n",
       "  (889, 9.20),\n",
       "  (224, 9.10),\n",
       "  (258, 9.10),\n",
       "  (278, 9.10),\n",
       "  (763, 9.10),\n",
       "  (792, 9.10),\n",
       "  (888, 9.10),\n",
       "  (317, 9.00),\n",
       "  (530, 9.00),\n",
       "  (766, 9.00),\n",
       "  (85, 8.90),\n",
       "  (125, 8.90),\n",
       "  (339, 8.90),\n",
       "  (840, 8.90),\n",
       "  (121, 8.80),\n",
       "  (704, 8.80),\n",
       "  (230, 8.70),\n",
       "  (113, 8.60),\n",
       "  (157, 8.60),\n",
       "  (257, 8.60),\n",
       "  (310, 8.60),\n",
       "  (816, 8.50),\n",
       "  (875, 8.50),\n",
       "  (79, 8.40),\n",
       "  (883, 8.40),\n",
       "  (98, 8.30),\n",
       "  (281, 8.30),\n",
       "  (393, 8.20),\n",
       "  (573, 8.00),\n",
       "  (817, 8.00),\n",
       "  (498, 7.90),\n",
       "  (655, 7.90),\n",
       "  (830, 7.90),\n",
       "  (897, 7.90),\n",
       "  (181, 7.80),\n",
       "  (463, 7.80),\n",
       "  (777, 7.80),\n",
       "  (226, 7.70),\n",
       "  (256, 7.70),\n",
       "  (303, 7.70),\n",
       "  (342, 7.70),\n",
       "  (375, 7.60),\n",
       "  (70, 7.50),\n",
       "  (270, 7.50),\n",
       "  (570, 7.50),\n",
       "  (851, 7.50),\n",
       "  (963, 7.50),\n",
       "  (107, 7.40),\n",
       "  (454, 7.40),\n",
       "  (587, 7.40),\n",
       "  (676, 7.40),\n",
       "  (44, 7.30),\n",
       "  (221, 7.30),\n",
       "  (337, 7.30),\n",
       "  (351, 7.30),\n",
       "  (361, 7.30),\n",
       "  (874, 7.30),\n",
       "  (902, 7.30),\n",
       "  (14, 7.20),\n",
       "  (245, 7.20),\n",
       "  (314, 7.20),\n",
       "  (732, 7.20),\n",
       "  (986, 7.20),\n",
       "  (1, 7.10),\n",
       "  (846, 7.10),\n",
       "  (906, 7.10),\n",
       "  (72, 7.00),\n",
       "  (101, 7.00),\n",
       "  (131, 7.00),\n",
       "  (269, 7.00),\n",
       "  (652, 7.00),\n",
       "  (667, 7.00),\n",
       "  (537, 6.90),\n",
       "  (543, 6.90),\n",
       "  (586, 6.90),\n",
       "  (624, 6.90),\n",
       "  (158, 6.80),\n",
       "  (236, 6.80),\n",
       "  (225, 6.70),\n",
       "  (890, 6.70),\n",
       "  (71, 6.60),\n",
       "  (218, 6.60),\n",
       "  (267, 6.60),\n",
       "  (330, 6.60),\n",
       "  (391, 6.60),\n",
       "  (632, 6.60),\n",
       "  (753, 6.60),\n",
       "  (685, 6.50),\n",
       "  (52, 6.40),\n",
       "  (286, 6.40),\n",
       "  (759, 6.40),\n",
       "  (867, 6.40),\n",
       "  (22, 6.30),\n",
       "  (516, 6.30),\n",
       "  (947, 6.30),\n",
       "  (63, 6.20),\n",
       "  (75, 6.20),\n",
       "  (91, 6.20),\n",
       "  (318, 6.20),\n",
       "  (389, 6.20),\n",
       "  (558, 6.20),\n",
       "  (694, 6.20),\n",
       "  (198, 6.10),\n",
       "  (294, 6.10),\n",
       "  (508, 6.10),\n",
       "  (571, 6.10),\n",
       "  (166, 6.00),\n",
       "  (360, 6.00),\n",
       "  (432, 5.90),\n",
       "  (172, 5.80),\n",
       "  (250, 5.80),\n",
       "  (462, 5.70),\n",
       "  (471, 5.70),\n",
       "  (569, 5.70),\n",
       "  (577, 5.70),\n",
       "  (142, 5.60),\n",
       "  (148, 5.60),\n",
       "  (185, 5.60),\n",
       "  (355, 5.60),\n",
       "  (362, 5.60),\n",
       "  (853, 5.60),\n",
       "  (984, 5.60),\n",
       "  (15, 5.50),\n",
       "  (120, 5.50),\n",
       "  (408, 5.50),\n",
       "  (482, 5.50),\n",
       "  (756, 5.50),\n",
       "  (18, 5.40),\n",
       "  (126, 5.40),\n",
       "  (163, 5.40),\n",
       "  (164, 5.40),\n",
       "  (606, 5.40),\n",
       "  (733, 5.40),\n",
       "  (787, 5.40),\n",
       "  (991, 5.40),\n",
       "  (313, 5.30),\n",
       "  (433, 5.30),\n",
       "  (491, 5.30),\n",
       "  (546, 5.30),\n",
       "  (620, 5.30),\n",
       "  (593, 5.20),\n",
       "  (397, 5.10),\n",
       "  (80, 5.00),\n",
       "  (169, 5.00),\n",
       "  (182, 5.00),\n",
       "  (467, 5.00),\n",
       "  (531, 5.00),\n",
       "  (597, 5.00),\n",
       "  (688, 5.00),\n",
       "  (842, 5.00),\n",
       "  (884, 5.00),\n",
       "  (920, 5.00),\n",
       "  (939, 5.00),\n",
       "  (990, 5.00),\n",
       "  (11, 4.90),\n",
       "  (83, 4.90),\n",
       "  (714, 4.90),\n",
       "  (845, 4.90),\n",
       "  (43, 4.80),\n",
       "  (192, 4.80),\n",
       "  (284, 4.80),\n",
       "  (786, 4.80),\n",
       "  (995, 4.80),\n",
       "  (160, 4.70),\n",
       "  (161, 4.70),\n",
       "  (528, 4.70),\n",
       "  (585, 4.70),\n",
       "  (778, 4.70),\n",
       "  (891, 4.70),\n",
       "  (999, 4.70),\n",
       "  (176, 4.60),\n",
       "  (280, 4.60),\n",
       "  (334, 4.60),\n",
       "  (358, 4.60),\n",
       "  (449, 4.60),\n",
       "  (477, 4.60),\n",
       "  (638, 4.60),\n",
       "  (699, 4.60),\n",
       "  (774, 4.60),\n",
       "  (782, 4.60),\n",
       "  (934, 4.60),\n",
       "  (207, 4.50),\n",
       "  (458, 4.50),\n",
       "  (672, 4.50),\n",
       "  (781, 4.50),\n",
       "  (847, 4.50),\n",
       "  (931, 4.50),\n",
       "  (951, 4.50),\n",
       "  (132, 4.40),\n",
       "  (141, 4.40),\n",
       "  (276, 4.40),\n",
       "  (329, 4.40),\n",
       "  (613, 4.40),\n",
       "  (900, 4.40),\n",
       "  (68, 4.30),\n",
       "  (89, 4.30),\n",
       "  (186, 4.30),\n",
       "  (320, 4.30),\n",
       "  (420, 4.30),\n",
       "  (527, 4.30),\n",
       "  (660, 4.30),\n",
       "  (93, 4.20),\n",
       "  (291, 4.20),\n",
       "  (331, 4.20),\n",
       "  (430, 4.20),\n",
       "  (503, 4.20),\n",
       "  (519, 4.20),\n",
       "  (608, 4.20),\n",
       "  (966, 4.20),\n",
       "  (117, 4.10),\n",
       "  (244, 4.10),\n",
       "  (336, 4.10),\n",
       "  (921, 4.10),\n",
       "  (156, 4.00),\n",
       "  (684, 4.00),\n",
       "  (820, 4.00),\n",
       "  (838, 4.00),\n",
       "  (49, 3.90),\n",
       "  (197, 3.90),\n",
       "  (210, 3.90),\n",
       "  (212, 3.90),\n",
       "  (298, 3.90),\n",
       "  (459, 3.90),\n",
       "  (653, 3.90),\n",
       "  (36, 3.80),\n",
       "  (486, 3.80),\n",
       "  (650, 3.80),\n",
       "  (677, 3.80),\n",
       "  (683, 3.80),\n",
       "  (994, 3.80),\n",
       "  (202, 3.70),\n",
       "  (259, 3.70),\n",
       "  (272, 3.70),\n",
       "  (386, 3.70),\n",
       "  (532, 3.70),\n",
       "  (957, 3.70),\n",
       "  (66, 3.60),\n",
       "  (145, 3.60),\n",
       "  (188, 3.60),\n",
       "  (265, 3.60),\n",
       "  (382, 3.60),\n",
       "  (426, 3.60),\n",
       "  (861, 3.60),\n",
       "  (38, 3.50),\n",
       "  (191, 3.50),\n",
       "  (205, 3.50),\n",
       "  (206, 3.50),\n",
       "  (344, 3.50),\n",
       "  (376, 3.50),\n",
       "  (513, 3.50),\n",
       "  (701, 3.50),\n",
       "  (814, 3.50),\n",
       "  (988, 3.50),\n",
       "  (341, 3.40),\n",
       "  (446, 3.40),\n",
       "  (481, 3.40),\n",
       "  (544, 3.40),\n",
       "  (989, 3.40),\n",
       "  (228, 3.30),\n",
       "  (297, 3.30),\n",
       "  (428, 3.30),\n",
       "  (87, 3.20),\n",
       "  (466, 3.20),\n",
       "  (483, 3.20),\n",
       "  (517, 3.20),\n",
       "  (596, 3.20),\n",
       "  (905, 3.20),\n",
       "  (17, 3.10),\n",
       "  (350, 3.10),\n",
       "  (535, 3.10),\n",
       "  (747, 3.10),\n",
       "  (751, 3.10),\n",
       "  (325, 3.00),\n",
       "  (385, 3.00),\n",
       "  (659, 3.00),\n",
       "  (678, 3.00),\n",
       "  (731, 3.00),\n",
       "  (821, 3.00),\n",
       "  (877, 3.00),\n",
       "  (232, 2.90),\n",
       "  (368, 2.90),\n",
       "  (452, 2.90),\n",
       "  (663, 2.90),\n",
       "  (769, 2.90),\n",
       "  (945, 2.90),\n",
       "  (295, 2.80),\n",
       "  (356, 2.80),\n",
       "  (359, 2.80),\n",
       "  (402, 2.80),\n",
       "  (475, 2.80),\n",
       "  (574, 2.80),\n",
       "  (749, 2.80),\n",
       "  (140, 2.70),\n",
       "  (146, 2.70),\n",
       "  (511, 2.70),\n",
       "  (540, 2.70),\n",
       "  (809, 2.70),\n",
       "  (873, 2.70),\n",
       "  (952, 2.70),\n",
       "  (962, 2.70),\n",
       "  (153, 2.60),\n",
       "  (179, 2.60),\n",
       "  (187, 2.60),\n",
       "  (629, 2.60),\n",
       "  (933, 2.60),\n",
       "  (21, 2.50),\n",
       "  (209, 2.50),\n",
       "  (213, 2.50),\n",
       "  (305, 2.50),\n",
       "  (370, 2.50),\n",
       "  (456, 2.50),\n",
       "  (521, 2.50),\n",
       "  (554, 2.50),\n",
       "  (567, 2.50),\n",
       "  (812, 2.50),\n",
       "  (896, 2.50),\n",
       "  (114, 2.40),\n",
       "  (371, 2.40),\n",
       "  (495, 2.40),\n",
       "  (505, 2.40),\n",
       "  (600, 2.40),\n",
       "  (761, 2.40),\n",
       "  (773, 2.40),\n",
       "  (807, 2.40),\n",
       "  (862, 2.40),\n",
       "  (932, 2.40),\n",
       "  (959, 2.40),\n",
       "  (174, 2.30),\n",
       "  (177, 2.30),\n",
       "  (369, 2.30),\n",
       "  (644, 2.30),\n",
       "  (674, 2.30),\n",
       "  (910, 2.30),\n",
       "  (923, 2.30),\n",
       "  (196, 2.20),\n",
       "  (283, 2.20),\n",
       "  (299, 2.20),\n",
       "  (493, 2.20),\n",
       "  (835, 2.20),\n",
       "  (869, 2.20),\n",
       "  (979, 2.20),\n",
       "  (139, 2.10),\n",
       "  (183, 2.10),\n",
       "  (542, 2.10),\n",
       "  (578, 2.10),\n",
       "  (622, 2.10),\n",
       "  (630, 2.10),\n",
       "  (876, 2.10),\n",
       "  (925, 2.10),\n",
       "  (10, 2.00),\n",
       "  (204, 2.00),\n",
       "  (268, 2.00),\n",
       "  (332, 2.00),\n",
       "  (377, 2.00),\n",
       "  (379, 2.00),\n",
       "  (400, 2.00),\n",
       "  (427, 2.00),\n",
       "  (502, 2.00),\n",
       "  (615, 2.00),\n",
       "  (664, 2.00),\n",
       "  (831, 2.00),\n",
       "  (33, 1.90),\n",
       "  (170, 1.90),\n",
       "  (194, 1.90),\n",
       "  (983, 1.90),\n",
       "  (175, 1.80),\n",
       "  (315, 1.80),\n",
       "  (592, 1.80),\n",
       "  (691, 1.80),\n",
       "  (996, 1.80),\n",
       "  (380, 1.70),\n",
       "  (551, 1.70),\n",
       "  (708, 1.70),\n",
       "  (860, 1.70),\n",
       "  (12, 1.60),\n",
       "  (32, 1.60),\n",
       "  (54, 1.60),\n",
       "  (165, 1.60),\n",
       "  (220, 1.60),\n",
       "  (223, 1.60),\n",
       "  (229, 1.60),\n",
       "  (287, 1.60),\n",
       "  (289, 1.60),\n",
       "  (367, 1.60),\n",
       "  (461, 1.60),\n",
       "  (919, 1.60),\n",
       "  (338, 1.50),\n",
       "  (610, 1.50),\n",
       "  (666, 1.50),\n",
       "  (799, 1.50),\n",
       "  (929, 1.50),\n",
       "  (122, 1.40),\n",
       "  (395, 1.40),\n",
       "  (418, 1.40),\n",
       "  (589, 1.40),\n",
       "  (374, 1.30),\n",
       "  (744, 1.30),\n",
       "  (767, 1.30),\n",
       "  (899, 1.30),\n",
       "  (95, 1.20),\n",
       "  (184, 1.20),\n",
       "  (322, 1.20),\n",
       "  (469, 1.20),\n",
       "  (484, 1.20),\n",
       "  (510, 1.20),\n",
       "  (595, 1.20),\n",
       "  (623, 1.20),\n",
       "  (736, 1.20),\n",
       "  (841, 1.20),\n",
       "  (922, 1.20),\n",
       "  (924, 1.20),\n",
       "  (69, 1.10),\n",
       "  (346, 1.10),\n",
       "  (364, 1.10),\n",
       "  (399, 1.10),\n",
       "  (404, 1.10),\n",
       "  (413, 1.10),\n",
       "  (434, 1.10),\n",
       "  (583, 1.10),\n",
       "  (726, 1.10),\n",
       "  (34, 1.00),\n",
       "  (81, 1.00),\n",
       "  (167, 1.00),\n",
       "  (240, 1.00),\n",
       "  (262, 1.00),\n",
       "  (279, 1.00),\n",
       "  (335, 1.00),\n",
       "  (372, 1.00),\n",
       "  (550, 1.00),\n",
       "  (662, 1.00),\n",
       "  (675, 1.00),\n",
       "  (755, 1.00),\n",
       "  (793, 1.00),\n",
       "  (797, 1.00),\n",
       "  (802, 1.00),\n",
       "  (941, 1.00),\n",
       "  (958, 1.00),\n",
       "  (35, 0.90),\n",
       "  (154, 0.90),\n",
       "  (261, 0.90),\n",
       "  (266, 0.90),\n",
       "  (366, 0.90),\n",
       "  (681, 0.90),\n",
       "  (690, 0.90),\n",
       "  (969, 0.90),\n",
       "  (16, 0.80),\n",
       "  (19, 0.80),\n",
       "  (373, 0.80),\n",
       "  (405, 0.80),\n",
       "  (437, 0.80),\n",
       "  (651, 0.80),\n",
       "  (686, 0.80),\n",
       "  (718, 0.80),\n",
       "  (720, 0.80),\n",
       "  (789, 0.80),\n",
       "  (795, 0.80),\n",
       "  (903, 0.80),\n",
       "  (73, 0.70),\n",
       "  (312, 0.70),\n",
       "  (394, 0.70),\n",
       "  (628, 0.70),\n",
       "  (633, 0.70),\n",
       "  (740, 0.70),\n",
       "  (785, 0.70),\n",
       "  (803, 0.70),\n",
       "  (930, 0.70),\n",
       "  (967, 0.70),\n",
       "  (5, 0.60),\n",
       "  (64, 0.60),\n",
       "  (78, 0.60),\n",
       "  (143, 0.60),\n",
       "  (246, 0.60),\n",
       "  (378, 0.60),\n",
       "  (381, 0.60),\n",
       "  (416, 0.60),\n",
       "  (421, 0.60),\n",
       "  (657, 0.60),\n",
       "  (702, 0.60),\n",
       "  (798, 0.60),\n",
       "  (277, 0.50),\n",
       "  (473, 0.50),\n",
       "  (715, 0.50),\n",
       "  (743, 0.50),\n",
       "  (745, 0.50),\n",
       "  (813, 0.50),\n",
       "  (827, 0.50),\n",
       "  (844, 0.50),\n",
       "  (26, 0.40),\n",
       "  (168, 0.40),\n",
       "  (435, 0.40),\n",
       "  (834, 0.40),\n",
       "  (848, 0.40),\n",
       "  (909, 0.40),\n",
       "  (972, 0.40),\n",
       "  (252, 0.30),\n",
       "  (324, 0.30),\n",
       "  (465, 0.30),\n",
       "  (499, 0.30),\n",
       "  (557, 0.30),\n",
       "  (871, 0.30),\n",
       "  (901, 0.30),\n",
       "  (943, 0.30),\n",
       "  (4, 0.20),\n",
       "  (105, 0.20),\n",
       "  (296, 0.20),\n",
       "  (326, 0.20),\n",
       "  (422, 0.20),\n",
       "  (500, 0.20),\n",
       "  (524, 0.20),\n",
       "  (647, 0.20),\n",
       "  (649, 0.20),\n",
       "  (712, 0.20),\n",
       "  (728, 0.20),\n",
       "  (913, 0.20),\n",
       "  (942, 0.20),\n",
       "  (2, 0.10),\n",
       "  (59, 0.10),\n",
       "  (112, 0.10),\n",
       "  (190, 0.10),\n",
       "  (357, 0.10),\n",
       "  (438, 0.10),\n",
       "  (478, 0.10),\n",
       "  (631, 0.10),\n",
       "  (680, 0.10),\n",
       "  (682, 0.10),\n",
       "  (771, 0.10),\n",
       "  (780, 0.10),\n",
       "  (818, 0.10),\n",
       "  (993, 0.10),\n",
       "  (3, 0.00),\n",
       "  (13, 0.00),\n",
       "  (20, 0.00),\n",
       "  (29, 0.00),\n",
       "  (103, 0.00),\n",
       "  (106, 0.00),\n",
       "  (111, 0.00),\n",
       "  (137, 0.00),\n",
       "  (147, 0.00),\n",
       "  (149, 0.00),\n",
       "  (150, 0.00),\n",
       "  (152, 0.00),\n",
       "  (233, 0.00),\n",
       "  (285, 0.00),\n",
       "  (333, 0.00),\n",
       "  (349, 0.00),\n",
       "  (384, 0.00),\n",
       "  (403, 0.00),\n",
       "  (415, 0.00),\n",
       "  (442, 0.00),\n",
       "  (450, 0.00),\n",
       "  (460, 0.00),\n",
       "  (480, 0.00),\n",
       "  (494, 0.00),\n",
       "  (501, 0.00),\n",
       "  (504, 0.00),\n",
       "  (525, 0.00),\n",
       "  (536, 0.00),\n",
       "  (548, 0.00),\n",
       "  (568, 0.00),\n",
       "  (590, 0.00),\n",
       "  (598, 0.00),\n",
       "  (617, 0.00),\n",
       "  (634, 0.00),\n",
       "  (648, 0.00),\n",
       "  (689, 0.00),\n",
       "  (713, 0.00),\n",
       "  (729, 0.00),\n",
       "  (739, 0.00),\n",
       "  (810, 0.00),\n",
       "  (833, 0.00),\n",
       "  (856, 0.00),\n",
       "  (895, 0.00),\n",
       "  (908, 0.00),\n",
       "  (914, 0.00),\n",
       "  (926, 0.00),\n",
       "  (928, 0.00),\n",
       "  (935, 0.00),\n",
       "  (940, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (964, 0.00),\n",
       "  (965, 0.00),\n",
       "  (970, 0.00),\n",
       "  (974, 0.00),\n",
       "  (975, 0.00),\n",
       "  (976, 0.00),\n",
       "  (977, 0.00),\n",
       "  (978, 0.00),\n",
       "  (980, 0.00)])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1ad9ffa518>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# on test\n",
    "sorted_hist = sorted(hist, key=lambda x: x[0], reverse = False)\n",
    "values = [elem[1] for elem in sorted_hist]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.4997)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "entropy(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fCvFG0VMKts"
   },
   "outputs": [],
   "source": [
    "def make_triplet_samples(z, margin, r2, r3):\n",
    "  positive_sample = z + random_vector_volume(z.shape, 0, margin).cuda() \n",
    "  negative_sample = z + random_vector_volume(z.shape, r2, r3).cuda()\n",
    "  return positive_sample, negative_sample\n",
    "\n",
    "def random_vector_surface(shape, r = 1.):\n",
    "  mat = torch.randn(size=shape).cuda()\n",
    "  norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "  return (mat/norm) * r\n",
    "\n",
    "def random_vector_volume(shape, inner_r, outer_r):\n",
    "  fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "  fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "  fraction.unsqueeze_(-1)\n",
    "  return random_vector_surface(shape, 1) * fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PizmBkGqMKtu"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_frequent(x):\n",
    "  return Counter(x).most_common(1)[0]\n",
    "\n",
    "def preds_around(center, radius, n_preds, model, dummy_img):\n",
    "  z_s = random_vector_volume([n_preds, 10], radius, radius + 0.01) + center[None]\n",
    "  noises = model.forward_z(z_s)\n",
    "  perturbed_imgs = noises + dummy_img \n",
    "  return torch.argmax(arch(perturbed_imgs), 1)\n",
    "  \n",
    "def most_freq_pred_around(center, radius, n_preds, model, dummy_img):\n",
    "  preds = preds_around(center, radius, n_preds, model, dummy_img)\n",
    "  most_freq = most_frequent(preds.tolist())\n",
    "  return (class_index_to_label(most_freq[0]), most_freq[1]/n_preds)\n",
    "\n",
    "def investigate_neighborhood(z, step, model, dummy_img):\n",
    "  with torch.no_grad():\n",
    "    result = []\n",
    "    for radius in np.arange(0.1, 6., step):\n",
    "#       print(\"creating {} more preds\".format(int(10 + 5 * (radius ** 2))))\n",
    "      most_freq_pred = most_freq_pred_around(z, radius, int(10 + 5 * (radius ** 2)), model, dummy_img)\n",
    "      result.append((radius, most_freq_pred))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-btRW4qMKtw",
    "outputId": "90e81f6a-1b9e-45a4-ae82-bda370319bd9"
   },
   "outputs": [],
   "source": [
    "#experiment 1\n",
    "\n",
    "z = torch.tensor([0.5] * 10).cuda()\n",
    "# z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "# z_s = z[None]\n",
    "\n",
    "model = learn.model.eval()\n",
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "  \n",
    "for i in range(6):\n",
    "  z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "  print(\"investigation for: \", z)\n",
    "  for elem in investigate_neighborhood(z, 0.5, model, x_img):\n",
    "    print(elem)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 1-1: modified investigate_z\n",
    "z_investigate_path = '/root/Derakhshani/adversarial/textual_notes/investigate_z_{}.txt'.format(env.save_filename)\n",
    "if Path(z_investigate_path).exists(): raise FileExistsError(\"file already exists\")\n",
    "file = open(str(z_investigate_path), 'w')\n",
    "        \n",
    "for i, (z, noise) in enumerate(zip(pruned_z_s, pruned_noises)):\n",
    "  hist = compute_prediction_histogram(learn, noise)\n",
    "  indexed_hist = [(i, val) for i, val in enumerate(hist)]\n",
    "  sorted_hist = sorted(indexed_hist, key=lambda x: x[1], reverse=True)\n",
    "  labeled_hist = [(class_index_to_label(i), count) for i, count in sorted_hist]\n",
    "  print(\"result {}:\".format(i))\n",
    "  print(big_vector_to_str(z))\n",
    "  print(labeled_hist[:6])\n",
    "  print(\"\\n\\n\")\n",
    "  \n",
    "  file.write(\"result {}:\\n\".format(i))\n",
    "  file.write(big_vector_to_str(z) + \"\\n\")\n",
    "  file.write(str(labeled_hist[:6]))\n",
    "  file.write(\"\\n\\n\\n\")\n",
    "  file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bp6YOnipMKtz"
   },
   "outputs": [],
   "source": [
    "#experiment 2\n",
    "import itertools\n",
    "z_s = [torch.tensor(t).cuda() for t in itertools.product( *([[-0.33, 0.33]] * 10) )]\n",
    "model = learn.model.eval()\n",
    "noises = []\n",
    "with torch.no_grad():\n",
    "  for z in z_s:\n",
    "    noises.append(model.forward_single_z(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "55lErWDyMKt1",
    "outputId": "93d8fb71-3fd5-44a5-d3ec-8013e13f17ba"
   },
   "outputs": [],
   "source": [
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "\n",
    "preds = []\n",
    "for noise in noises:\n",
    "  perturbed_img = x_img + noise\n",
    "  preds.append(torch.argmax(arch(perturbed_img[None]), 1)[0].item())\n",
    "\n",
    "from collections import Counter\n",
    "result = [(class_index_to_label(index), count) for index, count in Counter(preds).most_common(5)]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4WSg-wBFMKt5"
   },
   "outputs": [],
   "source": [
    "#experiment 3\n",
    "import itertools\n",
    "dimension_values = [[-0.9, 0.9]] * z_dim\n",
    "for i in range(z_dim):\n",
    "  if i % 100 != 0:\n",
    "    dimension_values[i] = [0.]\n",
    "# dimension_values[0] = [0.]\n",
    "# dimension_values[3] = [0.]\n",
    "# dimension_values[6] = [0.]\n",
    "# dimension_values[9] = [0.]\n",
    "pruned_z_s = [torch.tensor(t).cuda() for t in itertools.product(*dimension_values)]\n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 3: for the targeted-attack case\n",
    "pruned_z_s = []\n",
    "for i in range(z_dim):\n",
    "  new_z = torch.zeros(z_dim).cuda()\n",
    "  new_z[i] = 1.\n",
    "  pruned_z_s.append(new_z)\n",
    "  \n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 3-1: noises for \n",
    "pruned_z_s = []\n",
    "# for i in range(z_dim):\n",
    "#   new_z = torch.empty(z_dim).uniform_(0,1).cuda().detach()\n",
    "#   pruned_z_s.append(new_z)\n",
    "\n",
    "for i in range(z_dim):\n",
    "  new_z = torch.zeros(z_dim).cuda().detach()\n",
    "  new_z[i] = 1.\n",
    "  pruned_z_s.append(new_z)\n",
    "  \n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise in pruned_noises[0:200]:\n",
    "  img = noise_to_image(noise)\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider web\n",
    "z_values = [\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33],\n",
    "  [-0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33,  0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33,  0.33,  0.33],\n",
    "  [-0.33,  0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_values = [\n",
    "  # window screen\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuAVZzmKMKt9",
    "outputId": "7e6643e0-ce55-438b-e0ae-79bde3ee4cef"
   },
   "outputs": [],
   "source": [
    "#vgg-16_12 most repeated labels:\n",
    "l = [(611, 215.0),\n",
    "  (474, 194.1),\n",
    "  (398, 120.3),\n",
    "  (721, 79.6),\n",
    "  (741, 73.5),\n",
    "  (510, 62.5)]\n",
    "\n",
    "[(class_index_to_label(index), count) for index, count in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAgk-YyWc3rG"
   },
   "outputs": [],
   "source": [
    "# learn.recorder.plot_losses()\n",
    "# learn.recorder.plot_lr()\n",
    "# learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTHG4Bt7VDYp"
   },
   "outputs": [],
   "source": [
    "fooling_rates = []\n",
    "model = learn.model.eval()\n",
    "learn.metrics = [validation_single_perturbation]\n",
    "for i in range(10):\n",
    "  global_perturbations = model(torch.rand(1, 3, 224, 244).cuda())[0]\n",
    "  nag_util.global_perturbations = global_perturbations\n",
    "  fooling_rates.append(learn.validate()[1].cpu().item())\n",
    "  print(\"%d : %f\"%(i, fooling_rates[-1]))\n",
    "\n",
    "mean = np.mean(fooling_rates)\n",
    "stddev = np.std(fooling_rates)\n",
    "print(mean, stddev); print(fooling_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "OFCjzI7UaY3C",
    "outputId": "740185b4-dd54-46f4-b0af-79ee452568e1"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[200][0]\n",
    "x = normalize(x_img.data.cuda())\n",
    "z = torch.tensor([-0.33,  0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33], dtype=torch.float32).cuda()\n",
    "# z = torch.empty(z_dim).uniform_(-1,1).cuda()\n",
    "p = model.forward_single_z(z).detach()\n",
    "\n",
    "p_x = x + p\n",
    "# print(\"img range, noise range\")\n",
    "# print_range(x); print_range(p)\n",
    "adv_label = class_index_to_label(arch(p_x[None]).argmax(1).item())\n",
    "print_big_vector(arch(p_x[None])[0])\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0., 1.])\n",
    "p_img = Image(p)\n",
    "x_img.show()\n",
    "p_img.show()\n",
    "p_x_img.show()\n",
    "\n",
    "\n",
    "# print_range(p)\n",
    "# print_range(denormalize(x))\n",
    "# print_range(p_x)\n",
    "\n",
    "benign_label = class_index_to_label(arch(x[None]).argmax(1).item())\n",
    "\n",
    "print_big_vector(arch(x[None])[0])\n",
    "print(benign_label, adv_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzwsI2P1ZANz"
   },
   "outputs": [],
   "source": [
    "z1 = torch.tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p1 = model.forward_single_z(z1)\n",
    "\n",
    "z2 = torch.tensor([1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p2 = model.forward_single_z(z2)\n",
    "\n",
    "z3 = torch.tensor([1, 1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p3 = model.forward_single_z(z3)\n",
    "\n",
    "l2_distance(p1, p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eroI82OKSnAL"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[4][0]\n",
    "x = x_img.data[None].cuda()\n",
    "p = model(x)[0].squeeze().detach() \n",
    "x = x.squeeze()\n",
    "x = normalize(x)\n",
    "\n",
    "p_x = x + p\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0.,1.])\n",
    "p_img = Image(p)\n",
    "# x_img.show()\n",
    "p_img.show()\n",
    "# p_x_img.show()\n",
    "\n",
    "print_range(p)\n",
    "print_range(x)\n",
    "print_range(p_x)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NAG-tripletLossExperiment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
