{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ahmad-PH/nag-notebooks/blob/master/NAG_tripletLossExperiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqeZpz16do4y"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def run_shell_command(cmd):\n",
    "  p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "  print(str(p.communicate()[0], 'utf-8'))\n",
    "\n",
    "\n",
    "def detect_env():\n",
    "    import os\n",
    "    if 'content' in os.listdir('/'):\n",
    "      return \"colab\"\n",
    "    else:\n",
    "      return \"IBM\"\n",
    "  \n",
    "def create_env():\n",
    "  if detect_env() == \"IBM\":\n",
    "    return IBMEnv()\n",
    "  elif detect_env() == \"colab\":\n",
    "    return ColabEnv()\n",
    "\n",
    "\n",
    "class Env:\n",
    "  def get_nag_util_files(self):\n",
    "      import os\n",
    "      \n",
    "      print(\"\\ngetting git files ...\")\n",
    "      if os.path.isdir(self.python_files_path):\n",
    "        os.chdir(self.python_files_path)\n",
    "        run_shell_command('git pull')\n",
    "        os.chdir(self.root_folder)\n",
    "      else:\n",
    "        run_shell_command('git clone https://github.com/ahmad-PH/nag-public.git')\n",
    "      print(\"done.\")\n",
    "  \n",
    "\n",
    "class IBMEnv(Env):\n",
    "    def __init__(self):\n",
    "      self.root_folder = \"/root/Derakhshani/adversarial\"\n",
    "      self.temp_csv_path = self.root_folder + \"/temp\"\n",
    "      self.python_files_path = self.root_folder + \"/nag-public\"\n",
    "      self.python_files_dir = \"NAG-11May-beforeDenoiser\"\n",
    "      \n",
    "      import sys\n",
    "      sys.path.append('./nag/nag_util')\n",
    "      \n",
    "    def get_csv_path(self):\n",
    "      return self.root_folder + \"/textual_notes/CSVs/\" + self.save_filename\n",
    "    \n",
    "    def get_models_path(self):\n",
    "      return self.root_folder + \"/models/\" + self.save_filename\n",
    "      \n",
    "    def setup(self):\n",
    "      self.get_nag_util_files()\n",
    "      \n",
    "      import os; import torch;\n",
    "      os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "      cuda_index = 1\n",
    "      os.environ['CUDA_VISIBLE_DEVICES']=str(cuda_index)\n",
    "#       defaults.device = torch.device('cuda:' + str(cuda_index))\n",
    "#       print('cuda:' + str(cuda_index))\n",
    "#       torch.cuda.set_device('cuda:1')\n",
    "      \n",
    "    def load_dataset(self, compressed_name, unpacked_name):\n",
    "      pass\n",
    "\n",
    "    def load_test_dataset(self, root_folder):\n",
    "      pass\n",
    "    \n",
    "    def set_data_path(self, path):\n",
    "      self.data_path = Path(self.root_folder + '/datasets/' + path)\n",
    "    \n",
    "        \n",
    "class ColabEnv(Env):\n",
    "    def __init__(self):\n",
    "      self.root_folder = '/content'\n",
    "      self.temp_csv_path = self.root_folder\n",
    "      self.python_files_path = self.root_folder + '/nag-public'\n",
    "      self.python_files_dir = \"NAG-11May-beforeDenoiser\"\n",
    "      self.torchvision_upgraded = False\n",
    "      \n",
    "    def get_csv_path(self):\n",
    "      return self.root_folder + '/gdrive/My Drive/DL/textual_notes/CSVs/' + self.save_filename\n",
    "    \n",
    "    def get_models_path(self):\n",
    "      return self.root_folder + \"/gdrive/My Drive/DL/models/\" + self.save_filename\n",
    "        \n",
    "    def setup(self):\n",
    "        # ######################################################\n",
    "        # # TODO remove this once torchvision 0.3 is present by\n",
    "        # # default in Colab\n",
    "        # ######################################################\n",
    "        global torchvision_upgraded\n",
    "        try:\n",
    "            torchvision_upgraded\n",
    "        except NameError:\n",
    "          !pip uninstall -y torchvision\n",
    "          !pip install https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl\n",
    "          torchvision_upgraded = True\n",
    "        else:\n",
    "          print(\"torchvision already upgraded\")\n",
    "          \n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/gdrive')\n",
    "        \n",
    "        self.get_nag_util_files()\n",
    "        \n",
    "    def load_dataset(self, compressed_name, unpacked_name):\n",
    "      if compressed_name not in os.listdir('.'):\n",
    "        print(compressed_name + ' not found, getting it from drive')\n",
    "        shutil.copyfile(\"/content/gdrive/My Drive/DL/{}.tar.gz\".format(compressed_name), \"./{}.tar.gz\".format(compressed_name))\n",
    "\n",
    "        gunzip_arg = \"./{}.tar.gz\".format(compressed_name)\n",
    "        !gunzip -f $gunzip_arg\n",
    "\n",
    "        tar_arg = \"./{}.tar\".format(compressed_name)\n",
    "        !tar -xvf $tar_arg > /dev/null\n",
    "\n",
    "        os.rename(unpacked_name, compressed_name)\n",
    "\n",
    "    #     ls_arg = \"./{}/train/n01440764\".format(compressed_name)\n",
    "    #     !ls $ls_arg\n",
    "\n",
    "        !rm $tar_arg\n",
    "\n",
    "        print(\"done\") \n",
    "      else:\n",
    "        print(compressed_name + \" found\")\n",
    "        \n",
    "    def load_test_dataset(self, root_folder):\n",
    "      test_folder = root_folder + '/test/'\n",
    "      if 'test' not in os.listdir(root_folder):\n",
    "        print('getting test dataset from drive')\n",
    "        os.mkdir(test_folder)\n",
    "        for i in range(1,11):\n",
    "          shutil.copy(\"/content/gdrive/My Drive/DL/full_test_folder/{}.zip\".format(i), test_folder)\n",
    "          shutil.unpack_archive(test_folder + \"/{}.zip\".format(i), test_folder)\n",
    "          os.remove(test_folder + \"/{}.zip\".format(i))\n",
    "          print(\"done with the {}th fragment\".format(i))\n",
    "      else:\n",
    "        print('test dataset found.')\n",
    "        \n",
    "    def set_data_path(self, path):\n",
    "      self.data_path = Path('./' + path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "YyZUYSjBi9K9",
    "outputId": "5ef25a03-c55f-43de-8460-a1afde369fd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "getting git files ...\n",
      "Already up-to-date.\n",
      "\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "env = create_env()\n",
    "env.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Ev7jcRKoARg"
   },
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.imports import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys; import os; import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3_1aE41PZAMw"
   },
   "outputs": [],
   "source": [
    "sys.path.append(env.python_files_path + '/' + env.python_files_dir)\n",
    "\n",
    "from nag_util import *\n",
    "import nag_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tltucTv2ep9-"
   },
   "outputs": [],
   "source": [
    "# mode = \"sanity_check\"\n",
    "mode = \"normal\"\n",
    "# mode = \"div_metric_calc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50\n",
    "# model = models.resnet152\n",
    "# model = models.vgg16_bn\n",
    "# model = torchvision.models.googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SO1h55obXzOv",
    "outputId": "54414cc5-84d5-4f45-ecab-87374a58dd33"
   },
   "outputs": [],
   "source": [
    "if mode == \"normal\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "elif mode == \"sanity_check\":\n",
    "  env.load_dataset('dataset_sanity_check_small', 'dataset_sanity_check_small')  \n",
    "  env.set_data_path('dataset_sanity_check_small')\n",
    "elif mode == \"div_metric_calc\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "  env.load_test_dataset(str(env.data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "koaQZmjMom7w"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "gpu_flag = True\n",
    "nag_util.batch_size = batch_size; nag_util.gpu_flag = gpu_flag;\n",
    "# nag_util.set_globals(gpu_flag, batch_size)\n",
    "tfms = get_transforms(do_flip=False, max_rotate=0)\n",
    "data = (ImageList.from_folder(env.data_path)\n",
    "        .split_by_folder(valid=('test' if mode == 'div_metric_calc' else 'valid'))\n",
    "        .label_from_folder()\n",
    "        .transform(tfms, size=224)\n",
    "        .databunch(bs=batch_size, num_workers=1)\n",
    "        .normalize(imagenet_stats))\n",
    "\n",
    "# data.show_batch(rows=2, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDBkRV8yovwV"
   },
   "outputs": [],
   "source": [
    "model_name = model.__name__\n",
    "z_dim = 10\n",
    "label_dim = 1000\n",
    "gen_inp_dim = z_dim + label_dim\n",
    "arch = SoftmaxWrapper(model(pretrained=True).cuda().eval())\n",
    "nag_util.arch = arch\n",
    "requires_grad(arch, False)\n",
    "\n",
    "# vgg:\n",
    "# layers = []\n",
    "# blocks = [i-1 for i,o in enumerate(children(arch.features)) if isinstance(o, nn.MaxPool2d)]\n",
    "# layers = [arch.features[i] for i in blocks]\n",
    "# layer_weights = [1] * len(layers)\n",
    "\n",
    "layers = [\n",
    "    arch.softmax\n",
    "]\n",
    "\n",
    "layer_weights = [1.] * len(layers)\n",
    "\n",
    "# inception:\n",
    "# layers = [\n",
    "#     arch.Conv2d_1a_3x3,\n",
    "#     arch.Mixed_6e,\n",
    "#     arch.Mixed_7a,\n",
    "#     arch.fc    \n",
    "# ]\n",
    "# layer_weights = [1.0/4.0] * len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gen(nn.Module):\n",
    "  def __init__(self, inp_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "    super(Gen, self).__init__()\n",
    "\n",
    "    self.bs = None\n",
    "    self.inp_dim = inp_dim\n",
    "    self.gf_dim = gf_dim\n",
    "    self.y_dim = y_dim\n",
    "    self.df_dim = df_dim\n",
    "    self.image_shape = image_shape\n",
    "\n",
    "    self.z_ = nn.Linear(self.inp_dim, self.gf_dim * 7 * 4 * 4, bias=True)\n",
    "    self.z_.bias.data.fill_(0)\n",
    "    self.BN_ = nn.BatchNorm2d(self.gf_dim * 7)\n",
    "\n",
    "    self.half = max(self.gf_dim // 2, 1) \n",
    "    self.quarter = max(self.gf_dim // 4, 1)\n",
    "    self.eighth = max(self.gf_dim // 8, 1)\n",
    "    # sixteenth = max(self.gf_dim // 16, 1)\n",
    "    \n",
    "    self.CT2d_1 = deconv_layer(self.gf_dim * 8, self.gf_dim * 4, k_size = (5,5), pad = (2,2))\n",
    "    self.CT2d_2 = deconv_layer(self.gf_dim * 5, self.gf_dim * 2)    \n",
    "    self.CT2d_3 = deconv_layer(self.gf_dim * 2 + self.half, self.gf_dim * 1)\n",
    "    self.CT2d_4 = deconv_layer(self.gf_dim * 1 + self.quarter, self.gf_dim * 1)\n",
    "    self.CT2d_5 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "    self.CT2d_6 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "    self.CT2d_7 = deconv_layer(self.gf_dim * 1 + self.eighth, 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "  def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "    h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "    h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "    output = deconv_layer(h_input)\n",
    "    assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "            \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "            \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "    return output\n",
    "  \n",
    "  def forward_z(self, z):\n",
    "    self.bs = z.shape[0]\n",
    "    \n",
    "    h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "    assert h0.shape[2:] == (4, 4), \"Unexpected shape, it shoud be (4,4)\"\n",
    "\n",
    "    h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "    h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "    h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "    h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "    h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "    h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "    h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "    ksi = 10.0\n",
    "    output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "    # this coeff scales the output to be appropriate for images that are \n",
    "    # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "    # interval)\n",
    "    return output_coeff * torch.tanh(h7)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    self.bs = inputs.shape[0]\n",
    "\n",
    "    benign_preds_onehot = arch(inputs)\n",
    "    benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "    \n",
    "    labels = torch.zeros([self.bs, 1000]).cuda()\n",
    "    for i in range(self.bs):\n",
    "      random_label = self.randint(0,1000, exclude = benign_preds[i].item())\n",
    "      labels[i][random_label] = 1.\n",
    "    \n",
    "    z = torch.empty(self.bs, 10).cuda()\n",
    "    labels_and_z = torch.cat((labels, z), dim=1)\n",
    "    \n",
    "    z_out = self.forward_z(labels_and_z)\n",
    "    \n",
    "    return z_out, None, None, inputs, benign_preds_onehot, labels\n",
    "  \n",
    "#    def forward(self, inputs):\n",
    "#     self.bs = inputs.shape[0]\n",
    "\n",
    "#     benign_preds_onehot = arch(inputs)\n",
    "#     benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "    \n",
    "#     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "#     random_label = self.randint(0,1000, exclude = benign_preds.tolist())\n",
    "#     for i in range(self.bs):\n",
    "#       z[i][random_label] = 1.\n",
    "    \n",
    "#     z_out = self.forward_z(z)\n",
    "    \n",
    "#     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "  \n",
    "  @staticmethod\n",
    "  def randint(low, high, exclude):\n",
    "    temp = np.random.randint(low, high - 1)\n",
    "    if temp == exclude:\n",
    "      temp = temp + 1\n",
    "    return temp\n",
    "  \n",
    "  def forward_single_z(self, z):\n",
    "    return self.forward_z(z[None]).squeeze()\n",
    "           \n",
    "  \n",
    "  def make_triplet_samples(self, z, margin, r2, r3):\n",
    "    positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "    negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "    return positive_sample, negative_sample\n",
    "\n",
    "  def random_vector_surface(self, shape, r = 1.):\n",
    "    mat = torch.randn(size=shape).cuda()\n",
    "    norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "    return (mat/norm) * r\n",
    "\n",
    "  \n",
    "  def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "    fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "    fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "    fraction.unsqueeze_(-1)\n",
    "    return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "  def make_z(self, in_shape):\n",
    "    return torch.empty(in_shape).cuda().uniform_(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NkfbLWEQqRA_"
   },
   "outputs": [],
   "source": [
    "def js_distance(x1, x2):\n",
    "  m = 0.5 * (x1 + x2)\n",
    "  return 0.5 * (F.kl_div(x1, m) + F.kl_div(x2, m))\n",
    "\n",
    "def kl_distance(x1, x2):\n",
    "  inp = torch.log(x1)\n",
    "  target = x2\n",
    "  return F.kl_div(inp, target, reduction='batchmean')\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  x1 = tensorify(x1)\n",
    "  x2 = tensorify(x2)\n",
    "  x1 = x1 / torch.sum(x1)\n",
    "  x2 = x2 / torch.sum(x2)\n",
    "  return kl_distance(x1[None], x2[None])\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  if not isinstance(x1, torch.Tensor): x1 = torch.tensor(x1)\n",
    "  if not isinstance(x2, torch.Tensor): x2 = torch.tensor(x2)\n",
    "  x1 = x1 * 100. / torch.sum(x1)\n",
    "  x2 = x2 * 100. / torch.sum(x2)\n",
    "  return torch.norm(x1 - x2, 2)\n",
    "\n",
    "def distance_from_uniform(x):\n",
    "  return distrib_distance(x, [1.] * len(x))\n",
    "\n",
    "def wasserstein_distance(x1, x2):\n",
    "  return torch.mean(x1 - x2)\n",
    "\n",
    "def l1_distance(x1, x2):\n",
    "  return F.l1_loss(x1, x2)\n",
    "\n",
    "def l2_distance(x1, x2):\n",
    "  return F.mse_loss(x1 * 10, x2 * 10)\n",
    "\n",
    "def mse_loss(x1, x2):\n",
    "  return F.mse_loss(x1, x2)\n",
    "\n",
    "def cos_distance(x1, x2, dim = 1):\n",
    "  return -1 * torch.mean(F.cosine_similarity(x1, x2, dim=dim))\n",
    "\n",
    "triplet_call_cnt = 0\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, distance_func, margin):\n",
    "  # max distance when using l1_distance is 2\n",
    "  # max distacne when using l2-distance is sqrt(2)\n",
    "#   print(\"anchor: \", anchor.min(), anchor.max())\n",
    "  ap_dist = distance_func(anchor, positive)\n",
    "  an_dist = distance_func(anchor, negative)\n",
    "\n",
    "  global triplet_call_cnt\n",
    "  triplet_call_cnt += 1\n",
    "  if triplet_call_cnt % 10 in [0,1] : #and anchor.shape[1] == 1000:\n",
    "#     print(\"a: \", end=\"\"); print_big_vector(anchor[0])\n",
    "#     print(\"p: \", end=\"\"); print_big_vector(positive[0])\n",
    "#     print(\"n: \", end=\"\"); print_big_vector(negative[0])\n",
    "#     print(\"ap_dist: {}, an_dist: {}\".format(ap_dist, an_dist))\n",
    "    print(\"func:{}, ap_dist: {}, an_dist: {}\".format(distance_func.__name__, ap_dist, an_dist))\n",
    "    \n",
    "  return torch.mean(F.relu(ap_dist - an_dist + margin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsFgfiN8EV7z"
   },
   "outputs": [],
   "source": [
    "def diversity_loss(input, target):\n",
    "#   return -1 * torch.mean(torch.pow(f_x_a-f_x_s,2))\n",
    "  if input.shape[0] != batch_size:\n",
    "    print(\"input shape: \", input.shape)\n",
    "    print(\"target shape: \", target.shape, \"\\n\\n\")\n",
    "  return torch.mean(F.cosine_similarity(\n",
    "    input.view([batch_size, -1]),\n",
    "    target.view([batch_size, -1]), \n",
    "  ))\n",
    "\n",
    "# def fool_loss(input, target):\n",
    "#     true_class = torch.argmax(target, dim=1).view(-1,1).long()\n",
    "#     return -1 * torch.log(torch.mean(1 - input.gather(1, true_class)))\n",
    "\n",
    "def fool_loss_old(input, target, trash):\n",
    "  print(\"fool_loss:\")\n",
    "  true_class = torch.argmax(target, dim=1).view(-1,1).long()\n",
    "  print(true_class)\n",
    "  print(\"input: \", input.shape)\n",
    "  a = input.gather(1, true_class)\n",
    "  print(a)\n",
    "  print(1 - a)\n",
    "  print(torch.mean(1 - a))\n",
    "  print(torch.log(torch.mean(1-a)))\n",
    "  print(\"\\n\\n\")\n",
    "  # this is wrong! first log should be taken, THEN mean.\n",
    "  return -1 * torch.log(torch.mean(1 - input.gather(1, true_class)))\n",
    "\n",
    "fool_loss_count = 0\n",
    "\n",
    "def fool_loss(model_output, target_labels):\n",
    "  target_labels = target_labels.view(-1, 1).long().cuda()\n",
    "  target_probabilities = model_output.gather(1, target_labels)\n",
    "  epsilon = 1e-10\n",
    "  # highest possible fool_loss is - log(1e-10) == 23\n",
    "  result = torch.mean(-1 * torch.log(target_probabilities + epsilon))\n",
    "  \n",
    "  global fool_loss_count\n",
    "  fool_loss_count += 1\n",
    "  if fool_loss_count % 20 == 0:\n",
    "    print(\"target probs {}, loss: {}: \".format(target_probabilities, result))\n",
    "  \n",
    "  return result\n",
    "\n",
    "def validation(gen_output, target):\n",
    "  perturbations, _, _, clean_images, _, _ = gen_output\n",
    "  perturbed_images = clean_images + perturbations\n",
    "  benign_preds = torch.argmax(arch(clean_images), 1)\n",
    "  adversary_preds = torch.argmax(arch(perturbed_images), 1)\n",
    "  print(\"benign preds are: \", benign_preds)\n",
    "  print(\"adv preds are: \", adversary_preds)\n",
    "  print(\"pert range: \", perturbations.min(), \" \", perturbations.max())\n",
    "  return (benign_preds != adversary_preds).float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureLoss(nn.Module):\n",
    "    def __name__(self):\n",
    "      return \"feature_loss\"\n",
    "  \n",
    "    def __init__(self, dis, layers, layer_weights):\n",
    "        super().__init__()\n",
    "        \n",
    "        # define generator here \n",
    "        self.dis = dis\n",
    "        self.diversity_layers = layers\n",
    "        self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "        self.weights = layer_weights\n",
    "        self.metric_names = [\"fool_loss\"] #+ [f\"div_loss_{i}\" for i in range(len(layers))] #maybe Gram\n",
    "#         self.triplet_hooks = hook_outputs([arch.m.features[4]], detach=False)\n",
    "    \n",
    "    def make_features(self, x, clone=False):\n",
    "        y = self.dis(x)\n",
    "        return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "  \n",
    "    def forward(self, inp, target):\n",
    "      sigma_B, _, _, X_B, B_Y, labels_onehot = inp\n",
    "\n",
    "      X_A = X_B + sigma_B\n",
    "#       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "      A_Y, A_feat = self.make_features(X_A)\n",
    "#       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "      labels = labels_onehot.argmax(dim=1)\n",
    "      fooling_loss =  fool_loss(A_Y, labels)\n",
    "\n",
    "#       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "#       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "#       self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "#       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "      self.losses = [fooling_loss]\n",
    "      self.metrics = dict(zip(self.metric_names, [fooling_loss]))\n",
    "\n",
    "      return sum(self.losses)\n",
    "  \n",
    "    def add_perturbation_shuffled(self, inp, perturbation):\n",
    "#         j = torch.randperm(inp.shape[0])\n",
    "        j = derangement(inp.shape[0])\n",
    "        return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qd9gXUy_ovww"
   },
   "outputs": [],
   "source": [
    "feat_loss = FeatureLoss(arch, layers, layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfZKdYD2MSdi"
   },
   "outputs": [],
   "source": [
    "env.save_filename = 'resnet50_16'\n",
    "\n",
    "if Path(env.get_csv_path() + '.csv').exists(): raise FileExistsError(\"csv_path already exists\")\n",
    "if Path(env.get_models_path()).exists(): raise FileExistsError(\"models_path already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9J20CBLS8S9"
   },
   "outputs": [],
   "source": [
    "learn = None; gc.collect()\n",
    "csv_logger = partial(ImmediateCSVLogger, filename= env.temp_csv_path + '/' + env.save_filename)\n",
    "# learn = Learner(data, Gen(inp_dim=10), loss_func = feat_loss, metrics=[validation], callback_fns=LossMetrics, opt_func = optim.SGD)\n",
    "# learn = Learner(data, Gen(inp_dim=z_dim), loss_func = feat_loss, metrics=[validation], callback_fns=[LossMetrics, DiversityWeightsScheduler])\n",
    "learn = Learner(data, Gen(inp_dim=gen_inp_dim), loss_func = feat_loss, metrics=[validation], callback_fns=[LossMetrics, csv_logger])\n",
    "# load_starting_point(learn, model_name, z_dim)\n",
    "# random_seed(42, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wOZYzOHDEdB"
   },
   "outputs": [],
   "source": [
    "# !cp \"/content/gdrive/My Drive/DL/models/vgg16_12-last.pth\"  \"/content/\"\n",
    "# learn.load('/content/vgg16_12-last')\n",
    "\n",
    "# load_filename = 'resnet50_14/resnet50_14_49'\n",
    "# load_filename = 'vgg16_29/vgg16_29_69'\n",
    "# load_filename = 'vgg16_12-last'\n",
    "\n",
    "# learn.load('/root/Derakhshani/adversarial/models/' + load_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the selected settings are : \n",
      "\tmode: normal \n",
      "\tnetw-under-attack: resnet50 \n",
      "\tload filename: None \n",
      "\tsave filename: resnet50_16\n",
      "\n",
      "please MAKE SURE that the config is correct.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  load_filename\n",
    "except NameError:\n",
    "  load_filename = None\n",
    "\n",
    "print(\"the selected settings are : \")\n",
    "print(\"\\tmode: {} \\n\\tnetw-under-attack: {} \\n\\tload filename: {} \\n\\tsave filename: {}\\n\".format(\n",
    "  mode, model.__name__, load_filename , env.save_filename\n",
    "))\n",
    "print(\"please MAKE SURE that the config is correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='50', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/50 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='70' class='' max='1125', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      6.22% [70/1125 00:19<04:50 21.4781]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[1.9832e-08],\n",
      "        [2.5097e-14],\n",
      "        [3.8241e-15],\n",
      "        [9.9577e-11],\n",
      "        [6.5932e-15],\n",
      "        [2.5900e-10],\n",
      "        [7.4395e-14],\n",
      "        [5.3394e-10]], device='cuda:0', grad_fn=<GatherBackward>), loss: 21.886852264404297: \n",
      "target probs tensor([[4.3031e-17],\n",
      "        [3.2738e-15],\n",
      "        [1.6157e-11],\n",
      "        [2.8072e-06],\n",
      "        [6.6925e-13],\n",
      "        [1.7323e-17],\n",
      "        [2.0375e-10],\n",
      "        [1.7739e-12]], device='cuda:0', grad_fn=<GatherBackward>), loss: 21.58489227294922: \n",
      "target probs tensor([[8.2174e-05],\n",
      "        [2.6428e-12],\n",
      "        [7.5195e-15],\n",
      "        [1.0150e-11],\n",
      "        [1.2810e-11],\n",
      "        [3.4034e-16],\n",
      "        [2.1169e-12],\n",
      "        [5.9530e-10]], device='cuda:0', grad_fn=<GatherBackward>), loss: 21.048015594482422: \n"
     ]
    }
   ],
   "source": [
    "#last one was : 754 754\n",
    "if mode == \"sanity_check\":\n",
    "  print(\"\\n\\n\\nWARNING: you are training on a sanity_check dataset.\\n\\n\\n\\n\")\n",
    "\n",
    "saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=env.save_filename + \"-best\")\n",
    "saver_every_epoch = SaveModelCallback(learn, every='epoch', name=env.save_filename)\n",
    "\n",
    "learn.fit(50, lr=1e-02, wd = 0.001, callbacks=[saver_best, saver_every_epoch])\n",
    "# learn.fit(70, lr=1e-02, wd = 0.001, callbacks=[saver_best, saver_every_epoch])\n",
    "\n",
    "\n",
    "# learn.fit_one_cycle(8, max_lr=5e-01) #mohammad's setting that got 77 validation start on resnet with diversity loss on AdaptiveAvgPool2d\n",
    "# learn.fit_one_cycle(5, max_lr=2e-2) #used for vgg-19-bn\n",
    "# learn.fit_one_cycle(5, max_lr=3e-3) # used for resnet50\n",
    "\n",
    "shutil.copyfile(env.temp_csv_path + '/' + env.save_filename + \".csv\", env.get_csv_path() + '.csv')\n",
    "shutil.copytree(env.data_path/\"models\", env.get_models_path())\n",
    "shutil.rmtree(env.data_path/\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MO2fZ-hSSUzJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# z1 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "# z2 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "z1 = torch.tensor([0.8, -0.5] * 5).cuda()\n",
    "z2 = torch.tensor([-1.] * 10).cuda()\n",
    "print(\"z1: \", z1)\n",
    "print(\"z2: \", z2)\n",
    "print(\"distance: \", torch.norm(z1-z2,p=2))\n",
    "model = learn.model.eval()\n",
    "\n",
    "z_s = interpolate(z1, z2, 0.1)\n",
    "print(len(z_s))\n",
    "\n",
    "for i,z in enumerate(z_s):\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n",
    "  #img.save('./pics/' + str(i) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DGuGN7B7V0Xt"
   },
   "outputs": [],
   "source": [
    "def generate_perturbations(learn, n_perturbations):\n",
    "  initial_training_mode = learn.model.training\n",
    "  \n",
    "  model = learn.model.eval()\n",
    "  input_img = (learn.data.valid_ds[0][0].data)[None].cuda()\n",
    "  perturbations = []\n",
    "  for i in range(n_perturbations):\n",
    "    perturbation = model(input_img)[0].squeeze()\n",
    "    perturbations.append(perturbation)\n",
    "    \n",
    "  learn.model.train(initial_training_mode)  \n",
    "  return perturbations\n",
    "\n",
    "def compute_prediction_histogram(learn, perturbation, verbose=False):\n",
    "  pred_hist = [0] * 1000\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 and verbose: print (\"at batch no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbation[None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      pred_hist[pred] += 1\n",
    "  return pred_hist\n",
    "\n",
    "\n",
    "def compute_mean_prediction_histogram(learn, perturbations):\n",
    "  pred_histogram = torch.tensor([0] * 1000).detach_()\n",
    "  for j, perturbation in enumerate(perturbations):\n",
    "    pred_histogram_j = torch.tensor(compute_prediction_histogram(learn, perturbation, True)).detach_()\n",
    "    pred_histogram += pred_histogram_j\n",
    "    print(\"finished creating histogram for the {}th perturbation\".format(j))\n",
    "  \n",
    "  pred_histogram = pred_histogram.float() / len(perturbations)\n",
    "  return pred_histogram.tolist()\n",
    "\n",
    "\n",
    "def diversity(learn, n_perturbations, percentage):\n",
    "  pred_histogram = compute_mean_prediction_histogram(\n",
    "      learn, generate_perturbations(learn, n_perturbations)\n",
    "  )\n",
    "  print(\"finished creating the prediction histogram\")\n",
    "  pred_histogram_sum = np.sum(pred_histogram)\n",
    "\n",
    "  indexed_pred_histogram = [(i, hist_element) for i,hist_element in  \n",
    "                            enumerate(pred_histogram)]\n",
    "\n",
    "  indexed_pred_histogram.sort(key=lambda x: x[1], reverse = True)\n",
    "\n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  top_classes = []\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = indexed_pred_histogram[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / pred_histogram_sum) * 100.\n",
    "    top_classes.append(hist_elem[0])\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, indexed_pred_histogram, top_classes\n",
    "\n",
    "# idea : have 200 noises (1 for each class), then start iterating the dataset, and for each image, randomly apply one noise and record the result\n",
    "def targeted_diversity(learn, percentage):\n",
    "  model = learn.model.eval()\n",
    "\n",
    "  gen_inputs = [torch.empty(gen_inp_dim).uniform_(0,1).cuda().detach() for _ in range(200)]\n",
    "#   for i in range(gen_inp_dim):\n",
    "#     gen_inputs[i][i] = 1.\n",
    "\n",
    "  perturbations = [model.forward_single_z(z) for z in gen_inputs]\n",
    "\n",
    "  hist = [0.] * gen_inp_dim\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 : print(\"at batch_no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbations[np.random.randint(0,len(perturbations))][None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      hist[pred] += 1\n",
    "\n",
    "  pred_histogram_sum = np.sum(hist)\n",
    "  indexed_pred_histogram = [(i, hist_element) for i,hist_element in  \n",
    "                            enumerate(hist)]\n",
    "\n",
    "  indexed_pred_histogram.sort(key=lambda x: x[1], reverse = True)\n",
    "\n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = indexed_pred_histogram[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / pred_histogram_sum) * 100.\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, indexed_pred_histogram\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch_no 0\n",
      "at batch_no 100\n",
      "at batch_no 200\n",
      "at batch_no 300\n",
      "at batch_no 400\n",
      "at batch_no 500\n",
      "at batch_no 600\n",
      "at batch_no 700\n",
      "at batch_no 800\n",
      "at batch_no 900\n",
      "at batch_no 1000\n",
      "at batch_no 1100\n",
      "at batch_no 1200\n",
      "at batch_no 1300\n",
      "at batch_no 1400\n",
      "at batch_no 1500\n",
      "at batch_no 1600\n",
      "at batch_no 1700\n",
      "at batch_no 1800\n",
      "at batch_no 1900\n",
      "at batch_no 2000\n",
      "at batch_no 2100\n",
      "at batch_no 2200\n",
      "at batch_no 2300\n",
      "at batch_no 2400\n",
      "at batch_no 2500\n",
      "at batch_no 2600\n",
      "at batch_no 2700\n",
      "at batch_no 2800\n",
      "at batch_no 2900\n",
      "at batch_no 3000\n",
      "at batch_no 3100\n",
      "at batch_no 3200\n",
      "at batch_no 3300\n",
      "at batch_no 3400\n",
      "at batch_no 3500\n",
      "at batch_no 3600\n",
      "at batch_no 3700\n",
      "at batch_no 3800\n",
      "at batch_no 3900\n",
      "at batch_no 4000\n",
      "at batch_no 4100\n",
      "at batch_no 4200\n",
      "at batch_no 4300\n",
      "at batch_no 4400\n",
      "at batch_no 4500\n",
      "at batch_no 4600\n",
      "at batch_no 4700\n",
      "at batch_no 4800\n",
      "at batch_no 4900\n",
      "at batch_no 5000\n",
      "at batch_no 5100\n",
      "at batch_no 5200\n",
      "at batch_no 5300\n",
      "at batch_no 5400\n",
      "at batch_no 5500\n",
      "at batch_no 5600\n",
      "at batch_no 5700\n",
      "at batch_no 5800\n",
      "at batch_no 5900\n",
      "at batch_no 6000\n",
      "at batch_no 6100\n",
      "at batch_no 6200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(908,\n",
       " [(743, 144.00),\n",
       "  (834, 126.00),\n",
       "  (652, 110.00),\n",
       "  (674, 110.00),\n",
       "  (848, 102.00),\n",
       "  (483, 98.00),\n",
       "  (489, 98.00),\n",
       "  (611, 96.00),\n",
       "  (162, 94.00),\n",
       "  (497, 93.00),\n",
       "  (785, 93.00),\n",
       "  (598, 92.00),\n",
       "  (208, 86.00),\n",
       "  (424, 86.00),\n",
       "  (491, 86.00),\n",
       "  (646, 85.00),\n",
       "  (549, 84.00),\n",
       "  (877, 84.00),\n",
       "  (921, 84.00),\n",
       "  (679, 83.00),\n",
       "  (791, 82.00),\n",
       "  (839, 82.00),\n",
       "  (851, 81.00),\n",
       "  (425, 80.00),\n",
       "  (831, 80.00),\n",
       "  (700, 79.00),\n",
       "  (819, 79.00),\n",
       "  (909, 79.00),\n",
       "  (58, 78.00),\n",
       "  (234, 78.00),\n",
       "  (713, 78.00),\n",
       "  (256, 77.00),\n",
       "  (281, 77.00),\n",
       "  (480, 77.00),\n",
       "  (527, 77.00),\n",
       "  (833, 77.00),\n",
       "  (103, 76.00),\n",
       "  (343, 76.00),\n",
       "  (884, 76.00),\n",
       "  (382, 75.00),\n",
       "  (579, 75.00),\n",
       "  (614, 75.00),\n",
       "  (895, 75.00),\n",
       "  (82, 74.00),\n",
       "  (222, 74.00),\n",
       "  (349, 74.00),\n",
       "  (847, 74.00),\n",
       "  (3, 73.00),\n",
       "  (174, 73.00),\n",
       "  (265, 73.00),\n",
       "  (313, 73.00),\n",
       "  (471, 73.00),\n",
       "  (630, 73.00),\n",
       "  (904, 73.00),\n",
       "  (197, 72.00),\n",
       "  (366, 72.00),\n",
       "  (419, 72.00),\n",
       "  (458, 72.00),\n",
       "  (523, 72.00),\n",
       "  (203, 71.00),\n",
       "  (237, 71.00),\n",
       "  (245, 71.00),\n",
       "  (511, 71.00),\n",
       "  (706, 71.00),\n",
       "  (739, 71.00),\n",
       "  (762, 71.00),\n",
       "  (850, 71.00),\n",
       "  (923, 71.00),\n",
       "  (926, 71.00),\n",
       "  (164, 70.00),\n",
       "  (260, 70.00),\n",
       "  (477, 70.00),\n",
       "  (492, 70.00),\n",
       "  (703, 70.00),\n",
       "  (751, 70.00),\n",
       "  (843, 70.00),\n",
       "  (913, 70.00),\n",
       "  (379, 69.00),\n",
       "  (469, 69.00),\n",
       "  (539, 69.00),\n",
       "  (738, 69.00),\n",
       "  (896, 69.00),\n",
       "  (955, 69.00),\n",
       "  (48, 68.00),\n",
       "  (85, 68.00),\n",
       "  (154, 68.00),\n",
       "  (258, 68.00),\n",
       "  (279, 68.00),\n",
       "  (545, 68.00),\n",
       "  (577, 68.00),\n",
       "  (698, 68.00),\n",
       "  (855, 68.00),\n",
       "  (5, 67.00),\n",
       "  (6, 67.00),\n",
       "  (39, 67.00),\n",
       "  (243, 67.00),\n",
       "  (280, 67.00),\n",
       "  (420, 67.00),\n",
       "  (544, 67.00),\n",
       "  (570, 67.00),\n",
       "  (580, 67.00),\n",
       "  (666, 67.00),\n",
       "  (881, 67.00),\n",
       "  (973, 67.00),\n",
       "  (992, 67.00),\n",
       "  (269, 66.00),\n",
       "  (295, 66.00),\n",
       "  (332, 66.00),\n",
       "  (363, 66.00),\n",
       "  (403, 66.00),\n",
       "  (515, 66.00),\n",
       "  (607, 66.00),\n",
       "  (707, 66.00),\n",
       "  (714, 66.00),\n",
       "  (830, 66.00),\n",
       "  (912, 66.00),\n",
       "  (61, 65.00),\n",
       "  (72, 65.00),\n",
       "  (138, 65.00),\n",
       "  (192, 65.00),\n",
       "  (308, 65.00),\n",
       "  (314, 65.00),\n",
       "  (496, 65.00),\n",
       "  (561, 65.00),\n",
       "  (562, 65.00),\n",
       "  (582, 65.00),\n",
       "  (612, 65.00),\n",
       "  (722, 65.00),\n",
       "  (803, 65.00),\n",
       "  (842, 65.00),\n",
       "  (75, 64.00),\n",
       "  (178, 64.00),\n",
       "  (225, 64.00),\n",
       "  (249, 64.00),\n",
       "  (286, 64.00),\n",
       "  (336, 64.00),\n",
       "  (413, 64.00),\n",
       "  (548, 64.00),\n",
       "  (603, 64.00),\n",
       "  (766, 64.00),\n",
       "  (789, 64.00),\n",
       "  (846, 64.00),\n",
       "  (21, 63.00),\n",
       "  (78, 63.00),\n",
       "  (149, 63.00),\n",
       "  (202, 63.00),\n",
       "  (257, 63.00),\n",
       "  (288, 63.00),\n",
       "  (291, 63.00),\n",
       "  (361, 63.00),\n",
       "  (362, 63.00),\n",
       "  (406, 63.00),\n",
       "  (556, 63.00),\n",
       "  (583, 63.00),\n",
       "  (597, 63.00),\n",
       "  (682, 63.00),\n",
       "  (692, 63.00),\n",
       "  (747, 63.00),\n",
       "  (777, 63.00),\n",
       "  (867, 63.00),\n",
       "  (893, 63.00),\n",
       "  (988, 63.00),\n",
       "  (182, 62.00),\n",
       "  (330, 62.00),\n",
       "  (526, 62.00),\n",
       "  (593, 62.00),\n",
       "  (608, 62.00),\n",
       "  (632, 62.00),\n",
       "  (721, 62.00),\n",
       "  (746, 62.00),\n",
       "  (870, 62.00),\n",
       "  (927, 62.00),\n",
       "  (985, 62.00),\n",
       "  (999, 62.00),\n",
       "  (51, 61.00),\n",
       "  (70, 61.00),\n",
       "  (104, 61.00),\n",
       "  (272, 61.00),\n",
       "  (287, 61.00),\n",
       "  (292, 61.00),\n",
       "  (300, 61.00),\n",
       "  (334, 61.00),\n",
       "  (573, 61.00),\n",
       "  (621, 61.00),\n",
       "  (661, 61.00),\n",
       "  (783, 61.00),\n",
       "  (979, 61.00),\n",
       "  (995, 61.00),\n",
       "  (69, 60.00),\n",
       "  (172, 60.00),\n",
       "  (238, 60.00),\n",
       "  (261, 60.00),\n",
       "  (283, 60.00),\n",
       "  (342, 60.00),\n",
       "  (384, 60.00),\n",
       "  (428, 60.00),\n",
       "  (478, 60.00),\n",
       "  (553, 60.00),\n",
       "  (564, 60.00),\n",
       "  (565, 60.00),\n",
       "  (569, 60.00),\n",
       "  (655, 60.00),\n",
       "  (701, 60.00),\n",
       "  (716, 60.00),\n",
       "  (849, 60.00),\n",
       "  (900, 60.00),\n",
       "  (920, 60.00),\n",
       "  (981, 60.00),\n",
       "  (259, 59.00),\n",
       "  (284, 59.00),\n",
       "  (333, 59.00),\n",
       "  (468, 59.00),\n",
       "  (506, 59.00),\n",
       "  (518, 59.00),\n",
       "  (529, 59.00),\n",
       "  (604, 59.00),\n",
       "  (702, 59.00),\n",
       "  (741, 59.00),\n",
       "  (844, 59.00),\n",
       "  (871, 59.00),\n",
       "  (975, 59.00),\n",
       "  (13, 58.00),\n",
       "  (116, 58.00),\n",
       "  (180, 58.00),\n",
       "  (233, 58.00),\n",
       "  (250, 58.00),\n",
       "  (253, 58.00),\n",
       "  (301, 58.00),\n",
       "  (377, 58.00),\n",
       "  (454, 58.00),\n",
       "  (457, 58.00),\n",
       "  (475, 58.00),\n",
       "  (508, 58.00),\n",
       "  (769, 58.00),\n",
       "  (781, 58.00),\n",
       "  (822, 58.00),\n",
       "  (825, 58.00),\n",
       "  (866, 58.00),\n",
       "  (886, 58.00),\n",
       "  (974, 58.00),\n",
       "  (29, 57.00),\n",
       "  (46, 57.00),\n",
       "  (59, 57.00),\n",
       "  (181, 57.00),\n",
       "  (186, 57.00),\n",
       "  (254, 57.00),\n",
       "  (267, 57.00),\n",
       "  (372, 57.00),\n",
       "  (386, 57.00),\n",
       "  (398, 57.00),\n",
       "  (405, 57.00),\n",
       "  (431, 57.00),\n",
       "  (440, 57.00),\n",
       "  (495, 57.00),\n",
       "  (542, 57.00),\n",
       "  (584, 57.00),\n",
       "  (594, 57.00),\n",
       "  (627, 57.00),\n",
       "  (636, 57.00),\n",
       "  (645, 57.00),\n",
       "  (727, 57.00),\n",
       "  (730, 57.00),\n",
       "  (792, 57.00),\n",
       "  (820, 57.00),\n",
       "  (828, 57.00),\n",
       "  (925, 57.00),\n",
       "  (983, 57.00),\n",
       "  (18, 56.00),\n",
       "  (31, 56.00),\n",
       "  (67, 56.00),\n",
       "  (77, 56.00),\n",
       "  (157, 56.00),\n",
       "  (217, 56.00),\n",
       "  (293, 56.00),\n",
       "  (305, 56.00),\n",
       "  (391, 56.00),\n",
       "  (396, 56.00),\n",
       "  (437, 56.00),\n",
       "  (447, 56.00),\n",
       "  (532, 56.00),\n",
       "  (616, 56.00),\n",
       "  (628, 56.00),\n",
       "  (649, 56.00),\n",
       "  (657, 56.00),\n",
       "  (681, 56.00),\n",
       "  (776, 56.00),\n",
       "  (802, 56.00),\n",
       "  (806, 56.00),\n",
       "  (892, 56.00),\n",
       "  (951, 56.00),\n",
       "  (997, 56.00),\n",
       "  (23, 55.00),\n",
       "  (40, 55.00),\n",
       "  (71, 55.00),\n",
       "  (120, 55.00),\n",
       "  (135, 55.00),\n",
       "  (155, 55.00),\n",
       "  (161, 55.00),\n",
       "  (189, 55.00),\n",
       "  (195, 55.00),\n",
       "  (199, 55.00),\n",
       "  (223, 55.00),\n",
       "  (229, 55.00),\n",
       "  (239, 55.00),\n",
       "  (274, 55.00),\n",
       "  (289, 55.00),\n",
       "  (399, 55.00),\n",
       "  (688, 55.00),\n",
       "  (712, 55.00),\n",
       "  (774, 55.00),\n",
       "  (794, 55.00),\n",
       "  (796, 55.00),\n",
       "  (805, 55.00),\n",
       "  (864, 55.00),\n",
       "  (873, 55.00),\n",
       "  (874, 55.00),\n",
       "  (980, 55.00),\n",
       "  (991, 55.00),\n",
       "  (20, 54.00),\n",
       "  (30, 54.00),\n",
       "  (53, 54.00),\n",
       "  (60, 54.00),\n",
       "  (101, 54.00),\n",
       "  (132, 54.00),\n",
       "  (153, 54.00),\n",
       "  (168, 54.00),\n",
       "  (241, 54.00),\n",
       "  (263, 54.00),\n",
       "  (310, 54.00),\n",
       "  (312, 54.00),\n",
       "  (387, 54.00),\n",
       "  (401, 54.00),\n",
       "  (416, 54.00),\n",
       "  (422, 54.00),\n",
       "  (423, 54.00),\n",
       "  (441, 54.00),\n",
       "  (466, 54.00),\n",
       "  (487, 54.00),\n",
       "  (513, 54.00),\n",
       "  (568, 54.00),\n",
       "  (591, 54.00),\n",
       "  (763, 54.00),\n",
       "  (800, 54.00),\n",
       "  (894, 54.00),\n",
       "  (993, 54.00),\n",
       "  (25, 53.00),\n",
       "  (43, 53.00),\n",
       "  (66, 53.00),\n",
       "  (80, 53.00),\n",
       "  (94, 53.00),\n",
       "  (142, 53.00),\n",
       "  (145, 53.00),\n",
       "  (194, 53.00),\n",
       "  (264, 53.00),\n",
       "  (328, 53.00),\n",
       "  (331, 53.00),\n",
       "  (347, 53.00),\n",
       "  (368, 53.00),\n",
       "  (388, 53.00),\n",
       "  (404, 53.00),\n",
       "  (415, 53.00),\n",
       "  (444, 53.00),\n",
       "  (452, 53.00),\n",
       "  (464, 53.00),\n",
       "  (470, 53.00),\n",
       "  (500, 53.00),\n",
       "  (524, 53.00),\n",
       "  (543, 53.00),\n",
       "  (601, 53.00),\n",
       "  (617, 53.00),\n",
       "  (620, 53.00),\n",
       "  (641, 53.00),\n",
       "  (654, 53.00),\n",
       "  (667, 53.00),\n",
       "  (734, 53.00),\n",
       "  (753, 53.00),\n",
       "  (755, 53.00),\n",
       "  (771, 53.00),\n",
       "  (890, 53.00),\n",
       "  (984, 53.00),\n",
       "  (990, 53.00),\n",
       "  (10, 52.00),\n",
       "  (28, 52.00),\n",
       "  (33, 52.00),\n",
       "  (47, 52.00),\n",
       "  (73, 52.00),\n",
       "  (102, 52.00),\n",
       "  (171, 52.00),\n",
       "  (198, 52.00),\n",
       "  (275, 52.00),\n",
       "  (278, 52.00),\n",
       "  (299, 52.00),\n",
       "  (421, 52.00),\n",
       "  (436, 52.00),\n",
       "  (451, 52.00),\n",
       "  (453, 52.00),\n",
       "  (474, 52.00),\n",
       "  (535, 52.00),\n",
       "  (540, 52.00),\n",
       "  (546, 52.00),\n",
       "  (563, 52.00),\n",
       "  (669, 52.00),\n",
       "  (678, 52.00),\n",
       "  (691, 52.00),\n",
       "  (719, 52.00),\n",
       "  (732, 52.00),\n",
       "  (757, 52.00),\n",
       "  (788, 52.00),\n",
       "  (812, 52.00),\n",
       "  (852, 52.00),\n",
       "  (860, 52.00),\n",
       "  (878, 52.00),\n",
       "  (937, 52.00),\n",
       "  (49, 51.00),\n",
       "  (65, 51.00),\n",
       "  (90, 51.00),\n",
       "  (98, 51.00),\n",
       "  (129, 51.00),\n",
       "  (163, 51.00),\n",
       "  (219, 51.00),\n",
       "  (244, 51.00),\n",
       "  (247, 51.00),\n",
       "  (251, 51.00),\n",
       "  (255, 51.00),\n",
       "  (316, 51.00),\n",
       "  (318, 51.00),\n",
       "  (321, 51.00),\n",
       "  (340, 51.00),\n",
       "  (376, 51.00),\n",
       "  (450, 51.00),\n",
       "  (455, 51.00),\n",
       "  (505, 51.00),\n",
       "  (533, 51.00),\n",
       "  (547, 51.00),\n",
       "  (576, 51.00),\n",
       "  (640, 51.00),\n",
       "  (668, 51.00),\n",
       "  (677, 51.00),\n",
       "  (695, 51.00),\n",
       "  (718, 51.00),\n",
       "  (745, 51.00),\n",
       "  (780, 51.00),\n",
       "  (835, 51.00),\n",
       "  (854, 51.00),\n",
       "  (916, 51.00),\n",
       "  (939, 51.00),\n",
       "  (986, 51.00),\n",
       "  (76, 50.00),\n",
       "  (106, 50.00),\n",
       "  (107, 50.00),\n",
       "  (118, 50.00),\n",
       "  (148, 50.00),\n",
       "  (191, 50.00),\n",
       "  (221, 50.00),\n",
       "  (232, 50.00),\n",
       "  (273, 50.00),\n",
       "  (324, 50.00),\n",
       "  (327, 50.00),\n",
       "  (354, 50.00),\n",
       "  (407, 50.00),\n",
       "  (426, 50.00),\n",
       "  (439, 50.00),\n",
       "  (448, 50.00),\n",
       "  (595, 50.00),\n",
       "  (609, 50.00),\n",
       "  (671, 50.00),\n",
       "  (685, 50.00),\n",
       "  (760, 50.00),\n",
       "  (770, 50.00),\n",
       "  (799, 50.00),\n",
       "  (814, 50.00),\n",
       "  (815, 50.00),\n",
       "  (918, 50.00),\n",
       "  (946, 50.00),\n",
       "  (953, 50.00),\n",
       "  (14, 49.00),\n",
       "  (35, 49.00),\n",
       "  (41, 49.00),\n",
       "  (56, 49.00),\n",
       "  (57, 49.00),\n",
       "  (87, 49.00),\n",
       "  (92, 49.00),\n",
       "  (95, 49.00),\n",
       "  (109, 49.00),\n",
       "  (188, 49.00),\n",
       "  (216, 49.00),\n",
       "  (294, 49.00),\n",
       "  (320, 49.00),\n",
       "  (322, 49.00),\n",
       "  (329, 49.00),\n",
       "  (337, 49.00),\n",
       "  (353, 49.00),\n",
       "  (378, 49.00),\n",
       "  (409, 49.00),\n",
       "  (427, 49.00),\n",
       "  (445, 49.00),\n",
       "  (463, 49.00),\n",
       "  (560, 49.00),\n",
       "  (567, 49.00),\n",
       "  (610, 49.00),\n",
       "  (648, 49.00),\n",
       "  (684, 49.00),\n",
       "  (687, 49.00),\n",
       "  (816, 49.00),\n",
       "  (818, 49.00),\n",
       "  (859, 49.00),\n",
       "  (888, 49.00),\n",
       "  (917, 49.00),\n",
       "  (933, 49.00),\n",
       "  (950, 49.00),\n",
       "  (956, 49.00),\n",
       "  (8, 48.00),\n",
       "  (15, 48.00),\n",
       "  (24, 48.00),\n",
       "  (50, 48.00),\n",
       "  (89, 48.00),\n",
       "  (111, 48.00),\n",
       "  (114, 48.00),\n",
       "  (115, 48.00),\n",
       "  (117, 48.00),\n",
       "  (136, 48.00),\n",
       "  (137, 48.00),\n",
       "  (147, 48.00),\n",
       "  (150, 48.00),\n",
       "  (207, 48.00),\n",
       "  (326, 48.00),\n",
       "  (360, 48.00),\n",
       "  (392, 48.00),\n",
       "  (434, 48.00),\n",
       "  (462, 48.00),\n",
       "  (485, 48.00),\n",
       "  (519, 48.00),\n",
       "  (615, 48.00),\n",
       "  (626, 48.00),\n",
       "  (642, 48.00),\n",
       "  (670, 48.00),\n",
       "  (672, 48.00),\n",
       "  (726, 48.00),\n",
       "  (735, 48.00),\n",
       "  (765, 48.00),\n",
       "  (767, 48.00),\n",
       "  (773, 48.00),\n",
       "  (779, 48.00),\n",
       "  (829, 48.00),\n",
       "  (845, 48.00),\n",
       "  (880, 48.00),\n",
       "  (907, 48.00),\n",
       "  (948, 48.00),\n",
       "  (989, 48.00),\n",
       "  (998, 48.00),\n",
       "  (37, 47.00),\n",
       "  (96, 47.00),\n",
       "  (113, 47.00),\n",
       "  (151, 47.00),\n",
       "  (156, 47.00),\n",
       "  (209, 47.00),\n",
       "  (228, 47.00),\n",
       "  (302, 47.00),\n",
       "  (311, 47.00),\n",
       "  (364, 47.00),\n",
       "  (373, 47.00),\n",
       "  (380, 47.00),\n",
       "  (412, 47.00),\n",
       "  (432, 47.00),\n",
       "  (575, 47.00),\n",
       "  (586, 47.00),\n",
       "  (613, 47.00),\n",
       "  (619, 47.00),\n",
       "  (625, 47.00),\n",
       "  (637, 47.00),\n",
       "  (639, 47.00),\n",
       "  (724, 47.00),\n",
       "  (750, 47.00),\n",
       "  (826, 47.00),\n",
       "  (853, 47.00),\n",
       "  (922, 47.00),\n",
       "  (965, 47.00),\n",
       "  (9, 46.00),\n",
       "  (54, 46.00),\n",
       "  (63, 46.00),\n",
       "  (64, 46.00),\n",
       "  (81, 46.00),\n",
       "  (83, 46.00),\n",
       "  (100, 46.00),\n",
       "  (125, 46.00),\n",
       "  (128, 46.00),\n",
       "  (139, 46.00),\n",
       "  (166, 46.00),\n",
       "  (170, 46.00),\n",
       "  (204, 46.00),\n",
       "  (206, 46.00),\n",
       "  (224, 46.00),\n",
       "  (276, 46.00),\n",
       "  (307, 46.00),\n",
       "  (323, 46.00),\n",
       "  (410, 46.00),\n",
       "  (443, 46.00),\n",
       "  (476, 46.00),\n",
       "  (481, 46.00),\n",
       "  (486, 46.00),\n",
       "  (490, 46.00),\n",
       "  (498, 46.00),\n",
       "  (509, 46.00),\n",
       "  (554, 46.00),\n",
       "  (592, 46.00),\n",
       "  (650, 46.00),\n",
       "  (697, 46.00),\n",
       "  (699, 46.00),\n",
       "  (720, 46.00),\n",
       "  (725, 46.00),\n",
       "  (736, 46.00),\n",
       "  (759, 46.00),\n",
       "  (821, 46.00),\n",
       "  (823, 46.00),\n",
       "  (827, 46.00),\n",
       "  (840, 46.00),\n",
       "  (865, 46.00),\n",
       "  (868, 46.00),\n",
       "  (914, 46.00),\n",
       "  (938, 46.00),\n",
       "  (940, 46.00),\n",
       "  (949, 46.00),\n",
       "  (982, 46.00),\n",
       "  (1, 45.00),\n",
       "  (11, 45.00),\n",
       "  (16, 45.00),\n",
       "  (42, 45.00),\n",
       "  (91, 45.00),\n",
       "  (123, 45.00),\n",
       "  (127, 45.00),\n",
       "  (143, 45.00),\n",
       "  (177, 45.00),\n",
       "  (212, 45.00),\n",
       "  (252, 45.00),\n",
       "  (296, 45.00),\n",
       "  (306, 45.00),\n",
       "  (325, 45.00),\n",
       "  (339, 45.00),\n",
       "  (370, 45.00),\n",
       "  (381, 45.00),\n",
       "  (383, 45.00),\n",
       "  (460, 45.00),\n",
       "  (537, 45.00),\n",
       "  (574, 45.00),\n",
       "  (631, 45.00),\n",
       "  (634, 45.00),\n",
       "  (658, 45.00),\n",
       "  (659, 45.00),\n",
       "  (704, 45.00),\n",
       "  (761, 45.00),\n",
       "  (768, 45.00),\n",
       "  (787, 45.00),\n",
       "  (804, 45.00),\n",
       "  (863, 45.00),\n",
       "  (882, 45.00),\n",
       "  (915, 45.00),\n",
       "  (970, 45.00),\n",
       "  (17, 44.00),\n",
       "  (22, 44.00),\n",
       "  (88, 44.00),\n",
       "  (131, 44.00),\n",
       "  (133, 44.00),\n",
       "  (140, 44.00),\n",
       "  (144, 44.00),\n",
       "  (158, 44.00),\n",
       "  (218, 44.00),\n",
       "  (227, 44.00),\n",
       "  (230, 44.00),\n",
       "  (270, 44.00),\n",
       "  (303, 44.00),\n",
       "  (351, 44.00),\n",
       "  (397, 44.00),\n",
       "  (433, 44.00),\n",
       "  (473, 44.00),\n",
       "  (503, 44.00),\n",
       "  (507, 44.00),\n",
       "  (514, 44.00),\n",
       "  (517, 44.00),\n",
       "  (522, 44.00),\n",
       "  (602, 44.00),\n",
       "  (605, 44.00),\n",
       "  (653, 44.00),\n",
       "  (711, 44.00),\n",
       "  (772, 44.00),\n",
       "  (786, 44.00),\n",
       "  (795, 44.00),\n",
       "  (879, 44.00),\n",
       "  (883, 44.00),\n",
       "  (887, 44.00),\n",
       "  (934, 44.00),\n",
       "  (941, 44.00),\n",
       "  (994, 44.00),\n",
       "  (0, 43.00),\n",
       "  (19, 43.00),\n",
       "  (27, 43.00),\n",
       "  (44, 43.00),\n",
       "  (45, 43.00),\n",
       "  (84, 43.00),\n",
       "  (105, 43.00),\n",
       "  (122, 43.00),\n",
       "  (169, 43.00),\n",
       "  (190, 43.00),\n",
       "  (242, 43.00),\n",
       "  (262, 43.00),\n",
       "  (277, 43.00),\n",
       "  (298, 43.00),\n",
       "  (317, 43.00),\n",
       "  (344, 43.00),\n",
       "  (346, 43.00),\n",
       "  (350, 43.00),\n",
       "  (365, 43.00),\n",
       "  (389, 43.00),\n",
       "  (430, 43.00),\n",
       "  (465, 43.00),\n",
       "  (472, 43.00),\n",
       "  (504, 43.00),\n",
       "  (510, 43.00),\n",
       "  (512, 43.00),\n",
       "  (521, 43.00),\n",
       "  (528, 43.00),\n",
       "  (572, 43.00),\n",
       "  (581, 43.00),\n",
       "  (599, 43.00),\n",
       "  (635, 43.00),\n",
       "  (680, 43.00),\n",
       "  (694, 43.00),\n",
       "  (748, 43.00),\n",
       "  (764, 43.00),\n",
       "  (832, 43.00),\n",
       "  (857, 43.00),\n",
       "  (861, 43.00),\n",
       "  (897, 43.00),\n",
       "  (901, 43.00),\n",
       "  (903, 43.00),\n",
       "  (963, 43.00),\n",
       "  (4, 42.00),\n",
       "  (79, 42.00),\n",
       "  (97, 42.00),\n",
       "  (99, 42.00),\n",
       "  (108, 42.00),\n",
       "  (121, 42.00),\n",
       "  (126, 42.00),\n",
       "  (141, 42.00),\n",
       "  (146, 42.00),\n",
       "  (179, 42.00),\n",
       "  (214, 42.00),\n",
       "  (231, 42.00),\n",
       "  (268, 42.00),\n",
       "  (418, 42.00),\n",
       "  (435, 42.00),\n",
       "  (438, 42.00),\n",
       "  (550, 42.00),\n",
       "  (587, 42.00),\n",
       "  (588, 42.00),\n",
       "  (618, 42.00),\n",
       "  (647, 42.00),\n",
       "  (752, 42.00),\n",
       "  (801, 42.00),\n",
       "  (858, 42.00),\n",
       "  (959, 42.00),\n",
       "  (12, 41.00),\n",
       "  (338, 41.00),\n",
       "  (417, 41.00),\n",
       "  (461, 41.00),\n",
       "  (502, 41.00),\n",
       "  (520, 41.00),\n",
       "  (525, 41.00),\n",
       "  (530, 41.00),\n",
       "  (531, 41.00),\n",
       "  (629, 41.00),\n",
       "  (683, 41.00),\n",
       "  (715, 41.00),\n",
       "  (808, 41.00),\n",
       "  (824, 41.00),\n",
       "  (875, 41.00),\n",
       "  (889, 41.00),\n",
       "  (891, 41.00),\n",
       "  (902, 41.00),\n",
       "  (928, 41.00),\n",
       "  (957, 41.00),\n",
       "  (2, 40.00),\n",
       "  (38, 40.00),\n",
       "  (86, 40.00),\n",
       "  (160, 40.00),\n",
       "  (184, 40.00),\n",
       "  (196, 40.00),\n",
       "  (211, 40.00),\n",
       "  (235, 40.00),\n",
       "  (352, 40.00),\n",
       "  (374, 40.00),\n",
       "  (494, 40.00),\n",
       "  (538, 40.00),\n",
       "  (555, 40.00),\n",
       "  (606, 40.00),\n",
       "  (651, 40.00),\n",
       "  (723, 40.00),\n",
       "  (749, 40.00),\n",
       "  (784, 40.00),\n",
       "  (811, 40.00),\n",
       "  (905, 40.00),\n",
       "  (930, 40.00),\n",
       "  (964, 40.00),\n",
       "  (971, 40.00),\n",
       "  (977, 40.00),\n",
       "  (110, 39.00),\n",
       "  (183, 39.00),\n",
       "  (341, 39.00),\n",
       "  (355, 39.00),\n",
       "  (358, 39.00),\n",
       "  (375, 39.00),\n",
       "  (442, 39.00),\n",
       "  (501, 39.00),\n",
       "  (536, 39.00),\n",
       "  (644, 39.00),\n",
       "  (662, 39.00),\n",
       "  (665, 39.00),\n",
       "  (686, 39.00),\n",
       "  (690, 39.00),\n",
       "  (731, 39.00),\n",
       "  (754, 39.00),\n",
       "  (756, 39.00),\n",
       "  (807, 39.00),\n",
       "  (987, 39.00),\n",
       "  (26, 38.00),\n",
       "  (130, 38.00),\n",
       "  (297, 38.00),\n",
       "  (304, 38.00),\n",
       "  (359, 38.00),\n",
       "  (385, 38.00),\n",
       "  (395, 38.00),\n",
       "  (571, 38.00),\n",
       "  (624, 38.00),\n",
       "  (643, 38.00),\n",
       "  (705, 38.00),\n",
       "  (709, 38.00),\n",
       "  (742, 38.00),\n",
       "  (744, 38.00),\n",
       "  (798, 38.00),\n",
       "  (810, 38.00),\n",
       "  (872, 38.00),\n",
       "  (898, 38.00),\n",
       "  (910, 38.00),\n",
       "  (944, 38.00),\n",
       "  (954, 38.00),\n",
       "  (966, 38.00),\n",
       "  (996, 38.00),\n",
       "  (7, 37.00),\n",
       "  (52, 37.00),\n",
       "  (93, 37.00),\n",
       "  (112, 37.00),\n",
       "  (134, 37.00),\n",
       "  (185, 37.00),\n",
       "  (215, 37.00),\n",
       "  (319, 37.00),\n",
       "  (335, 37.00),\n",
       "  (402, 37.00),\n",
       "  (411, 37.00),\n",
       "  (446, 37.00),\n",
       "  (467, 37.00),\n",
       "  (484, 37.00),\n",
       "  (516, 37.00),\n",
       "  (589, 37.00),\n",
       "  (693, 37.00),\n",
       "  (841, 37.00),\n",
       "  (961, 37.00),\n",
       "  (34, 36.00),\n",
       "  (36, 36.00),\n",
       "  (152, 36.00),\n",
       "  (159, 36.00),\n",
       "  (201, 36.00),\n",
       "  (220, 36.00),\n",
       "  (236, 36.00),\n",
       "  (285, 36.00),\n",
       "  (459, 36.00),\n",
       "  (559, 36.00),\n",
       "  (664, 36.00),\n",
       "  (696, 36.00),\n",
       "  (737, 36.00),\n",
       "  (793, 36.00),\n",
       "  (817, 36.00),\n",
       "  (919, 36.00),\n",
       "  (932, 36.00),\n",
       "  (976, 36.00),\n",
       "  (187, 35.00),\n",
       "  (200, 35.00),\n",
       "  (205, 35.00),\n",
       "  (210, 35.00),\n",
       "  (367, 35.00),\n",
       "  (371, 35.00),\n",
       "  (393, 35.00),\n",
       "  (408, 35.00),\n",
       "  (456, 35.00),\n",
       "  (551, 35.00),\n",
       "  (558, 35.00),\n",
       "  (790, 35.00),\n",
       "  (809, 35.00),\n",
       "  (931, 35.00),\n",
       "  (935, 35.00),\n",
       "  (943, 35.00),\n",
       "  (960, 35.00),\n",
       "  (962, 35.00),\n",
       "  (972, 35.00),\n",
       "  (55, 34.00),\n",
       "  (124, 34.00),\n",
       "  (449, 34.00),\n",
       "  (488, 34.00),\n",
       "  (541, 34.00),\n",
       "  (557, 34.00),\n",
       "  (566, 34.00),\n",
       "  (633, 34.00),\n",
       "  (740, 34.00),\n",
       "  (775, 34.00),\n",
       "  (797, 34.00),\n",
       "  (813, 34.00),\n",
       "  (936, 34.00),\n",
       "  (947, 34.00),\n",
       "  (978, 34.00),\n",
       "  (165, 33.00),\n",
       "  (356, 33.00),\n",
       "  (429, 33.00),\n",
       "  (552, 33.00),\n",
       "  (675, 33.00),\n",
       "  (717, 33.00),\n",
       "  (958, 33.00),\n",
       "  (119, 32.00),\n",
       "  (246, 32.00),\n",
       "  (266, 32.00),\n",
       "  (290, 32.00),\n",
       "  (309, 32.00),\n",
       "  (534, 32.00),\n",
       "  (676, 32.00),\n",
       "  (708, 32.00),\n",
       "  (733, 32.00),\n",
       "  (908, 32.00),\n",
       "  (945, 32.00),\n",
       "  (952, 32.00),\n",
       "  (226, 31.00),\n",
       "  (710, 31.00),\n",
       "  (729, 31.00),\n",
       "  (778, 31.00),\n",
       "  (942, 31.00),\n",
       "  (176, 30.00),\n",
       "  (213, 30.00),\n",
       "  (357, 30.00),\n",
       "  (869, 30.00),\n",
       "  (175, 29.00),\n",
       "  (348, 29.00),\n",
       "  (638, 29.00),\n",
       "  (673, 29.00),\n",
       "  (837, 29.00),\n",
       "  (876, 29.00),\n",
       "  (968, 29.00),\n",
       "  (62, 28.00),\n",
       "  (193, 28.00),\n",
       "  (271, 28.00),\n",
       "  (390, 28.00),\n",
       "  (862, 28.00),\n",
       "  (369, 27.00),\n",
       "  (400, 27.00),\n",
       "  (656, 27.00),\n",
       "  (663, 27.00),\n",
       "  (240, 26.00),\n",
       "  (248, 26.00),\n",
       "  (315, 26.00),\n",
       "  (596, 26.00),\n",
       "  (782, 26.00),\n",
       "  (856, 26.00),\n",
       "  (68, 25.00),\n",
       "  (345, 25.00),\n",
       "  (660, 25.00),\n",
       "  (758, 25.00),\n",
       "  (924, 25.00),\n",
       "  (967, 25.00),\n",
       "  (969, 25.00),\n",
       "  (32, 24.00),\n",
       "  (74, 24.00),\n",
       "  (394, 24.00),\n",
       "  (585, 24.00),\n",
       "  (911, 24.00),\n",
       "  (929, 24.00),\n",
       "  (482, 22.00),\n",
       "  (173, 21.00),\n",
       "  (899, 21.00),\n",
       "  (414, 20.00),\n",
       "  (479, 20.00),\n",
       "  (499, 20.00),\n",
       "  (590, 20.00),\n",
       "  (623, 20.00),\n",
       "  (167, 19.00),\n",
       "  (578, 19.00),\n",
       "  (622, 19.00),\n",
       "  (493, 18.00),\n",
       "  (282, 17.00),\n",
       "  (600, 17.00),\n",
       "  (728, 15.00),\n",
       "  (689, 14.00),\n",
       "  (885, 14.00),\n",
       "  (906, 14.00),\n",
       "  (836, 13.00),\n",
       "  (838, 10.00)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "n, hist = targeted_diversity(learn, 95)\n",
    "n, hist\n",
    "# n, hist, tk = diversity(learn, 10, 95)\n",
    "# n, hist, tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8ab3fc4438>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_hist = sorted(hist, key=lambda x: x[0], reverse = False)\n",
    "values = [elem[1] for elem in sorted_hist]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.9102)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8846)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_from_uniform(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fCvFG0VMKts"
   },
   "outputs": [],
   "source": [
    "def make_triplet_samples(z, margin, r2, r3):\n",
    "  positive_sample = z + random_vector_volume(z.shape, 0, margin).cuda() \n",
    "  negative_sample = z + random_vector_volume(z.shape, r2, r3).cuda()\n",
    "  return positive_sample, negative_sample\n",
    "\n",
    "def random_vector_surface(shape, r = 1.):\n",
    "  mat = torch.randn(size=shape).cuda()\n",
    "  norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "  return (mat/norm) * r\n",
    "\n",
    "def random_vector_volume(shape, inner_r, outer_r):\n",
    "  fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "  fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "  fraction.unsqueeze_(-1)\n",
    "  return random_vector_surface(shape, 1) * fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PizmBkGqMKtu"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_frequent(x):\n",
    "  return Counter(x).most_common(1)[0]\n",
    "\n",
    "def preds_around(center, radius, n_preds, model, dummy_img):\n",
    "  z_s = random_vector_volume([n_preds, 10], radius, radius + 0.01) + center[None]\n",
    "  noises = model.forward_z(z_s)\n",
    "  perturbed_imgs = noises + dummy_img \n",
    "  return torch.argmax(arch(perturbed_imgs), 1)\n",
    "  \n",
    "def most_freq_pred_around(center, radius, n_preds, model, dummy_img):\n",
    "  preds = preds_around(center, radius, n_preds, model, dummy_img)\n",
    "  most_freq = most_frequent(preds.tolist())\n",
    "  return (class_index_to_label(most_freq[0]), most_freq[1]/n_preds)\n",
    "\n",
    "def investigate_neighborhood(z, step, model, dummy_img):\n",
    "  with torch.no_grad():\n",
    "    result = []\n",
    "    for radius in np.arange(0.1, 6., step):\n",
    "#       print(\"creating {} more preds\".format(int(10 + 5 * (radius ** 2))))\n",
    "      most_freq_pred = most_freq_pred_around(z, radius, int(10 + 5 * (radius ** 2)), model, dummy_img)\n",
    "      result.append((radius, most_freq_pred))\n",
    "    return result\n",
    "  \n",
    "def big_vector_to_str(x, thresh = 0.01):\n",
    "  torch.set_printoptions(precision=2, sci_mode=False, threshold=5000)  \n",
    "  result = \"[\"\n",
    "  for i, x_i in enumerate(x.data):\n",
    "    if abs(x_i) > thresh:\n",
    "      result += \"{}: {:.2f}\".format(i, x_i.item()) \n",
    "      result += \", \" if (i < x.shape[0]-1) else \"\"\n",
    "  result += \"]\"\n",
    "  return result\n",
    "\n",
    "def print_big_vector(x, thresh = 0.01):\n",
    "  print(big_vector_to_str(x, thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-btRW4qMKtw",
    "outputId": "90e81f6a-1b9e-45a4-ae82-bda370319bd9"
   },
   "outputs": [],
   "source": [
    "#experiment 1\n",
    "\n",
    "z = torch.tensor([0.5] * 10).cuda()\n",
    "# z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "# z_s = z[None]\n",
    "\n",
    "model = learn.model.eval()\n",
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "  \n",
    "for i in range(6):\n",
    "  z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "  print(\"investigation for: \", z)\n",
    "  for elem in investigate_neighborhood(z, 0.5, model, x_img):\n",
    "    print(elem)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 1-1: modified investigate_z\n",
    "z_investigate_path = '/root/Derakhshani/adversarial/textual_notes/investigate_z_{}.txt'.format(env.save_filename)\n",
    "if Path(z_investigate_path).exists(): raise FileExistsError(\"file already exists\")\n",
    "file = open(str(z_investigate_path), 'w')\n",
    "        \n",
    "for i, (z, noise) in enumerate(zip(pruned_z_s, pruned_noises)):\n",
    "  hist = compute_prediction_histogram(learn, noise)\n",
    "  indexed_hist = [(i, val) for i, val in enumerate(hist)]\n",
    "  sorted_hist = sorted(indexed_hist, key=lambda x: x[1], reverse=True)\n",
    "  labeled_hist = [(class_index_to_label(i), count) for i, count in sorted_hist]\n",
    "  print(\"result {}:\".format(i))\n",
    "  print(big_vector_to_str(z))\n",
    "  print(labeled_hist[:6])\n",
    "  print(\"\\n\\n\")\n",
    "  \n",
    "  file.write(\"result {}:\\n\".format(i))\n",
    "  file.write(big_vector_to_str(z) + \"\\n\")\n",
    "  file.write(str(labeled_hist[:6]))\n",
    "  file.write(\"\\n\\n\\n\")\n",
    "  file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bp6YOnipMKtz"
   },
   "outputs": [],
   "source": [
    "#experiment 2\n",
    "import itertools\n",
    "z_s = [torch.tensor(t).cuda() for t in itertools.product( *([[-0.33, 0.33]] * 10) )]\n",
    "model = learn.model.eval()\n",
    "noises = []\n",
    "with torch.no_grad():\n",
    "  for z in z_s:\n",
    "    noises.append(model.forward_single_z(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "55lErWDyMKt1",
    "outputId": "93d8fb71-3fd5-44a5-d3ec-8013e13f17ba"
   },
   "outputs": [],
   "source": [
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "\n",
    "preds = []\n",
    "for noise in noises:\n",
    "  perturbed_img = x_img + noise\n",
    "  preds.append(torch.argmax(arch(perturbed_img[None]), 1)[0].item())\n",
    "\n",
    "from collections import Counter\n",
    "result = [(class_index_to_label(index), count) for index, count in Counter(preds).most_common(5)]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4WSg-wBFMKt5"
   },
   "outputs": [],
   "source": [
    "#experiment 3\n",
    "import itertools\n",
    "dimension_values = [[-0.9, 0.9]] * z_dim\n",
    "for i in range(z_dim):\n",
    "  if i % 100 != 0:\n",
    "    dimension_values[i] = [0.]\n",
    "# dimension_values[0] = [0.]\n",
    "# dimension_values[3] = [0.]\n",
    "# dimension_values[6] = [0.]\n",
    "# dimension_values[9] = [0.]\n",
    "pruned_z_s = [torch.tensor(t).cuda() for t in itertools.product(*dimension_values)]\n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 3: for the targeted-attack case\n",
    "pruned_z_s = []\n",
    "for i in range(z_dim):\n",
    "  new_z = torch.zeros(z_dim).cuda()\n",
    "  new_z[i] = 1.\n",
    "  pruned_z_s.append(new_z)\n",
    "  \n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 3-1: noises for \n",
    "pruned_z_s = []\n",
    "# for i in range(z_dim):\n",
    "#   new_z = torch.empty(z_dim).uniform_(0,1).cuda().detach()\n",
    "#   pruned_z_s.append(new_z)\n",
    "\n",
    "for i in range(z_dim):\n",
    "  new_z = torch.zeros(z_dim).cuda().detach()\n",
    "  new_z[i] = 1.\n",
    "  pruned_z_s.append(new_z)\n",
    "  \n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise in pruned_noises[0:200]:\n",
    "  img = noise_to_image(noise)\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider web\n",
    "z_values = [\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33],\n",
    "  [-0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33,  0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33,  0.33,  0.33],\n",
    "  [-0.33,  0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_values = [\n",
    "  # window screen\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuAVZzmKMKt9",
    "outputId": "7e6643e0-ce55-438b-e0ae-79bde3ee4cef"
   },
   "outputs": [],
   "source": [
    "#vgg-16_12 most repeated labels:\n",
    "l = [(611, 215.0),\n",
    "  (474, 194.1),\n",
    "  (398, 120.3),\n",
    "  (721, 79.6),\n",
    "  (741, 73.5),\n",
    "  (510, 62.5)]\n",
    "\n",
    "[(class_index_to_label(index), count) for index, count in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAgk-YyWc3rG"
   },
   "outputs": [],
   "source": [
    "# learn.recorder.plot_losses()\n",
    "# learn.recorder.plot_lr()\n",
    "# learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTHG4Bt7VDYp"
   },
   "outputs": [],
   "source": [
    "fooling_rates = []\n",
    "model = learn.model.eval()\n",
    "learn.metrics = [validation_single_perturbation]\n",
    "for i in range(10):\n",
    "  global_perturbations = model(torch.rand(1, 3, 224, 244).cuda())[0]\n",
    "  nag_util.global_perturbations = global_perturbations\n",
    "  fooling_rates.append(learn.validate()[1].cpu().item())\n",
    "  print(\"%d : %f\"%(i, fooling_rates[-1]))\n",
    "\n",
    "mean = np.mean(fooling_rates)\n",
    "stddev = np.std(fooling_rates)\n",
    "print(mean, stddev); print(fooling_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "OFCjzI7UaY3C",
    "outputId": "740185b4-dd54-46f4-b0af-79ee452568e1"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[200][0]\n",
    "x = normalize(x_img.data.cuda())\n",
    "z = torch.tensor([-0.33,  0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33], dtype=torch.float32).cuda()\n",
    "# z = torch.empty(z_dim).uniform_(-1,1).cuda()\n",
    "p = model.forward_single_z(z).detach()\n",
    "\n",
    "p_x = x + p\n",
    "# print(\"img range, noise range\")\n",
    "# print_range(x); print_range(p)\n",
    "adv_label = class_index_to_label(arch(p_x[None]).argmax(1).item())\n",
    "print_big_vector(arch(p_x[None])[0])\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0., 1.])\n",
    "p_img = Image(p)\n",
    "x_img.show()\n",
    "p_img.show()\n",
    "p_x_img.show()\n",
    "\n",
    "\n",
    "# print_range(p)\n",
    "# print_range(denormalize(x))\n",
    "# print_range(p_x)\n",
    "\n",
    "benign_label = class_index_to_label(arch(x[None]).argmax(1).item())\n",
    "\n",
    "print_big_vector(arch(x[None])[0])\n",
    "print(benign_label, adv_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzwsI2P1ZANz"
   },
   "outputs": [],
   "source": [
    "z1 = torch.tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p1 = model.forward_single_z(z1)\n",
    "\n",
    "z2 = torch.tensor([1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p2 = model.forward_single_z(z2)\n",
    "\n",
    "z3 = torch.tensor([1, 1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p3 = model.forward_single_z(z3)\n",
    "\n",
    "l2_distance(p1, p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eroI82OKSnAL"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[4][0]\n",
    "x = x_img.data[None].cuda()\n",
    "p = model(x)[0].squeeze().detach() \n",
    "x = x.squeeze()\n",
    "x = normalize(x)\n",
    "\n",
    "p_x = x + p\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0.,1.])\n",
    "p_img = Image(p)\n",
    "# x_img.show()\n",
    "p_img.show()\n",
    "# p_x_img.show()\n",
    "\n",
    "print_range(p)\n",
    "print_range(x)\n",
    "print_range(p_x)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NAG-tripletLossExperiment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
