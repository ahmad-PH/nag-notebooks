{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ahmad-PH/nag-notebooks/blob/master/NAG_tripletLossExperiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqeZpz16do4y"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def run_shell_command(cmd):\n",
    "  p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "  print(str(p.communicate()[0], 'utf-8'))\n",
    "\n",
    "\n",
    "def detect_env():\n",
    "    import os\n",
    "    if 'content' in os.listdir('/'):\n",
    "      return \"colab\"\n",
    "    else:\n",
    "      return \"IBM\"\n",
    "  \n",
    "def create_env():\n",
    "  if detect_env() == \"IBM\":\n",
    "    return IBMEnv()\n",
    "  elif detect_env() == \"colab\":\n",
    "    return ColabEnv()\n",
    "\n",
    "\n",
    "class Env:\n",
    "  def get_nag_util_files(self):\n",
    "      import os\n",
    "      \n",
    "      print(\"\\ngetting git files ...\")\n",
    "      if os.path.isdir(self.python_files_path):\n",
    "        os.chdir(self.python_files_path)\n",
    "        run_shell_command('git pull')\n",
    "        os.chdir(self.root_folder)\n",
    "      else:\n",
    "        run_shell_command('git clone https://github.com/ahmad-PH/nag-public.git')\n",
    "      print(\"done.\")\n",
    "  \n",
    "\n",
    "class IBMEnv(Env):\n",
    "    def __init__(self):\n",
    "      self.root_folder = \"/root/Derakhshani/adversarial\"\n",
    "      self.temp_csv_path = self.root_folder + \"/temp\"\n",
    "      self.python_files_path = self.root_folder + \"/nag-public\"\n",
    "      self.python_files_dir = \"NAG-11May-beforeDenoiser\"\n",
    "      \n",
    "      import sys\n",
    "      sys.path.append('./nag/nag_util')\n",
    "      \n",
    "    def get_csv_path(self):\n",
    "      return self.root_folder + \"/textual_notes/CSVs/\" + self.save_filename\n",
    "    \n",
    "    def get_models_path(self):\n",
    "      return self.root_folder + \"/models/\" + self.save_filename\n",
    "      \n",
    "    def setup(self):\n",
    "      self.get_nag_util_files()\n",
    "      \n",
    "      import os; import torch;\n",
    "      os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "      cuda_index = 1\n",
    "      os.environ['CUDA_VISIBLE_DEVICES']=str(cuda_index)\n",
    "#       defaults.device = torch.device('cuda:' + str(cuda_index))\n",
    "#       print('cuda:' + str(cuda_index))\n",
    "#       torch.cuda.set_device('cuda:1')\n",
    "      \n",
    "    def load_dataset(self, compressed_name, unpacked_name):\n",
    "      pass\n",
    "\n",
    "    def load_test_dataset(self, root_folder):\n",
    "      pass\n",
    "    \n",
    "    def set_data_path(self, path):\n",
    "      self.data_path = Path(self.root_folder + '/datasets/' + path)\n",
    "    \n",
    "        \n",
    "class ColabEnv(Env):\n",
    "    def __init__(self):\n",
    "      self.root_folder = '/content'\n",
    "      self.temp_csv_path = self.root_folder\n",
    "      self.python_files_path = self.root_folder + '/nag-public'\n",
    "      self.python_files_dir = \"NAG-11May-beforeDenoiser\"\n",
    "      self.torchvision_upgraded = False\n",
    "      \n",
    "    def get_csv_path(self):\n",
    "      return self.root_folder + '/gdrive/My Drive/DL/textual_notes/CSVs/' + self.save_filename\n",
    "    \n",
    "    def get_models_path(self):\n",
    "      return self.root_folder + \"/gdrive/My Drive/DL/models/\" + self.save_filename\n",
    "        \n",
    "    def setup(self):\n",
    "        # ######################################################\n",
    "        # # TODO remove this once torchvision 0.3 is present by\n",
    "        # # default in Colab\n",
    "        # ######################################################\n",
    "        global torchvision_upgraded\n",
    "        try:\n",
    "            torchvision_upgraded\n",
    "        except NameError:\n",
    "          !pip uninstall -y torchvision\n",
    "          !pip install https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl\n",
    "          torchvision_upgraded = True\n",
    "        else:\n",
    "          print(\"torchvision already upgraded\")\n",
    "          \n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/gdrive')\n",
    "        \n",
    "        self.get_nag_util_files()\n",
    "        \n",
    "    def load_dataset(self, compressed_name, unpacked_name):\n",
    "      if compressed_name not in os.listdir('.'):\n",
    "        print(compressed_name + ' not found, getting it from drive')\n",
    "        shutil.copyfile(\"/content/gdrive/My Drive/DL/{}.tar.gz\".format(compressed_name), \"./{}.tar.gz\".format(compressed_name))\n",
    "\n",
    "        gunzip_arg = \"./{}.tar.gz\".format(compressed_name)\n",
    "        !gunzip -f $gunzip_arg\n",
    "\n",
    "        tar_arg = \"./{}.tar\".format(compressed_name)\n",
    "        !tar -xvf $tar_arg > /dev/null\n",
    "\n",
    "        os.rename(unpacked_name, compressed_name)\n",
    "\n",
    "    #     ls_arg = \"./{}/train/n01440764\".format(compressed_name)\n",
    "    #     !ls $ls_arg\n",
    "\n",
    "        !rm $tar_arg\n",
    "\n",
    "        print(\"done\") \n",
    "      else:\n",
    "        print(compressed_name + \" found\")\n",
    "        \n",
    "    def load_test_dataset(self, root_folder):\n",
    "      test_folder = root_folder + '/test/'\n",
    "      if 'test' not in os.listdir(root_folder):\n",
    "        print('getting test dataset from drive')\n",
    "        os.mkdir(test_folder)\n",
    "        for i in range(1,11):\n",
    "          shutil.copy(\"/content/gdrive/My Drive/DL/full_test_folder/{}.zip\".format(i), test_folder)\n",
    "          shutil.unpack_archive(test_folder + \"/{}.zip\".format(i), test_folder)\n",
    "          os.remove(test_folder + \"/{}.zip\".format(i))\n",
    "          print(\"done with the {}th fragment\".format(i))\n",
    "      else:\n",
    "        print('test dataset found.')\n",
    "        \n",
    "    def set_data_path(self, path):\n",
    "      self.data_path = Path('./' + path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "YyZUYSjBi9K9",
    "outputId": "5ef25a03-c55f-43de-8460-a1afde369fd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "getting git files ...\n",
      "Already up-to-date.\n",
      "\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "env = create_env()\n",
    "env.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Ev7jcRKoARg"
   },
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.imports import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys; import os; import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3_1aE41PZAMw"
   },
   "outputs": [],
   "source": [
    "sys.path.append(env.python_files_path + '/' + env.python_files_dir)\n",
    "\n",
    "from nag_util import *\n",
    "import nag_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tltucTv2ep9-"
   },
   "outputs": [],
   "source": [
    "# mode = \"sanity_check\"\n",
    "mode = \"normal\"\n",
    "# mode = \"div_metric_calc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50\n",
    "# model = models.resnet152\n",
    "# model = models.vgg16_bn\n",
    "# model = torchvision.models.googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SO1h55obXzOv",
    "outputId": "54414cc5-84d5-4f45-ecab-87374a58dd33"
   },
   "outputs": [],
   "source": [
    "if mode == \"normal\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "elif mode == \"sanity_check\":\n",
    "  env.load_dataset('dataset_sanity_check_small', 'dataset_sanity_check_small')  \n",
    "  env.set_data_path('dataset_sanity_check_small')\n",
    "elif mode == \"div_metric_calc\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "  env.load_test_dataset(str(env.data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "koaQZmjMom7w"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "gpu_flag = True\n",
    "nag_util.batch_size = batch_size; nag_util.gpu_flag = gpu_flag;\n",
    "# nag_util.set_globals(gpu_flag, batch_size)\n",
    "tfms = get_transforms(do_flip=False, max_rotate=0)\n",
    "data = (ImageList.from_folder(env.data_path)\n",
    "        .split_by_folder(valid=('test' if mode == 'div_metric_calc' else 'valid'))\n",
    "        .label_from_folder()\n",
    "        .transform(tfms, size=224)\n",
    "        .databunch(bs=batch_size, num_workers=1)\n",
    "        .normalize(imagenet_stats))\n",
    "\n",
    "# data.show_batch(rows=2, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDBkRV8yovwV"
   },
   "outputs": [],
   "source": [
    "model_name = model.__name__\n",
    "z_dim = 1000\n",
    "arch = SoftmaxWrapper(model(pretrained=True).cuda().eval())\n",
    "nag_util.arch = arch\n",
    "requires_grad(arch, False)\n",
    "\n",
    "# vgg:\n",
    "# layers = []\n",
    "# blocks = [i-1 for i,o in enumerate(children(arch.features)) if isinstance(o, nn.MaxPool2d)]\n",
    "# layers = [arch.features[i] for i in blocks]\n",
    "# layer_weights = [1] * len(layers)\n",
    "\n",
    "layers = [\n",
    "    arch.softmax\n",
    "]\n",
    "\n",
    "layer_weights = [1.] * len(layers)\n",
    "\n",
    "# inception:\n",
    "# layers = [\n",
    "#     arch.Conv2d_1a_3x3,\n",
    "#     arch.Mixed_6e,\n",
    "#     arch.Mixed_7a,\n",
    "#     arch.fc    \n",
    "# ]\n",
    "# layer_weights = [1.0/4.0] * len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gen(nn.Module):\n",
    "  def __init__(self, z_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "    super(Gen, self).__init__()\n",
    "\n",
    "    self.bs = None\n",
    "    self.z_dim = z_dim\n",
    "    self.gf_dim = gf_dim\n",
    "    self.y_dim = y_dim\n",
    "    self.df_dim = df_dim\n",
    "    self.image_shape = image_shape\n",
    "\n",
    "    self.z_ = nn.Linear(self.z_dim, self.gf_dim * 7 * 4 * 4, bias=True)\n",
    "    self.z_.bias.data.fill_(0)\n",
    "    self.BN_ = nn.BatchNorm2d(self.gf_dim * 7)\n",
    "\n",
    "    self.CT2d_1 = deconv_layer(self.gf_dim * 8, self.gf_dim * 4, k_size = (5,5), pad = (2,2))\n",
    "    \n",
    "    self.CT2d_2 = deconv_layer(self.gf_dim * 5, self.gf_dim * 2)\n",
    "\n",
    "    self.half = max(self.gf_dim // 2, 1) \n",
    "    self.CT2d_3 = deconv_layer(self.gf_dim * 2 + self.half, self.gf_dim * 1)\n",
    "\n",
    "    self.quarter = max(self.gf_dim // 4, 1)\n",
    "    self.CT2d_4 = deconv_layer(self.gf_dim * 1 + self.quarter, self.gf_dim * 1)\n",
    "\n",
    "    self.eighth = max(self.gf_dim // 8, 1)\n",
    "    self.CT2d_5 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "\n",
    "    # sixteenth = max(self.gf_dim // 16, 1)\n",
    "    self.CT2d_6 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "\n",
    "    # sixteenth = max(self.gf_dim // 16, 1)\n",
    "    self.CT2d_7 = deconv_layer(self.gf_dim * 1 + self.eighth, 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "  def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "    h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "    h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "    output = deconv_layer(h_input)\n",
    "    assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "            \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "            \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "    return output\n",
    "  \n",
    "  def forward_z(self, z):\n",
    "    self.bs = z.shape[0]\n",
    "    \n",
    "    h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "    assert h0.shape[2:] == (4, 4), \"Non-expected shape, it shoud be (4,4)\"\n",
    "\n",
    "    h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "    h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "    h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "    h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "    h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "    h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "    h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "    ksi = 10.0\n",
    "    output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "    # this coeff scales the output to be appropriate for images that are \n",
    "    # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "    # interval)\n",
    "    return output_coeff * torch.tanh(h7)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    self.bs = inputs.shape[0]\n",
    "\n",
    "    benign_preds_onehot = arch(inputs)\n",
    "    benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "    \n",
    "    z = torch.zeros([self.bs, 1000]).cuda()\n",
    "    random_label = self.randint(0,1000, exclude = benign_preds.tolist())\n",
    "    for i in range(self.bs):\n",
    "      z[i][random_label] = 1.\n",
    "    \n",
    "    z_out = self.forward_z(z)\n",
    "    \n",
    "    return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "  \n",
    "  @staticmethod\n",
    "  def randint(low, high, exclude: list):\n",
    "    temp = np.random.randint(low, high - len(exclude))\n",
    "    for val in exclude:\n",
    "      if temp >= val:\n",
    "        temp += 1\n",
    "    return temp\n",
    "  \n",
    "  def forward_single_z(self, z):\n",
    "    return self.forward_z(z[None]).squeeze()\n",
    "           \n",
    "  \n",
    "  def make_triplet_samples(self, z, margin, r2, r3):\n",
    "    positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "    negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "    return positive_sample, negative_sample\n",
    "\n",
    "  def random_vector_surface(self, shape, r = 1.):\n",
    "    mat = torch.randn(size=shape).cuda()\n",
    "    norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "    return (mat/norm) * r\n",
    "\n",
    "  \n",
    "  def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "    fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "    fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "    fraction.unsqueeze_(-1)\n",
    "    return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "  def make_z(self, in_shape):\n",
    "    return torch.empty(in_shape).cuda().uniform_(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NkfbLWEQqRA_"
   },
   "outputs": [],
   "source": [
    "def js_distance(x1, x2):\n",
    "  m = 0.5 * (x1 + x2)\n",
    "  return 0.5 * (F.kl_div(x1, m) + F.kl_div(x2, m))\n",
    "\n",
    "def kl_distance(x1, x2):\n",
    "  inp = torch.log(x1)\n",
    "  target = x2\n",
    "  return F.kl_div(inp, target, reduction='batchmean')\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  x1 = tensorify(x1)\n",
    "  x2 = tensorify(x2)\n",
    "  x1 = x1 / torch.sum(x1)\n",
    "  x2 = x2 / torch.sum(x2)\n",
    "  return kl_distance(x1[None], x2[None])\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  if not isinstance(x1, torch.Tensor): x1 = torch.tensor(x1)\n",
    "  if not isinstance(x2, torch.Tensor): x2 = torch.tensor(x2)\n",
    "  x1 = x1 * 100. / torch.sum(x1)\n",
    "  x2 = x2 * 100. / torch.sum(x2)\n",
    "  return torch.norm(x1 - x2, 2)\n",
    "\n",
    "def distance_from_uniform(x):\n",
    "  return distrib_distance(x, [1.] * len(x))\n",
    "\n",
    "def wasserstein_distance(x1, x2):\n",
    "  return torch.mean(x1 - x2)\n",
    "\n",
    "def l1_distance(x1, x2):\n",
    "  return F.l1_loss(x1, x2)\n",
    "\n",
    "def l2_distance(x1, x2):\n",
    "  return F.mse_loss(x1 * 10, x2 * 10)\n",
    "\n",
    "def mse_loss(x1, x2):\n",
    "  return F.mse_loss(x1, x2)\n",
    "\n",
    "def cos_distance(x1, x2, dim = 1):\n",
    "  return -1 * torch.mean(F.cosine_similarity(x1, x2, dim=dim))\n",
    "\n",
    "triplet_call_cnt = 0\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, distance_func, margin):\n",
    "  # max distance when using l1_distance is 2\n",
    "  # max distacne when using l2-distance is sqrt(2)\n",
    "#   print(\"anchor: \", anchor.min(), anchor.max())\n",
    "  ap_dist = distance_func(anchor, positive)\n",
    "  an_dist = distance_func(anchor, negative)\n",
    "\n",
    "  global triplet_call_cnt\n",
    "  triplet_call_cnt += 1\n",
    "  if triplet_call_cnt % 10 in [0,1] : #and anchor.shape[1] == 1000:\n",
    "#     print(\"a: \", end=\"\"); print_big_vector(anchor[0])\n",
    "#     print(\"p: \", end=\"\"); print_big_vector(positive[0])\n",
    "#     print(\"n: \", end=\"\"); print_big_vector(negative[0])\n",
    "#     print(\"ap_dist: {}, an_dist: {}\".format(ap_dist, an_dist))\n",
    "    print(\"func:{}, ap_dist: {}, an_dist: {}\".format(distance_func.__name__, ap_dist, an_dist))\n",
    "    \n",
    "  return torch.mean(F.relu(ap_dist - an_dist + margin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsFgfiN8EV7z"
   },
   "outputs": [],
   "source": [
    "def diversity_loss(input, target):\n",
    "#   return -1 * torch.mean(torch.pow(f_x_a-f_x_s,2))\n",
    "  if input.shape[0] != batch_size:\n",
    "    print(\"input shape: \", input.shape)\n",
    "    print(\"target shape: \", target.shape, \"\\n\\n\")\n",
    "  return torch.mean(F.cosine_similarity(\n",
    "    input.view([batch_size, -1]),\n",
    "    target.view([batch_size, -1]), \n",
    "  ))\n",
    "\n",
    "# def fool_loss(input, target):\n",
    "#     true_class = torch.argmax(target, dim=1).view(-1,1).long()\n",
    "#     return -1 * torch.log(torch.mean(1 - input.gather(1, true_class)))\n",
    "\n",
    "def fool_loss_old(input, target, trash):\n",
    "  print(\"fool_loss:\")\n",
    "  true_class = torch.argmax(target, dim=1).view(-1,1).long()\n",
    "  print(true_class)\n",
    "  print(\"input: \", input.shape)\n",
    "  a = input.gather(1, true_class)\n",
    "  print(a)\n",
    "  print(1 - a)\n",
    "  print(torch.mean(1 - a))\n",
    "  print(torch.log(torch.mean(1-a)))\n",
    "  print(\"\\n\\n\")\n",
    "  # this is wrong! first log should be taken, THEN mean.\n",
    "  return -1 * torch.log(torch.mean(1 - input.gather(1, true_class)))\n",
    "\n",
    "fool_loss_count = 0\n",
    "\n",
    "def fool_loss(model_output, target_labels):\n",
    "  target_labels = target_labels.view(-1, 1).long().cuda()\n",
    "  target_probabilities = model_output.gather(1, target_labels)\n",
    "  epsilon = 1e-10\n",
    "  # highest possible fool_loss is - log(1e-10) == 23\n",
    "  result = torch.mean(-1 * torch.log(target_probabilities + epsilon))\n",
    "  \n",
    "  global fool_loss_count\n",
    "  fool_loss_count += 1\n",
    "  if fool_loss_count % 20 == 0:\n",
    "    print(\"target probs {}, loss: {}: \".format(target_probabilities, result))\n",
    "  \n",
    "  return result\n",
    "\n",
    "def validation(gen_output, target):\n",
    "  perturbations, _, _, clean_images, _, _ = gen_output\n",
    "  perturbed_images = clean_images + perturbations\n",
    "  benign_preds = torch.argmax(arch(clean_images), 1)\n",
    "  adversary_preds = torch.argmax(arch(perturbed_images), 1)\n",
    "  return (benign_preds != adversary_preds).float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureLoss(nn.Module):\n",
    "    def __name__(self):\n",
    "      return \"feature_loss\"\n",
    "  \n",
    "    def __init__(self, dis, layers, layer_weights):\n",
    "        super().__init__()\n",
    "        \n",
    "        # define generator here \n",
    "        self.dis = dis\n",
    "        self.diversity_layers = layers\n",
    "        self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "        self.weights = layer_weights\n",
    "        self.metric_names = [\"fool_loss\"] #+ [f\"div_loss_{i}\" for i in range(len(layers))] #maybe Gram\n",
    "#         self.triplet_hooks = hook_outputs([arch.m.features[4]], detach=False)\n",
    "    \n",
    "    def make_features(self, x, clone=False):\n",
    "        y = self.dis(x)\n",
    "        return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "  \n",
    "    def forward(self, inp, target):\n",
    "      sigma_B, _, _, X_B, B_Y, z = inp\n",
    "\n",
    "      X_A = X_B + sigma_B\n",
    "#       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "      A_Y, A_feat = self.make_features(X_A)\n",
    "#       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "      chosen_labels = z.argmax(dim=1)\n",
    "      fooling_loss =  fool_loss(A_Y, chosen_labels)\n",
    "\n",
    "#       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "#       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "#       self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "#       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "      self.losses = [fooling_loss]\n",
    "      self.metrics = dict(zip(self.metric_names, [fooling_loss]))\n",
    "\n",
    "      return sum(self.losses)\n",
    "  \n",
    "    def add_perturbation_shuffled(self, inp, perturbation):\n",
    "#         j = torch.randperm(inp.shape[0])\n",
    "        j = derangement(inp.shape[0])\n",
    "        return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qd9gXUy_ovww"
   },
   "outputs": [],
   "source": [
    "feat_loss = FeatureLoss(arch, layers, layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfZKdYD2MSdi"
   },
   "outputs": [],
   "source": [
    "env.save_filename = 'resnet50_13'\n",
    "\n",
    "if Path(env.get_csv_path() + '.csv').exists(): raise FileExistsError(\"csv_path already exists\")\n",
    "if Path(env.get_models_path()).exists(): raise FileExistsError(\"models_path already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9J20CBLS8S9"
   },
   "outputs": [],
   "source": [
    "learn = None; gc.collect()\n",
    "csv_logger = partial(ImmediateCSVLogger, filename= env.temp_csv_path + '/' + env.save_filename)\n",
    "# learn = Learner(data, Gen(z_dim=10), loss_func = feat_loss, metrics=[validation], callback_fns=LossMetrics, opt_func = optim.SGD)\n",
    "# learn = Learner(data, Gen(z_dim=z_dim), loss_func = feat_loss, metrics=[validation], callback_fns=[LossMetrics, DiversityWeightsScheduler])\n",
    "learn = Learner(data, Gen(z_dim=z_dim), loss_func = feat_loss, metrics=[validation], callback_fns=[LossMetrics, csv_logger])\n",
    "# load_starting_point(learn, model_name, z_dim)\n",
    "# random_seed(42, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wk9E0AUm9rmn"
   },
   "outputs": [],
   "source": [
    "# learn.lr_find(1e-6, 1000)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wOZYzOHDEdB"
   },
   "outputs": [],
   "source": [
    "# !cp \"/content/gdrive/My Drive/DL/models/vgg16_12-last.pth\"  \"/content/\"\n",
    "# learn.load('/content/vgg16_12-last')\n",
    "\n",
    "# load_filename = 'resnet50_12/resnet50_12_69'\n",
    "# load_filename = 'vgg16_12-last'\n",
    "# load_filename = 'vgg16_27/vgg16_27_89'\n",
    "\n",
    "# learn.load('/root/Derakhshani/adversarial/models/' + load_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the selected settings are : \n",
      "\tmode: normal \n",
      "\tnetw-under-attack: resnet50 \n",
      "\tload filename: None \n",
      "\tsave filename: resnet50_13\n",
      "\n",
      "please MAKE SURE that the config is correct.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  load_filename\n",
    "except NameError:\n",
    "  load_filename = None\n",
    "\n",
    "print(\"the selected settings are : \")\n",
    "print(\"\\tmode: {} \\n\\tnetw-under-attack: {} \\n\\tload filename: {} \\n\\tsave filename: {}\\n\".format(\n",
    "  mode, model.__name__, load_filename , env.save_filename\n",
    "))\n",
    "print(\"please MAKE SURE that the config is correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 783
    },
    "colab_type": "code",
    "id": "LA1ffVbbEwQS",
    "outputId": "cb14c6fd-158b-4deb-c931-792b0fd7b3a2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='70', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/70 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='1125', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bening preds are : [903, 108, 215, 964, 375, 817, 403, 483]\n",
      "chose:  80\n",
      "bening preds are : [699, 690, 854, 931, 863, 48, 475, 409]\n",
      "chose:  111\n",
      "bening preds are : [118, 547, 128, 833, 724, 58, 50, 808]\n",
      "chose:  8\n",
      "bening preds are : [276, 823, 272, 829, 726, 715, 919, 547]\n",
      "chose:  68\n",
      "bening preds are : [888, 847, 982, 692, 722, 98, 170, 553]\n",
      "chose:  748\n",
      "bening preds are : [207, 528, 823, 433, 355, 841, 712, 505]\n",
      "chose:  645\n",
      "bening preds are : [1, 804, 528, 523, 976, 429, 754, 592]\n",
      "chose:  848\n",
      "bening preds are : [525, 368, 735, 565, 56, 529, 815, 801]\n",
      "chose:  606\n",
      "bening preds are : [382, 894, 253, 172, 59, 952, 763, 437]\n",
      "chose:  147\n",
      "bening preds are : [612, 232, 179, 585, 157, 245, 964, 818]\n",
      "chose:  692\n",
      "bening preds are : [292, 857, 730, 150, 437, 477, 806, 500]\n",
      "chose:  952\n",
      "bening preds are : [678, 2, 959, 277, 164, 980, 432, 670]\n",
      "chose:  371\n",
      "bening preds are : [129, 748, 303, 122, 87, 230, 767, 978]\n",
      "chose:  49\n",
      "bening preds are : [522, 881, 84, 369, 453, 976, 526, 258]\n",
      "chose:  763\n",
      "bening preds are : [628, 849, 947, 188, 876, 888, 402, 505]\n",
      "chose:  453\n",
      "bening preds are : [966, 429, 738, 511, 627, 906, 985, 152]\n",
      "chose:  346\n",
      "bening preds are : [21, 750, 83, 977, 868, 242, 549, 763]\n",
      "chose:  203\n",
      "bening preds are : [144, 308, 168, 510, 803, 156, 210, 890]\n",
      "chose:  361\n",
      "bening preds are : [19, 620, 242, 175, 281, 230, 14, 999]\n",
      "chose:  862\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-072040f6ea83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msaver_every_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveModelCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msaver_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver_every_epoch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# learn.fit_one_cycle(8, max_lr=5e-01) #mohammad's setting that got 77 validation start on resnet with diversity loss on AdaptiveAvgPool2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/fastprogress/fastprogress.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;34m\"Process and returns items from `DataLoader`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_batch\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmderakhshani/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmderakhshani/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if mode == \"sanity_check\":\n",
    "  print(\"\\n\\n\\nWARNING: you are training on a sanity_check dataset.\\n\\n\\n\\n\")\n",
    "\n",
    "saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=env.save_filename + \"-best\")\n",
    "saver_every_epoch = SaveModelCallback(learn, every='epoch', name=env.save_filename)\n",
    "\n",
    "learn.fit(70, lr=5e-03, wd = 0., callbacks=[saver_best, saver_every_epoch])\n",
    "\n",
    "# learn.fit_one_cycle(8, max_lr=5e-01) #mohammad's setting that got 77 validation start on resnet with diversity loss on AdaptiveAvgPool2d\n",
    "# learn.fit_one_cycle(5, max_lr=2e-2) #used for vgg-19-bn\n",
    "# learn.fit_one_cycle(5, max_lr=3e-3) # used for resnet50\n",
    "\n",
    "shutil.copyfile(env.temp_csv_path + '/' + env.save_filename + \".csv\", env.get_csv_path() + '.csv')\n",
    "shutil.copytree(env.data_path/\"models\", env.get_models_path())\n",
    "shutil.rmtree(env.data_path/\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MO2fZ-hSSUzJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# z1 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "# z2 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "z1 = torch.tensor([0.8, -0.5] * 5).cuda()\n",
    "z2 = torch.tensor([-1.] * 10).cuda()\n",
    "print(\"z1: \", z1)\n",
    "print(\"z2: \", z2)\n",
    "print(\"distance: \", torch.norm(z1-z2,p=2))\n",
    "model = learn.model.eval()\n",
    "\n",
    "z_s = interpolate(z1, z2, 0.1)\n",
    "print(len(z_s))\n",
    "\n",
    "for i,z in enumerate(z_s):\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n",
    "  #img.save('./pics/' + str(i) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DGuGN7B7V0Xt"
   },
   "outputs": [],
   "source": [
    "def generate_perturbations(learn, n_perturbations):\n",
    "  initial_training_mode = learn.model.training\n",
    "  \n",
    "  model = learn.model.eval()\n",
    "  input_img = (learn.data.valid_ds[0][0].data)[None].cuda()\n",
    "  perturbations = []\n",
    "  for i in range(n_perturbations):\n",
    "    perturbation = model(input_img)[0].squeeze()\n",
    "    perturbations.append(perturbation)\n",
    "    \n",
    "  learn.model.train(initial_training_mode)  \n",
    "  return perturbations\n",
    "\n",
    "def compute_prediction_histogram(learn, perturbation, verbose=False):\n",
    "  pred_hist = [0] * 1000\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 and verbose: print (\"at batch no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbation[None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      pred_hist[pred] += 1\n",
    "  return pred_hist\n",
    "\n",
    "\n",
    "def compute_mean_prediction_histogram(learn, perturbations):\n",
    "  pred_histogram = torch.tensor([0] * 1000).detach_()\n",
    "  for j, perturbation in enumerate(perturbations):\n",
    "    pred_histogram_j = torch.tensor(compute_prediction_histogram(learn, perturbation, True)).detach_()\n",
    "    pred_histogram += pred_histogram_j\n",
    "    print(\"finished creating histogram for the {}th perturbation\".format(j))\n",
    "  \n",
    "  pred_histogram = pred_histogram.float() / len(perturbations)\n",
    "  return pred_histogram.tolist()\n",
    "\n",
    "\n",
    "def diversity(learn, n_perturbations, percentage):\n",
    "  pred_histogram = compute_mean_prediction_histogram(\n",
    "      learn, generate_perturbations(learn, n_perturbations)\n",
    "  )\n",
    "  print(\"finished creating the prediction histogram\")\n",
    "  pred_histogram_sum = np.sum(pred_histogram)\n",
    "\n",
    "  indexed_pred_histogram = [(i, hist_element) for i,hist_element in  \n",
    "                            enumerate(pred_histogram)]\n",
    "\n",
    "  indexed_pred_histogram.sort(key=lambda x: x[1], reverse = True)\n",
    "\n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  top_classes = []\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = indexed_pred_histogram[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / pred_histogram_sum) * 100.\n",
    "    top_classes.append(hist_elem[0])\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, indexed_pred_histogram, top_classes\n",
    "\n",
    "# idea : have 200 noises (1 for each class), then start iterating the dataset, and for each image, randomly apply one noise and record the result\n",
    "def targeted_diversity(learn, percentage):\n",
    "  model = learn.model.eval()\n",
    "\n",
    "  one_hot_conditions = [torch.empty(z_dim).uniform_(0,1).cuda().detach() for _ in range(200)]\n",
    "#   for i in range(z_dim):\n",
    "#     one_hot_conditions[i][i] = 1.\n",
    "\n",
    "  perturbations = [model.forward_single_z(z) for z in one_hot_conditions]\n",
    "\n",
    "  hist = [0.] * z_dim\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 : print(\"at batch_no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbations[np.random.randint(0,len(perturbations))][None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      hist[pred] += 1\n",
    "\n",
    "  pred_histogram_sum = np.sum(hist)\n",
    "  indexed_pred_histogram = [(i, hist_element) for i,hist_element in  \n",
    "                            enumerate(hist)]\n",
    "\n",
    "  indexed_pred_histogram.sort(key=lambda x: x[1], reverse = True)\n",
    "\n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = indexed_pred_histogram[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / pred_histogram_sum) * 100.\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, indexed_pred_histogram\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch_no 0\n",
      "at batch_no 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(646,\n",
       " [(887, 12.00),\n",
       "  (640, 10.00),\n",
       "  (492, 9.00),\n",
       "  (868, 8.00),\n",
       "  (69, 7.00),\n",
       "  (735, 7.00),\n",
       "  (783, 6.00),\n",
       "  (857, 6.00),\n",
       "  (113, 5.00),\n",
       "  (593, 5.00),\n",
       "  (695, 5.00),\n",
       "  (800, 5.00),\n",
       "  (109, 4.00),\n",
       "  (182, 4.00),\n",
       "  (192, 4.00),\n",
       "  (222, 4.00),\n",
       "  (406, 4.00),\n",
       "  (463, 4.00),\n",
       "  (587, 4.00),\n",
       "  (611, 4.00),\n",
       "  (741, 4.00),\n",
       "  (778, 4.00),\n",
       "  (842, 4.00),\n",
       "  (864, 4.00),\n",
       "  (913, 4.00),\n",
       "  (30, 3.00),\n",
       "  (37, 3.00),\n",
       "  (49, 3.00),\n",
       "  (115, 3.00),\n",
       "  (119, 3.00),\n",
       "  (123, 3.00),\n",
       "  (125, 3.00),\n",
       "  (126, 3.00),\n",
       "  (179, 3.00),\n",
       "  (180, 3.00),\n",
       "  (217, 3.00),\n",
       "  (238, 3.00),\n",
       "  (310, 3.00),\n",
       "  (314, 3.00),\n",
       "  (363, 3.00),\n",
       "  (415, 3.00),\n",
       "  (440, 3.00),\n",
       "  (474, 3.00),\n",
       "  (488, 3.00),\n",
       "  (489, 3.00),\n",
       "  (497, 3.00),\n",
       "  (538, 3.00),\n",
       "  (558, 3.00),\n",
       "  (591, 3.00),\n",
       "  (643, 3.00),\n",
       "  (724, 3.00),\n",
       "  (762, 3.00),\n",
       "  (771, 3.00),\n",
       "  (787, 3.00),\n",
       "  (844, 3.00),\n",
       "  (858, 3.00),\n",
       "  (865, 3.00),\n",
       "  (870, 3.00),\n",
       "  (904, 3.00),\n",
       "  (4, 2.00),\n",
       "  (18, 2.00),\n",
       "  (25, 2.00),\n",
       "  (26, 2.00),\n",
       "  (32, 2.00),\n",
       "  (55, 2.00),\n",
       "  (57, 2.00),\n",
       "  (61, 2.00),\n",
       "  (62, 2.00),\n",
       "  (68, 2.00),\n",
       "  (77, 2.00),\n",
       "  (93, 2.00),\n",
       "  (108, 2.00),\n",
       "  (110, 2.00),\n",
       "  (114, 2.00),\n",
       "  (120, 2.00),\n",
       "  (121, 2.00),\n",
       "  (124, 2.00),\n",
       "  (128, 2.00),\n",
       "  (131, 2.00),\n",
       "  (134, 2.00),\n",
       "  (140, 2.00),\n",
       "  (151, 2.00),\n",
       "  (162, 2.00),\n",
       "  (187, 2.00),\n",
       "  (188, 2.00),\n",
       "  (189, 2.00),\n",
       "  (190, 2.00),\n",
       "  (198, 2.00),\n",
       "  (204, 2.00),\n",
       "  (206, 2.00),\n",
       "  (219, 2.00),\n",
       "  (231, 2.00),\n",
       "  (235, 2.00),\n",
       "  (243, 2.00),\n",
       "  (263, 2.00),\n",
       "  (264, 2.00),\n",
       "  (266, 2.00),\n",
       "  (280, 2.00),\n",
       "  (281, 2.00),\n",
       "  (287, 2.00),\n",
       "  (289, 2.00),\n",
       "  (291, 2.00),\n",
       "  (292, 2.00),\n",
       "  (317, 2.00),\n",
       "  (326, 2.00),\n",
       "  (327, 2.00),\n",
       "  (329, 2.00),\n",
       "  (334, 2.00),\n",
       "  (342, 2.00),\n",
       "  (345, 2.00),\n",
       "  (348, 2.00),\n",
       "  (350, 2.00),\n",
       "  (377, 2.00),\n",
       "  (395, 2.00),\n",
       "  (396, 2.00),\n",
       "  (397, 2.00),\n",
       "  (423, 2.00),\n",
       "  (447, 2.00),\n",
       "  (454, 2.00),\n",
       "  (455, 2.00),\n",
       "  (472, 2.00),\n",
       "  (476, 2.00),\n",
       "  (480, 2.00),\n",
       "  (496, 2.00),\n",
       "  (501, 2.00),\n",
       "  (506, 2.00),\n",
       "  (511, 2.00),\n",
       "  (513, 2.00),\n",
       "  (524, 2.00),\n",
       "  (536, 2.00),\n",
       "  (564, 2.00),\n",
       "  (572, 2.00),\n",
       "  (576, 2.00),\n",
       "  (586, 2.00),\n",
       "  (597, 2.00),\n",
       "  (599, 2.00),\n",
       "  (600, 2.00),\n",
       "  (619, 2.00),\n",
       "  (621, 2.00),\n",
       "  (624, 2.00),\n",
       "  (633, 2.00),\n",
       "  (639, 2.00),\n",
       "  (652, 2.00),\n",
       "  (654, 2.00),\n",
       "  (679, 2.00),\n",
       "  (698, 2.00),\n",
       "  (702, 2.00),\n",
       "  (707, 2.00),\n",
       "  (711, 2.00),\n",
       "  (725, 2.00),\n",
       "  (750, 2.00),\n",
       "  (761, 2.00),\n",
       "  (764, 2.00),\n",
       "  (766, 2.00),\n",
       "  (772, 2.00),\n",
       "  (777, 2.00),\n",
       "  (786, 2.00),\n",
       "  (801, 2.00),\n",
       "  (809, 2.00),\n",
       "  (818, 2.00),\n",
       "  (821, 2.00),\n",
       "  (822, 2.00),\n",
       "  (825, 2.00),\n",
       "  (826, 2.00),\n",
       "  (828, 2.00),\n",
       "  (829, 2.00),\n",
       "  (831, 2.00),\n",
       "  (843, 2.00),\n",
       "  (859, 2.00),\n",
       "  (866, 2.00),\n",
       "  (872, 2.00),\n",
       "  (888, 2.00),\n",
       "  (891, 2.00),\n",
       "  (894, 2.00),\n",
       "  (896, 2.00),\n",
       "  (907, 2.00),\n",
       "  (910, 2.00),\n",
       "  (911, 2.00),\n",
       "  (960, 2.00),\n",
       "  (971, 2.00),\n",
       "  (979, 2.00),\n",
       "  (982, 2.00),\n",
       "  (989, 2.00),\n",
       "  (996, 2.00),\n",
       "  (0, 1.00),\n",
       "  (1, 1.00),\n",
       "  (2, 1.00),\n",
       "  (5, 1.00),\n",
       "  (6, 1.00),\n",
       "  (7, 1.00),\n",
       "  (8, 1.00),\n",
       "  (9, 1.00),\n",
       "  (12, 1.00),\n",
       "  (13, 1.00),\n",
       "  (14, 1.00),\n",
       "  (15, 1.00),\n",
       "  (16, 1.00),\n",
       "  (17, 1.00),\n",
       "  (19, 1.00),\n",
       "  (20, 1.00),\n",
       "  (21, 1.00),\n",
       "  (22, 1.00),\n",
       "  (24, 1.00),\n",
       "  (27, 1.00),\n",
       "  (28, 1.00),\n",
       "  (31, 1.00),\n",
       "  (33, 1.00),\n",
       "  (35, 1.00),\n",
       "  (41, 1.00),\n",
       "  (42, 1.00),\n",
       "  (45, 1.00),\n",
       "  (46, 1.00),\n",
       "  (47, 1.00),\n",
       "  (48, 1.00),\n",
       "  (50, 1.00),\n",
       "  (51, 1.00),\n",
       "  (52, 1.00),\n",
       "  (53, 1.00),\n",
       "  (58, 1.00),\n",
       "  (59, 1.00),\n",
       "  (60, 1.00),\n",
       "  (63, 1.00),\n",
       "  (65, 1.00),\n",
       "  (70, 1.00),\n",
       "  (71, 1.00),\n",
       "  (72, 1.00),\n",
       "  (74, 1.00),\n",
       "  (75, 1.00),\n",
       "  (78, 1.00),\n",
       "  (79, 1.00),\n",
       "  (81, 1.00),\n",
       "  (83, 1.00),\n",
       "  (84, 1.00),\n",
       "  (86, 1.00),\n",
       "  (87, 1.00),\n",
       "  (88, 1.00),\n",
       "  (90, 1.00),\n",
       "  (91, 1.00),\n",
       "  (92, 1.00),\n",
       "  (94, 1.00),\n",
       "  (95, 1.00),\n",
       "  (96, 1.00),\n",
       "  (97, 1.00),\n",
       "  (98, 1.00),\n",
       "  (99, 1.00),\n",
       "  (100, 1.00),\n",
       "  (101, 1.00),\n",
       "  (102, 1.00),\n",
       "  (105, 1.00),\n",
       "  (106, 1.00),\n",
       "  (107, 1.00),\n",
       "  (111, 1.00),\n",
       "  (116, 1.00),\n",
       "  (117, 1.00),\n",
       "  (118, 1.00),\n",
       "  (122, 1.00),\n",
       "  (130, 1.00),\n",
       "  (133, 1.00),\n",
       "  (135, 1.00),\n",
       "  (136, 1.00),\n",
       "  (137, 1.00),\n",
       "  (139, 1.00),\n",
       "  (141, 1.00),\n",
       "  (142, 1.00),\n",
       "  (143, 1.00),\n",
       "  (144, 1.00),\n",
       "  (145, 1.00),\n",
       "  (146, 1.00),\n",
       "  (148, 1.00),\n",
       "  (150, 1.00),\n",
       "  (154, 1.00),\n",
       "  (155, 1.00),\n",
       "  (157, 1.00),\n",
       "  (158, 1.00),\n",
       "  (160, 1.00),\n",
       "  (161, 1.00),\n",
       "  (163, 1.00),\n",
       "  (164, 1.00),\n",
       "  (166, 1.00),\n",
       "  (168, 1.00),\n",
       "  (169, 1.00),\n",
       "  (170, 1.00),\n",
       "  (171, 1.00),\n",
       "  (175, 1.00),\n",
       "  (176, 1.00),\n",
       "  (177, 1.00),\n",
       "  (178, 1.00),\n",
       "  (183, 1.00),\n",
       "  (184, 1.00),\n",
       "  (186, 1.00),\n",
       "  (191, 1.00),\n",
       "  (193, 1.00),\n",
       "  (194, 1.00),\n",
       "  (195, 1.00),\n",
       "  (196, 1.00),\n",
       "  (197, 1.00),\n",
       "  (199, 1.00),\n",
       "  (200, 1.00),\n",
       "  (201, 1.00),\n",
       "  (202, 1.00),\n",
       "  (203, 1.00),\n",
       "  (205, 1.00),\n",
       "  (208, 1.00),\n",
       "  (209, 1.00),\n",
       "  (211, 1.00),\n",
       "  (213, 1.00),\n",
       "  (214, 1.00),\n",
       "  (216, 1.00),\n",
       "  (218, 1.00),\n",
       "  (221, 1.00),\n",
       "  (223, 1.00),\n",
       "  (226, 1.00),\n",
       "  (227, 1.00),\n",
       "  (228, 1.00),\n",
       "  (229, 1.00),\n",
       "  (230, 1.00),\n",
       "  (232, 1.00),\n",
       "  (234, 1.00),\n",
       "  (236, 1.00),\n",
       "  (241, 1.00),\n",
       "  (242, 1.00),\n",
       "  (246, 1.00),\n",
       "  (249, 1.00),\n",
       "  (252, 1.00),\n",
       "  (253, 1.00),\n",
       "  (256, 1.00),\n",
       "  (260, 1.00),\n",
       "  (270, 1.00),\n",
       "  (271, 1.00),\n",
       "  (273, 1.00),\n",
       "  (274, 1.00),\n",
       "  (275, 1.00),\n",
       "  (276, 1.00),\n",
       "  (279, 1.00),\n",
       "  (282, 1.00),\n",
       "  (284, 1.00),\n",
       "  (286, 1.00),\n",
       "  (290, 1.00),\n",
       "  (293, 1.00),\n",
       "  (294, 1.00),\n",
       "  (295, 1.00),\n",
       "  (296, 1.00),\n",
       "  (297, 1.00),\n",
       "  (298, 1.00),\n",
       "  (300, 1.00),\n",
       "  (301, 1.00),\n",
       "  (302, 1.00),\n",
       "  (303, 1.00),\n",
       "  (304, 1.00),\n",
       "  (305, 1.00),\n",
       "  (306, 1.00),\n",
       "  (307, 1.00),\n",
       "  (309, 1.00),\n",
       "  (312, 1.00),\n",
       "  (313, 1.00),\n",
       "  (315, 1.00),\n",
       "  (316, 1.00),\n",
       "  (318, 1.00),\n",
       "  (319, 1.00),\n",
       "  (320, 1.00),\n",
       "  (321, 1.00),\n",
       "  (322, 1.00),\n",
       "  (324, 1.00),\n",
       "  (325, 1.00),\n",
       "  (328, 1.00),\n",
       "  (330, 1.00),\n",
       "  (331, 1.00),\n",
       "  (332, 1.00),\n",
       "  (335, 1.00),\n",
       "  (336, 1.00),\n",
       "  (337, 1.00),\n",
       "  (339, 1.00),\n",
       "  (340, 1.00),\n",
       "  (341, 1.00),\n",
       "  (344, 1.00),\n",
       "  (347, 1.00),\n",
       "  (349, 1.00),\n",
       "  (352, 1.00),\n",
       "  (354, 1.00),\n",
       "  (355, 1.00),\n",
       "  (357, 1.00),\n",
       "  (358, 1.00),\n",
       "  (360, 1.00),\n",
       "  (361, 1.00),\n",
       "  (362, 1.00),\n",
       "  (364, 1.00),\n",
       "  (365, 1.00),\n",
       "  (366, 1.00),\n",
       "  (367, 1.00),\n",
       "  (369, 1.00),\n",
       "  (372, 1.00),\n",
       "  (374, 1.00),\n",
       "  (375, 1.00),\n",
       "  (376, 1.00),\n",
       "  (378, 1.00),\n",
       "  (379, 1.00),\n",
       "  (380, 1.00),\n",
       "  (381, 1.00),\n",
       "  (382, 1.00),\n",
       "  (384, 1.00),\n",
       "  (386, 1.00),\n",
       "  (387, 1.00),\n",
       "  (388, 1.00),\n",
       "  (389, 1.00),\n",
       "  (391, 1.00),\n",
       "  (392, 1.00),\n",
       "  (393, 1.00),\n",
       "  (394, 1.00),\n",
       "  (398, 1.00),\n",
       "  (399, 1.00),\n",
       "  (401, 1.00),\n",
       "  (402, 1.00),\n",
       "  (403, 1.00),\n",
       "  (405, 1.00),\n",
       "  (408, 1.00),\n",
       "  (409, 1.00),\n",
       "  (411, 1.00),\n",
       "  (413, 1.00),\n",
       "  (417, 1.00),\n",
       "  (418, 1.00),\n",
       "  (420, 1.00),\n",
       "  (422, 1.00),\n",
       "  (425, 1.00),\n",
       "  (428, 1.00),\n",
       "  (429, 1.00),\n",
       "  (432, 1.00),\n",
       "  (433, 1.00),\n",
       "  (434, 1.00),\n",
       "  (436, 1.00),\n",
       "  (438, 1.00),\n",
       "  (439, 1.00),\n",
       "  (442, 1.00),\n",
       "  (443, 1.00),\n",
       "  (448, 1.00),\n",
       "  (449, 1.00),\n",
       "  (450, 1.00),\n",
       "  (451, 1.00),\n",
       "  (457, 1.00),\n",
       "  (459, 1.00),\n",
       "  (461, 1.00),\n",
       "  (464, 1.00),\n",
       "  (468, 1.00),\n",
       "  (477, 1.00),\n",
       "  (483, 1.00),\n",
       "  (484, 1.00),\n",
       "  (486, 1.00),\n",
       "  (490, 1.00),\n",
       "  (491, 1.00),\n",
       "  (495, 1.00),\n",
       "  (498, 1.00),\n",
       "  (502, 1.00),\n",
       "  (503, 1.00),\n",
       "  (504, 1.00),\n",
       "  (505, 1.00),\n",
       "  (507, 1.00),\n",
       "  (508, 1.00),\n",
       "  (512, 1.00),\n",
       "  (514, 1.00),\n",
       "  (517, 1.00),\n",
       "  (518, 1.00),\n",
       "  (519, 1.00),\n",
       "  (520, 1.00),\n",
       "  (522, 1.00),\n",
       "  (523, 1.00),\n",
       "  (526, 1.00),\n",
       "  (527, 1.00),\n",
       "  (528, 1.00),\n",
       "  (530, 1.00),\n",
       "  (531, 1.00),\n",
       "  (533, 1.00),\n",
       "  (534, 1.00),\n",
       "  (535, 1.00),\n",
       "  (537, 1.00),\n",
       "  (539, 1.00),\n",
       "  (540, 1.00),\n",
       "  (541, 1.00),\n",
       "  (543, 1.00),\n",
       "  (544, 1.00),\n",
       "  (546, 1.00),\n",
       "  (547, 1.00),\n",
       "  (550, 1.00),\n",
       "  (551, 1.00),\n",
       "  (554, 1.00),\n",
       "  (555, 1.00),\n",
       "  (560, 1.00),\n",
       "  (561, 1.00),\n",
       "  (562, 1.00),\n",
       "  (565, 1.00),\n",
       "  (566, 1.00),\n",
       "  (567, 1.00),\n",
       "  (569, 1.00),\n",
       "  (570, 1.00),\n",
       "  (573, 1.00),\n",
       "  (575, 1.00),\n",
       "  (577, 1.00),\n",
       "  (579, 1.00),\n",
       "  (580, 1.00),\n",
       "  (581, 1.00),\n",
       "  (582, 1.00),\n",
       "  (584, 1.00),\n",
       "  (592, 1.00),\n",
       "  (595, 1.00),\n",
       "  (604, 1.00),\n",
       "  (605, 1.00),\n",
       "  (606, 1.00),\n",
       "  (607, 1.00),\n",
       "  (608, 1.00),\n",
       "  (609, 1.00),\n",
       "  (612, 1.00),\n",
       "  (614, 1.00),\n",
       "  (615, 1.00),\n",
       "  (616, 1.00),\n",
       "  (618, 1.00),\n",
       "  (620, 1.00),\n",
       "  (622, 1.00),\n",
       "  (625, 1.00),\n",
       "  (626, 1.00),\n",
       "  (628, 1.00),\n",
       "  (629, 1.00),\n",
       "  (634, 1.00),\n",
       "  (636, 1.00),\n",
       "  (637, 1.00),\n",
       "  (641, 1.00),\n",
       "  (642, 1.00),\n",
       "  (645, 1.00),\n",
       "  (646, 1.00),\n",
       "  (647, 1.00),\n",
       "  (656, 1.00),\n",
       "  (658, 1.00),\n",
       "  (661, 1.00),\n",
       "  (663, 1.00),\n",
       "  (665, 1.00),\n",
       "  (666, 1.00),\n",
       "  (668, 1.00),\n",
       "  (670, 1.00),\n",
       "  (671, 1.00),\n",
       "  (672, 1.00),\n",
       "  (673, 1.00),\n",
       "  (674, 1.00),\n",
       "  (677, 1.00),\n",
       "  (682, 1.00),\n",
       "  (683, 1.00),\n",
       "  (684, 1.00),\n",
       "  (685, 1.00),\n",
       "  (687, 1.00),\n",
       "  (688, 1.00),\n",
       "  (689, 1.00),\n",
       "  (690, 1.00),\n",
       "  (693, 1.00),\n",
       "  (694, 1.00),\n",
       "  (696, 1.00),\n",
       "  (699, 1.00),\n",
       "  (700, 1.00),\n",
       "  (701, 1.00),\n",
       "  (703, 1.00),\n",
       "  (708, 1.00),\n",
       "  (709, 1.00),\n",
       "  (712, 1.00),\n",
       "  (715, 1.00),\n",
       "  (716, 1.00),\n",
       "  (718, 1.00),\n",
       "  (719, 1.00),\n",
       "  (720, 1.00),\n",
       "  (721, 1.00),\n",
       "  (723, 1.00),\n",
       "  (729, 1.00),\n",
       "  (730, 1.00),\n",
       "  (732, 1.00),\n",
       "  (734, 1.00),\n",
       "  (736, 1.00),\n",
       "  (737, 1.00),\n",
       "  (738, 1.00),\n",
       "  (743, 1.00),\n",
       "  (745, 1.00),\n",
       "  (746, 1.00),\n",
       "  (751, 1.00),\n",
       "  (752, 1.00),\n",
       "  (753, 1.00),\n",
       "  (755, 1.00),\n",
       "  (757, 1.00),\n",
       "  (759, 1.00),\n",
       "  (763, 1.00),\n",
       "  (765, 1.00),\n",
       "  (768, 1.00),\n",
       "  (769, 1.00),\n",
       "  (770, 1.00),\n",
       "  (774, 1.00),\n",
       "  (775, 1.00),\n",
       "  (776, 1.00),\n",
       "  (780, 1.00),\n",
       "  (781, 1.00),\n",
       "  (784, 1.00),\n",
       "  (789, 1.00),\n",
       "  (791, 1.00),\n",
       "  (793, 1.00),\n",
       "  (794, 1.00),\n",
       "  (796, 1.00),\n",
       "  (799, 1.00),\n",
       "  (802, 1.00),\n",
       "  (803, 1.00),\n",
       "  (805, 1.00),\n",
       "  (806, 1.00),\n",
       "  (808, 1.00),\n",
       "  (810, 1.00),\n",
       "  (814, 1.00),\n",
       "  (815, 1.00),\n",
       "  (816, 1.00),\n",
       "  (817, 1.00),\n",
       "  (820, 1.00),\n",
       "  (823, 1.00),\n",
       "  (830, 1.00),\n",
       "  (832, 1.00),\n",
       "  (833, 1.00),\n",
       "  (834, 1.00),\n",
       "  (835, 1.00),\n",
       "  (836, 1.00),\n",
       "  (837, 1.00),\n",
       "  (845, 1.00),\n",
       "  (848, 1.00),\n",
       "  (849, 1.00),\n",
       "  (850, 1.00),\n",
       "  (852, 1.00),\n",
       "  (853, 1.00),\n",
       "  (854, 1.00),\n",
       "  (855, 1.00),\n",
       "  (860, 1.00),\n",
       "  (862, 1.00),\n",
       "  (867, 1.00),\n",
       "  (873, 1.00),\n",
       "  (874, 1.00),\n",
       "  (875, 1.00),\n",
       "  (877, 1.00),\n",
       "  (878, 1.00),\n",
       "  (879, 1.00),\n",
       "  (880, 1.00),\n",
       "  (881, 1.00),\n",
       "  (882, 1.00),\n",
       "  (883, 1.00),\n",
       "  (884, 1.00),\n",
       "  (885, 1.00),\n",
       "  (886, 1.00),\n",
       "  (890, 1.00),\n",
       "  (892, 1.00),\n",
       "  (893, 1.00),\n",
       "  (898, 1.00),\n",
       "  (900, 1.00),\n",
       "  (901, 1.00),\n",
       "  (902, 1.00),\n",
       "  (905, 1.00),\n",
       "  (906, 1.00),\n",
       "  (909, 1.00),\n",
       "  (915, 1.00),\n",
       "  (918, 1.00),\n",
       "  (919, 1.00),\n",
       "  (920, 1.00),\n",
       "  (921, 1.00),\n",
       "  (923, 1.00),\n",
       "  (924, 1.00),\n",
       "  (925, 1.00),\n",
       "  (926, 1.00),\n",
       "  (932, 1.00),\n",
       "  (933, 1.00),\n",
       "  (934, 1.00),\n",
       "  (937, 1.00),\n",
       "  (939, 1.00),\n",
       "  (940, 1.00),\n",
       "  (941, 1.00),\n",
       "  (942, 1.00),\n",
       "  (944, 1.00),\n",
       "  (946, 1.00),\n",
       "  (948, 1.00),\n",
       "  (949, 1.00),\n",
       "  (952, 1.00),\n",
       "  (953, 1.00),\n",
       "  (955, 1.00),\n",
       "  (956, 1.00),\n",
       "  (957, 1.00),\n",
       "  (959, 1.00),\n",
       "  (962, 1.00),\n",
       "  (963, 1.00),\n",
       "  (968, 1.00),\n",
       "  (973, 1.00),\n",
       "  (976, 1.00),\n",
       "  (978, 1.00),\n",
       "  (980, 1.00),\n",
       "  (981, 1.00),\n",
       "  (983, 1.00),\n",
       "  (984, 1.00),\n",
       "  (985, 1.00),\n",
       "  (987, 1.00),\n",
       "  (988, 1.00),\n",
       "  (990, 1.00),\n",
       "  (991, 1.00),\n",
       "  (992, 1.00),\n",
       "  (997, 1.00),\n",
       "  (3, 0.00),\n",
       "  (10, 0.00),\n",
       "  (11, 0.00),\n",
       "  (23, 0.00),\n",
       "  (29, 0.00),\n",
       "  (34, 0.00),\n",
       "  (36, 0.00),\n",
       "  (38, 0.00),\n",
       "  (39, 0.00),\n",
       "  (40, 0.00),\n",
       "  (43, 0.00),\n",
       "  (44, 0.00),\n",
       "  (54, 0.00),\n",
       "  (56, 0.00),\n",
       "  (64, 0.00),\n",
       "  (66, 0.00),\n",
       "  (67, 0.00),\n",
       "  (73, 0.00),\n",
       "  (76, 0.00),\n",
       "  (80, 0.00),\n",
       "  (82, 0.00),\n",
       "  (85, 0.00),\n",
       "  (89, 0.00),\n",
       "  (103, 0.00),\n",
       "  (104, 0.00),\n",
       "  (112, 0.00),\n",
       "  (127, 0.00),\n",
       "  (129, 0.00),\n",
       "  (132, 0.00),\n",
       "  (138, 0.00),\n",
       "  (147, 0.00),\n",
       "  (149, 0.00),\n",
       "  (152, 0.00),\n",
       "  (153, 0.00),\n",
       "  (156, 0.00),\n",
       "  (159, 0.00),\n",
       "  (165, 0.00),\n",
       "  (167, 0.00),\n",
       "  (172, 0.00),\n",
       "  (173, 0.00),\n",
       "  (174, 0.00),\n",
       "  (181, 0.00),\n",
       "  (185, 0.00),\n",
       "  (207, 0.00),\n",
       "  (210, 0.00),\n",
       "  (212, 0.00),\n",
       "  (215, 0.00),\n",
       "  (220, 0.00),\n",
       "  (224, 0.00),\n",
       "  (225, 0.00),\n",
       "  (233, 0.00),\n",
       "  (237, 0.00),\n",
       "  (239, 0.00),\n",
       "  (240, 0.00),\n",
       "  (244, 0.00),\n",
       "  (245, 0.00),\n",
       "  (247, 0.00),\n",
       "  (248, 0.00),\n",
       "  (250, 0.00),\n",
       "  (251, 0.00),\n",
       "  (254, 0.00),\n",
       "  (255, 0.00),\n",
       "  (257, 0.00),\n",
       "  (258, 0.00),\n",
       "  (259, 0.00),\n",
       "  (261, 0.00),\n",
       "  (262, 0.00),\n",
       "  (265, 0.00),\n",
       "  (267, 0.00),\n",
       "  (268, 0.00),\n",
       "  (269, 0.00),\n",
       "  (272, 0.00),\n",
       "  (277, 0.00),\n",
       "  (278, 0.00),\n",
       "  (283, 0.00),\n",
       "  (285, 0.00),\n",
       "  (288, 0.00),\n",
       "  (299, 0.00),\n",
       "  (308, 0.00),\n",
       "  (311, 0.00),\n",
       "  (323, 0.00),\n",
       "  (333, 0.00),\n",
       "  (338, 0.00),\n",
       "  (343, 0.00),\n",
       "  (346, 0.00),\n",
       "  (351, 0.00),\n",
       "  (353, 0.00),\n",
       "  (356, 0.00),\n",
       "  (359, 0.00),\n",
       "  (368, 0.00),\n",
       "  (370, 0.00),\n",
       "  (371, 0.00),\n",
       "  (373, 0.00),\n",
       "  (383, 0.00),\n",
       "  (385, 0.00),\n",
       "  (390, 0.00),\n",
       "  (400, 0.00),\n",
       "  (404, 0.00),\n",
       "  (407, 0.00),\n",
       "  (410, 0.00),\n",
       "  (412, 0.00),\n",
       "  (414, 0.00),\n",
       "  (416, 0.00),\n",
       "  (419, 0.00),\n",
       "  (421, 0.00),\n",
       "  (424, 0.00),\n",
       "  (426, 0.00),\n",
       "  (427, 0.00),\n",
       "  (430, 0.00),\n",
       "  (431, 0.00),\n",
       "  (435, 0.00),\n",
       "  (437, 0.00),\n",
       "  (441, 0.00),\n",
       "  (444, 0.00),\n",
       "  (445, 0.00),\n",
       "  (446, 0.00),\n",
       "  (452, 0.00),\n",
       "  (453, 0.00),\n",
       "  (456, 0.00),\n",
       "  (458, 0.00),\n",
       "  (460, 0.00),\n",
       "  (462, 0.00),\n",
       "  (465, 0.00),\n",
       "  (466, 0.00),\n",
       "  (467, 0.00),\n",
       "  (469, 0.00),\n",
       "  (470, 0.00),\n",
       "  (471, 0.00),\n",
       "  (473, 0.00),\n",
       "  (475, 0.00),\n",
       "  (478, 0.00),\n",
       "  (479, 0.00),\n",
       "  (481, 0.00),\n",
       "  (482, 0.00),\n",
       "  (485, 0.00),\n",
       "  (487, 0.00),\n",
       "  (493, 0.00),\n",
       "  (494, 0.00),\n",
       "  (499, 0.00),\n",
       "  (500, 0.00),\n",
       "  (509, 0.00),\n",
       "  (510, 0.00),\n",
       "  (515, 0.00),\n",
       "  (516, 0.00),\n",
       "  (521, 0.00),\n",
       "  (525, 0.00),\n",
       "  (529, 0.00),\n",
       "  (532, 0.00),\n",
       "  (542, 0.00),\n",
       "  (545, 0.00),\n",
       "  (548, 0.00),\n",
       "  (549, 0.00),\n",
       "  (552, 0.00),\n",
       "  (553, 0.00),\n",
       "  (556, 0.00),\n",
       "  (557, 0.00),\n",
       "  (559, 0.00),\n",
       "  (563, 0.00),\n",
       "  (568, 0.00),\n",
       "  (571, 0.00),\n",
       "  (574, 0.00),\n",
       "  (578, 0.00),\n",
       "  (583, 0.00),\n",
       "  (585, 0.00),\n",
       "  (588, 0.00),\n",
       "  (589, 0.00),\n",
       "  (590, 0.00),\n",
       "  (594, 0.00),\n",
       "  (596, 0.00),\n",
       "  (598, 0.00),\n",
       "  (601, 0.00),\n",
       "  (602, 0.00),\n",
       "  (603, 0.00),\n",
       "  (610, 0.00),\n",
       "  (613, 0.00),\n",
       "  (617, 0.00),\n",
       "  (623, 0.00),\n",
       "  (627, 0.00),\n",
       "  (630, 0.00),\n",
       "  (631, 0.00),\n",
       "  (632, 0.00),\n",
       "  (635, 0.00),\n",
       "  (638, 0.00),\n",
       "  (644, 0.00),\n",
       "  (648, 0.00),\n",
       "  (649, 0.00),\n",
       "  (650, 0.00),\n",
       "  (651, 0.00),\n",
       "  (653, 0.00),\n",
       "  (655, 0.00),\n",
       "  (657, 0.00),\n",
       "  (659, 0.00),\n",
       "  (660, 0.00),\n",
       "  (662, 0.00),\n",
       "  (664, 0.00),\n",
       "  (667, 0.00),\n",
       "  (669, 0.00),\n",
       "  (675, 0.00),\n",
       "  (676, 0.00),\n",
       "  (678, 0.00),\n",
       "  (680, 0.00),\n",
       "  (681, 0.00),\n",
       "  (686, 0.00),\n",
       "  (691, 0.00),\n",
       "  (692, 0.00),\n",
       "  (697, 0.00),\n",
       "  (704, 0.00),\n",
       "  (705, 0.00),\n",
       "  (706, 0.00),\n",
       "  (710, 0.00),\n",
       "  (713, 0.00),\n",
       "  (714, 0.00),\n",
       "  (717, 0.00),\n",
       "  (722, 0.00),\n",
       "  (726, 0.00),\n",
       "  (727, 0.00),\n",
       "  (728, 0.00),\n",
       "  (731, 0.00),\n",
       "  (733, 0.00),\n",
       "  (739, 0.00),\n",
       "  (740, 0.00),\n",
       "  (742, 0.00),\n",
       "  (744, 0.00),\n",
       "  (747, 0.00),\n",
       "  (748, 0.00),\n",
       "  (749, 0.00),\n",
       "  (754, 0.00),\n",
       "  (756, 0.00),\n",
       "  (758, 0.00),\n",
       "  (760, 0.00),\n",
       "  (767, 0.00),\n",
       "  (773, 0.00),\n",
       "  (779, 0.00),\n",
       "  (782, 0.00),\n",
       "  (785, 0.00),\n",
       "  (788, 0.00),\n",
       "  (790, 0.00),\n",
       "  (792, 0.00),\n",
       "  (795, 0.00),\n",
       "  (797, 0.00),\n",
       "  (798, 0.00),\n",
       "  (804, 0.00),\n",
       "  (807, 0.00),\n",
       "  (811, 0.00),\n",
       "  (812, 0.00),\n",
       "  (813, 0.00),\n",
       "  (819, 0.00),\n",
       "  (824, 0.00),\n",
       "  (827, 0.00),\n",
       "  (838, 0.00),\n",
       "  (839, 0.00),\n",
       "  (840, 0.00),\n",
       "  (841, 0.00),\n",
       "  (846, 0.00),\n",
       "  (847, 0.00),\n",
       "  (851, 0.00),\n",
       "  (856, 0.00),\n",
       "  (861, 0.00),\n",
       "  (863, 0.00),\n",
       "  (869, 0.00),\n",
       "  (871, 0.00),\n",
       "  (876, 0.00),\n",
       "  (889, 0.00),\n",
       "  (895, 0.00),\n",
       "  (897, 0.00),\n",
       "  (899, 0.00),\n",
       "  (903, 0.00),\n",
       "  (908, 0.00),\n",
       "  (912, 0.00),\n",
       "  (914, 0.00),\n",
       "  (916, 0.00),\n",
       "  (917, 0.00),\n",
       "  (922, 0.00),\n",
       "  (927, 0.00),\n",
       "  (928, 0.00),\n",
       "  (929, 0.00),\n",
       "  (930, 0.00),\n",
       "  (931, 0.00),\n",
       "  (935, 0.00),\n",
       "  (936, 0.00),\n",
       "  (938, 0.00),\n",
       "  (943, 0.00),\n",
       "  (945, 0.00),\n",
       "  (947, 0.00),\n",
       "  (950, 0.00),\n",
       "  (951, 0.00),\n",
       "  (954, 0.00),\n",
       "  (958, 0.00),\n",
       "  (961, 0.00),\n",
       "  (964, 0.00),\n",
       "  (965, 0.00),\n",
       "  (966, 0.00),\n",
       "  (967, 0.00),\n",
       "  (969, 0.00),\n",
       "  (970, 0.00),\n",
       "  (972, 0.00),\n",
       "  (974, 0.00),\n",
       "  (975, 0.00),\n",
       "  (977, 0.00),\n",
       "  (986, 0.00),\n",
       "  (993, 0.00),\n",
       "  (994, 0.00),\n",
       "  (995, 0.00),\n",
       "  (998, 0.00),\n",
       "  (999, 0.00)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "n, hist = targeted_diversity(learn, 95)\n",
    "n, hist\n",
    "# n, hist, tk = diversity(learn, 10, 95)\n",
    "# n, hist, tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff1d62327b8>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_hist = sorted(hist, key=lambda x: x[0], reverse = False)\n",
    "values = [elem[1] for elem in sorted_hist]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.2046)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4409)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_from_uniform(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch_no 0\n",
      "at batch_no 100\n",
      "at batch_no 200\n",
      "at batch_no 300\n",
      "at batch_no 400\n",
      "at batch_no 500\n",
      "at batch_no 600\n",
      "at batch_no 700\n",
      "at batch_no 800\n",
      "at batch_no 900\n",
      "at batch_no 1000\n",
      "at batch_no 1100\n",
      "at batch_no 1200\n",
      "at batch_no 1300\n",
      "at batch_no 1400\n",
      "at batch_no 1500\n",
      "at batch_no 1600\n",
      "at batch_no 1700\n",
      "at batch_no 1800\n",
      "at batch_no 1900\n",
      "at batch_no 2000\n",
      "at batch_no 2100\n",
      "at batch_no 2200\n",
      "at batch_no 2300\n",
      "at batch_no 2400\n",
      "at batch_no 2500\n",
      "at batch_no 2600\n",
      "at batch_no 2700\n",
      "at batch_no 2800\n",
      "at batch_no 2900\n",
      "at batch_no 3000\n",
      "at batch_no 3100\n",
      "at batch_no 3200\n",
      "at batch_no 3300\n",
      "at batch_no 3400\n",
      "at batch_no 3500\n",
      "at batch_no 3600\n",
      "at batch_no 3700\n",
      "at batch_no 3800\n",
      "at batch_no 3900\n",
      "at batch_no 4000\n",
      "at batch_no 4100\n",
      "at batch_no 4200\n",
      "at batch_no 4300\n",
      "at batch_no 4400\n",
      "at batch_no 4500\n",
      "at batch_no 4600\n",
      "at batch_no 4700\n",
      "at batch_no 4800\n",
      "at batch_no 4900\n",
      "at batch_no 5000\n",
      "at batch_no 5100\n",
      "at batch_no 5200\n",
      "at batch_no 5300\n",
      "at batch_no 5400\n",
      "at batch_no 5500\n",
      "at batch_no 5600\n",
      "at batch_no 5700\n",
      "at batch_no 5800\n",
      "at batch_no 5900\n",
      "at batch_no 6000\n",
      "at batch_no 6100\n",
      "at batch_no 6200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(860,\n",
       " [(887, 489.00),\n",
       "  (640, 371.00),\n",
       "  (741, 307.00),\n",
       "  (857, 303.00),\n",
       "  (611, 298.00),\n",
       "  (492, 227.00),\n",
       "  (69, 194.00),\n",
       "  (695, 191.00),\n",
       "  (735, 181.00),\n",
       "  (406, 171.00),\n",
       "  (109, 168.00),\n",
       "  (800, 165.00),\n",
       "  (750, 159.00),\n",
       "  (497, 154.00),\n",
       "  (891, 154.00),\n",
       "  (771, 153.00),\n",
       "  (464, 147.00),\n",
       "  (126, 145.00),\n",
       "  (868, 142.00),\n",
       "  (643, 139.00),\n",
       "  (116, 133.00),\n",
       "  (962, 131.00),\n",
       "  (199, 128.00),\n",
       "  (455, 126.00),\n",
       "  (646, 125.00),\n",
       "  (794, 124.00),\n",
       "  (973, 124.00),\n",
       "  (753, 123.00),\n",
       "  (192, 121.00),\n",
       "  (783, 119.00),\n",
       "  (593, 118.00),\n",
       "  (652, 117.00),\n",
       "  (124, 114.00),\n",
       "  (858, 114.00),\n",
       "  (784, 113.00),\n",
       "  (913, 113.00),\n",
       "  (189, 112.00),\n",
       "  (884, 112.00),\n",
       "  (599, 110.00),\n",
       "  (509, 108.00),\n",
       "  (115, 106.00),\n",
       "  (911, 106.00),\n",
       "  (677, 105.00),\n",
       "  (679, 105.00),\n",
       "  (721, 105.00),\n",
       "  (828, 105.00),\n",
       "  (626, 104.00),\n",
       "  (663, 104.00),\n",
       "  (878, 104.00),\n",
       "  (454, 103.00),\n",
       "  (586, 103.00),\n",
       "  (786, 101.00),\n",
       "  (506, 100.00),\n",
       "  (162, 99.00),\n",
       "  (182, 99.00),\n",
       "  (896, 98.00),\n",
       "  (33, 97.00),\n",
       "  (237, 97.00),\n",
       "  (621, 96.00),\n",
       "  (743, 96.00),\n",
       "  (864, 96.00),\n",
       "  (37, 95.00),\n",
       "  (772, 95.00),\n",
       "  (342, 94.00),\n",
       "  (505, 94.00),\n",
       "  (489, 92.00),\n",
       "  (440, 91.00),\n",
       "  (577, 91.00),\n",
       "  (562, 90.00),\n",
       "  (894, 90.00),\n",
       "  (36, 89.00),\n",
       "  (119, 89.00),\n",
       "  (272, 89.00),\n",
       "  (457, 89.00),\n",
       "  (834, 89.00),\n",
       "  (219, 88.00),\n",
       "  (6, 87.00),\n",
       "  (58, 87.00),\n",
       "  (125, 87.00),\n",
       "  (564, 87.00),\n",
       "  (668, 87.00),\n",
       "  (77, 86.00),\n",
       "  (327, 86.00),\n",
       "  (113, 85.00),\n",
       "  (401, 85.00),\n",
       "  (549, 85.00),\n",
       "  (264, 84.00),\n",
       "  (317, 84.00),\n",
       "  (597, 84.00),\n",
       "  (123, 83.00),\n",
       "  (203, 83.00),\n",
       "  (222, 83.00),\n",
       "  (483, 83.00),\n",
       "  (523, 83.00),\n",
       "  (524, 83.00),\n",
       "  (538, 83.00),\n",
       "  (436, 81.00),\n",
       "  (654, 81.00),\n",
       "  (51, 80.00),\n",
       "  (60, 80.00),\n",
       "  (208, 80.00),\n",
       "  (281, 80.00),\n",
       "  (516, 80.00),\n",
       "  (561, 80.00),\n",
       "  (616, 80.00),\n",
       "  (830, 80.00),\n",
       "  (979, 80.00),\n",
       "  (348, 79.00),\n",
       "  (507, 79.00),\n",
       "  (581, 79.00),\n",
       "  (698, 79.00),\n",
       "  (787, 79.00),\n",
       "  (893, 79.00),\n",
       "  (791, 78.00),\n",
       "  (844, 78.00),\n",
       "  (904, 78.00),\n",
       "  (94, 77.00),\n",
       "  (377, 77.00),\n",
       "  (579, 77.00),\n",
       "  (865, 77.00),\n",
       "  (314, 76.00),\n",
       "  (395, 76.00),\n",
       "  (468, 76.00),\n",
       "  (488, 76.00),\n",
       "  (754, 76.00),\n",
       "  (762, 76.00),\n",
       "  (777, 76.00),\n",
       "  (988, 75.00),\n",
       "  (180, 74.00),\n",
       "  (61, 73.00),\n",
       "  (82, 73.00),\n",
       "  (539, 73.00),\n",
       "  (824, 73.00),\n",
       "  (963, 73.00),\n",
       "  (39, 72.00),\n",
       "  (242, 72.00),\n",
       "  (411, 72.00),\n",
       "  (423, 72.00),\n",
       "  (724, 72.00),\n",
       "  (805, 72.00),\n",
       "  (197, 71.00),\n",
       "  (879, 71.00),\n",
       "  (101, 70.00),\n",
       "  (107, 70.00),\n",
       "  (217, 70.00),\n",
       "  (451, 70.00),\n",
       "  (892, 70.00),\n",
       "  (76, 69.00),\n",
       "  (191, 69.00),\n",
       "  (601, 69.00),\n",
       "  (671, 69.00),\n",
       "  (151, 68.00),\n",
       "  (184, 68.00),\n",
       "  (201, 68.00),\n",
       "  (458, 68.00),\n",
       "  (591, 68.00),\n",
       "  (737, 68.00),\n",
       "  (855, 68.00),\n",
       "  (304, 67.00),\n",
       "  (380, 67.00),\n",
       "  (474, 67.00),\n",
       "  (658, 67.00),\n",
       "  (850, 67.00),\n",
       "  (79, 66.00),\n",
       "  (164, 66.00),\n",
       "  (188, 66.00),\n",
       "  (512, 66.00),\n",
       "  (832, 66.00),\n",
       "  (880, 66.00),\n",
       "  (23, 65.00),\n",
       "  (227, 65.00),\n",
       "  (311, 65.00),\n",
       "  (331, 65.00),\n",
       "  (420, 65.00),\n",
       "  (425, 65.00),\n",
       "  (471, 65.00),\n",
       "  (815, 65.00),\n",
       "  (121, 64.00),\n",
       "  (270, 64.00),\n",
       "  (329, 64.00),\n",
       "  (645, 64.00),\n",
       "  (703, 64.00),\n",
       "  (849, 64.00),\n",
       "  (47, 63.00),\n",
       "  (533, 63.00),\n",
       "  (565, 63.00),\n",
       "  (763, 63.00),\n",
       "  (22, 62.00),\n",
       "  (541, 62.00),\n",
       "  (642, 62.00),\n",
       "  (730, 62.00),\n",
       "  (748, 62.00),\n",
       "  (822, 62.00),\n",
       "  (839, 62.00),\n",
       "  (57, 61.00),\n",
       "  (70, 61.00),\n",
       "  (234, 61.00),\n",
       "  (241, 61.00),\n",
       "  (382, 61.00),\n",
       "  (444, 61.00),\n",
       "  (491, 61.00),\n",
       "  (594, 61.00),\n",
       "  (725, 61.00),\n",
       "  (826, 61.00),\n",
       "  (992, 61.00),\n",
       "  (49, 60.00),\n",
       "  (74, 60.00),\n",
       "  (85, 60.00),\n",
       "  (110, 60.00),\n",
       "  (228, 60.00),\n",
       "  (293, 60.00),\n",
       "  (463, 60.00),\n",
       "  (543, 60.00),\n",
       "  (618, 60.00),\n",
       "  (808, 60.00),\n",
       "  (854, 60.00),\n",
       "  (50, 59.00),\n",
       "  (218, 59.00),\n",
       "  (307, 59.00),\n",
       "  (363, 59.00),\n",
       "  (396, 59.00),\n",
       "  (490, 59.00),\n",
       "  (532, 59.00),\n",
       "  (556, 59.00),\n",
       "  (700, 59.00),\n",
       "  (776, 59.00),\n",
       "  (778, 59.00),\n",
       "  (793, 59.00),\n",
       "  (956, 59.00),\n",
       "  (983, 59.00),\n",
       "  (65, 58.00),\n",
       "  (195, 58.00),\n",
       "  (301, 58.00),\n",
       "  (367, 58.00),\n",
       "  (496, 58.00),\n",
       "  (569, 58.00),\n",
       "  (576, 58.00),\n",
       "  (582, 58.00),\n",
       "  (707, 58.00),\n",
       "  (770, 58.00),\n",
       "  (926, 58.00),\n",
       "  (25, 57.00),\n",
       "  (48, 57.00),\n",
       "  (135, 57.00),\n",
       "  (266, 57.00),\n",
       "  (298, 57.00),\n",
       "  (328, 57.00),\n",
       "  (461, 57.00),\n",
       "  (514, 57.00),\n",
       "  (608, 57.00),\n",
       "  (639, 57.00),\n",
       "  (840, 57.00),\n",
       "  (866, 57.00),\n",
       "  (21, 56.00),\n",
       "  (90, 56.00),\n",
       "  (138, 56.00),\n",
       "  (169, 56.00),\n",
       "  (181, 56.00),\n",
       "  (280, 56.00),\n",
       "  (292, 56.00),\n",
       "  (310, 56.00),\n",
       "  (312, 56.00),\n",
       "  (326, 56.00),\n",
       "  (362, 56.00),\n",
       "  (373, 56.00),\n",
       "  (441, 56.00),\n",
       "  (452, 56.00),\n",
       "  (480, 56.00),\n",
       "  (540, 56.00),\n",
       "  (666, 56.00),\n",
       "  (684, 56.00),\n",
       "  (953, 56.00),\n",
       "  (114, 55.00),\n",
       "  (265, 55.00),\n",
       "  (283, 55.00),\n",
       "  (300, 55.00),\n",
       "  (404, 55.00),\n",
       "  (424, 55.00),\n",
       "  (477, 55.00),\n",
       "  (563, 55.00),\n",
       "  (614, 55.00),\n",
       "  (683, 55.00),\n",
       "  (746, 55.00),\n",
       "  (765, 55.00),\n",
       "  (792, 55.00),\n",
       "  (806, 55.00),\n",
       "  (863, 55.00),\n",
       "  (24, 54.00),\n",
       "  (46, 54.00),\n",
       "  (62, 54.00),\n",
       "  (142, 54.00),\n",
       "  (212, 54.00),\n",
       "  (584, 54.00),\n",
       "  (716, 54.00),\n",
       "  (766, 54.00),\n",
       "  (809, 54.00),\n",
       "  (816, 54.00),\n",
       "  (829, 54.00),\n",
       "  (946, 54.00),\n",
       "  (55, 53.00),\n",
       "  (295, 53.00),\n",
       "  (379, 53.00),\n",
       "  (606, 53.00),\n",
       "  (607, 53.00),\n",
       "  (779, 53.00),\n",
       "  (870, 53.00),\n",
       "  (35, 52.00),\n",
       "  (71, 52.00),\n",
       "  (161, 52.00),\n",
       "  (214, 52.00),\n",
       "  (274, 52.00),\n",
       "  (275, 52.00),\n",
       "  (318, 52.00),\n",
       "  (336, 52.00),\n",
       "  (443, 52.00),\n",
       "  (476, 52.00),\n",
       "  (513, 52.00),\n",
       "  (609, 52.00),\n",
       "  (661, 52.00),\n",
       "  (667, 52.00),\n",
       "  (688, 52.00),\n",
       "  (835, 52.00),\n",
       "  (895, 52.00),\n",
       "  (985, 52.00),\n",
       "  (8, 51.00),\n",
       "  (66, 51.00),\n",
       "  (171, 51.00),\n",
       "  (225, 51.00),\n",
       "  (232, 51.00),\n",
       "  (251, 51.00),\n",
       "  (340, 51.00),\n",
       "  (410, 51.00),\n",
       "  (487, 51.00),\n",
       "  (612, 51.00),\n",
       "  (619, 51.00),\n",
       "  (843, 51.00),\n",
       "  (845, 51.00),\n",
       "  (932, 51.00),\n",
       "  (949, 51.00),\n",
       "  (987, 51.00),\n",
       "  (999, 51.00),\n",
       "  (134, 50.00),\n",
       "  (216, 50.00),\n",
       "  (238, 50.00),\n",
       "  (276, 50.00),\n",
       "  (277, 50.00),\n",
       "  (291, 50.00),\n",
       "  (294, 50.00),\n",
       "  (350, 50.00),\n",
       "  (472, 50.00),\n",
       "  (535, 50.00),\n",
       "  (558, 50.00),\n",
       "  (641, 50.00),\n",
       "  (702, 50.00),\n",
       "  (775, 50.00),\n",
       "  (990, 50.00),\n",
       "  (4, 49.00),\n",
       "  (9, 49.00),\n",
       "  (26, 49.00),\n",
       "  (154, 49.00),\n",
       "  (157, 49.00),\n",
       "  (170, 49.00),\n",
       "  (172, 49.00),\n",
       "  (210, 49.00),\n",
       "  (231, 49.00),\n",
       "  (235, 49.00),\n",
       "  (253, 49.00),\n",
       "  (255, 49.00),\n",
       "  (313, 49.00),\n",
       "  (316, 49.00),\n",
       "  (321, 49.00),\n",
       "  (387, 49.00),\n",
       "  (397, 49.00),\n",
       "  (447, 49.00),\n",
       "  (448, 49.00),\n",
       "  (580, 49.00),\n",
       "  (670, 49.00),\n",
       "  (685, 49.00),\n",
       "  (821, 49.00),\n",
       "  (927, 49.00),\n",
       "  (15, 48.00),\n",
       "  (30, 48.00),\n",
       "  (78, 48.00),\n",
       "  (120, 48.00),\n",
       "  (128, 48.00),\n",
       "  (139, 48.00),\n",
       "  (178, 48.00),\n",
       "  (185, 48.00),\n",
       "  (260, 48.00),\n",
       "  (347, 48.00),\n",
       "  (398, 48.00),\n",
       "  (537, 48.00),\n",
       "  (566, 48.00),\n",
       "  (587, 48.00),\n",
       "  (627, 48.00),\n",
       "  (690, 48.00),\n",
       "  (696, 48.00),\n",
       "  (761, 48.00),\n",
       "  (833, 48.00),\n",
       "  (881, 48.00),\n",
       "  (937, 48.00),\n",
       "  (991, 48.00),\n",
       "  (12, 47.00),\n",
       "  (18, 47.00),\n",
       "  (32, 47.00),\n",
       "  (44, 47.00),\n",
       "  (73, 47.00),\n",
       "  (81, 47.00),\n",
       "  (95, 47.00),\n",
       "  (102, 47.00),\n",
       "  (129, 47.00),\n",
       "  (136, 47.00),\n",
       "  (205, 47.00),\n",
       "  (236, 47.00),\n",
       "  (239, 47.00),\n",
       "  (243, 47.00),\n",
       "  (288, 47.00),\n",
       "  (319, 47.00),\n",
       "  (337, 47.00),\n",
       "  (343, 47.00),\n",
       "  (372, 47.00),\n",
       "  (391, 47.00),\n",
       "  (508, 47.00),\n",
       "  (526, 47.00),\n",
       "  (528, 47.00),\n",
       "  (546, 47.00),\n",
       "  (567, 47.00),\n",
       "  (572, 47.00),\n",
       "  (709, 47.00),\n",
       "  (780, 47.00),\n",
       "  (795, 47.00),\n",
       "  (814, 47.00),\n",
       "  (819, 47.00),\n",
       "  (872, 47.00),\n",
       "  (886, 47.00),\n",
       "  (0, 46.00),\n",
       "  (3, 46.00),\n",
       "  (28, 46.00),\n",
       "  (31, 46.00),\n",
       "  (42, 46.00),\n",
       "  (80, 46.00),\n",
       "  (89, 46.00),\n",
       "  (104, 46.00),\n",
       "  (112, 46.00),\n",
       "  (133, 46.00),\n",
       "  (143, 46.00),\n",
       "  (160, 46.00),\n",
       "  (206, 46.00),\n",
       "  (284, 46.00),\n",
       "  (305, 46.00),\n",
       "  (354, 46.00),\n",
       "  (460, 46.00),\n",
       "  (573, 46.00),\n",
       "  (625, 46.00),\n",
       "  (628, 46.00),\n",
       "  (692, 46.00),\n",
       "  (697, 46.00),\n",
       "  (711, 46.00),\n",
       "  (875, 46.00),\n",
       "  (971, 46.00),\n",
       "  (995, 46.00),\n",
       "  (13, 45.00),\n",
       "  (14, 45.00),\n",
       "  (17, 45.00),\n",
       "  (19, 45.00),\n",
       "  (84, 45.00),\n",
       "  (88, 45.00),\n",
       "  (127, 45.00),\n",
       "  (159, 45.00),\n",
       "  (177, 45.00),\n",
       "  (196, 45.00),\n",
       "  (259, 45.00),\n",
       "  (309, 45.00),\n",
       "  (323, 45.00),\n",
       "  (358, 45.00),\n",
       "  (364, 45.00),\n",
       "  (383, 45.00),\n",
       "  (390, 45.00),\n",
       "  (428, 45.00),\n",
       "  (473, 45.00),\n",
       "  (503, 45.00),\n",
       "  (522, 45.00),\n",
       "  (547, 45.00),\n",
       "  (583, 45.00),\n",
       "  (630, 45.00),\n",
       "  (637, 45.00),\n",
       "  (653, 45.00),\n",
       "  (701, 45.00),\n",
       "  (704, 45.00),\n",
       "  (738, 45.00),\n",
       "  (751, 45.00),\n",
       "  (774, 45.00),\n",
       "  (781, 45.00),\n",
       "  (790, 45.00),\n",
       "  (801, 45.00),\n",
       "  (812, 45.00),\n",
       "  (874, 45.00),\n",
       "  (950, 45.00),\n",
       "  (952, 45.00),\n",
       "  (955, 45.00),\n",
       "  (11, 44.00),\n",
       "  (53, 44.00),\n",
       "  (75, 44.00),\n",
       "  (118, 44.00),\n",
       "  (132, 44.00),\n",
       "  (140, 44.00),\n",
       "  (176, 44.00),\n",
       "  (190, 44.00),\n",
       "  (215, 44.00),\n",
       "  (287, 44.00),\n",
       "  (299, 44.00),\n",
       "  (357, 44.00),\n",
       "  (421, 44.00),\n",
       "  (449, 44.00),\n",
       "  (592, 44.00),\n",
       "  (603, 44.00),\n",
       "  (659, 44.00),\n",
       "  (687, 44.00),\n",
       "  (726, 44.00),\n",
       "  (820, 44.00),\n",
       "  (877, 44.00),\n",
       "  (883, 44.00),\n",
       "  (910, 44.00),\n",
       "  (916, 44.00),\n",
       "  (917, 44.00),\n",
       "  (944, 44.00),\n",
       "  (989, 44.00),\n",
       "  (96, 43.00),\n",
       "  (97, 43.00),\n",
       "  (100, 43.00),\n",
       "  (117, 43.00),\n",
       "  (141, 43.00),\n",
       "  (155, 43.00),\n",
       "  (194, 43.00),\n",
       "  (279, 43.00),\n",
       "  (308, 43.00),\n",
       "  (325, 43.00),\n",
       "  (333, 43.00),\n",
       "  (351, 43.00),\n",
       "  (389, 43.00),\n",
       "  (407, 43.00),\n",
       "  (486, 43.00),\n",
       "  (511, 43.00),\n",
       "  (517, 43.00),\n",
       "  (571, 43.00),\n",
       "  (613, 43.00),\n",
       "  (636, 43.00),\n",
       "  (655, 43.00),\n",
       "  (825, 43.00),\n",
       "  (902, 43.00),\n",
       "  (981, 43.00),\n",
       "  (984, 43.00),\n",
       "  (10, 42.00),\n",
       "  (20, 42.00),\n",
       "  (83, 42.00),\n",
       "  (99, 42.00),\n",
       "  (179, 42.00),\n",
       "  (244, 42.00),\n",
       "  (245, 42.00),\n",
       "  (247, 42.00),\n",
       "  (290, 42.00),\n",
       "  (339, 42.00),\n",
       "  (353, 42.00),\n",
       "  (360, 42.00),\n",
       "  (399, 42.00),\n",
       "  (502, 42.00),\n",
       "  (544, 42.00),\n",
       "  (560, 42.00),\n",
       "  (570, 42.00),\n",
       "  (600, 42.00),\n",
       "  (699, 42.00),\n",
       "  (710, 42.00),\n",
       "  (788, 42.00),\n",
       "  (882, 42.00),\n",
       "  (986, 42.00),\n",
       "  (993, 42.00),\n",
       "  (43, 41.00),\n",
       "  (93, 41.00),\n",
       "  (98, 41.00),\n",
       "  (131, 41.00),\n",
       "  (144, 41.00),\n",
       "  (168, 41.00),\n",
       "  (213, 41.00),\n",
       "  (230, 41.00),\n",
       "  (248, 41.00),\n",
       "  (334, 41.00),\n",
       "  (352, 41.00),\n",
       "  (375, 41.00),\n",
       "  (392, 41.00),\n",
       "  (433, 41.00),\n",
       "  (510, 41.00),\n",
       "  (575, 41.00),\n",
       "  (595, 41.00),\n",
       "  (604, 41.00),\n",
       "  (674, 41.00),\n",
       "  (757, 41.00),\n",
       "  (764, 41.00),\n",
       "  (769, 41.00),\n",
       "  (802, 41.00),\n",
       "  (823, 41.00),\n",
       "  (831, 41.00),\n",
       "  (837, 41.00),\n",
       "  (860, 41.00),\n",
       "  (888, 41.00),\n",
       "  (889, 41.00),\n",
       "  (890, 41.00),\n",
       "  (941, 41.00),\n",
       "  (67, 40.00),\n",
       "  (92, 40.00),\n",
       "  (108, 40.00),\n",
       "  (137, 40.00),\n",
       "  (156, 40.00),\n",
       "  (193, 40.00),\n",
       "  (198, 40.00),\n",
       "  (209, 40.00),\n",
       "  (254, 40.00),\n",
       "  (261, 40.00),\n",
       "  (269, 40.00),\n",
       "  (286, 40.00),\n",
       "  (289, 40.00),\n",
       "  (324, 40.00),\n",
       "  (338, 40.00),\n",
       "  (365, 40.00),\n",
       "  (388, 40.00),\n",
       "  (415, 40.00),\n",
       "  (416, 40.00),\n",
       "  (635, 40.00),\n",
       "  (734, 40.00),\n",
       "  (782, 40.00),\n",
       "  (796, 40.00),\n",
       "  (900, 40.00),\n",
       "  (903, 40.00),\n",
       "  (924, 40.00),\n",
       "  (939, 40.00),\n",
       "  (959, 40.00),\n",
       "  (7, 39.00),\n",
       "  (91, 39.00),\n",
       "  (211, 39.00),\n",
       "  (221, 39.00),\n",
       "  (285, 39.00),\n",
       "  (306, 39.00),\n",
       "  (322, 39.00),\n",
       "  (349, 39.00),\n",
       "  (366, 39.00),\n",
       "  (417, 39.00),\n",
       "  (484, 39.00),\n",
       "  (548, 39.00),\n",
       "  (693, 39.00),\n",
       "  (694, 39.00),\n",
       "  (739, 39.00),\n",
       "  (852, 39.00),\n",
       "  (56, 38.00),\n",
       "  (200, 38.00),\n",
       "  (252, 38.00),\n",
       "  (267, 38.00),\n",
       "  (296, 38.00),\n",
       "  (303, 38.00),\n",
       "  (370, 38.00),\n",
       "  (498, 38.00),\n",
       "  (518, 38.00),\n",
       "  (650, 38.00),\n",
       "  (736, 38.00),\n",
       "  (752, 38.00),\n",
       "  (847, 38.00),\n",
       "  (859, 38.00),\n",
       "  (861, 38.00),\n",
       "  (873, 38.00),\n",
       "  (915, 38.00),\n",
       "  (936, 38.00),\n",
       "  (957, 38.00),\n",
       "  (87, 37.00),\n",
       "  (148, 37.00),\n",
       "  (173, 37.00),\n",
       "  (223, 37.00),\n",
       "  (256, 37.00),\n",
       "  (262, 37.00),\n",
       "  (361, 37.00),\n",
       "  (374, 37.00),\n",
       "  (384, 37.00),\n",
       "  (386, 37.00),\n",
       "  (393, 37.00),\n",
       "  (649, 37.00),\n",
       "  (715, 37.00),\n",
       "  (804, 37.00),\n",
       "  (807, 37.00),\n",
       "  (867, 37.00),\n",
       "  (41, 36.00),\n",
       "  (72, 36.00),\n",
       "  (224, 36.00),\n",
       "  (302, 36.00),\n",
       "  (320, 36.00),\n",
       "  (368, 36.00),\n",
       "  (376, 36.00),\n",
       "  (378, 36.00),\n",
       "  (439, 36.00),\n",
       "  (453, 36.00),\n",
       "  (531, 36.00),\n",
       "  (559, 36.00),\n",
       "  (574, 36.00),\n",
       "  (731, 36.00),\n",
       "  (768, 36.00),\n",
       "  (827, 36.00),\n",
       "  (871, 36.00),\n",
       "  (918, 36.00),\n",
       "  (982, 36.00),\n",
       "  (5, 35.00),\n",
       "  (27, 35.00),\n",
       "  (68, 35.00),\n",
       "  (106, 35.00),\n",
       "  (130, 35.00),\n",
       "  (204, 35.00),\n",
       "  (273, 35.00),\n",
       "  (330, 35.00),\n",
       "  (346, 35.00),\n",
       "  (475, 35.00),\n",
       "  (644, 35.00),\n",
       "  (665, 35.00),\n",
       "  (723, 35.00),\n",
       "  (732, 35.00),\n",
       "  (919, 35.00),\n",
       "  (933, 35.00),\n",
       "  (943, 35.00),\n",
       "  (975, 35.00),\n",
       "  (976, 35.00),\n",
       "  (16, 34.00),\n",
       "  (86, 34.00),\n",
       "  (145, 34.00),\n",
       "  (355, 34.00),\n",
       "  (408, 34.00),\n",
       "  (422, 34.00),\n",
       "  (430, 34.00),\n",
       "  (432, 34.00),\n",
       "  (466, 34.00),\n",
       "  (851, 34.00),\n",
       "  (909, 34.00),\n",
       "  (921, 34.00),\n",
       "  (968, 34.00),\n",
       "  (40, 33.00),\n",
       "  (45, 33.00),\n",
       "  (105, 33.00),\n",
       "  (111, 33.00),\n",
       "  (186, 33.00),\n",
       "  (220, 33.00),\n",
       "  (249, 33.00),\n",
       "  (258, 33.00),\n",
       "  (341, 33.00),\n",
       "  (344, 33.00),\n",
       "  (402, 33.00),\n",
       "  (450, 33.00),\n",
       "  (545, 33.00),\n",
       "  (602, 33.00),\n",
       "  (610, 33.00),\n",
       "  (678, 33.00),\n",
       "  (717, 33.00),\n",
       "  (719, 33.00),\n",
       "  (727, 33.00),\n",
       "  (755, 33.00),\n",
       "  (862, 33.00),\n",
       "  (29, 32.00),\n",
       "  (207, 32.00),\n",
       "  (226, 32.00),\n",
       "  (250, 32.00),\n",
       "  (278, 32.00),\n",
       "  (413, 32.00),\n",
       "  (427, 32.00),\n",
       "  (437, 32.00),\n",
       "  (467, 32.00),\n",
       "  (470, 32.00),\n",
       "  (500, 32.00),\n",
       "  (527, 32.00),\n",
       "  (557, 32.00),\n",
       "  (705, 32.00),\n",
       "  (803, 32.00),\n",
       "  (920, 32.00),\n",
       "  (63, 31.00),\n",
       "  (147, 31.00),\n",
       "  (297, 31.00),\n",
       "  (403, 31.00),\n",
       "  (419, 31.00),\n",
       "  (426, 31.00),\n",
       "  (456, 31.00),\n",
       "  (459, 31.00),\n",
       "  (501, 31.00),\n",
       "  (555, 31.00),\n",
       "  (714, 31.00),\n",
       "  (749, 31.00),\n",
       "  (759, 31.00),\n",
       "  (760, 31.00),\n",
       "  (842, 31.00),\n",
       "  (174, 30.00),\n",
       "  (183, 30.00),\n",
       "  (202, 30.00),\n",
       "  (335, 30.00),\n",
       "  (356, 30.00),\n",
       "  (434, 30.00),\n",
       "  (445, 30.00),\n",
       "  (620, 30.00),\n",
       "  (682, 30.00),\n",
       "  (720, 30.00),\n",
       "  (767, 30.00),\n",
       "  (848, 30.00),\n",
       "  (912, 30.00),\n",
       "  (914, 30.00),\n",
       "  (922, 30.00),\n",
       "  (934, 30.00),\n",
       "  (938, 30.00),\n",
       "  (994, 30.00),\n",
       "  (1, 29.00),\n",
       "  (122, 29.00),\n",
       "  (268, 29.00),\n",
       "  (345, 29.00),\n",
       "  (412, 29.00),\n",
       "  (442, 29.00),\n",
       "  (521, 29.00),\n",
       "  (676, 29.00),\n",
       "  (789, 29.00),\n",
       "  (813, 29.00),\n",
       "  (817, 29.00),\n",
       "  (818, 29.00),\n",
       "  (929, 29.00),\n",
       "  (2, 28.00),\n",
       "  (59, 28.00),\n",
       "  (166, 28.00),\n",
       "  (175, 28.00),\n",
       "  (187, 28.00),\n",
       "  (263, 28.00),\n",
       "  (332, 28.00),\n",
       "  (381, 28.00),\n",
       "  (405, 28.00),\n",
       "  (409, 28.00),\n",
       "  (494, 28.00),\n",
       "  (495, 28.00),\n",
       "  (504, 28.00),\n",
       "  (520, 28.00),\n",
       "  (530, 28.00),\n",
       "  (552, 28.00),\n",
       "  (590, 28.00),\n",
       "  (624, 28.00),\n",
       "  (672, 28.00),\n",
       "  (758, 28.00),\n",
       "  (907, 28.00),\n",
       "  (928, 28.00),\n",
       "  (947, 28.00),\n",
       "  (958, 28.00),\n",
       "  (965, 28.00),\n",
       "  (52, 27.00),\n",
       "  (429, 27.00),\n",
       "  (553, 27.00),\n",
       "  (923, 27.00),\n",
       "  (931, 27.00),\n",
       "  (997, 27.00),\n",
       "  (38, 26.00),\n",
       "  (103, 26.00),\n",
       "  (152, 26.00),\n",
       "  (233, 26.00),\n",
       "  (400, 26.00),\n",
       "  (431, 26.00),\n",
       "  (462, 26.00),\n",
       "  (469, 26.00),\n",
       "  (536, 26.00),\n",
       "  (554, 26.00),\n",
       "  (629, 26.00),\n",
       "  (634, 26.00),\n",
       "  (846, 26.00),\n",
       "  (853, 26.00),\n",
       "  (146, 25.00),\n",
       "  (240, 25.00),\n",
       "  (542, 25.00),\n",
       "  (550, 25.00),\n",
       "  (722, 25.00),\n",
       "  (733, 25.00),\n",
       "  (756, 25.00),\n",
       "  (773, 25.00),\n",
       "  (798, 25.00),\n",
       "  (901, 25.00),\n",
       "  (945, 25.00),\n",
       "  (951, 25.00),\n",
       "  (964, 25.00),\n",
       "  (54, 24.00),\n",
       "  (149, 24.00),\n",
       "  (371, 24.00),\n",
       "  (479, 24.00),\n",
       "  (481, 24.00),\n",
       "  (482, 24.00),\n",
       "  (499, 24.00),\n",
       "  (605, 24.00),\n",
       "  (615, 24.00),\n",
       "  (898, 24.00),\n",
       "  (908, 24.00),\n",
       "  (998, 24.00),\n",
       "  (385, 23.00),\n",
       "  (394, 23.00),\n",
       "  (414, 23.00),\n",
       "  (515, 23.00),\n",
       "  (589, 23.00),\n",
       "  (708, 23.00),\n",
       "  (925, 23.00),\n",
       "  (970, 23.00),\n",
       "  (229, 22.00),\n",
       "  (271, 22.00),\n",
       "  (359, 22.00),\n",
       "  (485, 22.00),\n",
       "  (519, 22.00),\n",
       "  (568, 22.00),\n",
       "  (598, 22.00),\n",
       "  (633, 22.00),\n",
       "  (657, 22.00),\n",
       "  (660, 22.00),\n",
       "  (718, 22.00),\n",
       "  (948, 22.00),\n",
       "  (996, 22.00),\n",
       "  (588, 21.00),\n",
       "  (596, 21.00),\n",
       "  (647, 21.00),\n",
       "  (656, 21.00),\n",
       "  (838, 21.00),\n",
       "  (978, 21.00),\n",
       "  (150, 20.00),\n",
       "  (165, 20.00),\n",
       "  (246, 20.00),\n",
       "  (369, 20.00),\n",
       "  (493, 20.00),\n",
       "  (585, 20.00),\n",
       "  (686, 20.00),\n",
       "  (706, 20.00),\n",
       "  (745, 20.00),\n",
       "  (811, 20.00),\n",
       "  (930, 20.00),\n",
       "  (960, 20.00),\n",
       "  (966, 20.00),\n",
       "  (158, 19.00),\n",
       "  (418, 19.00),\n",
       "  (967, 19.00),\n",
       "  (980, 19.00),\n",
       "  (34, 18.00),\n",
       "  (282, 18.00),\n",
       "  (632, 18.00),\n",
       "  (669, 18.00),\n",
       "  (797, 18.00),\n",
       "  (954, 18.00),\n",
       "  (163, 17.00),\n",
       "  (529, 17.00),\n",
       "  (689, 17.00),\n",
       "  (712, 17.00),\n",
       "  (869, 17.00),\n",
       "  (906, 17.00),\n",
       "  (972, 17.00),\n",
       "  (977, 17.00),\n",
       "  (465, 16.00),\n",
       "  (551, 16.00),\n",
       "  (623, 16.00),\n",
       "  (675, 16.00),\n",
       "  (841, 16.00),\n",
       "  (856, 16.00),\n",
       "  (64, 15.00),\n",
       "  (153, 15.00),\n",
       "  (435, 15.00),\n",
       "  (478, 15.00),\n",
       "  (622, 15.00),\n",
       "  (638, 15.00),\n",
       "  (744, 15.00),\n",
       "  (897, 15.00),\n",
       "  (617, 14.00),\n",
       "  (905, 14.00),\n",
       "  (315, 13.00),\n",
       "  (525, 13.00),\n",
       "  (578, 13.00),\n",
       "  (713, 13.00),\n",
       "  (740, 13.00),\n",
       "  (747, 13.00),\n",
       "  (942, 13.00),\n",
       "  (680, 12.00),\n",
       "  (691, 12.00),\n",
       "  (729, 12.00),\n",
       "  (974, 12.00),\n",
       "  (167, 11.00),\n",
       "  (446, 11.00),\n",
       "  (534, 11.00),\n",
       "  (648, 11.00),\n",
       "  (664, 11.00),\n",
       "  (785, 10.00),\n",
       "  (631, 9.00),\n",
       "  (885, 9.00),\n",
       "  (799, 8.00),\n",
       "  (257, 7.00),\n",
       "  (438, 7.00),\n",
       "  (662, 7.00),\n",
       "  (876, 7.00),\n",
       "  (728, 6.00),\n",
       "  (742, 6.00),\n",
       "  (940, 6.00),\n",
       "  (651, 5.00),\n",
       "  (673, 5.00),\n",
       "  (810, 5.00),\n",
       "  (836, 5.00),\n",
       "  (961, 5.00),\n",
       "  (969, 5.00),\n",
       "  (681, 4.00),\n",
       "  (935, 4.00),\n",
       "  (899, 2.00)])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "n, hist = targeted_diversity(learn, 95)\n",
    "n, hist\n",
    "# n, hist, tk = diversity(learn, 10, 95)\n",
    "# n, hist, tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fCvFG0VMKts"
   },
   "outputs": [],
   "source": [
    "def make_triplet_samples(z, margin, r2, r3):\n",
    "  positive_sample = z + random_vector_volume(z.shape, 0, margin).cuda() \n",
    "  negative_sample = z + random_vector_volume(z.shape, r2, r3).cuda()\n",
    "  return positive_sample, negative_sample\n",
    "\n",
    "def random_vector_surface(shape, r = 1.):\n",
    "  mat = torch.randn(size=shape).cuda()\n",
    "  norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "  return (mat/norm) * r\n",
    "\n",
    "def random_vector_volume(shape, inner_r, outer_r):\n",
    "  fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "  fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "  fraction.unsqueeze_(-1)\n",
    "  return random_vector_surface(shape, 1) * fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PizmBkGqMKtu"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_frequent(x):\n",
    "  return Counter(x).most_common(1)[0]\n",
    "\n",
    "def preds_around(center, radius, n_preds, model, dummy_img):\n",
    "  z_s = random_vector_volume([n_preds, 10], radius, radius + 0.01) + center[None]\n",
    "  noises = model.forward_z(z_s)\n",
    "  perturbed_imgs = noises + dummy_img \n",
    "  return torch.argmax(arch(perturbed_imgs), 1)\n",
    "  \n",
    "def most_freq_pred_around(center, radius, n_preds, model, dummy_img):\n",
    "  preds = preds_around(center, radius, n_preds, model, dummy_img)\n",
    "  most_freq = most_frequent(preds.tolist())\n",
    "  return (class_index_to_label(most_freq[0]), most_freq[1]/n_preds)\n",
    "\n",
    "def investigate_neighborhood(z, step, model, dummy_img):\n",
    "  with torch.no_grad():\n",
    "    result = []\n",
    "    for radius in np.arange(0.1, 6., step):\n",
    "#       print(\"creating {} more preds\".format(int(10 + 5 * (radius ** 2))))\n",
    "      most_freq_pred = most_freq_pred_around(z, radius, int(10 + 5 * (radius ** 2)), model, dummy_img)\n",
    "      result.append((radius, most_freq_pred))\n",
    "    return result\n",
    "  \n",
    "def big_vector_to_str(x, thresh = 0.01):\n",
    "  torch.set_printoptions(precision=2, sci_mode=False, threshold=5000)  \n",
    "  result = \"[\"\n",
    "  for i, x_i in enumerate(x.data):\n",
    "    if abs(x_i) > thresh:\n",
    "      result += \"{}: {:.2f}\".format(i, x_i.item()) \n",
    "      result += \", \" if (i < x.shape[0]-1) else \"\"\n",
    "  result += \"]\"\n",
    "  return result\n",
    "\n",
    "def print_big_vector(x, thresh = 0.01):\n",
    "  print(big_vector_to_str(x, thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-btRW4qMKtw",
    "outputId": "90e81f6a-1b9e-45a4-ae82-bda370319bd9"
   },
   "outputs": [],
   "source": [
    "#experiment 1\n",
    "\n",
    "z = torch.tensor([0.5] * 10).cuda()\n",
    "# z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "# z_s = z[None]\n",
    "\n",
    "model = learn.model.eval()\n",
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "  \n",
    "for i in range(6):\n",
    "  z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "  print(\"investigation for: \", z)\n",
    "  for elem in investigate_neighborhood(z, 0.5, model, x_img):\n",
    "    print(elem)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 1-1: modified investigate_z\n",
    "z_investigate_path = '/root/Derakhshani/adversarial/textual_notes/investigate_z_{}.txt'.format(env.save_filename)\n",
    "if Path(z_investigate_path).exists(): raise FileExistsError(\"file already exists\")\n",
    "file = open(str(z_investigate_path), 'w')\n",
    "        \n",
    "for i, (z, noise) in enumerate(zip(pruned_z_s, pruned_noises)):\n",
    "  hist = compute_prediction_histogram(learn, noise)\n",
    "  indexed_hist = [(i, val) for i, val in enumerate(hist)]\n",
    "  sorted_hist = sorted(indexed_hist, key=lambda x: x[1], reverse=True)\n",
    "  labeled_hist = [(class_index_to_label(i), count) for i, count in sorted_hist]\n",
    "  print(\"result {}:\".format(i))\n",
    "  print(big_vector_to_str(z))\n",
    "  print(labeled_hist[:6])\n",
    "  print(\"\\n\\n\")\n",
    "  \n",
    "  file.write(\"result {}:\\n\".format(i))\n",
    "  file.write(big_vector_to_str(z) + \"\\n\")\n",
    "  file.write(str(labeled_hist[:6]))\n",
    "  file.write(\"\\n\\n\\n\")\n",
    "  file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bp6YOnipMKtz"
   },
   "outputs": [],
   "source": [
    "#experiment 2\n",
    "import itertools\n",
    "z_s = [torch.tensor(t).cuda() for t in itertools.product( *([[-0.33, 0.33]] * 10) )]\n",
    "model = learn.model.eval()\n",
    "noises = []\n",
    "with torch.no_grad():\n",
    "  for z in z_s:\n",
    "    noises.append(model.forward_single_z(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "55lErWDyMKt1",
    "outputId": "93d8fb71-3fd5-44a5-d3ec-8013e13f17ba"
   },
   "outputs": [],
   "source": [
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "\n",
    "preds = []\n",
    "for noise in noises:\n",
    "  perturbed_img = x_img + noise\n",
    "  preds.append(torch.argmax(arch(perturbed_img[None]), 1)[0].item())\n",
    "\n",
    "from collections import Counter\n",
    "result = [(class_index_to_label(index), count) for index, count in Counter(preds).most_common(5)]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4WSg-wBFMKt5"
   },
   "outputs": [],
   "source": [
    "#experiment 3\n",
    "import itertools\n",
    "dimension_values = [[-0.9, 0.9]] * z_dim\n",
    "for i in range(z_dim):\n",
    "  if i % 100 != 0:\n",
    "    dimension_values[i] = [0.]\n",
    "# dimension_values[0] = [0.]\n",
    "# dimension_values[3] = [0.]\n",
    "# dimension_values[6] = [0.]\n",
    "# dimension_values[9] = [0.]\n",
    "pruned_z_s = [torch.tensor(t).cuda() for t in itertools.product(*dimension_values)]\n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 3: for the targeted-attack case\n",
    "pruned_z_s = []\n",
    "for i in range(z_dim):\n",
    "  new_z = torch.zeros(z_dim).cuda()\n",
    "  new_z[i] = 1.\n",
    "  pruned_z_s.append(new_z)\n",
    "  \n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 3-1: noises for \n",
    "pruned_z_s = []\n",
    "# for i in range(z_dim):\n",
    "#   new_z = torch.empty(z_dim).uniform_(0,1).cuda().detach()\n",
    "#   pruned_z_s.append(new_z)\n",
    "\n",
    "for i in range(z_dim):\n",
    "  new_z = torch.zeros(z_dim).cuda().detach()\n",
    "  new_z[i] = 1.\n",
    "  pruned_z_s.append(new_z)\n",
    "  \n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise in pruned_noises[0:200]:\n",
    "  img = noise_to_image(noise)\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider web\n",
    "z_values = [\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33],\n",
    "  [-0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33,  0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33,  0.33,  0.33],\n",
    "  [-0.33,  0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_values = [\n",
    "  # window screen\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuAVZzmKMKt9",
    "outputId": "7e6643e0-ce55-438b-e0ae-79bde3ee4cef"
   },
   "outputs": [],
   "source": [
    "#vgg-16_12 most repeated labels:\n",
    "l = [(611, 215.0),\n",
    "  (474, 194.1),\n",
    "  (398, 120.3),\n",
    "  (721, 79.6),\n",
    "  (741, 73.5),\n",
    "  (510, 62.5)]\n",
    "\n",
    "[(class_index_to_label(index), count) for index, count in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAgk-YyWc3rG"
   },
   "outputs": [],
   "source": [
    "# learn.recorder.plot_losses()\n",
    "# learn.recorder.plot_lr()\n",
    "# learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTHG4Bt7VDYp"
   },
   "outputs": [],
   "source": [
    "fooling_rates = []\n",
    "model = learn.model.eval()\n",
    "learn.metrics = [validation_single_perturbation]\n",
    "for i in range(10):\n",
    "  global_perturbations = model(torch.rand(1, 3, 224, 244).cuda())[0]\n",
    "  nag_util.global_perturbations = global_perturbations\n",
    "  fooling_rates.append(learn.validate()[1].cpu().item())\n",
    "  print(\"%d : %f\"%(i, fooling_rates[-1]))\n",
    "\n",
    "mean = np.mean(fooling_rates)\n",
    "stddev = np.std(fooling_rates)\n",
    "print(mean, stddev); print(fooling_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "OFCjzI7UaY3C",
    "outputId": "740185b4-dd54-46f4-b0af-79ee452568e1"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[200][0]\n",
    "x = normalize(x_img.data.cuda())\n",
    "z = torch.tensor([-0.33,  0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33], dtype=torch.float32).cuda()\n",
    "# z = torch.empty(z_dim).uniform_(-1,1).cuda()\n",
    "p = model.forward_single_z(z).detach()\n",
    "\n",
    "p_x = x + p\n",
    "# print(\"img range, noise range\")\n",
    "# print_range(x); print_range(p)\n",
    "adv_label = class_index_to_label(arch(p_x[None]).argmax(1).item())\n",
    "print_big_vector(arch(p_x[None])[0])\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0., 1.])\n",
    "p_img = Image(p)\n",
    "x_img.show()\n",
    "p_img.show()\n",
    "p_x_img.show()\n",
    "\n",
    "\n",
    "# print_range(p)\n",
    "# print_range(denormalize(x))\n",
    "# print_range(p_x)\n",
    "\n",
    "benign_label = class_index_to_label(arch(x[None]).argmax(1).item())\n",
    "\n",
    "print_big_vector(arch(x[None])[0])\n",
    "print(benign_label, adv_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzwsI2P1ZANz"
   },
   "outputs": [],
   "source": [
    "z1 = torch.tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p1 = model.forward_single_z(z1)\n",
    "\n",
    "z2 = torch.tensor([1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p2 = model.forward_single_z(z2)\n",
    "\n",
    "z3 = torch.tensor([1, 1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p3 = model.forward_single_z(z3)\n",
    "\n",
    "l2_distance(p1, p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eroI82OKSnAL"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[4][0]\n",
    "x = x_img.data[None].cuda()\n",
    "p = model(x)[0].squeeze().detach() \n",
    "x = x.squeeze()\n",
    "x = normalize(x)\n",
    "\n",
    "p_x = x + p\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0.,1.])\n",
    "p_img = Image(p)\n",
    "# x_img.show()\n",
    "p_img.show()\n",
    "# p_x_img.show()\n",
    "\n",
    "print_range(p)\n",
    "print_range(x)\n",
    "print_range(p_x)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NAG-tripletLossExperiment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
