{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqeZpz16do4y"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def run_shell_command(cmd):\n",
    "  p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "  print(str(p.communicate()[0], 'utf-8'))\n",
    "  \n",
    "def detect_env():\n",
    "    import os\n",
    "    if 'content' in os.listdir('/'):\n",
    "      return \"colab\"\n",
    "    else:\n",
    "      return \"IBM\"\n",
    "    \n",
    "def create_env():\n",
    "  if detect_env() == \"IBM\":\n",
    "    return IBMEnv()\n",
    "  elif detect_env() == \"colab\":\n",
    "    return ColabEnv()\n",
    "\n",
    "\n",
    "class Env:\n",
    "  def get_nag_util_files(self):\n",
    "      import os\n",
    "      \n",
    "      print(\"\\ngetting git files ...\")\n",
    "      if os.path.isdir(self.python_files_path):\n",
    "        os.chdir(self.python_files_path)\n",
    "        run_shell_command('git pull')\n",
    "        os.chdir(self.root_folder)\n",
    "      else:\n",
    "        run_shell_command('git clone https://github.com/ahmad-PH/nag-public.git')\n",
    "      print(\"done.\")\n",
    "  \n",
    "\n",
    "class IBMEnv(Env):\n",
    "    def __init__(self):\n",
    "      self.root_folder = \"/root/Derakhshani/adversarial\"\n",
    "      self.temp_csv_path = self.root_folder + \"/temp\"\n",
    "      self.python_files_path = self.root_folder + \"/nag-public\"\n",
    "      self.python_files_dir = \"NAG-11May-beforeDenoiser\"\n",
    "      \n",
    "      import sys\n",
    "      sys.path.append('./nag/nag_util')\n",
    "      \n",
    "    def get_csv_path(self):\n",
    "      return self.root_folder + \"/textual_notes/CSVs/\" + self.save_filename\n",
    "    \n",
    "    def get_models_path(self):\n",
    "      return self.root_folder + \"/models/\" + self.save_filename\n",
    "      \n",
    "    def setup(self):\n",
    "      self.get_nag_util_files()\n",
    "#       defaults.device = torch.device('cuda:0')\n",
    "      import os;\n",
    "      os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "      os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "      \n",
    "    def load_dataset(self, compressed_name, unpacked_name):\n",
    "      pass\n",
    "\n",
    "    def load_test_dataset(self, root_folder):\n",
    "      pass\n",
    "    \n",
    "    def set_data_path(self, path):\n",
    "      self.data_path = Path(self.root_folder + '/datasets/' + path)\n",
    "    \n",
    "        \n",
    "class ColabEnv(Env):\n",
    "    def __init__(self):\n",
    "      self.root_folder = '/content'\n",
    "      self.temp_csv_path = self.root_folder\n",
    "      self.python_files_path = self.root_folder + '/nag-public'\n",
    "      self.python_files_dir = \"NAG-11May-beforeDenoiser\"\n",
    "      self.torchvision_upgraded = False\n",
    "      \n",
    "    def get_csv_path(self):\n",
    "      return self.root_folder + '/gdrive/My Drive/DL/textual_notes/CSVs/' + self.save_filename\n",
    "    \n",
    "    def get_models_path(self):\n",
    "      return self.root_folder + \"/gdrive/My Drive/DL/models/\" + self.save_filename\n",
    "        \n",
    "    def setup(self):\n",
    "        # ######################################################\n",
    "        # # TODO remove this once torchvision 0.3 is present by\n",
    "        # # default in Colab\n",
    "        # ######################################################\n",
    "        global torchvision_upgraded\n",
    "        try:\n",
    "            torchvision_upgraded\n",
    "        except NameError:\n",
    "          !pip uninstall -y torchvision\n",
    "          !pip install https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl\n",
    "          torchvision_upgraded = True\n",
    "        else:\n",
    "          print(\"torchvision already upgraded\")\n",
    "          \n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/gdrive')\n",
    "        \n",
    "        self.get_nag_util_files()\n",
    "        \n",
    "    def load_dataset(self, compressed_name, unpacked_name):\n",
    "      if compressed_name not in os.listdir('.'):\n",
    "        print(compressed_name + ' not found, getting it from drive')\n",
    "        shutil.copyfile(\"/content/gdrive/My Drive/DL/{}.tar.gz\".format(compressed_name), \"./{}.tar.gz\".format(compressed_name))\n",
    "\n",
    "        gunzip_arg = \"./{}.tar.gz\".format(compressed_name)\n",
    "        !gunzip -f $gunzip_arg\n",
    "\n",
    "        tar_arg = \"./{}.tar\".format(compressed_name)\n",
    "        !tar -xvf $tar_arg > /dev/null\n",
    "\n",
    "        os.rename(unpacked_name, compressed_name)\n",
    "\n",
    "    #     ls_arg = \"./{}/train/n01440764\".format(compressed_name)\n",
    "    #     !ls $ls_arg\n",
    "\n",
    "        !rm $tar_arg\n",
    "\n",
    "        print(\"done\") \n",
    "      else:\n",
    "        print(compressed_name + \" found\")\n",
    "        \n",
    "    def load_test_dataset(self, root_folder):\n",
    "      test_folder = root_folder + '/test/'\n",
    "      if 'test' not in os.listdir(root_folder):\n",
    "        print('getting test dataset from drive')\n",
    "        os.mkdir(test_folder)\n",
    "        for i in range(1,11):\n",
    "          shutil.copy(\"/content/gdrive/My Drive/DL/full_test_folder/{}.zip\".format(i), test_folder)\n",
    "          shutil.unpack_archive(test_folder + \"/{}.zip\".format(i), test_folder)\n",
    "          os.remove(test_folder + \"/{}.zip\".format(i))\n",
    "          print(\"done with the {}th fragment\".format(i))\n",
    "      else:\n",
    "        print('test dataset found.')\n",
    "        \n",
    "    def set_data_path(self, path):\n",
    "      self.data_path = Path('./' + path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "YyZUYSjBi9K9",
    "outputId": "a88472cd-6bbe-474d-c505-1c5791d6de13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "getting git files ...\n",
      "Already up-to-date.\n",
      "\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "env = create_env()\n",
    "env.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Ev7jcRKoARg"
   },
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.imports import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys; import os; import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3_1aE41PZAMw"
   },
   "outputs": [],
   "source": [
    "sys.path.append(env.python_files_path + '/' + env.python_files_dir)\n",
    "\n",
    "from nag_util import *\n",
    "import nag_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tltucTv2ep9-"
   },
   "outputs": [],
   "source": [
    "# mode = \"sanity_check\"\n",
    "mode = \"normal\"\n",
    "# mode = \"div_metric_calc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "SO1h55obXzOv",
    "outputId": "32b4001a-9389-4ae2-bd0c-c09953991995"
   },
   "outputs": [],
   "source": [
    "if mode == \"normal\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "elif mode == \"sanity_check\":\n",
    "  env.load_dataset('dataset_sanity_check_small', 'dataset_sanity_check_small')  \n",
    "  env.set_data_path('dataset_sanity_check_small')\n",
    "elif mode == \"div_metric_calc\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "  env.load_test_dataset(str(env.data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "koaQZmjMom7w"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "gpu_flag = True\n",
    "nag_util.batch_size = batch_size; nag_util.gpu_flag = gpu_flag;\n",
    "# nag_util.set_globals(gpu_flag, batch_size)\n",
    "tfms = get_transforms(do_flip=False, max_rotate=0)\n",
    "data = (ImageList.from_folder(env.data_path)\n",
    "        .split_by_folder(valid=('test' if mode == 'div_metric_calc' else 'valid'))\n",
    "        .label_from_folder()\n",
    "        .transform(tfms, size=224)\n",
    "        .databunch(bs=batch_size, num_workers=1)\n",
    "        .normalize(imagenet_stats))\n",
    "\n",
    "# data.show_batch(rows=2, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDBkRV8yovwV"
   },
   "outputs": [],
   "source": [
    "# model = models.resnet50\n",
    "model = models.vgg16_bn\n",
    "# model = torchvision.models.googlenet\n",
    "model_name = model.__name__\n",
    "z_dim = 10\n",
    "\n",
    "class SoftmaxWrapper(nn.Module):\n",
    "  def __init__(self, m):\n",
    "    super().__init__()\n",
    "    self.m = m\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "  def forward(self, inp):\n",
    "    out = self.m(inp)\n",
    "    return self.softmax(out)\n",
    "  \n",
    "arch = SoftmaxWrapper(model(pretrained=True).cuda().eval())\n",
    "nag_util.arch = arch\n",
    "requires_grad(arch, False)\n",
    "\n",
    "# vgg:\n",
    "# layers = []\n",
    "# blocks = [i-1 for i,o in enumerate(children(arch.features)) if isinstance(o, nn.MaxPool2d)]\n",
    "# layers = [arch.features[i] for i in blocks]\n",
    "# layer_weights = [1] * len(layers)\n",
    "\n",
    "# resnet:\n",
    "# layers = [\n",
    "#   arch.layer2[0].downsample,\n",
    "#   arch.layer3[0].downsample,\n",
    "#   arch.layer4[0].downsample\n",
    "# ]\n",
    "layers = [\n",
    "    arch.softmax\n",
    "]\n",
    "\n",
    "layer_weights = [1.] * len(layers)\n",
    "\n",
    "# layers = []\n",
    "# last_layer = None\n",
    "# for o in children(arch):\n",
    "#   if isinstance(o, nn.AdaptiveAvgPool2d):\n",
    "#     layers.append(last_layer)\n",
    "#   last_layer = o\n",
    "    \n",
    "# # layers = [arch.fc]\n",
    "\n",
    "# layer_weights = [1] * len(layers)\n",
    "\n",
    "# inception:\n",
    "# layers = [\n",
    "#     arch.Conv2d_1a_3x3,\n",
    "#     arch.Mixed_6e,\n",
    "#     arch.Mixed_7a,\n",
    "#     arch.fc    \n",
    "# ]\n",
    "# layer_weights = [1.0/4.0] * len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QdqhsfBNWx66"
   },
   "outputs": [],
   "source": [
    "class Gen(nn.Module):\n",
    "  def __init__(self, z_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "    super(Gen, self).__init__()\n",
    "\n",
    "    self.bs = None\n",
    "    self.z_dim = z_dim\n",
    "    self.gf_dim = gf_dim\n",
    "    self.y_dim = y_dim\n",
    "    self.df_dim = df_dim\n",
    "    self.image_shape = image_shape\n",
    "\n",
    "    self.z_ = nn.Linear(self.z_dim, self.gf_dim * 7 * 4 * 4, bias=True)\n",
    "    self.z_.bias.data.fill_(0)\n",
    "    self.BN_ = nn.BatchNorm2d(self.gf_dim * 7)\n",
    "\n",
    "    self.CT2d_1 = deconv_layer(self.gf_dim * 8, \n",
    "                             self.gf_dim * 4,\n",
    "                              k_size = (5,5), s = (2,2), pad = (2,2))\n",
    "    self.CT2d_2 = deconv_layer(self.gf_dim * 5, self.gf_dim * 2)\n",
    "\n",
    "    self.half = self.gf_dim // 2\n",
    "    if self.half == 0:\n",
    "      self.half == 1\n",
    "    self.CT2d_3 = deconv_layer(self.gf_dim * 2 + self.half, self.gf_dim * 1)\n",
    "\n",
    "    self.quarter = self.gf_dim // 4\n",
    "    if self.quarter == 0:\n",
    "      self.quarter == 1\n",
    "    self.CT2d_4 = deconv_layer(self.gf_dim * 1 + self.quarter, self.gf_dim * 1)\n",
    "\n",
    "    self.eighth = self.gf_dim // 8\n",
    "    if self.eighth == 0:\n",
    "      self.eighth == 1\n",
    "    self.CT2d_5 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "\n",
    "    # sixteenth = self.gf_dim // 16\n",
    "    # if half == 0:\n",
    "      # half == 1\n",
    "    self.CT2d_6 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "\n",
    "    # sixteenth = self.gf_dim // 16\n",
    "    # if half == 0:\n",
    "      # half == 1\n",
    "    self.CT2d_7 = deconv_layer(self.gf_dim * 1 + self.eighth, 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "\n",
    "  def forward_z(self, z):\n",
    "    self.bs = z.shape[0]\n",
    "      \n",
    "    # define generator here\n",
    "    # input: bs * 100\n",
    "    # Linear (z_dim, gf_dim * 7 * 4 * 4), bias = (True, init with zero), \n",
    "    # Reshape (bs, gf_dim * 7 * 4 * 4) -> (bs, gf_dim * 7, 4 , 4)\n",
    "    # Virtual Batch Norm = VBN\n",
    "    # ReLU\n",
    "    # h0 <- relu output\n",
    "    h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "    assert h0.shape[2:] == (4, 4), \"Non-expected shape, it shoud be (4,4)\"\n",
    "\n",
    "    # h0z = self.make_z([bs, gf_dim, 4, 4])\n",
    "    # h0 = torch.cat([h0, h0z], dim=1)\n",
    "    # h1 = deconv(gf_dim * 8, gf_dim * 4, kernel = (5, 5), stride = (2,2), padding = (2,2), bias = (True, 0))\n",
    "    # h1 = ReLU(VBN(h1))\n",
    "    h0z = self.make_z([self.bs, self.gf_dim, 4, 4])\n",
    "    h0 = torch.cat([h0, h0z], dim=1)\n",
    "    h1 = self.CT2d_1(h0)\n",
    "    assert h1.shape[2:] == (7, 7), \"Non-expected shape, it shoud be (7,7)\"\n",
    "\n",
    "    # h1z = self.make_z([bs, gf_dim, 7, 7])\n",
    "    # h1 = torch.cat([h1, h1z], dim=1)\n",
    "    # h2 = deconv(gf_dim * 5, gf_dim * 2, kernel = (5, 5), stride = (2,2), padding = (2,2), bias = (True, 0))\n",
    "    # h2 = ReLU(VBN(h2))\n",
    "    # assert output size (14,14)\n",
    "    h1z = self.make_z([self.bs, self.gf_dim, 7, 7])\n",
    "    h1 = torch.cat([h1, h1z], dim=1)\n",
    "    h2 = self.CT2d_2(h1)\n",
    "    assert h2.shape[2:] == (14,14), \"Non-expected shape, it shoud be (14,14)\"\n",
    "\n",
    "    # h2z = self.make_z([bs, half, 14, 14])\n",
    "    # h2 = torch.cat([h2, h2z], dim=1)\n",
    "    # h3 = deconv(gf_dim  2 + half, gf_dim  1, kernel = (5, 5), stride = (2,2), padding = (2,2), bias = (True, 0))\n",
    "    # h3 = ReLU(VBN(h3))\n",
    "    h2z = self.make_z([self.bs, self.half, 14, 14])\n",
    "    h2 = torch.cat([h2, h2z], dim=1)\n",
    "    h3 = self.CT2d_3(h2)\n",
    "    assert h3.shape[2:] == (28,28), \"Non-expected shape, it shoud be (28,28)\"\n",
    "\n",
    "    # h3z = self.make_z([bs, quarter, 28, 28])\n",
    "    # h3 = torch.cat([h3, h3z], dim=1)\n",
    "    # h4 = deconv(gf_dim * 1 + quarter, gf_dim * 1, kernel = (5, 5), stride = (2,2), padding = (2,2), bias = (True, 0))\n",
    "    # h4 = ReLU(VBN(h4))\n",
    "    h3z = self.make_z([self.bs, self.quarter, 28, 28])\n",
    "    h3 = torch.cat([h3, h3z], dim=1)\n",
    "    h4 = self.CT2d_4(h3)\n",
    "    assert h4.shape[2:] == (56,56), \"Non-expected shape, it shoud be (56,56)\"\n",
    "\n",
    "    # h4z = self.make_z([bs, self.eighth, 56, 56])\n",
    "    # h4 = torch.cat([h4, h4z], dim=1)\n",
    "    # h5 = deconv(gf_dim * 1 + eighth, gf_dim * 1, kernel = (5, 5), stride = (2,2), padding = (2,2), bias = (True, 0))\n",
    "    # h5 = ReLU(VBN(h5))\n",
    "\n",
    "    h4z = self.make_z([self.bs, self.eighth, 56, 56])\n",
    "    h4 = torch.cat([h4, h4z], dim=1)\n",
    "    h5 = self.CT2d_5(h4)\n",
    "    assert h5.shape[2:] == (112,112), \"Non-expected shape, it shoud be (112,112)\"\n",
    "\n",
    "    # h5z = self.make_z([bs, eighth, 112, 112])\n",
    "    # h5 = torch.cat([h5, h5z], dim=1)\n",
    "    # h6 = deconv(gf_dim * 1 + eighth, gf_dim * 1, kernel = (5, 5), stride = (2,2), padding = (2,2), bias = (True, 0))\n",
    "    # h6 = ReLU(VBN(h5))\n",
    "    h5z = self.make_z([self.bs, self.eighth, 112, 112])\n",
    "    h5 = torch.cat([h5, h5z], dim=1)\n",
    "    h6 = self.CT2d_6(h5)\n",
    "    assert h6.shape[2:] == (224,224), \"Non-expected shape, it shoud be (224,224)\"\n",
    "\n",
    "    # h6z = self.make_z([bs, eighth, 224, 224])\n",
    "    # h6 = torch.cat([h6, h6z], dim=1)\n",
    "    # h7 = deconv(gf_dim * 1 + eighth, 3, kernel = (5, 5), stride = (2,2), padding = (2,2), bias = (True, 0))\n",
    "    # h7 = ReLU(VBN(h7))\n",
    "    h6z = self.make_z([self.bs, self.eighth, 224, 224])\n",
    "    h6 = torch.cat([h6, h6z], dim=1)\n",
    "    h7 = self.CT2d_7(h6)\n",
    "    assert h7.shape[2:] == (224,224), \"Non-expected shape, it shoud be (448,448)\"\n",
    "\n",
    "    # out = 10*tanh(h7)\n",
    "\n",
    "    #     return 10 *F.tanh(h7)\n",
    "    ksi = 10.0\n",
    "    output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "    # this coeff scales the output to be appropriate for images that are \n",
    "    # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "    # interval)\n",
    "    return output_coeff * torch.tanh(h7)\n",
    "    # return 0.15 * torch.tanh(h7)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    self.bs = inputs.shape[0]\n",
    "    z = inputs.new_empty([self.bs, self.z_dim]).uniform_(-1,1).cuda()\n",
    "    p, n = self.make_triplet_samples(z, 0.1, 0.1, 2.)\n",
    "    \n",
    "    z_out = self.forward_z(z)\n",
    "    p_out = self.forward_z(p)\n",
    "    n_out = self.forward_z(n)\n",
    "    \n",
    "    return z_out, p_out, n_out, inputs\n",
    "  \n",
    "  def forward_single_z(self, z):\n",
    "    return self.forward_z(z[None]).squeeze()\n",
    "           \n",
    "  \n",
    "  def make_triplet_samples(self, z, margin, r2, r3):\n",
    "    positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "    negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "#     negative_sample = z + self.random_vector_volume(z.shape, margin, margin * scale).cuda()\n",
    "    return positive_sample, negative_sample\n",
    "\n",
    "  def random_vector_surface(self, shape, r = 1.):\n",
    "    mat = torch.randn(size=shape).cuda()\n",
    "    norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "    return (mat/norm) * r\n",
    "\n",
    "#   def random_vector_volume(shape, inner_r = 0, outer_r):\n",
    "#     d = torch.zeros(shape[0]).uniform_()   ** (1/int(np.prod(shape[0])))\n",
    "#     d.unsqueeze_(-1)\n",
    "#     return random_vector_surface(shape, outer_r) * d\n",
    "  \n",
    "  def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "#     d = torch.zeros(shape[0]).uniform_(0, outer_r - inner_r).cuda()\n",
    "    fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "    fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "    fraction.unsqueeze_(-1)\n",
    "#     return self.random_vector_surface(shape, 1) * d + inner_r\n",
    "    return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "  def make_z(self, in_shape):\n",
    "    return torch.empty(in_shape).cuda().uniform_(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wULy7qXNYeVv"
   },
   "outputs": [],
   "source": [
    "def load_starting_point(learn, name, z_dim):\n",
    "  if detect_env() != \"colab\":\n",
    "    raise NotImplementedError(\"load_starting_point not implemented for non-colab environments yet.\")\n",
    "  import os\n",
    "  identity_token = name + '-zdim' + str(z_dim)\n",
    "  address = '/content/gdrive/My Drive/DL/model_starting_points/' + identity_token\n",
    "  starting_point_exists = os.path.isfile(address + '.pth')\n",
    "  if not starting_point_exists:\n",
    "    print(\"\\n\\nno starting point found for model:\" + identity_token + \". creating one from the current learner.\\n\\n\")\n",
    "    learn.save(address)\n",
    "  learn.load(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h8KJQaKHZANA"
   },
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=2, sci_mode=False, threshold=5000)\n",
    "\n",
    "def print_softmax_tensor(x):\n",
    "  print(\"[\", end=\"\")\n",
    "  for i, x_i in enumerate(x.data):\n",
    "    if abs(x_i) > 0.01:\n",
    "      print(\"{}: {:.2f}\".format(i, x_i.item()), end=(\", \" if (i < x.shape[0]-1) else \"\"))\n",
    "  print(\"]\")\n",
    "  \n",
    "# print_softmax_tensor(torch.tensor([0.01, 2.5, 5.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NkfbLWEQqRA_"
   },
   "outputs": [],
   "source": [
    "def js_distance(x1, x2):\n",
    "  m = 0.5 * (x1 + x2)\n",
    "  return 0.5 * (F.kl_div(x1, m) + F.kl_div(x2, m))\n",
    "\n",
    "def kl_distance(x1, x2):\n",
    "  inp = torch.log(x1)\n",
    "  target = x2\n",
    "  return F.kl_div(inp, target, reduction='batchmean')\n",
    "\n",
    "def wasserstein_distance(x1, x2):\n",
    "  pass\n",
    "\n",
    "def l1_distance(x1, x2):\n",
    "  return F.l1_loss(x1, x2)\n",
    "\n",
    "def l2_distance(x1, x2):\n",
    "  return F.mse_loss(x1 * 10, x2 * 10)\n",
    "\n",
    "def cos_distance(x1, x2):\n",
    "    return -1 * torch.mean(F.cosine_similarity(x1, x2))\n",
    "\n",
    "triplet_call_cnt = 0\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, distance_func, margin):\n",
    "  # max distance when using l1_distance is 2\n",
    "  # max distacne when using l2-distance is sqrt(2)\n",
    "#   print(\"anchor: \", anchor.min(), anchor.max())\n",
    "  ap_dist = distance_func(anchor, positive)\n",
    "  an_dist = distance_func(anchor, negative)\n",
    "\n",
    "  global triplet_call_cnt\n",
    "  triplet_call_cnt += 1\n",
    "  if triplet_call_cnt % 10 == 0:\n",
    "    print(\"a: \", end=\"\"); print_softmax_tensor(anchor[0])\n",
    "    print(\"p: \", end=\"\"); print_softmax_tensor(positive[0])\n",
    "    print(\"n: \", end=\"\"); print_softmax_tensor(negative[0])\n",
    "    print(\"ap_dist: {}, an_dist: {}\".format(ap_dist, an_dist))\n",
    "    \n",
    "  return torch.mean(F.relu(ap_dist - an_dist + margin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsFgfiN8EV7z"
   },
   "outputs": [],
   "source": [
    "def diversity_loss(input, target):\n",
    "#   return -1 * torch.mean(torch.pow(f_x_a-f_x_s,2))\n",
    "  if input.shape[0] != batch_size:\n",
    "    print(\"input shape: \", input.shape)\n",
    "    print(\"target shape: \", target.shape, \"\\n\\n\")\n",
    "  return torch.mean(F.cosine_similarity(\n",
    "    input.view([batch_size, -1]),\n",
    "    target.view([batch_size, -1]), \n",
    "  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eqJsujkVZANH"
   },
   "outputs": [],
   "source": [
    "# z1 = torch.tensor([[1., 0.]])\n",
    "# z2 = torch.tensor([[-1., 0]])\n",
    "# cos_sim(z1,z2)\n",
    "\n",
    "# z1 = torch.tensor([[0.5, 0.5]])\n",
    "# z2 = torch.tensor([[0.55, 0.45]])\n",
    "# F.kl_div(z1, z2, reduction='batchmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FVegHeYovws"
   },
   "outputs": [],
   "source": [
    "class FeatureLoss(nn.Module):\n",
    "    def __name__(self):\n",
    "      return \"feature_loss\"\n",
    "  \n",
    "    def __init__(self, dis, layers, layer_weights):\n",
    "        super().__init__()\n",
    "        \n",
    "        # define generator here \n",
    "        self.dis = dis\n",
    "        self.diversity_layers = layers\n",
    "        self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "        self.weights = layer_weights\n",
    "        self.metric_names = [\"fool_loss\"] + [f\"div_loss_{i}\" for i in range(len(layers))] + ['triplet_loss']# Maybe Gram\n",
    "        self.triplet_weight = 10.\n",
    "    \n",
    "    def make_features(self, x, clone=False):\n",
    "        y = self.dis(x)\n",
    "        return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "    \n",
    "    def forward(self, inp, target):\n",
    "        sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "        X_A = self.add_perturbation(X_B, sigma_B) \n",
    "        X_A_pos = self.add_perturbation(X_B, sigma_pos)\n",
    "        X_A_neg = self.add_perturbation(X_B, sigma_neg) \n",
    "        \n",
    "        X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "        \n",
    "        B_Y, _ = self.make_features(X_B)\n",
    "        A_Y, A_feat = self.make_features(X_A)\n",
    "        _, S_feat = self.make_features(X_S)\n",
    "        pos_softmax, _ = self.make_features(X_A_pos)\n",
    "        neg_softmax, _ = self.make_features(X_A_neg)\n",
    "        \n",
    "        \n",
    "        fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "      \n",
    "        raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "        weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "        raw_triplet_loss = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "        weighted_triplet_loss = raw_triplet_loss * self.triplet_weight\n",
    "    \n",
    "        self.losses = [fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss]\n",
    "        self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses + [weighted_triplet_loss]))\n",
    "\n",
    "        return sum(self.losses)\n",
    "\n",
    "#     def forward(self, inp, target):\n",
    "#       sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "#       X_A = self.add_perturbation(X_B, sigma_B) \n",
    "\n",
    "#       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "#       B_Y, _ = self.make_features(X_B)\n",
    "#       A_Y, A_feat = self.make_features(X_A)\n",
    "#       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "#       fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "#       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "#       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "#       raw_triplet_loss = triplet_loss(sigma_B, sigma_pos, sigma_neg, l2_distance, 5.)\n",
    "#       weighted_triplet_loss = raw_triplet_loss * self.triplet_weight\n",
    "\n",
    "#       self.losses = [fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss]\n",
    "#       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses + [weighted_triplet_loss]))\n",
    "\n",
    "#       return sum(self.losses)\n",
    "  \n",
    "  \n",
    "    def add_perturbation(self, inp, perturbation):\n",
    "        return inp.add(perturbation)\n",
    "  \n",
    "    def add_perturbation_shuffled(self, inp, perturbation):\n",
    "#         j = torch.randperm(inp.shape[0])\n",
    "        j = derangement(inp.shape[0])\n",
    "        return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qd9gXUy_ovww"
   },
   "outputs": [],
   "source": [
    "feat_loss = FeatureLoss(arch, layers, layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfZKdYD2MSdi"
   },
   "outputs": [],
   "source": [
    "env.save_filename = 'vgg16_13'\n",
    "\n",
    "if Path(env.get_csv_path() + '.csv').exists(): raise FileExistsError(\"csv_path already exists\")\n",
    "if Path(env.get_models_path()).exists(): raise FileExistsError(\"models_path already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9J20CBLS8S9"
   },
   "outputs": [],
   "source": [
    "learn = None; gc.collect()\n",
    "csv_logger = partial(ImmediateCSVLogger, filename= env.temp_csv_path + '/' + env.save_filename)\n",
    "# learn = Learner(data, Gen(z_dim=10), loss_func = feat_loss, metrics=[validation], callback_fns=LossMetrics, opt_func = optim.SGD)\n",
    "# learn = Learner(data, Gen(z_dim=z_dim), loss_func = feat_loss, metrics=[validation], callback_fns=[LossMetrics, DiversityWeightsScheduler])\n",
    "learn = Learner(data, Gen(z_dim=z_dim), loss_func = feat_loss, metrics=[validation], callback_fns=[LossMetrics, csv_logger])\n",
    "# load_starting_point(learn, model_name, z_dim)\n",
    "# random_seed(42, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wk9E0AUm9rmn"
   },
   "outputs": [],
   "source": [
    "# learn.lr_find(1e-6, 1000)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0wOZYzOHDEdB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (9000 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "n02454379,n02454379,n02454379,n02454379,n02454379\n",
       "Path: /root/Derakhshani/adversarial/datasets/dataset;\n",
       "\n",
       "Valid: LabelList (1000 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "n02454379,n02397096,n02090379,n01729977,n02268853\n",
       "Path: /root/Derakhshani/adversarial/datasets/dataset;\n",
       "\n",
       "Test: None, model=Gen(\n",
       "  (z_): Linear(in_features=10, out_features=7168, bias=True)\n",
       "  (BN_): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (CT2d_1): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (BN2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_2): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(320, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_3): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(160, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_4): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(80, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_5): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_6): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_7): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (BN2d): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FeatureLoss(\n",
       "  (dis): SoftmaxWrapper(\n",
       "    (m): VGG(\n",
       "      (features): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace)\n",
       "        (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (9): ReLU(inplace)\n",
       "        (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (12): ReLU(inplace)\n",
       "        (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (16): ReLU(inplace)\n",
       "        (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (19): ReLU(inplace)\n",
       "        (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (22): ReLU(inplace)\n",
       "        (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (26): ReLU(inplace)\n",
       "        (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (29): ReLU(inplace)\n",
       "        (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (32): ReLU(inplace)\n",
       "        (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (36): ReLU(inplace)\n",
       "        (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (39): ReLU(inplace)\n",
       "        (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (42): ReLU(inplace)\n",
       "        (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "      (classifier): Sequential(\n",
       "        (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.5)\n",
       "        (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (4): ReLU(inplace)\n",
       "        (5): Dropout(p=0.5)\n",
       "        (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (softmax): Softmax()\n",
       "  )\n",
       "), metrics=[<function validation at 0x7f193c9aae18>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/Derakhshani/adversarial/datasets/dataset'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), <class 'fastai.callbacks.loss_metrics.LossMetrics'>, functools.partial(<class 'nag_util.ImmediateCSVLogger'>, filename='/root/Derakhshani/adversarial/temp/vgg16_13')], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): Linear(in_features=10, out_features=7168, bias=True)\n",
       "  (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): ConvTranspose2d(320, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ConvTranspose2d(160, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (8): ConvTranspose2d(80, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (12): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ConvTranspose2d(72, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (15): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")], add_time=True, silent=None)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !cp \"/content/gdrive/My Drive/DL/models/vgg16_12-last.pth\"  \"/content/\"\n",
    "# learn.load('/content/vgg16_12-last')\n",
    "\n",
    "learn.load('/root/Derakhshani/adversarial/models/vgg16_11/vgg16_11_29')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "LA1ffVbbEwQS",
    "outputId": "682ac419-ee54-497a-b89f-496f789cf368",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='30', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/30 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>div_loss_0</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='186' class='' max='1125', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      16.53% [186/1125 00:52<04:27 1.4521]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if mode == \"sanity_check\":\n",
    "  print(\"\\n\\n\\nWARNING: you are training on a sanity_check dataset.\\n\\n\\n\\n\")\n",
    "if len(learn.callback_fns) == 1:\n",
    "  print(\"\\n\\n\\nWARNING: you are not using the DiversityWeightsScheduler callback.\\n\\n\\n\")\n",
    "\n",
    "    \n",
    "saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=env.save_filename + \"-best\")\n",
    "saver_every_epoch = SaveModelCallback(learn, every='epoch', name=env.save_filename)\n",
    "\n",
    "# import cProfile\n",
    "\n",
    "# pr = cProfile.Profile()\n",
    "# pr.enable()\n",
    "learn.fit(30, lr=5e-03, callbacks=[saver_best, saver_every_epoch])\n",
    "# pr.disable()\n",
    "\n",
    "# learn.fit(30, lr=5e-03, wd=0.005, callbacks=[saver_best, saver_every_epoch])\n",
    "# learn.fit_one_cycle(20, max_lr=5e-1, callbacks=[saver_callback])\n",
    "# learn.fit_one_cycle(8, max_lr=5e-01) #mohammad's setting that got 77 validation start on resnet with diversity loss on AdaptiveAvgPool2d\n",
    "# learn.fit_one_cycle(5, max_lr=2e-2) #used for vgg-19-bn\n",
    "# learn.fit_one_cycle(5, max_lr=3e-3) # used for resnet50\n",
    "\n",
    "shutil.copyfile(env.temp_csv_path + '/' + env.save_filename + \".csv\", env.get_csv_path() + '.csv')\n",
    "shutil.copytree(env.data_path/\"models\", env.get_models_path())\n",
    "\n",
    "# pr.print_stats()\n",
    "\n",
    "# shutil.copyfile(\"/content/dataset/models/\" + save_filename + \"-best.pth\", \"/content/gdrive/My Drive/DL/models/\" + save_filename + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-BmJ8cESVIay"
   },
   "outputs": [],
   "source": [
    "!cp \"/content/gdrive/My Drive/DL/models/resnet50-dir/resnet50-dir-best.pth\" \"/content/resnet50-best.pth\"\n",
    "learn.load(\"/content/resnet50-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "obTWhste2pZo"
   },
   "outputs": [],
   "source": [
    "learn.fit(1, lr = 0., wd=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zUz0oXLbVVB0"
   },
   "outputs": [],
   "source": [
    "learn.validate(metrics=[feat_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MO2fZ-hSSUzJ"
   },
   "outputs": [],
   "source": [
    "z1 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "z2 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "# print(\"z1: \", z1)\n",
    "# print(\"z2: \", z2)\n",
    "print(\"distance: \", torch.norm(z1-z2,p=2))\n",
    "model = learn.model.eval()\n",
    "\n",
    "z_s = interpolate(z1, z2, 0.15)\n",
    "print(len(z_s))\n",
    "for i,z in enumerate(z_s):\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n",
    "  img.save('./pics/' + str(i) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DGuGN7B7V0Xt"
   },
   "outputs": [],
   "source": [
    "def generate_perturbations(learn, n_perturbations):\n",
    "  initial_training_mode = learn.model.training\n",
    "  \n",
    "  model = learn.model.eval()\n",
    "  input_img = (learn.data.valid_ds[0][0].data)[None].cuda()\n",
    "  perturbations = []\n",
    "  for i in range(n_perturbations):\n",
    "    perturbation = model(input_img)[0].squeeze()\n",
    "    perturbations.append(perturbation)\n",
    "    \n",
    "  learn.model.train(initial_training_mode)  \n",
    "  return perturbations\n",
    "  \n",
    "  \n",
    "# def compute_mean_prediction_histogram(learn, perturbations):\n",
    "#   pred_histogram = [0] * 1000\n",
    "#   for j, perturbation in enumerate(perturbations):\n",
    "#     for i in range(len(learn.data.valid_ds)):\n",
    "#       img = learn.data.valid_ds[i][0].data[None].cuda()\n",
    "#       perturbed_img = img + perturbation\n",
    "#       pred = torch.argmax(arch(perturbed_img).squeeze())\n",
    "#       pred_histogram[pred]+= 1./len(perturbations)\n",
    "#     print(\"finished creating histogram for the %dth perturbation\"%j)\n",
    "#   return pred_histogram\n",
    "\n",
    "  \n",
    "def compute_mean_prediction_histogram(learn, perturbations):\n",
    "  pred_histogram = [0] * 1000\n",
    "  for j, perturbation in enumerate(perturbations):\n",
    "    batch_no = -1\n",
    "    for batch, _ in learn.data.valid_dl:\n",
    "      batch_no += 1\n",
    "      if batch_no % 100 == 0 : print(\"at batch no {}\".format(batch_no))\n",
    "      perturbed_batch = batch + perturbation[None]\n",
    "      preds = arch(perturbed_batch).argmax(1)\n",
    "      for pred in preds:\n",
    "        pred_histogram[pred]+= 1. / len(perturbations)\n",
    "    print(\"finished creating histogram for the %dth perturbation\"%j)\n",
    "\n",
    "  pred_histogram = np.asarray(np.array(pred_histogram) / len(perturbations))\n",
    "\n",
    "  return pred_histogram\n",
    "\n",
    "\n",
    "def diversity(learn, n_perturbations, percentage):\n",
    "  pred_histogram = compute_mean_prediction_histogram(\n",
    "      learn, generate_perturbations(learn, n_perturbations)\n",
    "  )\n",
    "  print(\"finished creating the prediction histogram\")\n",
    "  pred_histogram_sum = np.sum(pred_histogram)\n",
    "  \n",
    "  indexed_pred_histogram = [(i, hist_element) for i,hist_element in  \n",
    "                            enumerate(pred_histogram)]\n",
    "  \n",
    "  indexed_pred_histogram.sort(key=lambda x: x[1], reverse = True)\n",
    "  \n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  top_classes = []\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = indexed_pred_histogram[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / pred_histogram_sum) * 100.\n",
    "    top_classes.append(hist_elem[0])\n",
    "    n_used_classes += 1\n",
    "  \n",
    "  #top_classes is a useful piece of info that is currently unused\n",
    "  return n_used_classes, indexed_pred_histogram, top_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 978
    },
    "colab_type": "code",
    "id": "Q8VUc3YH4vj5",
    "outputId": "aff7c648-2f10-4938-c9cd-378f6c8d0c91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "at batch no 3200\n",
      "at batch no 3300\n",
      "at batch no 3400\n",
      "at batch no 3500\n",
      "at batch no 3600\n",
      "at batch no 3700\n",
      "at batch no 3800\n",
      "at batch no 3900\n",
      "at batch no 4000\n",
      "at batch no 4100\n",
      "at batch no 4200\n",
      "at batch no 4300\n",
      "at batch no 4400\n",
      "at batch no 4500\n",
      "at batch no 4600\n",
      "at batch no 4700\n",
      "at batch no 4800\n",
      "at batch no 4900\n",
      "at batch no 5000\n",
      "at batch no 5100\n",
      "at batch no 5200\n",
      "at batch no 5300\n",
      "at batch no 5400\n",
      "at batch no 5500\n",
      "at batch no 5600\n",
      "at batch no 5700\n",
      "at batch no 5800\n",
      "at batch no 5900\n",
      "at batch no 6000\n",
      "at batch no 6100\n",
      "at batch no 6200\n",
      "finished creating histogram for the 0th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "at batch no 3200\n",
      "at batch no 3300\n",
      "at batch no 3400\n",
      "at batch no 3500\n",
      "at batch no 3600\n",
      "at batch no 3700\n",
      "at batch no 3800\n",
      "at batch no 3900\n",
      "at batch no 4000\n",
      "at batch no 4100\n",
      "at batch no 4200\n",
      "at batch no 4300\n",
      "at batch no 4400\n",
      "at batch no 4500\n",
      "at batch no 4600\n",
      "at batch no 4700\n",
      "at batch no 4800\n",
      "at batch no 4900\n",
      "at batch no 5000\n",
      "at batch no 5100\n",
      "at batch no 5200\n",
      "at batch no 5300\n",
      "at batch no 5400\n",
      "at batch no 5500\n",
      "at batch no 5600\n",
      "at batch no 5700\n",
      "at batch no 5800\n",
      "at batch no 5900\n",
      "at batch no 6000\n",
      "at batch no 6100\n",
      "at batch no 6200\n",
      "finished creating histogram for the 1th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "at batch no 3200\n",
      "at batch no 3300\n",
      "at batch no 3400\n",
      "at batch no 3500\n",
      "at batch no 3600\n",
      "at batch no 3700\n",
      "at batch no 3800\n",
      "at batch no 3900\n",
      "at batch no 4000\n",
      "at batch no 4100\n",
      "at batch no 4200\n",
      "at batch no 4300\n",
      "at batch no 4400\n",
      "at batch no 4500\n",
      "at batch no 4600\n",
      "at batch no 4700\n",
      "at batch no 4800\n",
      "at batch no 4900\n",
      "at batch no 5000\n",
      "at batch no 5100\n",
      "at batch no 5200\n",
      "at batch no 5300\n",
      "at batch no 5400\n",
      "at batch no 5500\n",
      "at batch no 5600\n",
      "at batch no 5700\n",
      "at batch no 5800\n",
      "at batch no 5900\n",
      "at batch no 6000\n",
      "at batch no 6100\n",
      "at batch no 6200\n",
      "finished creating histogram for the 2th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "at batch no 3200\n",
      "at batch no 3300\n",
      "at batch no 3400\n",
      "at batch no 3500\n",
      "at batch no 3600\n",
      "at batch no 3700\n",
      "at batch no 3800\n",
      "at batch no 3900\n",
      "at batch no 4000\n",
      "at batch no 4100\n",
      "at batch no 4200\n",
      "at batch no 4300\n",
      "at batch no 4400\n",
      "at batch no 4500\n",
      "at batch no 4600\n",
      "at batch no 4700\n",
      "at batch no 4800\n",
      "at batch no 4900\n",
      "at batch no 5000\n",
      "at batch no 5100\n",
      "at batch no 5200\n",
      "at batch no 5300\n",
      "at batch no 5400\n",
      "at batch no 5500\n",
      "at batch no 5600\n",
      "at batch no 5700\n",
      "at batch no 5800\n",
      "at batch no 5900\n",
      "at batch no 6000\n",
      "at batch no 6100\n",
      "at batch no 6200\n",
      "finished creating histogram for the 3th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "at batch no 3200\n",
      "at batch no 3300\n",
      "at batch no 3400\n",
      "at batch no 3500\n",
      "at batch no 3600\n",
      "at batch no 3700\n",
      "at batch no 3800\n",
      "at batch no 3900\n",
      "at batch no 4000\n",
      "at batch no 4100\n",
      "at batch no 4200\n",
      "at batch no 4300\n",
      "at batch no 4400\n",
      "at batch no 4500\n",
      "at batch no 4600\n",
      "at batch no 4700\n",
      "at batch no 4800\n",
      "at batch no 4900\n",
      "at batch no 5000\n",
      "at batch no 5100\n",
      "at batch no 5200\n",
      "at batch no 5300\n",
      "at batch no 5400\n",
      "at batch no 5500\n",
      "at batch no 5600\n",
      "at batch no 5700\n",
      "at batch no 5800\n",
      "at batch no 5900\n",
      "at batch no 6000\n",
      "at batch no 6100\n",
      "at batch no 6200\n",
      "finished creating histogram for the 4th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "at batch no 3200\n",
      "at batch no 3300\n",
      "at batch no 3400\n",
      "at batch no 3500\n",
      "at batch no 3600\n",
      "at batch no 3700\n",
      "at batch no 3800\n",
      "at batch no 3900\n",
      "at batch no 4000\n",
      "at batch no 4100\n",
      "at batch no 4200\n",
      "at batch no 4300\n",
      "at batch no 4400\n",
      "at batch no 4500\n",
      "at batch no 4600\n",
      "at batch no 4700\n",
      "at batch no 4800\n",
      "at batch no 4900\n",
      "at batch no 5000\n",
      "at batch no 5100\n",
      "at batch no 5200\n",
      "at batch no 5300\n",
      "at batch no 5400\n",
      "at batch no 5500\n",
      "at batch no 5600\n",
      "at batch no 5700\n",
      "at batch no 5800\n",
      "at batch no 5900\n",
      "at batch no 6000\n",
      "at batch no 6100\n",
      "at batch no 6200\n",
      "finished creating histogram for the 5th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "at batch no 3200\n",
      "at batch no 3300\n",
      "at batch no 3400\n",
      "at batch no 3500\n",
      "at batch no 3600\n",
      "at batch no 3700\n",
      "at batch no 3800\n",
      "at batch no 3900\n",
      "at batch no 4000\n",
      "at batch no 4100\n",
      "at batch no 4200\n",
      "at batch no 4300\n",
      "at batch no 4400\n",
      "at batch no 4500\n",
      "at batch no 4600\n",
      "at batch no 4700\n",
      "at batch no 4800\n",
      "at batch no 4900\n",
      "at batch no 5000\n",
      "at batch no 5100\n",
      "at batch no 5200\n",
      "at batch no 5300\n",
      "at batch no 5400\n",
      "at batch no 5500\n",
      "at batch no 5600\n",
      "at batch no 5700\n",
      "at batch no 5800\n",
      "at batch no 5900\n",
      "at batch no 6000\n",
      "at batch no 6100\n",
      "at batch no 6200\n",
      "finished creating histogram for the 6th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "at batch no 3200\n",
      "at batch no 3300\n",
      "at batch no 3400\n",
      "at batch no 3500\n",
      "at batch no 3600\n",
      "at batch no 3700\n",
      "at batch no 3800\n",
      "at batch no 3900\n",
      "at batch no 4000\n",
      "at batch no 4100\n",
      "at batch no 4200\n",
      "at batch no 4300\n",
      "at batch no 4400\n",
      "at batch no 4500\n",
      "at batch no 4600\n",
      "at batch no 4700\n",
      "at batch no 4800\n",
      "at batch no 4900\n",
      "at batch no 5000\n",
      "at batch no 5100\n",
      "at batch no 5200\n",
      "at batch no 5300\n",
      "at batch no 5400\n",
      "at batch no 5500\n",
      "at batch no 5600\n",
      "at batch no 5700\n",
      "at batch no 5800\n",
      "at batch no 5900\n",
      "at batch no 6000\n",
      "at batch no 6100\n",
      "at batch no 6200\n",
      "finished creating histogram for the 7th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "at batch no 3200\n",
      "at batch no 3300\n",
      "at batch no 3400\n",
      "at batch no 3500\n",
      "at batch no 3600\n",
      "at batch no 3700\n",
      "at batch no 3800\n",
      "at batch no 3900\n",
      "at batch no 4000\n",
      "at batch no 4100\n",
      "at batch no 4200\n",
      "at batch no 4300\n",
      "at batch no 4400\n",
      "at batch no 4500\n",
      "at batch no 4600\n",
      "at batch no 4700\n",
      "at batch no 4800\n",
      "at batch no 4900\n",
      "at batch no 5000\n",
      "at batch no 5100\n",
      "at batch no 5200\n",
      "at batch no 5300\n",
      "at batch no 5400\n",
      "at batch no 5500\n",
      "at batch no 5600\n",
      "at batch no 5700\n",
      "at batch no 5800\n",
      "at batch no 5900\n",
      "at batch no 6000\n",
      "at batch no 6100\n",
      "at batch no 6200\n",
      "finished creating histogram for the 8th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "at batch no 3200\n",
      "at batch no 3300\n",
      "at batch no 3400\n",
      "at batch no 3500\n",
      "at batch no 3600\n",
      "at batch no 3700\n",
      "at batch no 3800\n",
      "at batch no 3900\n",
      "at batch no 4000\n",
      "at batch no 4100\n",
      "at batch no 4200\n",
      "at batch no 4300\n",
      "at batch no 4400\n",
      "at batch no 4500\n",
      "at batch no 4600\n",
      "at batch no 4700\n",
      "at batch no 4800\n",
      "at batch no 4900\n",
      "at batch no 5000\n",
      "at batch no 5100\n",
      "at batch no 5200\n",
      "at batch no 5300\n",
      "at batch no 5400\n",
      "at batch no 5500\n",
      "at batch no 5600\n",
      "at batch no 5700\n",
      "at batch no 5800\n",
      "at batch no 5900\n",
      "at batch no 6000\n",
      "at batch no 6100\n",
      "at batch no 6200\n",
      "finished creating histogram for the 9th perturbation\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(807,\n",
       " [(594, 1423.070000003424),\n",
       "  (815, 793.3700000011331),\n",
       "  (839, 342.25999999979825),\n",
       "  (854, 68.66000000000875),\n",
       "  (904, 31.27000000000026),\n",
       "  (794, 22.199999999999168),\n",
       "  (580, 15.559999999999544),\n",
       "  (905, 10.199999999999848),\n",
       "  (828, 10.159999999999851),\n",
       "  (84, 9.909999999999865),\n",
       "  (556, 8.879999999999923),\n",
       "  (611, 8.789999999999928),\n",
       "  (489, 8.24999999999996),\n",
       "  (709, 5.660000000000053),\n",
       "  (545, 5.460000000000051),\n",
       "  (497, 5.280000000000048),\n",
       "  (363, 5.240000000000047),\n",
       "  (646, 5.240000000000047),\n",
       "  (781, 5.210000000000047),\n",
       "  (741, 5.180000000000047),\n",
       "  (288, 5.170000000000046),\n",
       "  (922, 5.170000000000046),\n",
       "  (688, 5.120000000000045),\n",
       "  (61, 5.110000000000046),\n",
       "  (806, 5.0400000000000444),\n",
       "  (67, 4.970000000000043),\n",
       "  (824, 4.940000000000043),\n",
       "  (687, 4.890000000000042),\n",
       "  (109, 4.860000000000042),\n",
       "  (987, 4.860000000000042),\n",
       "  (893, 4.800000000000042),\n",
       "  (401, 4.68000000000004),\n",
       "  (292, 4.660000000000039),\n",
       "  (58, 4.550000000000038),\n",
       "  (718, 4.550000000000038),\n",
       "  (39, 4.5300000000000376),\n",
       "  (509, 4.5300000000000376),\n",
       "  (671, 4.520000000000037),\n",
       "  (82, 4.500000000000037),\n",
       "  (614, 4.410000000000036),\n",
       "  (721, 4.350000000000035),\n",
       "  (562, 4.270000000000033),\n",
       "  (955, 4.250000000000034),\n",
       "  (982, 4.220000000000033),\n",
       "  (420, 4.190000000000032),\n",
       "  (582, 4.180000000000033),\n",
       "  (858, 4.180000000000033),\n",
       "  (381, 4.140000000000032),\n",
       "  (581, 4.140000000000032),\n",
       "  (992, 4.140000000000032),\n",
       "  (701, 4.130000000000032),\n",
       "  (748, 4.120000000000031),\n",
       "  (116, 4.110000000000031),\n",
       "  (546, 4.0700000000000305),\n",
       "  (664, 4.060000000000031),\n",
       "  (340, 4.050000000000031),\n",
       "  (791, 4.0300000000000304),\n",
       "  (48, 4.02000000000003),\n",
       "  (293, 3.9700000000000295),\n",
       "  (474, 3.9100000000000286),\n",
       "  (588, 3.8800000000000283),\n",
       "  (599, 3.780000000000027),\n",
       "  (698, 3.7600000000000264),\n",
       "  (468, 3.750000000000026),\n",
       "  (343, 3.740000000000026),\n",
       "  (286, 3.720000000000026),\n",
       "  (383, 3.7000000000000255),\n",
       "  (533, 3.7000000000000255),\n",
       "  (703, 3.660000000000025),\n",
       "  (398, 3.650000000000025),\n",
       "  (848, 3.650000000000025),\n",
       "  (457, 3.6400000000000245),\n",
       "  (716, 3.6400000000000245),\n",
       "  (652, 3.6200000000000245),\n",
       "  (37, 3.6100000000000243),\n",
       "  (66, 3.600000000000024),\n",
       "  (219, 3.600000000000024),\n",
       "  (425, 3.600000000000024),\n",
       "  (565, 3.590000000000024),\n",
       "  (56, 3.580000000000024),\n",
       "  (409, 3.5600000000000236),\n",
       "  (579, 3.5600000000000236),\n",
       "  (162, 3.5500000000000234),\n",
       "  (208, 3.5500000000000234),\n",
       "  (819, 3.5500000000000234),\n",
       "  (161, 3.540000000000023),\n",
       "  (424, 3.540000000000023),\n",
       "  (640, 3.540000000000023),\n",
       "  (327, 3.520000000000023),\n",
       "  (25, 3.510000000000023),\n",
       "  (184, 3.510000000000023),\n",
       "  (352, 3.510000000000023),\n",
       "  (679, 3.510000000000023),\n",
       "  (834, 3.510000000000023),\n",
       "  (555, 3.5000000000000226),\n",
       "  (23, 3.4800000000000226),\n",
       "  (135, 3.4700000000000224),\n",
       "  (431, 3.4700000000000224),\n",
       "  (237, 3.460000000000022),\n",
       "  (354, 3.460000000000022),\n",
       "  (917, 3.4400000000000217),\n",
       "  (138, 3.430000000000022),\n",
       "  (382, 3.430000000000022),\n",
       "  (783, 3.430000000000022),\n",
       "  (754, 3.4000000000000212),\n",
       "  (880, 3.4000000000000212),\n",
       "  (234, 3.390000000000021),\n",
       "  (406, 3.380000000000021),\n",
       "  (458, 3.380000000000021),\n",
       "  (825, 3.380000000000021),\n",
       "  (851, 3.380000000000021),\n",
       "  (591, 3.370000000000021),\n",
       "  (213, 3.3600000000000207),\n",
       "  (645, 3.3600000000000207),\n",
       "  (334, 3.3400000000000203),\n",
       "  (561, 3.3400000000000203),\n",
       "  (203, 3.3200000000000203),\n",
       "  (238, 3.31000000000002),\n",
       "  (289, 3.31000000000002),\n",
       "  (477, 3.31000000000002),\n",
       "  (526, 3.31000000000002),\n",
       "  (217, 3.30000000000002),\n",
       "  (868, 3.2900000000000196),\n",
       "  (674, 3.28000000000002),\n",
       "  (300, 3.2700000000000196),\n",
       "  (632, 3.2600000000000193),\n",
       "  (102, 3.250000000000019),\n",
       "  (415, 3.250000000000019),\n",
       "  (314, 3.240000000000019),\n",
       "  (404, 3.240000000000019),\n",
       "  (743, 3.230000000000019),\n",
       "  (64, 3.220000000000019),\n",
       "  (284, 3.220000000000019),\n",
       "  (850, 3.220000000000019),\n",
       "  (866, 3.220000000000019),\n",
       "  (483, 3.2100000000000186),\n",
       "  (779, 3.2000000000000184),\n",
       "  (981, 3.2000000000000184),\n",
       "  (49, 3.190000000000018),\n",
       "  (496, 3.190000000000018),\n",
       "  (281, 3.1800000000000184),\n",
       "  (843, 3.1800000000000184),\n",
       "  (751, 3.170000000000018),\n",
       "  (863, 3.170000000000018),\n",
       "  (99, 3.160000000000018),\n",
       "  (151, 3.160000000000018),\n",
       "  (274, 3.160000000000018),\n",
       "  (407, 3.1500000000000177),\n",
       "  (738, 3.1300000000000177),\n",
       "  (251, 3.1200000000000174),\n",
       "  (185, 3.110000000000017),\n",
       "  (387, 3.110000000000017),\n",
       "  (867, 3.110000000000017),\n",
       "  (878, 3.110000000000017),\n",
       "  (50, 3.100000000000017),\n",
       "  (800, 3.100000000000017),\n",
       "  (211, 3.0900000000000167),\n",
       "  (386, 3.080000000000017),\n",
       "  (722, 3.080000000000017),\n",
       "  (790, 3.0700000000000167),\n",
       "  (918, 3.0700000000000167),\n",
       "  (724, 3.0600000000000165),\n",
       "  (788, 3.040000000000016),\n",
       "  (319, 3.0300000000000162),\n",
       "  (361, 3.020000000000016),\n",
       "  (886, 3.020000000000016),\n",
       "  (218, 3.0100000000000158),\n",
       "  (417, 3.0100000000000158),\n",
       "  (584, 3.0100000000000158),\n",
       "  (597, 3.0100000000000158),\n",
       "  (661, 3.0000000000000155),\n",
       "  (414, 2.9900000000000153),\n",
       "  (491, 2.9900000000000153),\n",
       "  (96, 2.9800000000000155),\n",
       "  (926, 2.9700000000000153),\n",
       "  (884, 2.960000000000015),\n",
       "  (260, 2.950000000000015),\n",
       "  (253, 2.9400000000000146),\n",
       "  (570, 2.9400000000000146),\n",
       "  (759, 2.9400000000000146),\n",
       "  (349, 2.930000000000015),\n",
       "  (621, 2.930000000000015),\n",
       "  (973, 2.930000000000015),\n",
       "  (128, 2.9200000000000146),\n",
       "  (520, 2.9200000000000146),\n",
       "  (77, 2.9100000000000144),\n",
       "  (168, 2.9100000000000144),\n",
       "  (445, 2.9100000000000144),\n",
       "  (762, 2.9100000000000144),\n",
       "  (881, 2.9100000000000144),\n",
       "  (155, 2.900000000000014),\n",
       "  (249, 2.900000000000014),\n",
       "  (76, 2.880000000000014),\n",
       "  (182, 2.880000000000014),\n",
       "  (259, 2.880000000000014),\n",
       "  (385, 2.880000000000014),\n",
       "  (549, 2.880000000000014),\n",
       "  (874, 2.880000000000014),\n",
       "  (132, 2.870000000000014),\n",
       "  (216, 2.870000000000014),\n",
       "  (498, 2.8600000000000136),\n",
       "  (91, 2.8500000000000134),\n",
       "  (191, 2.8500000000000134),\n",
       "  (221, 2.8500000000000134),\n",
       "  (495, 2.8500000000000134),\n",
       "  (753, 2.8500000000000134),\n",
       "  (963, 2.8500000000000134),\n",
       "  (133, 2.840000000000013),\n",
       "  (24, 2.820000000000013),\n",
       "  (454, 2.820000000000013),\n",
       "  (508, 2.820000000000013),\n",
       "  (57, 2.810000000000013),\n",
       "  (257, 2.810000000000013),\n",
       "  (275, 2.810000000000013),\n",
       "  (355, 2.8000000000000127),\n",
       "  (950, 2.7900000000000125),\n",
       "  (71, 2.7800000000000127),\n",
       "  (331, 2.7800000000000127),\n",
       "  (359, 2.7800000000000127),\n",
       "  (563, 2.7800000000000127),\n",
       "  (197, 2.7700000000000125),\n",
       "  (279, 2.7700000000000125),\n",
       "  (280, 2.7700000000000125),\n",
       "  (344, 2.7700000000000125),\n",
       "  (453, 2.7700000000000125),\n",
       "  (876, 2.7700000000000125),\n",
       "  (186, 2.760000000000012),\n",
       "  (365, 2.760000000000012),\n",
       "  (316, 2.750000000000012),\n",
       "  (706, 2.750000000000012),\n",
       "  (752, 2.750000000000012),\n",
       "  (192, 2.7400000000000118),\n",
       "  (766, 2.7400000000000118),\n",
       "  (142, 2.730000000000012),\n",
       "  (159, 2.7200000000000117),\n",
       "  (299, 2.7200000000000117),\n",
       "  (455, 2.7200000000000117),\n",
       "  (829, 2.7200000000000117),\n",
       "  (890, 2.7200000000000117),\n",
       "  (156, 2.7100000000000115),\n",
       "  (201, 2.7100000000000115),\n",
       "  (476, 2.7100000000000115),\n",
       "  (490, 2.7100000000000115),\n",
       "  (554, 2.7000000000000113),\n",
       "  (669, 2.7000000000000113),\n",
       "  (178, 2.690000000000011),\n",
       "  (328, 2.690000000000011),\n",
       "  (333, 2.690000000000011),\n",
       "  (538, 2.690000000000011),\n",
       "  (337, 2.6800000000000113),\n",
       "  (692, 2.6800000000000113),\n",
       "  (515, 2.670000000000011),\n",
       "  (625, 2.670000000000011),\n",
       "  (985, 2.670000000000011),\n",
       "  (330, 2.660000000000011),\n",
       "  (624, 2.660000000000011),\n",
       "  (642, 2.660000000000011),\n",
       "  (949, 2.660000000000011),\n",
       "  (215, 2.6500000000000106),\n",
       "  (336, 2.6500000000000106),\n",
       "  (998, 2.6500000000000106),\n",
       "  (55, 2.6400000000000103),\n",
       "  (72, 2.6400000000000103),\n",
       "  (242, 2.6400000000000103),\n",
       "  (263, 2.6400000000000103),\n",
       "  (283, 2.6400000000000103),\n",
       "  (313, 2.6400000000000103),\n",
       "  (446, 2.6400000000000103),\n",
       "  (447, 2.6400000000000103),\n",
       "  (467, 2.6400000000000103),\n",
       "  (511, 2.6400000000000103),\n",
       "  (60, 2.6300000000000106),\n",
       "  (157, 2.6300000000000106),\n",
       "  (277, 2.6300000000000106),\n",
       "  (440, 2.6300000000000106),\n",
       "  (506, 2.6300000000000106),\n",
       "  (939, 2.6300000000000106),\n",
       "  (199, 2.6200000000000103),\n",
       "  (269, 2.61000000000001),\n",
       "  (849, 2.61000000000001),\n",
       "  (655, 2.60000000000001),\n",
       "  (920, 2.5900000000000096),\n",
       "  (971, 2.5900000000000096),\n",
       "  (527, 2.58000000000001),\n",
       "  (52, 2.5700000000000096),\n",
       "  (273, 2.5700000000000096),\n",
       "  (882, 2.5700000000000096),\n",
       "  (912, 2.5700000000000096),\n",
       "  (239, 2.5600000000000094),\n",
       "  (522, 2.5600000000000094),\n",
       "  (934, 2.5600000000000094),\n",
       "  (734, 2.550000000000009),\n",
       "  (892, 2.550000000000009),\n",
       "  (9, 2.540000000000009),\n",
       "  (110, 2.540000000000009),\n",
       "  (154, 2.540000000000009),\n",
       "  (410, 2.540000000000009),\n",
       "  (923, 2.540000000000009),\n",
       "  (41, 2.530000000000009),\n",
       "  (214, 2.530000000000009),\n",
       "  (308, 2.530000000000009),\n",
       "  (916, 2.530000000000009),\n",
       "  (46, 2.520000000000009),\n",
       "  (443, 2.520000000000009),\n",
       "  (627, 2.520000000000009),\n",
       "  (953, 2.520000000000009),\n",
       "  (100, 2.5100000000000087),\n",
       "  (232, 2.5100000000000087),\n",
       "  (548, 2.5100000000000087),\n",
       "  (616, 2.5100000000000087),\n",
       "  (842, 2.5100000000000087),\n",
       "  (195, 2.490000000000008),\n",
       "  (245, 2.490000000000008),\n",
       "  (323, 2.490000000000008),\n",
       "  (568, 2.490000000000008),\n",
       "  (85, 2.4800000000000084),\n",
       "  (134, 2.4800000000000084),\n",
       "  (153, 2.4800000000000084),\n",
       "  (171, 2.4800000000000084),\n",
       "  (230, 2.4800000000000084),\n",
       "  (318, 2.4800000000000084),\n",
       "  (707, 2.4800000000000084),\n",
       "  (984, 2.4800000000000084),\n",
       "  (63, 2.470000000000008),\n",
       "  (631, 2.470000000000008),\n",
       "  (250, 2.460000000000008),\n",
       "  (294, 2.460000000000008),\n",
       "  (373, 2.460000000000008),\n",
       "  (411, 2.460000000000008),\n",
       "  (8, 2.4500000000000077),\n",
       "  (205, 2.4500000000000077),\n",
       "  (225, 2.4500000000000077),\n",
       "  (256, 2.4500000000000077),\n",
       "  (648, 2.4500000000000077),\n",
       "  (792, 2.4500000000000077),\n",
       "  (805, 2.4500000000000077),\n",
       "  (888, 2.4500000000000077),\n",
       "  (45, 2.4400000000000075),\n",
       "  (62, 2.4400000000000075),\n",
       "  (90, 2.4400000000000075),\n",
       "  (243, 2.4400000000000075),\n",
       "  (658, 2.4400000000000075),\n",
       "  (823, 2.4400000000000075),\n",
       "  (906, 2.4400000000000075),\n",
       "  (946, 2.4400000000000075),\n",
       "  (69, 2.4300000000000077),\n",
       "  (189, 2.4300000000000077),\n",
       "  (198, 2.4300000000000077),\n",
       "  (290, 2.4300000000000077),\n",
       "  (426, 2.4300000000000077),\n",
       "  (870, 2.4300000000000077),\n",
       "  (264, 2.4200000000000075),\n",
       "  (603, 2.4200000000000075),\n",
       "  (877, 2.4200000000000075),\n",
       "  (129, 2.4100000000000072),\n",
       "  (136, 2.4100000000000072),\n",
       "  (177, 2.4100000000000072),\n",
       "  (207, 2.4100000000000072),\n",
       "  (332, 2.4100000000000072),\n",
       "  (396, 2.4100000000000072),\n",
       "  (735, 2.4100000000000072),\n",
       "  (799, 2.4100000000000072),\n",
       "  (954, 2.4100000000000072),\n",
       "  (43, 2.400000000000007),\n",
       "  (181, 2.400000000000007),\n",
       "  (360, 2.400000000000007),\n",
       "  (464, 2.400000000000007),\n",
       "  (907, 2.400000000000007),\n",
       "  (298, 2.390000000000007),\n",
       "  (342, 2.390000000000007),\n",
       "  (847, 2.390000000000007),\n",
       "  (254, 2.380000000000007),\n",
       "  (711, 2.380000000000007),\n",
       "  (765, 2.380000000000007),\n",
       "  (713, 2.3700000000000068),\n",
       "  (925, 2.3700000000000068),\n",
       "  (927, 2.3700000000000068),\n",
       "  (79, 2.3600000000000065),\n",
       "  (144, 2.3600000000000065),\n",
       "  (172, 2.3600000000000065),\n",
       "  (227, 2.3600000000000065),\n",
       "  (357, 2.3600000000000065),\n",
       "  (727, 2.3600000000000065),\n",
       "  (947, 2.3600000000000065),\n",
       "  (978, 2.3600000000000065),\n",
       "  (13, 2.3500000000000063),\n",
       "  (130, 2.3500000000000063),\n",
       "  (353, 2.3500000000000063),\n",
       "  (681, 2.3500000000000063),\n",
       "  (732, 2.3500000000000063),\n",
       "  (737, 2.3500000000000063),\n",
       "  (777, 2.3500000000000063),\n",
       "  (986, 2.3500000000000063),\n",
       "  (40, 2.340000000000006),\n",
       "  (312, 2.340000000000006),\n",
       "  (372, 2.340000000000006),\n",
       "  (770, 2.340000000000006),\n",
       "  (780, 2.340000000000006),\n",
       "  (821, 2.340000000000006),\n",
       "  (937, 2.340000000000006),\n",
       "  (88, 2.3300000000000063),\n",
       "  (164, 2.3300000000000063),\n",
       "  (367, 2.3300000000000063),\n",
       "  (605, 2.3300000000000063),\n",
       "  (662, 2.3300000000000063),\n",
       "  (730, 2.3300000000000063),\n",
       "  (812, 2.3300000000000063),\n",
       "  (141, 2.320000000000006),\n",
       "  (339, 2.320000000000006),\n",
       "  (472, 2.320000000000006),\n",
       "  (564, 2.320000000000006),\n",
       "  (83, 2.310000000000006),\n",
       "  (104, 2.310000000000006),\n",
       "  (271, 2.310000000000006),\n",
       "  (651, 2.310000000000006),\n",
       "  (932, 2.310000000000006),\n",
       "  (370, 2.3000000000000056),\n",
       "  (518, 2.3000000000000056),\n",
       "  (532, 2.3000000000000056),\n",
       "  (670, 2.3000000000000056),\n",
       "  (787, 2.3000000000000056),\n",
       "  (816, 2.3000000000000056),\n",
       "  (15, 2.2900000000000054),\n",
       "  (28, 2.2900000000000054),\n",
       "  (38, 2.2900000000000054),\n",
       "  (54, 2.2900000000000054),\n",
       "  (124, 2.2900000000000054),\n",
       "  (295, 2.2900000000000054),\n",
       "  (364, 2.2900000000000054),\n",
       "  (628, 2.2900000000000054),\n",
       "  (746, 2.2900000000000054),\n",
       "  (871, 2.2900000000000054),\n",
       "  (118, 2.2800000000000056),\n",
       "  (258, 2.2800000000000056),\n",
       "  (261, 2.2800000000000056),\n",
       "  (310, 2.2800000000000056),\n",
       "  (7, 2.2700000000000053),\n",
       "  (223, 2.2700000000000053),\n",
       "  (419, 2.2700000000000053),\n",
       "  (452, 2.2700000000000053),\n",
       "  (471, 2.2700000000000053),\n",
       "  (535, 2.2700000000000053),\n",
       "  (789, 2.2700000000000053),\n",
       "  (803, 2.2700000000000053),\n",
       "  (0, 2.260000000000005),\n",
       "  (5, 2.260000000000005),\n",
       "  (65, 2.260000000000005),\n",
       "  (301, 2.260000000000005),\n",
       "  (47, 2.250000000000005),\n",
       "  (276, 2.250000000000005),\n",
       "  (462, 2.250000000000005),\n",
       "  (641, 2.250000000000005),\n",
       "  (801, 2.250000000000005),\n",
       "  (898, 2.250000000000005),\n",
       "  (21, 2.2400000000000047),\n",
       "  (44, 2.2400000000000047),\n",
       "  (487, 2.2400000000000047),\n",
       "  (602, 2.2400000000000047),\n",
       "  (650, 2.2400000000000047),\n",
       "  (654, 2.2400000000000047),\n",
       "  (695, 2.2400000000000047),\n",
       "  (894, 2.2400000000000047),\n",
       "  (226, 2.230000000000005),\n",
       "  (262, 2.230000000000005),\n",
       "  (291, 2.230000000000005),\n",
       "  (481, 2.230000000000005),\n",
       "  (539, 2.230000000000005),\n",
       "  (720, 2.230000000000005),\n",
       "  (993, 2.230000000000005),\n",
       "  (42, 2.2200000000000046),\n",
       "  (51, 2.2200000000000046),\n",
       "  (94, 2.2200000000000046),\n",
       "  (113, 2.2200000000000046),\n",
       "  (368, 2.2200000000000046),\n",
       "  (375, 2.2200000000000046),\n",
       "  (376, 2.2200000000000046),\n",
       "  (510, 2.2200000000000046),\n",
       "  (593, 2.2200000000000046),\n",
       "  (855, 2.2200000000000046),\n",
       "  (889, 2.2200000000000046),\n",
       "  (202, 2.2100000000000044),\n",
       "  (760, 2.2100000000000044),\n",
       "  (785, 2.2100000000000044),\n",
       "  (80, 2.200000000000004),\n",
       "  (180, 2.200000000000004),\n",
       "  (265, 2.200000000000004),\n",
       "  (750, 2.200000000000004),\n",
       "  (114, 2.190000000000004),\n",
       "  (278, 2.190000000000004),\n",
       "  (321, 2.190000000000004),\n",
       "  (547, 2.190000000000004),\n",
       "  (592, 2.190000000000004),\n",
       "  (617, 2.190000000000004),\n",
       "  (697, 2.190000000000004),\n",
       "  (756, 2.190000000000004),\n",
       "  (247, 2.180000000000004),\n",
       "  (421, 2.180000000000004),\n",
       "  (879, 2.180000000000004),\n",
       "  (131, 2.170000000000004),\n",
       "  (204, 2.170000000000004),\n",
       "  (612, 2.170000000000004),\n",
       "  (725, 2.170000000000004),\n",
       "  (798, 2.170000000000004),\n",
       "  (873, 2.170000000000004),\n",
       "  (123, 2.1600000000000037),\n",
       "  (255, 2.1600000000000037),\n",
       "  (378, 2.1600000000000037),\n",
       "  (659, 2.1600000000000037),\n",
       "  (668, 2.1600000000000037),\n",
       "  (699, 2.1600000000000037),\n",
       "  (763, 2.1600000000000037),\n",
       "  (944, 2.1600000000000037),\n",
       "  (210, 2.1500000000000035),\n",
       "  (577, 2.1500000000000035),\n",
       "  (595, 2.1500000000000035),\n",
       "  (714, 2.1500000000000035),\n",
       "  (529, 2.1400000000000032),\n",
       "  (619, 2.1400000000000032),\n",
       "  (14, 2.1300000000000034),\n",
       "  (145, 2.1300000000000034),\n",
       "  (598, 2.1300000000000034),\n",
       "  (826, 2.1300000000000034),\n",
       "  (174, 2.120000000000003),\n",
       "  (193, 2.120000000000003),\n",
       "  (212, 2.120000000000003),\n",
       "  (441, 2.120000000000003),\n",
       "  (630, 2.120000000000003),\n",
       "  (322, 2.110000000000003),\n",
       "  (346, 2.110000000000003),\n",
       "  (377, 2.110000000000003),\n",
       "  (769, 2.110000000000003),\n",
       "  (831, 2.110000000000003),\n",
       "  (844, 2.110000000000003),\n",
       "  (996, 2.110000000000003),\n",
       "  (335, 2.1000000000000028),\n",
       "  (392, 2.1000000000000028),\n",
       "  (492, 2.1000000000000028),\n",
       "  (988, 2.1000000000000028),\n",
       "  (991, 2.1000000000000028),\n",
       "  (148, 2.0900000000000025),\n",
       "  (384, 2.0900000000000025),\n",
       "  (388, 2.0900000000000025),\n",
       "  (391, 2.0900000000000025),\n",
       "  (402, 2.0900000000000025),\n",
       "  (436, 2.0900000000000025),\n",
       "  (485, 2.0900000000000025),\n",
       "  (604, 2.0900000000000025),\n",
       "  (229, 2.0800000000000027),\n",
       "  (233, 2.0800000000000027),\n",
       "  (285, 2.0800000000000027),\n",
       "  (303, 2.0800000000000027),\n",
       "  (347, 2.0800000000000027),\n",
       "  (399, 2.0800000000000027),\n",
       "  (626, 2.0800000000000027),\n",
       "  (846, 2.0800000000000027),\n",
       "  (30, 2.0700000000000025),\n",
       "  (475, 2.0700000000000025),\n",
       "  (519, 2.0700000000000025),\n",
       "  (620, 2.0700000000000025),\n",
       "  (694, 2.0700000000000025),\n",
       "  (772, 2.0700000000000025),\n",
       "  (962, 2.0700000000000025),\n",
       "  (995, 2.0700000000000025),\n",
       "  (18, 2.0600000000000023),\n",
       "  (127, 2.0600000000000023),\n",
       "  (433, 2.0600000000000023),\n",
       "  (500, 2.0600000000000023),\n",
       "  (572, 2.0600000000000023),\n",
       "  (764, 2.0600000000000023),\n",
       "  (990, 2.0600000000000023),\n",
       "  (78, 2.050000000000002),\n",
       "  (187, 2.050000000000002),\n",
       "  (311, 2.050000000000002),\n",
       "  (615, 2.050000000000002),\n",
       "  (802, 2.050000000000002),\n",
       "  (910, 2.050000000000002),\n",
       "  (935, 2.050000000000002),\n",
       "  (938, 2.050000000000002),\n",
       "  (70, 2.040000000000002),\n",
       "  (200, 2.040000000000002),\n",
       "  (413, 2.040000000000002),\n",
       "  (466, 2.040000000000002),\n",
       "  (573, 2.040000000000002),\n",
       "  (98, 2.030000000000002),\n",
       "  (272, 2.030000000000002),\n",
       "  (302, 2.030000000000002),\n",
       "  (351, 2.030000000000002),\n",
       "  (542, 2.030000000000002),\n",
       "  (636, 2.030000000000002),\n",
       "  (975, 2.030000000000002),\n",
       "  (1, 2.020000000000002),\n",
       "  (137, 2.020000000000002),\n",
       "  (139, 2.020000000000002),\n",
       "  (209, 2.020000000000002),\n",
       "  (397, 2.020000000000002),\n",
       "  (444, 2.020000000000002),\n",
       "  (92, 2.0100000000000016),\n",
       "  (120, 2.0100000000000016),\n",
       "  (170, 2.0100000000000016),\n",
       "  (448, 2.0100000000000016),\n",
       "  (726, 2.0100000000000016),\n",
       "  (761, 2.0100000000000016),\n",
       "  (27, 2.0000000000000013),\n",
       "  (160, 2.0000000000000013),\n",
       "  (439, 2.0000000000000013),\n",
       "  (514, 2.0000000000000013),\n",
       "  (524, 2.0000000000000013),\n",
       "  (543, 2.0000000000000013),\n",
       "  (553, 2.0000000000000013),\n",
       "  (22, 1.9900000000000013),\n",
       "  (95, 1.9900000000000013),\n",
       "  (108, 1.9900000000000013),\n",
       "  (252, 1.9900000000000013),\n",
       "  (379, 1.9900000000000013),\n",
       "  (427, 1.9900000000000013),\n",
       "  (537, 1.9900000000000013),\n",
       "  (637, 1.9900000000000013),\n",
       "  (745, 1.9900000000000013),\n",
       "  (865, 1.9900000000000013),\n",
       "  (248, 1.980000000000001),\n",
       "  (348, 1.980000000000001),\n",
       "  (395, 1.980000000000001),\n",
       "  (449, 1.980000000000001),\n",
       "  (704, 1.980000000000001),\n",
       "  (206, 1.970000000000001),\n",
       "  (236, 1.970000000000001),\n",
       "  (350, 1.970000000000001),\n",
       "  (456, 1.970000000000001),\n",
       "  (607, 1.970000000000001),\n",
       "  (814, 1.970000000000001),\n",
       "  (921, 1.970000000000001),\n",
       "  (983, 1.970000000000001),\n",
       "  (12, 1.9600000000000009),\n",
       "  (105, 1.9600000000000009),\n",
       "  (224, 1.9600000000000009),\n",
       "  (864, 1.9600000000000009),\n",
       "  (957, 1.9600000000000009),\n",
       "  (31, 1.9500000000000006),\n",
       "  (125, 1.9500000000000006),\n",
       "  (169, 1.9500000000000006),\n",
       "  (228, 1.9500000000000006),\n",
       "  (307, 1.9500000000000006),\n",
       "  (358, 1.9500000000000006),\n",
       "  (470, 1.9500000000000006),\n",
       "  (560, 1.9500000000000006),\n",
       "  (576, 1.9500000000000006),\n",
       "  (860, 1.9500000000000006),\n",
       "  (11, 1.9400000000000006),\n",
       "  (17, 1.9400000000000006),\n",
       "  (194, 1.9400000000000006),\n",
       "  (220, 1.9400000000000006),\n",
       "  (235, 1.9400000000000006),\n",
       "  (853, 1.9400000000000006),\n",
       "  (900, 1.9400000000000006),\n",
       "  (19, 1.9300000000000004),\n",
       "  (75, 1.9300000000000004),\n",
       "  (163, 1.9300000000000004),\n",
       "  (488, 1.9300000000000004),\n",
       "  (531, 1.9300000000000004),\n",
       "  (933, 1.9300000000000004),\n",
       "  (968, 1.9300000000000004),\n",
       "  (240, 1.9200000000000004),\n",
       "  (587, 1.9200000000000004),\n",
       "  (609, 1.9200000000000004),\n",
       "  (872, 1.9200000000000004),\n",
       "  (86, 1.9100000000000001),\n",
       "  (196, 1.9100000000000001),\n",
       "  (685, 1.9100000000000001),\n",
       "  (723, 1.9100000000000001),\n",
       "  (820, 1.9100000000000001),\n",
       "  (840, 1.9100000000000001),\n",
       "  (97, 1.9),\n",
       "  (325, 1.9),\n",
       "  (647, 1.9),\n",
       "  (845, 1.9),\n",
       "  (997, 1.9),\n",
       "  (53, 1.89),\n",
       "  (121, 1.89),\n",
       "  (822, 1.89),\n",
       "  (977, 1.89),\n",
       "  (87, 1.8799999999999997),\n",
       "  (504, 1.8799999999999997),\n",
       "  (677, 1.8799999999999997),\n",
       "  (796, 1.8799999999999997),\n",
       "  (989, 1.8799999999999997),\n",
       "  (3, 1.8699999999999997),\n",
       "  (106, 1.8699999999999997),\n",
       "  (112, 1.8699999999999997),\n",
       "  (305, 1.8699999999999997),\n",
       "  (362, 1.8699999999999997),\n",
       "  (428, 1.8699999999999997),\n",
       "  (784, 1.8699999999999997),\n",
       "  (974, 1.8699999999999997),\n",
       "  (149, 1.8599999999999994),\n",
       "  (774, 1.8599999999999994),\n",
       "  (807, 1.8599999999999994),\n",
       "  (909, 1.8599999999999994),\n",
       "  (143, 1.8499999999999992),\n",
       "  (244, 1.8499999999999992),\n",
       "  (418, 1.8499999999999992),\n",
       "  (451, 1.8499999999999992),\n",
       "  (473, 1.8499999999999992),\n",
       "  (505, 1.8499999999999992),\n",
       "  (513, 1.8499999999999992),\n",
       "  (122, 1.8399999999999992),\n",
       "  (719, 1.8399999999999992),\n",
       "  (852, 1.8399999999999992),\n",
       "  (956, 1.8399999999999992),\n",
       "  (93, 1.829999999999999),\n",
       "  (268, 1.829999999999999),\n",
       "  (683, 1.829999999999999),\n",
       "  (786, 1.829999999999999),\n",
       "  (608, 1.819999999999999),\n",
       "  (739, 1.819999999999999),\n",
       "  (755, 1.819999999999999),\n",
       "  (757, 1.819999999999999),\n",
       "  (150, 1.8099999999999987),\n",
       "  (166, 1.8099999999999987),\n",
       "  (682, 1.8099999999999987),\n",
       "  (705, 1.8099999999999987),\n",
       "  (964, 1.8099999999999987),\n",
       "  (574, 1.7999999999999985),\n",
       "  (634, 1.7999999999999985),\n",
       "  (768, 1.7999999999999985),\n",
       "  (951, 1.7999999999999985),\n",
       "  (10, 1.7899999999999985),\n",
       "  (107, 1.7899999999999985),\n",
       "  (317, 1.7899999999999985),\n",
       "  (324, 1.7899999999999985),\n",
       "  (329, 1.7899999999999985),\n",
       "  (416, 1.7899999999999985),\n",
       "  (629, 1.7899999999999985),\n",
       "  (809, 1.7899999999999985),\n",
       "  (101, 1.7799999999999983),\n",
       "  (267, 1.7799999999999983),\n",
       "  (566, 1.7799999999999983),\n",
       "  (690, 1.7799999999999983),\n",
       "  (857, 1.7799999999999983),\n",
       "  (979, 1.7799999999999983),\n",
       "  (306, 1.7699999999999982),\n",
       "  (326, 1.7699999999999982),\n",
       "  (341, 1.7699999999999982),\n",
       "  (569, 1.7699999999999982),\n",
       "  (691, 1.7699999999999982),\n",
       "  (771, 1.7699999999999982),\n",
       "  (833, 1.7699999999999982),\n",
       "  (837, 1.7699999999999982),\n",
       "  (959, 1.7699999999999982),\n",
       "  (81, 1.759999999999998),\n",
       "  (575, 1.759999999999998),\n",
       "  (586, 1.759999999999998),\n",
       "  (665, 1.759999999999998),\n",
       "  (811, 1.759999999999998),\n",
       "  (945, 1.759999999999998),\n",
       "  (117, 1.7499999999999978),\n",
       "  (429, 1.7499999999999978),\n",
       "  (528, 1.7499999999999978),\n",
       "  (657, 1.7499999999999978),\n",
       "  (767, 1.7499999999999978),\n",
       "  (804, 1.7499999999999978),\n",
       "  (304, 1.7399999999999978),\n",
       "  (450, 1.7399999999999978),\n",
       "  (895, 1.7399999999999978),\n",
       "  (89, 1.7299999999999975),\n",
       "  (241, 1.7299999999999975),\n",
       "  (270, 1.7299999999999975),\n",
       "  (432, 1.7299999999999975),\n",
       "  (544, 1.7299999999999975),\n",
       "  (639, 1.7299999999999975),\n",
       "  (2, 1.7199999999999975),\n",
       "  (29, 1.7199999999999975),\n",
       "  (179, 1.7199999999999975),\n",
       "  (422, 1.7199999999999975),\n",
       "  (530, 1.7199999999999975),\n",
       "  (571, 1.7199999999999975),\n",
       "  (684, 1.7199999999999975),\n",
       "  (835, 1.7199999999999975),\n",
       "  (883, 1.7199999999999975),\n",
       "  (948, 1.7199999999999975),\n",
       "  (4, 1.7099999999999973),\n",
       "  (16, 1.7099999999999973),\n",
       "  (412, 1.7099999999999973),\n",
       "  (552, 1.7099999999999973),\n",
       "  (832, 1.7099999999999973),\n",
       "  (140, 1.699999999999997),\n",
       "  (393, 1.699999999999997),\n",
       "  (463, 1.699999999999997),\n",
       "  (507, 1.699999999999997),\n",
       "  (672, 1.699999999999997),\n",
       "  (782, 1.699999999999997),\n",
       "  (115, 1.689999999999997),\n",
       "  (356, 1.689999999999997),\n",
       "  (366, 1.689999999999997),\n",
       "  (676, 1.689999999999997),\n",
       "  (930, 1.689999999999997),\n",
       "  (936, 1.689999999999997),\n",
       "  (173, 1.6799999999999968),\n",
       "  (320, 1.6799999999999968),\n",
       "  (437, 1.6799999999999968),\n",
       "  (680, 1.6799999999999968),\n",
       "  (35, 1.6699999999999968),\n",
       "  (165, 1.6699999999999968),\n",
       "  (717, 1.6699999999999968),\n",
       "  (902, 1.6699999999999968),\n",
       "  (26, 1.6599999999999966),\n",
       "  (190, 1.6599999999999966),\n",
       "  (693, 1.6599999999999966),\n",
       "  (960, 1.6599999999999966),\n",
       "  (338, 1.6499999999999964),\n",
       "  (423, 1.6499999999999964),\n",
       "  (613, 1.6499999999999964),\n",
       "  (885, 1.6499999999999964),\n",
       "  (941, 1.6499999999999964),\n",
       "  (282, 1.6399999999999963),\n",
       "  (369, 1.6399999999999963),\n",
       "  (525, 1.6399999999999963),\n",
       "  (558, 1.6399999999999963),\n",
       "  (175, 1.6299999999999961),\n",
       "  (287, 1.6299999999999961),\n",
       "  (486, 1.6299999999999961),\n",
       "  (503, 1.6299999999999961),\n",
       "  (517, 1.6299999999999961),\n",
       "  (606, 1.6299999999999961),\n",
       "  (678, 1.6299999999999961),\n",
       "  (736, 1.6299999999999961),\n",
       "  (817, 1.6299999999999961),\n",
       "  (20, 1.619999999999996),\n",
       "  (147, 1.619999999999996),\n",
       "  (635, 1.619999999999996),\n",
       "  (712, 1.619999999999996),\n",
       "  (152, 1.6099999999999959),\n",
       "  (903, 1.6099999999999959),\n",
       "  (952, 1.6099999999999959),\n",
       "  (231, 1.5999999999999959),\n",
       "  (297, 1.5999999999999959),\n",
       "  (994, 1.5999999999999959),\n",
       "  (6, 1.5899999999999959),\n",
       "  (389, 1.5899999999999959),\n",
       "  (512, 1.5899999999999959),\n",
       "  (540, 1.5899999999999959),\n",
       "  (643, 1.5899999999999959),\n",
       "  (601, 1.579999999999996),\n",
       "  (793, 1.579999999999996),\n",
       "  (875, 1.579999999999996),\n",
       "  (958, 1.579999999999996),\n",
       "  (915, 1.569999999999996),\n",
       "  (222, 1.559999999999996),\n",
       "  (638, 1.559999999999996),\n",
       "  (749, 1.559999999999996),\n",
       "  (913, 1.559999999999996),\n",
       "  (408, 1.549999999999996),\n",
       "  (551, 1.549999999999996),\n",
       "  (928, 1.549999999999996),\n",
       "  (589, 1.539999999999996),\n",
       "  (667, 1.539999999999996),\n",
       "  (430, 1.5299999999999963),\n",
       "  (523, 1.5299999999999963),\n",
       "  (795, 1.5299999999999963),\n",
       "  (919, 1.5299999999999963),\n",
       "  (309, 1.5199999999999962),\n",
       "  (494, 1.5199999999999962),\n",
       "  (666, 1.4999999999999962),\n",
       "  (887, 1.4999999999999962),\n",
       "  (484, 1.4899999999999962),\n",
       "  (583, 1.4899999999999962),\n",
       "  (797, 1.4899999999999962),\n",
       "  (897, 1.4899999999999962),\n",
       "  (480, 1.4799999999999964),\n",
       "  (550, 1.4799999999999964),\n",
       "  (696, 1.4799999999999964),\n",
       "  (966, 1.4799999999999964),\n",
       "  (315, 1.4699999999999964),\n",
       "  (585, 1.4699999999999964),\n",
       "  (33, 1.4599999999999964),\n",
       "  (74, 1.4599999999999964),\n",
       "  (435, 1.4599999999999964),\n",
       "  (502, 1.4599999999999964),\n",
       "  (521, 1.4599999999999964),\n",
       "  (896, 1.4599999999999964),\n",
       "  (188, 1.4499999999999964),\n",
       "  (296, 1.4499999999999964),\n",
       "  (403, 1.4499999999999964),\n",
       "  (559, 1.4499999999999964),\n",
       "  (808, 1.4399999999999964),\n",
       "  (649, 1.4299999999999966),\n",
       "  (176, 1.4199999999999966),\n",
       "  (183, 1.4199999999999966),\n",
       "  (861, 1.4199999999999966),\n",
       "  (969, 1.4199999999999966),\n",
       "  (126, 1.4099999999999966),\n",
       "  (460, 1.4099999999999966),\n",
       "  (859, 1.4099999999999966),\n",
       "  (36, 1.3999999999999966),\n",
       "  (758, 1.3999999999999966),\n",
       "  (371, 1.3899999999999966),\n",
       "  (536, 1.3899999999999966),\n",
       "  (773, 1.3899999999999966),\n",
       "  (943, 1.3899999999999966),\n",
       "  (914, 1.3799999999999968),\n",
       "  (394, 1.3699999999999968),\n",
       "  (469, 1.3699999999999968),\n",
       "  (869, 1.3699999999999968),\n",
       "  (891, 1.3699999999999968),\n",
       "  (702, 1.3599999999999968),\n",
       "  (972, 1.3599999999999968),\n",
       "  (578, 1.3499999999999968),\n",
       "  (715, 1.3499999999999968),\n",
       "  (776, 1.3499999999999968),\n",
       "  (778, 1.3499999999999968),\n",
       "  (940, 1.3499999999999968),\n",
       "  (146, 1.3399999999999967),\n",
       "  (390, 1.3399999999999967),\n",
       "  (73, 1.329999999999997),\n",
       "  (710, 1.329999999999997),\n",
       "  (980, 1.329999999999997),\n",
       "  (731, 1.319999999999997),\n",
       "  (813, 1.309999999999997),\n",
       "  (742, 1.299999999999997),\n",
       "  (931, 1.299999999999997),\n",
       "  (686, 1.2799999999999971),\n",
       "  (700, 1.2699999999999971),\n",
       "  (541, 1.2499999999999971),\n",
       "  (911, 1.2499999999999971),\n",
       "  (345, 1.239999999999997),\n",
       "  (380, 1.239999999999997),\n",
       "  (534, 1.239999999999997),\n",
       "  (656, 1.239999999999997),\n",
       "  (59, 1.2299999999999973),\n",
       "  (374, 1.2199999999999973),\n",
       "  (747, 1.2199999999999973),\n",
       "  (929, 1.2199999999999973),\n",
       "  (34, 1.2099999999999973),\n",
       "  (158, 1.2099999999999973),\n",
       "  (266, 1.2099999999999973),\n",
       "  (775, 1.2099999999999973),\n",
       "  (729, 1.1999999999999973),\n",
       "  (862, 1.1999999999999973),\n",
       "  (103, 1.1899999999999973),\n",
       "  (830, 1.1699999999999975),\n",
       "  (405, 1.1599999999999975),\n",
       "  (442, 1.1599999999999975),\n",
       "  (622, 1.1599999999999975),\n",
       "  (633, 1.1599999999999975),\n",
       "  (111, 1.1499999999999975),\n",
       "  (965, 1.1499999999999975),\n",
       "  (32, 1.1399999999999975),\n",
       "  (856, 1.1399999999999975),\n",
       "  (465, 1.1299999999999977),\n",
       "  (660, 1.1299999999999977),\n",
       "  (744, 1.1299999999999977),\n",
       "  (924, 1.1299999999999977),\n",
       "  (708, 1.1199999999999977),\n",
       "  (967, 1.1099999999999977),\n",
       "  (459, 1.0799999999999979),\n",
       "  (478, 1.0799999999999979),\n",
       "  (644, 1.0799999999999979),\n",
       "  (976, 1.0699999999999978),\n",
       "  (516, 1.0599999999999978),\n",
       "  (653, 1.0599999999999978),\n",
       "  (400, 1.0499999999999978),\n",
       "  (942, 1.0499999999999978),\n",
       "  (675, 1.0399999999999978),\n",
       "  (438, 1.019999999999998),\n",
       "  (618, 1.009999999999998),\n",
       "  (901, 0.999999999999998),\n",
       "  (119, 0.9899999999999981),\n",
       "  (557, 0.9899999999999981),\n",
       "  (596, 0.9899999999999981),\n",
       "  (810, 0.9899999999999981),\n",
       "  (610, 0.9799999999999981),\n",
       "  (479, 0.9599999999999982),\n",
       "  (999, 0.9599999999999982),\n",
       "  (434, 0.9299999999999983),\n",
       "  (501, 0.9299999999999983),\n",
       "  (461, 0.9099999999999984),\n",
       "  (567, 0.9099999999999984),\n",
       "  (733, 0.9099999999999984),\n",
       "  (246, 0.8999999999999984),\n",
       "  (590, 0.8999999999999984),\n",
       "  (740, 0.8999999999999984),\n",
       "  (673, 0.8699999999999986),\n",
       "  (908, 0.8399999999999986),\n",
       "  (818, 0.8299999999999986),\n",
       "  (663, 0.8199999999999987),\n",
       "  (623, 0.8099999999999987),\n",
       "  (499, 0.7999999999999987),\n",
       "  (600, 0.7999999999999987),\n",
       "  (689, 0.7699999999999989),\n",
       "  (827, 0.729999999999999),\n",
       "  (899, 0.7199999999999991),\n",
       "  (482, 0.7099999999999991),\n",
       "  (728, 0.6999999999999991),\n",
       "  (970, 0.6999999999999991),\n",
       "  (836, 0.6499999999999992),\n",
       "  (493, 0.5999999999999994),\n",
       "  (68, 0.5399999999999997),\n",
       "  (961, 0.4899999999999999),\n",
       "  (841, 0.4699999999999999),\n",
       "  (838, 0.45999999999999996),\n",
       "  (167, 0.43000000000000005)],\n",
       " [594,\n",
       "  815,\n",
       "  839,\n",
       "  854,\n",
       "  904,\n",
       "  794,\n",
       "  580,\n",
       "  905,\n",
       "  828,\n",
       "  84,\n",
       "  556,\n",
       "  611,\n",
       "  489,\n",
       "  709,\n",
       "  545,\n",
       "  497,\n",
       "  363,\n",
       "  646,\n",
       "  781,\n",
       "  741,\n",
       "  288,\n",
       "  922,\n",
       "  688,\n",
       "  61,\n",
       "  806,\n",
       "  67,\n",
       "  824,\n",
       "  687,\n",
       "  109,\n",
       "  987,\n",
       "  893,\n",
       "  401,\n",
       "  292,\n",
       "  58,\n",
       "  718,\n",
       "  39,\n",
       "  509,\n",
       "  671,\n",
       "  82,\n",
       "  614,\n",
       "  721,\n",
       "  562,\n",
       "  955,\n",
       "  982,\n",
       "  420,\n",
       "  582,\n",
       "  858,\n",
       "  381,\n",
       "  581,\n",
       "  992,\n",
       "  701,\n",
       "  748,\n",
       "  116,\n",
       "  546,\n",
       "  664,\n",
       "  340,\n",
       "  791,\n",
       "  48,\n",
       "  293,\n",
       "  474,\n",
       "  588,\n",
       "  599,\n",
       "  698,\n",
       "  468,\n",
       "  343,\n",
       "  286,\n",
       "  383,\n",
       "  533,\n",
       "  703,\n",
       "  398,\n",
       "  848,\n",
       "  457,\n",
       "  716,\n",
       "  652,\n",
       "  37,\n",
       "  66,\n",
       "  219,\n",
       "  425,\n",
       "  565,\n",
       "  56,\n",
       "  409,\n",
       "  579,\n",
       "  162,\n",
       "  208,\n",
       "  819,\n",
       "  161,\n",
       "  424,\n",
       "  640,\n",
       "  327,\n",
       "  25,\n",
       "  184,\n",
       "  352,\n",
       "  679,\n",
       "  834,\n",
       "  555,\n",
       "  23,\n",
       "  135,\n",
       "  431,\n",
       "  237,\n",
       "  354,\n",
       "  917,\n",
       "  138,\n",
       "  382,\n",
       "  783,\n",
       "  754,\n",
       "  880,\n",
       "  234,\n",
       "  406,\n",
       "  458,\n",
       "  825,\n",
       "  851,\n",
       "  591,\n",
       "  213,\n",
       "  645,\n",
       "  334,\n",
       "  561,\n",
       "  203,\n",
       "  238,\n",
       "  289,\n",
       "  477,\n",
       "  526,\n",
       "  217,\n",
       "  868,\n",
       "  674,\n",
       "  300,\n",
       "  632,\n",
       "  102,\n",
       "  415,\n",
       "  314,\n",
       "  404,\n",
       "  743,\n",
       "  64,\n",
       "  284,\n",
       "  850,\n",
       "  866,\n",
       "  483,\n",
       "  779,\n",
       "  981,\n",
       "  49,\n",
       "  496,\n",
       "  281,\n",
       "  843,\n",
       "  751,\n",
       "  863,\n",
       "  99,\n",
       "  151,\n",
       "  274,\n",
       "  407,\n",
       "  738,\n",
       "  251,\n",
       "  185,\n",
       "  387,\n",
       "  867,\n",
       "  878,\n",
       "  50,\n",
       "  800,\n",
       "  211,\n",
       "  386,\n",
       "  722,\n",
       "  790,\n",
       "  918,\n",
       "  724,\n",
       "  788,\n",
       "  319,\n",
       "  361,\n",
       "  886,\n",
       "  218,\n",
       "  417,\n",
       "  584,\n",
       "  597,\n",
       "  661,\n",
       "  414,\n",
       "  491,\n",
       "  96,\n",
       "  926,\n",
       "  884,\n",
       "  260,\n",
       "  253,\n",
       "  570,\n",
       "  759,\n",
       "  349,\n",
       "  621,\n",
       "  973,\n",
       "  128,\n",
       "  520,\n",
       "  77,\n",
       "  168,\n",
       "  445,\n",
       "  762,\n",
       "  881,\n",
       "  155,\n",
       "  249,\n",
       "  76,\n",
       "  182,\n",
       "  259,\n",
       "  385,\n",
       "  549,\n",
       "  874,\n",
       "  132,\n",
       "  216,\n",
       "  498,\n",
       "  91,\n",
       "  191,\n",
       "  221,\n",
       "  495,\n",
       "  753,\n",
       "  963,\n",
       "  133,\n",
       "  24,\n",
       "  454,\n",
       "  508,\n",
       "  57,\n",
       "  257,\n",
       "  275,\n",
       "  355,\n",
       "  950,\n",
       "  71,\n",
       "  331,\n",
       "  359,\n",
       "  563,\n",
       "  197,\n",
       "  279,\n",
       "  280,\n",
       "  344,\n",
       "  453,\n",
       "  876,\n",
       "  186,\n",
       "  365,\n",
       "  316,\n",
       "  706,\n",
       "  752,\n",
       "  192,\n",
       "  766,\n",
       "  142,\n",
       "  159,\n",
       "  299,\n",
       "  455,\n",
       "  829,\n",
       "  890,\n",
       "  156,\n",
       "  201,\n",
       "  476,\n",
       "  490,\n",
       "  554,\n",
       "  669,\n",
       "  178,\n",
       "  328,\n",
       "  333,\n",
       "  538,\n",
       "  337,\n",
       "  692,\n",
       "  515,\n",
       "  625,\n",
       "  985,\n",
       "  330,\n",
       "  624,\n",
       "  642,\n",
       "  949,\n",
       "  215,\n",
       "  336,\n",
       "  998,\n",
       "  55,\n",
       "  72,\n",
       "  242,\n",
       "  263,\n",
       "  283,\n",
       "  313,\n",
       "  446,\n",
       "  447,\n",
       "  467,\n",
       "  511,\n",
       "  60,\n",
       "  157,\n",
       "  277,\n",
       "  440,\n",
       "  506,\n",
       "  939,\n",
       "  199,\n",
       "  269,\n",
       "  849,\n",
       "  655,\n",
       "  920,\n",
       "  971,\n",
       "  527,\n",
       "  52,\n",
       "  273,\n",
       "  882,\n",
       "  912,\n",
       "  239,\n",
       "  522,\n",
       "  934,\n",
       "  734,\n",
       "  892,\n",
       "  9,\n",
       "  110,\n",
       "  154,\n",
       "  410,\n",
       "  923,\n",
       "  41,\n",
       "  214,\n",
       "  308,\n",
       "  916,\n",
       "  46,\n",
       "  443,\n",
       "  627,\n",
       "  953,\n",
       "  100,\n",
       "  232,\n",
       "  548,\n",
       "  616,\n",
       "  842,\n",
       "  195,\n",
       "  245,\n",
       "  323,\n",
       "  568,\n",
       "  85,\n",
       "  134,\n",
       "  153,\n",
       "  171,\n",
       "  230,\n",
       "  318,\n",
       "  707,\n",
       "  984,\n",
       "  63,\n",
       "  631,\n",
       "  250,\n",
       "  294,\n",
       "  373,\n",
       "  411,\n",
       "  8,\n",
       "  205,\n",
       "  225,\n",
       "  256,\n",
       "  648,\n",
       "  792,\n",
       "  805,\n",
       "  888,\n",
       "  45,\n",
       "  62,\n",
       "  90,\n",
       "  243,\n",
       "  658,\n",
       "  823,\n",
       "  906,\n",
       "  946,\n",
       "  69,\n",
       "  189,\n",
       "  198,\n",
       "  290,\n",
       "  426,\n",
       "  870,\n",
       "  264,\n",
       "  603,\n",
       "  877,\n",
       "  129,\n",
       "  136,\n",
       "  177,\n",
       "  207,\n",
       "  332,\n",
       "  396,\n",
       "  735,\n",
       "  799,\n",
       "  954,\n",
       "  43,\n",
       "  181,\n",
       "  360,\n",
       "  464,\n",
       "  907,\n",
       "  298,\n",
       "  342,\n",
       "  847,\n",
       "  254,\n",
       "  711,\n",
       "  765,\n",
       "  713,\n",
       "  925,\n",
       "  927,\n",
       "  79,\n",
       "  144,\n",
       "  172,\n",
       "  227,\n",
       "  357,\n",
       "  727,\n",
       "  947,\n",
       "  978,\n",
       "  13,\n",
       "  130,\n",
       "  353,\n",
       "  681,\n",
       "  732,\n",
       "  737,\n",
       "  777,\n",
       "  986,\n",
       "  40,\n",
       "  312,\n",
       "  372,\n",
       "  770,\n",
       "  780,\n",
       "  821,\n",
       "  937,\n",
       "  88,\n",
       "  164,\n",
       "  367,\n",
       "  605,\n",
       "  662,\n",
       "  730,\n",
       "  812,\n",
       "  141,\n",
       "  339,\n",
       "  472,\n",
       "  564,\n",
       "  83,\n",
       "  104,\n",
       "  271,\n",
       "  651,\n",
       "  932,\n",
       "  370,\n",
       "  518,\n",
       "  532,\n",
       "  670,\n",
       "  787,\n",
       "  816,\n",
       "  15,\n",
       "  28,\n",
       "  38,\n",
       "  54,\n",
       "  124,\n",
       "  295,\n",
       "  364,\n",
       "  628,\n",
       "  746,\n",
       "  871,\n",
       "  118,\n",
       "  258,\n",
       "  261,\n",
       "  310,\n",
       "  7,\n",
       "  223,\n",
       "  419,\n",
       "  452,\n",
       "  471,\n",
       "  535,\n",
       "  789,\n",
       "  803,\n",
       "  0,\n",
       "  5,\n",
       "  65,\n",
       "  301,\n",
       "  47,\n",
       "  276,\n",
       "  462,\n",
       "  641,\n",
       "  801,\n",
       "  898,\n",
       "  21,\n",
       "  44,\n",
       "  487,\n",
       "  602,\n",
       "  650,\n",
       "  654,\n",
       "  695,\n",
       "  894,\n",
       "  226,\n",
       "  262,\n",
       "  291,\n",
       "  481,\n",
       "  539,\n",
       "  720,\n",
       "  993,\n",
       "  42,\n",
       "  51,\n",
       "  94,\n",
       "  113,\n",
       "  368,\n",
       "  375,\n",
       "  376,\n",
       "  510,\n",
       "  593,\n",
       "  855,\n",
       "  889,\n",
       "  202,\n",
       "  760,\n",
       "  785,\n",
       "  80,\n",
       "  180,\n",
       "  265,\n",
       "  750,\n",
       "  114,\n",
       "  278,\n",
       "  321,\n",
       "  547,\n",
       "  592,\n",
       "  617,\n",
       "  697,\n",
       "  756,\n",
       "  247,\n",
       "  421,\n",
       "  879,\n",
       "  131,\n",
       "  204,\n",
       "  612,\n",
       "  725,\n",
       "  798,\n",
       "  873,\n",
       "  123,\n",
       "  255,\n",
       "  378,\n",
       "  659,\n",
       "  668,\n",
       "  699,\n",
       "  763,\n",
       "  944,\n",
       "  210,\n",
       "  577,\n",
       "  595,\n",
       "  714,\n",
       "  529,\n",
       "  619,\n",
       "  14,\n",
       "  145,\n",
       "  598,\n",
       "  826,\n",
       "  174,\n",
       "  193,\n",
       "  212,\n",
       "  441,\n",
       "  630,\n",
       "  322,\n",
       "  346,\n",
       "  377,\n",
       "  769,\n",
       "  831,\n",
       "  844,\n",
       "  996,\n",
       "  335,\n",
       "  392,\n",
       "  492,\n",
       "  988,\n",
       "  991,\n",
       "  148,\n",
       "  384,\n",
       "  388,\n",
       "  391,\n",
       "  402,\n",
       "  436,\n",
       "  485,\n",
       "  604,\n",
       "  229,\n",
       "  233,\n",
       "  285,\n",
       "  303,\n",
       "  347,\n",
       "  399,\n",
       "  626,\n",
       "  846,\n",
       "  30,\n",
       "  475,\n",
       "  519,\n",
       "  620,\n",
       "  694,\n",
       "  772,\n",
       "  962,\n",
       "  995,\n",
       "  18,\n",
       "  127,\n",
       "  433,\n",
       "  500,\n",
       "  572,\n",
       "  764,\n",
       "  990,\n",
       "  78,\n",
       "  187,\n",
       "  311,\n",
       "  615,\n",
       "  802,\n",
       "  910,\n",
       "  935,\n",
       "  938,\n",
       "  70,\n",
       "  200,\n",
       "  413,\n",
       "  466,\n",
       "  573,\n",
       "  98,\n",
       "  272,\n",
       "  302,\n",
       "  351,\n",
       "  542,\n",
       "  636,\n",
       "  975,\n",
       "  1,\n",
       "  137,\n",
       "  139,\n",
       "  209,\n",
       "  397,\n",
       "  444,\n",
       "  92,\n",
       "  120,\n",
       "  170,\n",
       "  448,\n",
       "  726,\n",
       "  761,\n",
       "  27,\n",
       "  160,\n",
       "  439,\n",
       "  514,\n",
       "  524,\n",
       "  543,\n",
       "  553,\n",
       "  22,\n",
       "  95,\n",
       "  108,\n",
       "  252,\n",
       "  379,\n",
       "  427,\n",
       "  537,\n",
       "  637,\n",
       "  745,\n",
       "  865,\n",
       "  248,\n",
       "  348,\n",
       "  395,\n",
       "  449,\n",
       "  704,\n",
       "  206,\n",
       "  236,\n",
       "  350,\n",
       "  456,\n",
       "  607,\n",
       "  814,\n",
       "  921,\n",
       "  983,\n",
       "  12,\n",
       "  105,\n",
       "  224,\n",
       "  864,\n",
       "  957,\n",
       "  31,\n",
       "  125,\n",
       "  169,\n",
       "  228,\n",
       "  307,\n",
       "  358,\n",
       "  470,\n",
       "  560,\n",
       "  576,\n",
       "  860,\n",
       "  11,\n",
       "  17,\n",
       "  194,\n",
       "  220,\n",
       "  235,\n",
       "  853,\n",
       "  900,\n",
       "  19,\n",
       "  75,\n",
       "  163,\n",
       "  488,\n",
       "  531,\n",
       "  933,\n",
       "  968,\n",
       "  240,\n",
       "  587,\n",
       "  609,\n",
       "  872,\n",
       "  86,\n",
       "  196,\n",
       "  685,\n",
       "  723,\n",
       "  820,\n",
       "  840,\n",
       "  97,\n",
       "  325,\n",
       "  647,\n",
       "  845,\n",
       "  997,\n",
       "  53,\n",
       "  121,\n",
       "  822,\n",
       "  977,\n",
       "  87,\n",
       "  504,\n",
       "  677,\n",
       "  796,\n",
       "  989,\n",
       "  3,\n",
       "  106,\n",
       "  112,\n",
       "  305,\n",
       "  362,\n",
       "  428,\n",
       "  784,\n",
       "  974,\n",
       "  149,\n",
       "  774,\n",
       "  807,\n",
       "  909,\n",
       "  143,\n",
       "  244,\n",
       "  418,\n",
       "  451,\n",
       "  473,\n",
       "  505,\n",
       "  513,\n",
       "  122,\n",
       "  719,\n",
       "  852,\n",
       "  956,\n",
       "  93,\n",
       "  268,\n",
       "  683,\n",
       "  786,\n",
       "  608,\n",
       "  739,\n",
       "  755,\n",
       "  757,\n",
       "  150,\n",
       "  166,\n",
       "  682,\n",
       "  705,\n",
       "  964,\n",
       "  574,\n",
       "  634,\n",
       "  768,\n",
       "  951,\n",
       "  10,\n",
       "  107,\n",
       "  317,\n",
       "  324,\n",
       "  329,\n",
       "  416,\n",
       "  629,\n",
       "  809,\n",
       "  101,\n",
       "  267,\n",
       "  566,\n",
       "  690,\n",
       "  857,\n",
       "  979,\n",
       "  306,\n",
       "  326,\n",
       "  341,\n",
       "  569,\n",
       "  691,\n",
       "  771,\n",
       "  833,\n",
       "  837,\n",
       "  959,\n",
       "  81,\n",
       "  575,\n",
       "  586,\n",
       "  665,\n",
       "  811,\n",
       "  945,\n",
       "  117,\n",
       "  429,\n",
       "  528,\n",
       "  657,\n",
       "  767,\n",
       "  804,\n",
       "  304,\n",
       "  450,\n",
       "  895,\n",
       "  89,\n",
       "  241,\n",
       "  270,\n",
       "  432,\n",
       "  544,\n",
       "  639,\n",
       "  2,\n",
       "  29,\n",
       "  179,\n",
       "  422,\n",
       "  530,\n",
       "  571,\n",
       "  684,\n",
       "  835,\n",
       "  883,\n",
       "  948,\n",
       "  4,\n",
       "  16,\n",
       "  412,\n",
       "  552,\n",
       "  832,\n",
       "  140,\n",
       "  393,\n",
       "  463,\n",
       "  507,\n",
       "  672,\n",
       "  782,\n",
       "  115,\n",
       "  356,\n",
       "  366,\n",
       "  676,\n",
       "  930,\n",
       "  936,\n",
       "  173,\n",
       "  320,\n",
       "  437,\n",
       "  680,\n",
       "  35,\n",
       "  165,\n",
       "  717,\n",
       "  902,\n",
       "  26,\n",
       "  190,\n",
       "  693])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, hist, tk = diversity(learn, 10, 95)\n",
    "n, hist, tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAgk-YyWc3rG"
   },
   "outputs": [],
   "source": [
    "# learn.recorder.plot_losses()\n",
    "# learn.recorder.plot_lr()\n",
    "# learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTHG4Bt7VDYp"
   },
   "outputs": [],
   "source": [
    "fooling_rates = []\n",
    "model = learn.model.eval()\n",
    "learn.metrics = [validation_single_perturbation]\n",
    "for i in range(10):\n",
    "  global_perturbations = model(torch.rand(1, 3, 224, 244).cuda())[0]\n",
    "  nag_util.global_perturbations = global_perturbations\n",
    "  fooling_rates.append(learn.validate()[1].cpu().item())\n",
    "  print(\"%d : %f\"%(i, fooling_rates[-1]))\n",
    "\n",
    "mean = np.mean(fooling_rates)\n",
    "stddev = np.std(fooling_rates)\n",
    "print(mean, stddev); print(fooling_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OFCjzI7UaY3C"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[4][0]\n",
    "x = x_img.data.cuda()\n",
    "z = torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.float32).cuda()\n",
    "# z = torch.empty(z_dim).uniform_(-1,1).cuda()\n",
    "p = model.forward_single_z(z).detach()\n",
    "x = normalize(x)\n",
    "\n",
    "p_x = x + p\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0., 1.])\n",
    "p_img = Image(p)\n",
    "# x_img.show()\n",
    "p_img.show()\n",
    "# p_x_img.show()\n",
    "\n",
    "# print_range(p)\n",
    "# print_range(x)\n",
    "# print_range(p_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzwsI2P1ZANz"
   },
   "outputs": [],
   "source": [
    "z1 = torch.tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p1 = model.forward_single_z(z1)\n",
    "\n",
    "z2 = torch.tensor([1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p2 = model.forward_single_z(z2)\n",
    "\n",
    "z3 = torch.tensor([1, 1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p3 = model.forward_single_z(z3)\n",
    "\n",
    "l2_distance(p1, p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eroI82OKSnAL"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[4][0]\n",
    "x = x_img.data[None].cuda()\n",
    "p = model(x)[0].squeeze().detach() \n",
    "x = x.squeeze()\n",
    "x = normalize(x)\n",
    "\n",
    "p_x = x + p\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0.,1.])\n",
    "p_img = Image(p)\n",
    "# x_img.show()\n",
    "p_img.show()\n",
    "# p_x_img.show()\n",
    "\n",
    "print_range(p)\n",
    "print_range(x)\n",
    "print_range(p_x)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NAG-tripletLossExperiment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
