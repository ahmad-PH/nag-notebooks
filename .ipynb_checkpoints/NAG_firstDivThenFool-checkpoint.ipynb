{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ahmad-PH/nag-notebooks/blob/master/NAG_tripletLossExperiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqeZpz16do4y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up-to-date.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os; import subprocess\n",
    "\n",
    "def detect_env():\n",
    "    return 'colab' if 'content' in os.listdir('/') else 'IBM'\n",
    "  \n",
    "def run_shell_command(cmd):\n",
    "  p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "  print(str(p.communicate()[0], 'utf-8'))\n",
    "  \n",
    "if detect_env() == 'colab': root_folder = '/content'\n",
    "elif detect_env() == 'IBM' : root_folder = '/root/Derakhshani/adversarial'\n",
    "python_files_path = root_folder + '/nag-public'\n",
    "\n",
    "if os.path.isdir(python_files_path):\n",
    "  initial_dir = os.getcwd()\n",
    "  os.chdir(python_files_path)\n",
    "  run_shell_command('git pull')\n",
    "  os.chdir(initial_dir)\n",
    "else:\n",
    "  os.chdir('/root/Derakhshani/adversarial')\n",
    "  run_shell_command('git clone https://github.com/ahmad-PH/nag-public.git')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(python_files_path + '/NAG-11May-beforeDenoiser')\n",
    "\n",
    "from nag_util import *\n",
    "import nag_util\n",
    "from environment import *\n",
    "from visualization import *\n",
    "\n",
    "env = create_env()\n",
    "env.setup(cuda_index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Ev7jcRKoARg"
   },
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.imports import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys; import os; import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralRelu(nn.Module):\n",
    "  def __init__(self, leak=None, sub=None, maxv=None):\n",
    "    super().__init__()\n",
    "    self.leak,self.sub,self.maxv = leak,sub,maxv\n",
    "\n",
    "  def forward(self, x): \n",
    "    x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n",
    "    if self.sub is not None: x.sub_(self.sub)\n",
    "    if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "    return x\n",
    "  \n",
    "class deconv_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k_size = (4,4), s = (2,2), pad = (1,1), b = True, activation = True):\n",
    "        super(deconv_layer, self).__init__()\n",
    "\n",
    "        self.CT2d = nn.ConvTranspose2d(in_channels = in_ch,\n",
    "                                  out_channels = out_ch,\n",
    "                                  kernel_size = k_size,\n",
    "                                  stride = s, \n",
    "                                  padding = pad,\n",
    "                                  bias = b)\n",
    "        self.BN2d = nn.BatchNorm2d(out_ch)\n",
    "        \n",
    "        self.activation = activation\n",
    "        if self.activation:\n",
    "            self.relu = GeneralRelu(0, 0.2, 5)\n",
    "        \n",
    "        self.weight_init()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.activation:\n",
    "            return self.relu(self.BN2d(self.CT2d(input)))\n",
    "        else:\n",
    "            return self.BN2d(self.CT2d(input))\n",
    "\n",
    "    def weight_init(self):\n",
    "        self.CT2d.weight.data.normal_(mean = 0, std = 0.02)\n",
    "        self.CT2d.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Gen(nn.Module):\n",
    "#   def __init__(self, z_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "#     super(Gen, self).__init__()\n",
    "\n",
    "#     self.bs = None\n",
    "#     self.z_dim = z_dim\n",
    "#     self.gf_dim = gf_dim\n",
    "#     self.y_dim = y_dim\n",
    "#     self.df_dim = df_dim\n",
    "#     self.image_shape = image_shape\n",
    "\n",
    "#     self.z_ = nn.Linear(self.z_dim, self.gf_dim * 7 * 4 * 4, bias=True)\n",
    "#     self.z_.bias.data.fill_(0)\n",
    "#     self.BN_ = nn.BatchNorm2d(self.gf_dim * 7)\n",
    "\n",
    "#     self.half = max(self.gf_dim // 2, 1) \n",
    "#     self.quarter = max(self.gf_dim // 4, 1)\n",
    "#     self.eighth = max(self.gf_dim // 8, 1)\n",
    "#     # sixteenth = max(self.gf_dim // 16, 1)\n",
    "    \n",
    "#     self.CT2d_1 = deconv_layer(self.gf_dim * 8, self.gf_dim * 4, k_size = (5,5), pad = (2,2))\n",
    "#     self.CT2d_2 = deconv_layer(self.gf_dim * 5, self.gf_dim * 2)    \n",
    "#     self.CT2d_3 = deconv_layer(self.gf_dim * 2 + self.half, self.gf_dim * 1)\n",
    "#     self.CT2d_4 = deconv_layer(self.gf_dim * 1 + self.quarter, self.gf_dim * 1)\n",
    "#     self.CT2d_5 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "#     self.CT2d_6 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "#     self.CT2d_7 = deconv_layer(self.gf_dim * 1 + self.eighth, 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "#   def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "#     h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "#     h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "#     output = deconv_layer(h_input)\n",
    "#     assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "#             \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "#             \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "#     return output\n",
    "  \n",
    "#   def forward_z(self, z):\n",
    "#     self.bs = z.shape[0]\n",
    "    \n",
    "#     h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "#     assert h0.shape[2:] == (4, 4), \"Unexpected shape, it shoud be (4,4)\"\n",
    "\n",
    "#     h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "#     h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "#     h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "#     h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "#     h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "#     h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "#     h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "#     ksi = 10.0\n",
    "#     output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "#     # this coeff scales the output to be appropriate for images that are \n",
    "#     # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "#     # interval)\n",
    "#     return output_coeff * torch.tanh(h7)\n",
    "  \n",
    "# #   # blind-selection\n",
    "#   def forward(self, inputs):\n",
    "#     self.bs = inputs.shape[0]\n",
    "\n",
    "#     benign_preds_onehot = arch(inputs)\n",
    "#     benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "    \n",
    "#     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "#     for i in range(self.bs):\n",
    "#       random_label = self.randint(0,1000, exclude = benign_preds[i].item())\n",
    "#       z[i][random_label] = 1.\n",
    "    \n",
    "#     z_out = self.forward_z(z)\n",
    "    \n",
    "#     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "\n",
    "# #   #second-best selection: made validation so much worse\n",
    "# #   def forward(self, inputs):\n",
    "# #     self.bs = inputs.shape[0]\n",
    "\n",
    "# #     benign_preds_onehot = arch(inputs)\n",
    "# #     target_preds = torch.topk(benign_preds_onehot, 2, dim = 1).indices[:, 1:]\n",
    "    \n",
    "# #     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "# #     for i in range(self.bs):\n",
    "# #       z[i][target_preds[i]] = 1.\n",
    "    \n",
    "# #     z_out = self.forward_z(z)\n",
    "    \n",
    "# #     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "  \n",
    "# #    def forward(self, inputs):\n",
    "# #     self.bs = inputs.shape[0]\n",
    "\n",
    "# #     benign_preds_onehot = arch(inputs)\n",
    "# #     benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "    \n",
    "# #     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "# #     random_label = self.randint(0,1000, exclude = benign_preds.tolist())\n",
    "# #     for i in range(self.bs):\n",
    "# #       z[i][random_label] = 1.\n",
    "    \n",
    "# #     z_out = self.forward_z(z)\n",
    "    \n",
    "# #     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "  \n",
    "#   @staticmethod\n",
    "#   def randint(low, high, exclude):\n",
    "#     temp = np.random.randint(low, high - 1)\n",
    "#     if temp == exclude:\n",
    "#       temp = temp + 1\n",
    "#     return temp\n",
    "  \n",
    "#   def forward_single_z(self, z):\n",
    "#     return self.forward_z(z[None]).squeeze()\n",
    "           \n",
    "  \n",
    "#   def make_triplet_samples(self, z, margin, r2, r3):\n",
    "#     positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "#     negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "#     return positive_sample, negative_sample\n",
    "\n",
    "#   def random_vector_surface(self, shape, r = 1.):\n",
    "#     mat = torch.randn(size=shape).cuda()\n",
    "#     norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "#     return (mat/norm) * r\n",
    "\n",
    "  \n",
    "#   def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "#     fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "#     fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "#     fraction.unsqueeze_(-1)\n",
    "#     return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "#   def make_z(self, in_shape):\n",
    "#     return torch.empty(in_shape).cuda().uniform_(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-targeted Gen\n",
    "class Gen(nn.Module):\n",
    "  def __init__(self, z_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "    super(Gen, self).__init__()\n",
    "\n",
    "    self.bs = None\n",
    "    self.z_dim = z_dim\n",
    "    self.gf_dim = gf_dim\n",
    "    self.y_dim = y_dim\n",
    "    self.df_dim = df_dim\n",
    "    self.image_shape = image_shape\n",
    "\n",
    "    self.z_ = nn.Linear(self.z_dim, self.gf_dim * 7 * 4 * 4, bias=True)\n",
    "    self.z_.bias.data.fill_(0)\n",
    "    self.BN_ = nn.BatchNorm2d(self.gf_dim * 7)\n",
    "\n",
    "    self.half = max(self.gf_dim // 2, 1) \n",
    "    self.quarter = max(self.gf_dim // 4, 1)\n",
    "    self.eighth = max(self.gf_dim // 8, 1)\n",
    "    # sixteenth = max(self.gf_dim // 16, 1)\n",
    "\n",
    "    self.CT2d_1 = deconv_layer(self.gf_dim * 8, self.gf_dim * 4, k_size = (5,5), pad = (2,2))\n",
    "    self.CT2d_2 = deconv_layer(self.gf_dim * 5, self.gf_dim * 2)    \n",
    "    self.CT2d_3 = deconv_layer(self.gf_dim * 2 + self.half, self.gf_dim * 1)\n",
    "    self.CT2d_4 = deconv_layer(self.gf_dim * 1 + self.quarter, self.gf_dim * 1)\n",
    "    self.CT2d_5 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "    self.CT2d_6 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "    self.CT2d_7 = deconv_layer(self.gf_dim * 1 + self.eighth, 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "  def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "    h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "    h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "    output = deconv_layer(h_input)\n",
    "    assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "            \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "            \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "    return output\n",
    "  \n",
    "  def forward_z(self, z):\n",
    "    self.bs = z.shape[0]\n",
    "    \n",
    "    h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "    assert h0.shape[2:] == (4, 4), \"Non-expected shape, it shoud be (4,4)\"\n",
    "\n",
    "    h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "    h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "    h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "    h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "    h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "    h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "    h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "    ksi = 10.0\n",
    "    output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "    # this coeff scales the output to be appropriate for images that are \n",
    "    # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "    # interval)\n",
    "    return output_coeff * torch.tanh(h7)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    self.bs = inputs.shape[0]\n",
    "    z = inputs.new_empty([self.bs, self.z_dim]).uniform_(-1,1).cuda()\n",
    "    p, n = self.make_triplet_samples(z, 0.1, 0.1, 2.)\n",
    "    \n",
    "    z_out = self.forward_z(z)\n",
    "    p_out = self.forward_z(p)\n",
    "    n_out = self.forward_z(n)\n",
    "    \n",
    "    return z_out, p_out, n_out, inputs\n",
    "#     return z_out, None, None, inputs\n",
    "  \n",
    "  def forward_single_z(self, z):\n",
    "    return self.forward_z(z[None]).squeeze()\n",
    "  \n",
    "  def generate_single_noise(self):\n",
    "    z = torch.empty(self.z_dim).uniform_(-1,1).cuda()\n",
    "    return self.forward_single_z(z)\n",
    "           \n",
    "  \n",
    "  def make_triplet_samples(self, z, margin, r2, r3):\n",
    "    positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "    negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "    return positive_sample, negative_sample\n",
    "\n",
    "  def random_vector_surface(self, shape, r = 1.):\n",
    "    mat = torch.randn(size=shape).cuda()\n",
    "    norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "    return (mat/norm) * r\n",
    "\n",
    "  \n",
    "  def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "    fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "    fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "    fraction.unsqueeze_(-1)\n",
    "    return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "  def make_z(self, in_shape):\n",
    "    return torch.empty(in_shape).cuda().uniform_(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def js_distance(x1, x2):\n",
    "  m = 0.5 * (x1 + x2)\n",
    "  return 0.5 * (F.kl_div(x1, m) + F.kl_div(x2, m))\n",
    "\n",
    "def kl_distance(x1, x2):\n",
    "  inp = torch.log(x1)\n",
    "  target = x2\n",
    "  return F.kl_div(inp, target, reduction='batchmean')\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  x1 = tensorify(x1)\n",
    "  x2 = tensorify(x2)\n",
    "  x1 = x1 / torch.sum(x1)\n",
    "  x2 = x2 / torch.sum(x2)\n",
    "  return kl_distance(x1[None], x2[None])\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  if not isinstance(x1, torch.Tensor): x1 = torch.tensor(x1)\n",
    "  if not isinstance(x2, torch.Tensor): x2 = torch.tensor(x2)\n",
    "  x1 = x1 * 100. / torch.sum(x1)\n",
    "  x2 = x2 * 100. / torch.sum(x2)\n",
    "  return torch.norm(x1 - x2, 2)\n",
    "\n",
    "def distance_from_uniform(x):\n",
    "  return distrib_distance(x, [1.] * len(x))\n",
    "\n",
    "def wasserstein_distance(x1, x2):\n",
    "  return torch.mean(x1 - x2)\n",
    "\n",
    "def l1_distance(x1, x2):\n",
    "  return F.l1_loss(x1, x2)\n",
    "\n",
    "def l2_distance(x1, x2):\n",
    "  return F.mse_loss(x1 * 10, x2 * 10)\n",
    "\n",
    "def mse_loss(x1, x2):\n",
    "  return F.mse_loss(x1, x2)\n",
    "\n",
    "def cos_distance(x1, x2, dim = 1):\n",
    "  return -1 * torch.mean(F.cosine_similarity(x1, x2, dim=dim))\n",
    "\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, distance_func, margin):\n",
    "  # max distance when using l1_distance is 2\n",
    "  # max distacne when using l2-distance is sqrt(2)\n",
    "#   print(\"anchor: \", anchor.min(), anchor.max())\n",
    "  ap_dist = distance_func(anchor, positive)\n",
    "  an_dist = distance_func(anchor, negative)\n",
    "\n",
    "  triplet_loss.call_count += 1\n",
    "  if triplet_loss.call_count % 200 == 0 : #and anchor.shape[1] == 1000:\n",
    "#     print(\"a: \", end=\"\"); print_big_vector(anchor[0])\n",
    "#     print(\"p: \", end=\"\"); print_big_vector(positive[0])\n",
    "#     print(\"n: \", end=\"\"); print_big_vector(negative[0])\n",
    "    print(\"func:{}, ap_dist: {}, an_dist: {}\".format(distance_func.__name__, ap_dist, an_dist))\n",
    "    \n",
    "  return torch.mean(F.relu(ap_dist - an_dist + margin))\n",
    "\n",
    "triplet_loss.call_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diversity_loss(input, target):\n",
    "#   return -1 * torch.mean(torch.pow(f_x_a-f_x_s,2))\n",
    "  if input.shape[0] != batch_size:\n",
    "    print(\"input shape: \", input.shape)\n",
    "    print(\"target shape: \", target.shape, \"\\n\\n\")\n",
    "  return torch.mean(F.cosine_similarity(\n",
    "    input.view([batch_size, -1]),\n",
    "    target.view([batch_size, -1]), \n",
    "  ))\n",
    "\n",
    "\n",
    "def fool_loss(input, target):\n",
    "  true_class = torch.argmax(target, dim=1).view(-1,1).long().cuda()\n",
    "  target_probabilities = input.gather(1, true_class)\n",
    "  epsilon = 1e-10\n",
    "  result =  torch.mean(-1 * torch.log(1 - target_probabilities + epsilon))\n",
    "  \n",
    "  fool_loss.call_count += 1\n",
    "  if fool_loss.call_count % 200 == 0:\n",
    "    print(\"target probs {}, loss: {}: \".format(target_probabilities, result))\n",
    "    \n",
    "  return result\n",
    "\n",
    "fool_loss.call_count = 0\n",
    "\n",
    "# def fool_loss(model_output, target_labels):\n",
    "#   target_labels = target_labels.view(-1, 1).long().cuda()\n",
    "#   target_probabilities = model_output.gather(1, target_labels)\n",
    "#   epsilon = 1e-10\n",
    "#   # highest possible fool_loss is - log(1e-10) == 23\n",
    "#   result = torch.mean(-1 * torch.log(target_probabilities + epsilon))\n",
    "  \n",
    "#   global fool_loss_count\n",
    "#   fool_loss_count += 1\n",
    "#   if fool_loss_count % 20 == 0:\n",
    "#     print(\"target probs {}, loss: {}: \".format(target_probabilities, result))\n",
    "  \n",
    "#   return result\n",
    "\n",
    "# # targeted \n",
    "# def validation(gen_output, target):\n",
    "#   perturbations, _, _, clean_images, _, _ = gen_output\n",
    "#   return validation_(perturbations, clean_images)\n",
    "\n",
    "def validation(gen_output, target):\n",
    "  perturbations, _, _, clean_images = gen_output\n",
    "  return validation_(perturbations, clean_images)\n",
    "\n",
    "unfooled_histogram = np.array([0.] * 1000)\n",
    "valid_cnt = 0\n",
    "\n",
    "def validation_(perturbations, clean_images):\n",
    "  perturbed_images = clean_images + perturbations\n",
    "  benign_preds = torch.argmax(arch(clean_images), 1)\n",
    "  adversary_preds = torch.argmax(arch(perturbed_images), 1)\n",
    "\n",
    "#   print('benign, adversary, unfooled')\n",
    "#   print(benign_preds)\n",
    "#   print(adversary_preds)\n",
    "\n",
    "  is_unfooled = (benign_preds == adversary_preds)\n",
    "  for i , unfooled in enumerate(is_unfooled):\n",
    "    if unfooled == 1:\n",
    "      unfooled_histogram[benign_preds[i]] += 1\n",
    "  \n",
    "  global valid_cnt\n",
    "  valid_cnt += 1\n",
    "  if valid_cnt % 10 == 0:\n",
    "    indexed = [(i, u) for i, u in enumerate(unfooled_histogram)]\n",
    "    summarized = list(filter(lambda x: x[1] > 0.0, indexed))\n",
    "    percent = [(i, 100. * u / np.sum(unfooled_histogram)) for i, u in summarized]\n",
    "    print('\\nhist: ')\n",
    "    print(sorted(summarized, key=lambda x: x[1], reverse = True))\n",
    "    print('\\npercent: ')\n",
    "    print(sorted(percent, key =lambda x: x[1], reverse = True))\n",
    "    print('\\n')\n",
    "    \n",
    "  return (benign_preds != adversary_preds).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FeatureLoss(nn.Module):\n",
    "#     def __name__(self):\n",
    "#       return \"feature_loss\"\n",
    "  \n",
    "#     def __init__(self, dis, layers, layer_weights):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # define generator here \n",
    "#         self.dis = dis\n",
    "#         self.diversity_layers = layers\n",
    "#         self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "#         self.weights = layer_weights\n",
    "#         self.metric_names = [\"fool_loss\"] #+ [f\"div_loss_{i}\" for i in range(len(layers))] #maybe Gram\n",
    "# #         self.triplet_hooks = hook_outputs([arch.m.features[4]], detach=False)\n",
    "    \n",
    "#     def make_features(self, x, clone=False):\n",
    "#         y = self.dis(x)\n",
    "#         return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "  \n",
    "#     def forward(self, inp, target):\n",
    "#       sigma_B, _, _, X_B, B_Y, z = inp\n",
    "\n",
    "#       X_A = X_B + sigma_B\n",
    "# #       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "#       A_Y, A_feat = self.make_features(X_A)\n",
    "# #       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "#       chosen_labels = z.argmax(dim=1)\n",
    "#       fooling_loss =  fool_loss(A_Y, chosen_labels)\n",
    "\n",
    "# #       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "# #       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "# #       self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "# #       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "#       self.losses = [fooling_loss]\n",
    "#       self.metrics = dict(zip(self.metric_names, [fooling_loss]))\n",
    "\n",
    "#       return sum(self.losses)\n",
    "  \n",
    "#     def add_perturbation_shuffled(self, inp, perturbation):\n",
    "# #         j = torch.randperm(inp.shape[0])\n",
    "#         j = derangement(inp.shape[0])\n",
    "#         return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-targeted FeatureLoss\n",
    "class FeatureLoss(nn.Module):\n",
    "    def __name__(self):\n",
    "      return \"feature_loss\"\n",
    "  \n",
    "    def __init__(self, dis, layers, layer_weights):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dis = dis\n",
    "        self.diversity_layers = layers\n",
    "        self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "        self.weights = layer_weights\n",
    "        \n",
    "        self.metric_names = [\"fool_loss\"] + [f\"div_loss_{i}\" for i in range(len(layers))] + ['triplet_loss']# Maybe Gram\n",
    "#         self.metric_names = [\"fool_loss\"] + ['triplet_loss']# Maybe Gram\n",
    "#         self.metric_names = [\"fool_loss\"] + ['div_loss']# Maybe Gram\n",
    "        self.triplet_weight = 4.\n",
    "        self.div_weight = 1.\n",
    "        self.fooling_weight = 2.\n",
    "    \n",
    "    def make_features(self, x, clone=False):\n",
    "        y = self.dis(x)\n",
    "        return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "    \n",
    "#     triplet loss\n",
    "    def forward(self, inp, target):\n",
    "        sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "        X_A = self.add_perturbation(X_B, sigma_B) \n",
    "        X_A_pos = self.add_perturbation(X_B, sigma_pos)\n",
    "        X_A_neg = self.add_perturbation(X_B, sigma_neg) \n",
    "        \n",
    "        X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "        \n",
    "        B_Y, _ = self.make_features(X_B)\n",
    "        A_Y, A_feat = self.make_features(X_A)\n",
    "        _, S_feat = self.make_features(X_S)\n",
    "        pos_softmax, _ = self.make_features(X_A_pos)\n",
    "        neg_softmax, _ = self.make_features(X_A_neg)\n",
    "        \n",
    "        raw_fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "        weighted_fooling_loss = self.fooling_weight * raw_fooling_loss\n",
    "      \n",
    "        raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "        weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "        raw_triplet_loss = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "        weighted_triplet_loss = raw_triplet_loss * self.triplet_weight\n",
    "    \n",
    "#         self.losses = weighted_diversity_losses + [weighted_triplet_loss]\n",
    "#         self.metrics = dict(zip(self.metric_names, raw_diversity_losses + [raw_triplet_loss]))\n",
    "        \n",
    "        self.losses = [weighted_fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss]\n",
    "        raw_losses = [raw_fooling_loss] + raw_diversity_losses + [raw_triplet_loss]\n",
    "        if len(self.metric_names) != len(raw_losses):\n",
    "          raise Exception(\"length of metric names unequals length of losses\")\n",
    "        self.metrics = dict(zip(self.metric_names, raw_losses))\n",
    "        \n",
    "#         self.losses = [fooling_loss] + [weighted_triplet_loss]\n",
    "#         self.metrics = dict(zip(self.metric_names, [fooling_loss] + [raw_triplet_loss]))\n",
    "        \n",
    "        return sum(self.losses)\n",
    "\n",
    "\n",
    "#     #use two types of triplet losses\n",
    "#     def forward(self, inp, target):\n",
    "#       sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "#       X_A = self.add_perturbation(X_B, sigma_B) \n",
    "#       X_A_pos = self.add_perturbation(X_B, sigma_pos)\n",
    "#       X_A_neg = self.add_perturbation(X_B, sigma_neg) \n",
    "\n",
    "#       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "#       B_Y, _ = self.make_features(X_B)\n",
    "#       A_Y, A_feat = self.make_features(X_A)\n",
    "#       _, S_feat = self.make_features(X_S)\n",
    "#       pos_softmax, _ = self.make_features(X_A_pos)\n",
    "#       neg_softmax, _ = self.make_features(X_A_neg)\n",
    "\n",
    "#       fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "#       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "#       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "      \n",
    "#       raw_triplet_loss_sm = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "#       weighted_triplet_loss_sm = raw_triplet_loss_sm * self.triplet_weight_sm\n",
    "      \n",
    "#       raw_triplet_loss_noise = triplet_loss(sigma_B, sigma_pos, sigma_neg, l2_distance, 5.)\n",
    "#       weighted_triplet_loss_noise = raw_triplet_loss_noise * self.triplet_weight_noise\n",
    "\n",
    "#       self.losses = [fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss_sm, weighted_triplet_loss_noise] \n",
    "#       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses + [weighted_triplet_loss_sm, weighted_triplet_loss_noise]))\n",
    "\n",
    "#       return sum(self.losses)\n",
    "\n",
    "#     # just fooling and diversity\n",
    "#     def forward(self, inp, target):\n",
    "#       sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "#       X_A = self.add_perturbation(X_B, sigma_B) \n",
    "\n",
    "#       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "#       B_Y, _ = self.make_features(X_B)\n",
    "#       A_Y, A_feat = self.make_features(X_A)\n",
    "#       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "#       fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "#       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "#       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "#       self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "#       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "#       return sum(self.losses)\n",
    "  \n",
    "  \n",
    "    def add_perturbation(self, inp, perturbation):\n",
    "        return inp.add(perturbation)\n",
    "  \n",
    "    def add_perturbation_shuffled(self, inp, perturbation):\n",
    "        j = derangement(inp.shape[0])\n",
    "        return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def produce_summary(root_folder, n_files):\n",
    "  def writeline(file, values, fmt_string):\n",
    "    file.write(', '.join(fmt_string.format(v) for v in values) + '\\n')\n",
    "  \n",
    "  last_rows = []\n",
    "  for i in range(n_files):\n",
    "    prefix = '/root/Derakhshani/adversarial/textual_notes/CSVs'\n",
    "    df = pd.read_csv(\"{}/{}/{}.csv\".format(prefix, root_folder, i))\n",
    "    last_rows.append(df.iloc[-1][1:-1].values.tolist())\n",
    "  \n",
    "  last_rows = np.array(last_rows)\n",
    "  \n",
    "  labels = list(df.columns[1:-1])\n",
    "  means = np.mean(last_rows, axis=0).tolist()\n",
    "  outfile = open('{}/{}/summary.txt'.format(prefix, root_folder), 'w+')\n",
    "  outfile.write('means: \\n')\n",
    "  writeline(outfile, labels, '{: >11}')\n",
    "  writeline(outfile, means, '{: >11.3}')\n",
    "  outfile.write('\\n')\n",
    "      \n",
    "  operations = []\n",
    "  for column in df.columns[1:-1]:\n",
    "    if column in ['train_loss', 'valid_loss', 'fool_loss', 'triplet_loss'] or column[:8] == 'div_loss':\n",
    "      operations.append('min')\n",
    "    elif column in ['validation', 'div_metric']:\n",
    "      operations.append('max')\n",
    "    else:\n",
    "      raise ValueError('column {} is not recognized'.format(column))\n",
    "    \n",
    "  results = []\n",
    "  indexes = []\n",
    "  \n",
    "  for i in range(len(operations)):\n",
    "    values = last_rows[:, i]\n",
    "    if operations[i] == 'max': operation = np.max\n",
    "    elif operations[i] == 'min': operation = np.min\n",
    "    result = operation(values)\n",
    "    results.append(result)\n",
    "    indexes.append(values.tolist().index(result))\n",
    "  \n",
    "  outfile.write('bests: \\n')\n",
    "  writeline(outfile, labels, '{: >11}')\n",
    "  writeline(outfile, operations, '{: >11}')\n",
    "  writeline(outfile, results, '{: >11.3}')\n",
    "  writeline(outfile, indexes, '{: >11}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils import dir_util \n",
    "\n",
    "def investigate_initial_settings(n_settings, n_epochs, lr, wd, results_dir):\n",
    "  os.mkdir(env.get_csv_dir() + results_dir)\n",
    "  os.mkdir(env.get_models_dir() + results_dir)\n",
    "  \n",
    "  for setting_ind in range(n_settings):\n",
    "    print(f\"investigation no: {n_settings}\")\n",
    "    learn = None; gen = None; gc.collect()\n",
    "    gen = Gen(z_dim = z_dim)\n",
    "    init_cnn(gen, True)\n",
    "    div_metric = DiversityMetric(10, 95)\n",
    "    \n",
    "    tmp_csv_filename =  env.temp_csv_path + '/' + results_dir + '/' + str(setting_ind)\n",
    "    csv_logger = partial(ImmediateCSVLogger, filename=tmp_csv_filename)\n",
    "    \n",
    "    learn = Learner(data, gen, loss_func = feat_loss, metrics=[validation, div_metric], \n",
    "                    model_dir = env.get_learner_models_dir(), callback_fns=[LossMetrics, csv_logger])\n",
    "    div_metric.set_learner(learn)\n",
    "    \n",
    "    saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=model.__name__ + \"-best\")\n",
    "    saver_every_epoch = SaveModelCallback(learn, every='epoch', name=model.__name__)\n",
    "\n",
    "    learn.fit(n_epochs, lr=lr, wd = wd, callbacks=[saver_best, saver_every_epoch])\n",
    "    \n",
    "    shutil.copyfile(tmp_csv_filename + \".csv\", env.get_csv_dir() + results_dir + '/' + str(setting_ind) + '.csv')\n",
    "    \n",
    "    model_dest = env.get_models_dir() + results_dir + '/' + str(setting_ind)\n",
    "    os.mkdir(model_dest)\n",
    "    dir_util.copy_tree(env.data_path/env.get_learner_models_dir(), model_dest)\n",
    "    shutil.rmtree(env.data_path/env.get_learner_models_dir())  \n",
    "    \n",
    "  produce_summary(results_dir, n_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_perturbations(learn, n_perturbations):\n",
    "  initial_training_mode = learn.model.training\n",
    "  gen = learn.model.eval()\n",
    "  perturbations = [gen.generate_single_noise() for _ in range(n_perturbations)]\n",
    "  learn.model.train(initial_training_mode)  \n",
    "  return perturbations\n",
    "\n",
    "def compute_prediction_histogram(learn, perturbation, verbose=False):\n",
    "  pred_hist = [0] * 1000\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 and verbose: print (\"at batch no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbation[None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      pred_hist[pred] += 1\n",
    "  return pred_hist\n",
    "\n",
    "\n",
    "def compute_mean_prediction_histogram(learn, perturbations):\n",
    "  pred_hist = torch.tensor([0] * 1000).detach_()\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 5 == 0 : print(f\"at batch no {batch_no}\")\n",
    "    for j, perturbation in enumerate(perturbations):\n",
    "      perturbed_batch = batch + perturbation[None]\n",
    "      preds = arch(perturbed_batch).argmax(1)\n",
    "      for pred in preds:\n",
    "        pred_hist[pred] += 1\n",
    "  pred_hist = pred_hist.float() / len(perturbations)\n",
    "  return pred_hist.tolist()\n",
    "\n",
    "\n",
    "def classes_needed_to_reach(percentage, hist):\n",
    "  hist_sum = np.sum(hist)\n",
    "  indexed_hist = [(i, hist_element) for i,hist_element in  \n",
    "                          enumerate(hist)]\n",
    "  sorted_hist = sorted(indexed_hist, key=lambda x: x[1], reverse = True)\n",
    "  \n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = sorted_hist[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / hist_sum) * 100.\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, sorted_hist\n",
    "\n",
    "def diversity(learn, n_perturbations, percentage = 95, p = None):\n",
    "  pred_histogram = compute_mean_prediction_histogram(\n",
    "      learn, generate_perturbations(learn, n_perturbations)\n",
    "  )\n",
    "  print(\"finished creating the prediction histogram\")\n",
    "\n",
    "  return classes_needed_to_reach(95, pred_histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiversityMetric(Callback):\n",
    "  def __init__(self, n_perturbations, percentage):\n",
    "    super().__init__()\n",
    "    self.name = \"div_metric\"\n",
    "    self.n_perturbations = n_perturbations\n",
    "    self.percentage = percentage\n",
    "    self.learn = None\n",
    "  \n",
    "  def set_learner(self, learn):\n",
    "    self.learn = learn\n",
    "    \n",
    "  def on_epoch_begin(self, **kwargs):\n",
    "    global learn\n",
    "    self.perturbations = generate_perturbations(self.learn, self.n_perturbations)\n",
    "    self.pred_hist = torch.tensor([0] * 1000).detach_()\n",
    "    \n",
    "  def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "    _, _, _, images = last_output\n",
    "    for j, perturbation in enumerate(self.perturbations):\n",
    "      perturbed_batch = images + perturbation[None]\n",
    "      preds = arch(perturbed_batch).argmax(1)\n",
    "      for pred in preds:\n",
    "        self.pred_hist[pred] += 1\n",
    "  \n",
    "  def on_epoch_end(self, last_metrics, **kwargs):\n",
    "    self.pred_hist = (self.pred_hist.float() / len(self.perturbations)).tolist()\n",
    "    div_metric = classes_needed_to_reach(self.percentage, self.pred_hist)[0]\n",
    "    return add_metrics(last_metrics, div_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tltucTv2ep9-"
   },
   "outputs": [],
   "source": [
    "# mode = \"sanity_check\"\n",
    "# mode = \"normal\"\n",
    "mode = \"div_metric_calc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50\n",
    "# model = models.resnet152\n",
    "# model = models.vgg16_bn\n",
    "# model = torchvision.models.googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SO1h55obXzOv",
    "outputId": "54414cc5-84d5-4f45-ecab-87374a58dd33"
   },
   "outputs": [],
   "source": [
    "if mode == \"normal\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "elif mode == \"sanity_check\":\n",
    "  env.load_dataset('dataset_sanity_check_small', 'dataset_sanity_check_small')  \n",
    "  env.set_data_path('dataset_sanity_check_small')\n",
    "elif mode == \"div_metric_calc\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "  env.load_test_dataset(str(env.data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "koaQZmjMom7w"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "gpu_flag = True\n",
    "nag_util.batch_size = batch_size; nag_util.gpu_flag = gpu_flag;\n",
    "tfms = get_transforms(do_flip=False, max_rotate=0)\n",
    "data = (ImageList.from_folder(env.data_path)\n",
    "        .split_by_folder(valid=('test' if mode == 'div_metric_calc' else 'valid'))\n",
    "        .label_from_folder()\n",
    "        .transform(tfms, size=224)\n",
    "        .databunch(bs=batch_size, num_workers=1)\n",
    "        .normalize(imagenet_stats))\n",
    "\n",
    "# data.show_batch(rows=2, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDBkRV8yovwV"
   },
   "outputs": [],
   "source": [
    "z_dim = 10\n",
    "arch = SoftmaxWrapper(model(pretrained=True).cuda().eval())\n",
    "nag_util.arch = arch\n",
    "requires_grad(arch, False)\n",
    "\n",
    "# vgg:\n",
    "# layers = []\n",
    "# blocks = [i-1 for i,o in enumerate(children(arch.features)) if isinstance(o, nn.MaxPool2d)]\n",
    "# layers = [arch.features[i] for i in blocks]\n",
    "# layer_weights = [1] * len(layers)\n",
    "\n",
    "layers = [\n",
    "    arch.softmax\n",
    "]\n",
    "\n",
    "layer_weights = [1.] * len(layers)\n",
    "\n",
    "# inception:\n",
    "# layers = [\n",
    "#     arch.Conv2d_1a_3x3,\n",
    "#     arch.Mixed_6e,\n",
    "#     arch.Mixed_7a,\n",
    "#     arch.fc    \n",
    "# ]\n",
    "# layer_weights = [1.0/4.0] * len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qd9gXUy_ovww"
   },
   "outputs": [],
   "source": [
    "feat_loss = FeatureLoss(arch, layers, layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfZKdYD2MSdi"
   },
   "outputs": [],
   "source": [
    "env.save_filename = 'resnet50_x2' #resnet50_60\n",
    "# env.save_filename = 'resnet50_17'\n",
    "# env.save_filename = 'vgg16_32'\n",
    "\n",
    "if Path(env.get_csv_path() + '.csv').exists(): raise FileExistsError(\"csv_path already exists\")\n",
    "if Path(env.get_models_path()).exists(): raise FileExistsError(\"models_path already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9J20CBLS8S9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models_directory returned is:  models/174\n"
     ]
    }
   ],
   "source": [
    "learn = None; gen = None; gc.collect()\n",
    "csv_logger = partial(ImmediateCSVLogger, filename= env.temp_csv_path + '/' + env.save_filename)\n",
    "gen = Gen(z_dim=z_dim)\n",
    "init_cnn(gen, True)\n",
    "div_metric = DiversityMetric(10, 95)\n",
    "\n",
    "learn = Learner(data, gen, loss_func = feat_loss, model_dir = env.get_learner_models_dir(),\n",
    "                metrics=[validation, div_metric], callback_fns=[LossMetrics, csv_logger])\n",
    "\n",
    "div_metric.set_learner(learn)\n",
    "\n",
    "# learn = Learner(data, Gen(z_dim=10), loss_func = feat_loss, metrics=[validation], callback_fns=LossMetrics, opt_func = optim.SGD)\n",
    "# learn = Learner(data, Gen(z_dim=z_dim), loss_func = feat_loss, metrics=[validation], callback_fns=[LossMetrics, DiversityWeightsScheduler])\n",
    "\n",
    "# load_starting_point(learn, model.__name__, z_dim)\n",
    "# random_seed(42, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wOZYzOHDEdB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GeneralRelu. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type FeatureLoss. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Gen. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type deconv_layer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (9000 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "n02454379,n02454379,n02454379,n02454379,n02454379\n",
       "Path: /root/Derakhshani/adversarial/datasets/dataset;\n",
       "\n",
       "Valid: LabelList (50000 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "n02454379,n02454379,n02454379,n02454379,n02454379\n",
       "Path: /root/Derakhshani/adversarial/datasets/dataset;\n",
       "\n",
       "Test: None, model=Gen(\n",
       "  (z_): Linear(in_features=10, out_features=7168, bias=True)\n",
       "  (BN_): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (CT2d_1): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (BN2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_2): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(320, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_3): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(160, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_4): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(80, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_5): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_6): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_7): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (BN2d): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FeatureLoss(\n",
       "  (dis): SoftmaxWrapper(\n",
       "    (m): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax()\n",
       "  )\n",
       "), metrics=[<function validation at 0x7f568884d8c8>, DiversityMetric\n",
       "n_perturbations: 10\n",
       "percentage: 95], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/Derakhshani/adversarial/datasets/dataset'), model_dir='models/174', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), <class 'fastai.callbacks.loss_metrics.LossMetrics'>, functools.partial(<class 'nag_util.ImmediateCSVLogger'>, filename='/root/Derakhshani/adversarial/temp/resnet50_x2')], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): Linear(in_features=10, out_features=7168, bias=True)\n",
       "  (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): GeneralRelu()\n",
       "  (5): ConvTranspose2d(320, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): GeneralRelu()\n",
       "  (8): ConvTranspose2d(160, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): GeneralRelu()\n",
       "  (11): ConvTranspose2d(80, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): GeneralRelu()\n",
       "  (14): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (16): GeneralRelu()\n",
       "  (17): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): GeneralRelu()\n",
       "  (20): ConvTranspose2d(72, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (21): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")], add_time=True, silent=None)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !cp \"/content/gdrive/My Drive/DL/models/vgg16_12-last.pth\"  \"/content/\"\n",
    "# learn.load('/content/vgg16_12-last')\n",
    "\n",
    "# load_filename = 'resnet50-11_39'\n",
    "# load_filename = 'resnet50_startpoint_0'\n",
    "load_filename = 'resnet50_60/resnet50_60_77'\n",
    "# load_filename = 'investigate_resnet50_7/7/resnet50_3'\n",
    "# load_filename = 'vgg16_30/vgg16_30_69'\n",
    "# load_filename = 'vgg16_12-last'\n",
    "\n",
    "learn.load('/root/Derakhshani/adversarial/models/' + load_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the selected settings are : \n",
      "\tmode: div_metric_calc \n",
      "\tnetw-under-attack: resnet50 \n",
      "\tload filename: resnet50_60/resnet50_60_77 \n",
      "      \tsave filename: resnet50_x2\n",
      "\tmetric names: ['fool_loss', 'div_loss_0', 'triplet_loss']\n",
      "\n",
      "please MAKE SURE that the config is correct.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  load_filename\n",
    "except NameError:\n",
    "  load_filename = None\n",
    "\n",
    "print(\"the selected settings are : \")\n",
    "print('''\\tmode: {} \\n\\tnetw-under-attack: {} \\n\\tload filename: {} \n",
    "      \\tsave filename: {}\\n\\tmetric names: {}\\n'''.format(\n",
    "      mode, model.__name__, load_filename , env.save_filename, feat_loss.metric_names\n",
    "))\n",
    "print(\"please MAKE SURE that the config is correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find(1e-6, 100)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dir = 'investigate_resnet50_7'\n",
    "# investigate_initial_settings(8, 4, lr = 1e-2, wd = 0.0, results_dir = results_dir)\n",
    "# shutil.rmtree(env.get_models_dir() + results_dir)\n",
    "# shutil.rmtree(env.get_csv_dir() + results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WeightTuner(LearnerCallback):\n",
    "#   def __init__(self, learn: Learner):\n",
    "#     super().__init__(learn)\n",
    "#     self.fooling_weight = learn.loss_func.fooling_weight\n",
    "    \n",
    "#   def on_epoch_begin(self, **kwargs):\n",
    "#     fooling_rate = \n",
    "#     print(\"by how much to increase the fooling_weight? (current value: {})\".format(fooling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoolingWeightScheduler(LearnerCallback):\n",
    "  def __init__(self, learn: Learner, fooling_loss_index):\n",
    "    super().__init__(learn)\n",
    "    self.fooling_loss_index = fooling_loss_index\n",
    "    self.weights_history = []\n",
    "    self.fooling_loss_history = []\n",
    "  \n",
    "  def on_epoch_end(self, last_metrics, **kwargs):\n",
    "    # history keeping\n",
    "    self.weights_history.append((kwargs['epoch'], self.learn.loss_func.fooling_weight))\n",
    "    \n",
    "    # the actualy functionality\n",
    "    fooling_loss = last_metrics[self.fooling_loss_index]\n",
    "    self.fooling_loss_history.append(fooling_loss)\n",
    "    \n",
    "    if len(self.weights_history) < 2:\n",
    "      return\n",
    "    \n",
    "    if self.fooling_loss_history[-1] > self.fooling_loss_history[-2]:\n",
    "      self.learn.loss_func.fooling_weight += 0.3    \n",
    "      print('fooling weight increased to {} at the end of epoch {}'.format(\n",
    "        self.learn.loss_func.fooling_weight, kwargs['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>div_metric</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>div_loss_0</th>\n",
       "      <th>triplet_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.484675</td>\n",
       "      <td>6.603043</td>\n",
       "      <td>0.661000</td>\n",
       "      <td>802</td>\n",
       "      <td>0.946450</td>\n",
       "      <td>0.557192</td>\n",
       "      <td>1.038238</td>\n",
       "      <td>06:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.239085</td>\n",
       "      <td>6.349411</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>576</td>\n",
       "      <td>0.742049</td>\n",
       "      <td>0.585446</td>\n",
       "      <td>1.069967</td>\n",
       "      <td>06:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.012896</td>\n",
       "      <td>6.351116</td>\n",
       "      <td>0.711000</td>\n",
       "      <td>422</td>\n",
       "      <td>0.788913</td>\n",
       "      <td>0.562791</td>\n",
       "      <td>1.052624</td>\n",
       "      <td>06:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.247818</td>\n",
       "      <td>6.608007</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>484</td>\n",
       "      <td>0.752701</td>\n",
       "      <td>0.562747</td>\n",
       "      <td>1.078512</td>\n",
       "      <td>06:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.212513</td>\n",
       "      <td>6.460710</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>390</td>\n",
       "      <td>0.711930</td>\n",
       "      <td>0.566027</td>\n",
       "      <td>1.064311</td>\n",
       "      <td>06:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.315418</td>\n",
       "      <td>6.252168</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>475</td>\n",
       "      <td>0.653542</td>\n",
       "      <td>0.540468</td>\n",
       "      <td>1.052138</td>\n",
       "      <td>06:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.986650</td>\n",
       "      <td>6.262525</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>315</td>\n",
       "      <td>0.650308</td>\n",
       "      <td>0.569699</td>\n",
       "      <td>1.049279</td>\n",
       "      <td>06:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>5.935610</td>\n",
       "      <td>6.184744</td>\n",
       "      <td>0.736000</td>\n",
       "      <td>420</td>\n",
       "      <td>0.650551</td>\n",
       "      <td>0.538362</td>\n",
       "      <td>1.037528</td>\n",
       "      <td>06:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>6.150433</td>\n",
       "      <td>6.540987</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>429</td>\n",
       "      <td>0.672047</td>\n",
       "      <td>0.546790</td>\n",
       "      <td>1.061718</td>\n",
       "      <td>06:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>6.268784</td>\n",
       "      <td>6.431576</td>\n",
       "      <td>0.753000</td>\n",
       "      <td>429</td>\n",
       "      <td>0.592627</td>\n",
       "      <td>0.571051</td>\n",
       "      <td>1.035477</td>\n",
       "      <td>06:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.238379</td>\n",
       "      <td>6.630941</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>356</td>\n",
       "      <td>0.662677</td>\n",
       "      <td>0.556786</td>\n",
       "      <td>1.038098</td>\n",
       "      <td>06:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>6.627360</td>\n",
       "      <td>6.661795</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>413</td>\n",
       "      <td>0.592584</td>\n",
       "      <td>0.572391</td>\n",
       "      <td>1.048284</td>\n",
       "      <td>06:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>6.647898</td>\n",
       "      <td>6.492095</td>\n",
       "      <td>0.767000</td>\n",
       "      <td>345</td>\n",
       "      <td>0.582658</td>\n",
       "      <td>0.556385</td>\n",
       "      <td>1.017802</td>\n",
       "      <td>06:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>6.242447</td>\n",
       "      <td>6.807240</td>\n",
       "      <td>0.758000</td>\n",
       "      <td>377</td>\n",
       "      <td>0.647733</td>\n",
       "      <td>0.553120</td>\n",
       "      <td>1.045344</td>\n",
       "      <td>06:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>6.502340</td>\n",
       "      <td>6.756387</td>\n",
       "      <td>0.763000</td>\n",
       "      <td>383</td>\n",
       "      <td>0.593949</td>\n",
       "      <td>0.533053</td>\n",
       "      <td>1.036128</td>\n",
       "      <td>06:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>6.596348</td>\n",
       "      <td>6.685573</td>\n",
       "      <td>0.773000</td>\n",
       "      <td>481</td>\n",
       "      <td>0.570606</td>\n",
       "      <td>0.548707</td>\n",
       "      <td>1.034936</td>\n",
       "      <td>06:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>6.563083</td>\n",
       "      <td>6.753512</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>389</td>\n",
       "      <td>0.596150</td>\n",
       "      <td>0.545419</td>\n",
       "      <td>1.030392</td>\n",
       "      <td>06:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>6.838840</td>\n",
       "      <td>6.737998</td>\n",
       "      <td>0.762000</td>\n",
       "      <td>313</td>\n",
       "      <td>0.570185</td>\n",
       "      <td>0.539864</td>\n",
       "      <td>1.007858</td>\n",
       "      <td>06:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>6.682962</td>\n",
       "      <td>6.827616</td>\n",
       "      <td>0.759000</td>\n",
       "      <td>365</td>\n",
       "      <td>0.564441</td>\n",
       "      <td>0.530439</td>\n",
       "      <td>1.038075</td>\n",
       "      <td>06:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>6.640796</td>\n",
       "      <td>6.651606</td>\n",
       "      <td>0.783000</td>\n",
       "      <td>430</td>\n",
       "      <td>0.504648</td>\n",
       "      <td>0.542363</td>\n",
       "      <td>1.047896</td>\n",
       "      <td>06:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.470023</td>\n",
       "      <td>6.562579</td>\n",
       "      <td>0.769000</td>\n",
       "      <td>297</td>\n",
       "      <td>0.517575</td>\n",
       "      <td>0.528980</td>\n",
       "      <td>1.016703</td>\n",
       "      <td>06:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>6.841472</td>\n",
       "      <td>6.715772</td>\n",
       "      <td>0.782000</td>\n",
       "      <td>310</td>\n",
       "      <td>0.523232</td>\n",
       "      <td>0.544226</td>\n",
       "      <td>1.006574</td>\n",
       "      <td>06:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>6.968253</td>\n",
       "      <td>6.986688</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>352</td>\n",
       "      <td>0.524743</td>\n",
       "      <td>0.546226</td>\n",
       "      <td>1.032898</td>\n",
       "      <td>06:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>6.785932</td>\n",
       "      <td>7.022553</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>379</td>\n",
       "      <td>0.508993</td>\n",
       "      <td>0.553154</td>\n",
       "      <td>1.019283</td>\n",
       "      <td>06:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>6.878831</td>\n",
       "      <td>6.993742</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>328</td>\n",
       "      <td>0.498133</td>\n",
       "      <td>0.559350</td>\n",
       "      <td>1.023291</td>\n",
       "      <td>06:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>6.927871</td>\n",
       "      <td>7.098584</td>\n",
       "      <td>0.782000</td>\n",
       "      <td>203</td>\n",
       "      <td>0.509882</td>\n",
       "      <td>0.549138</td>\n",
       "      <td>1.038250</td>\n",
       "      <td>06:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>6.990757</td>\n",
       "      <td>7.223417</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>356</td>\n",
       "      <td>0.502009</td>\n",
       "      <td>0.570384</td>\n",
       "      <td>1.035747</td>\n",
       "      <td>06:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>6.922608</td>\n",
       "      <td>6.959058</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>221</td>\n",
       "      <td>0.466117</td>\n",
       "      <td>0.551850</td>\n",
       "      <td>1.019156</td>\n",
       "      <td>06:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>6.971678</td>\n",
       "      <td>7.136505</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>331</td>\n",
       "      <td>0.485790</td>\n",
       "      <td>0.567741</td>\n",
       "      <td>1.034953</td>\n",
       "      <td>06:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>6.990115</td>\n",
       "      <td>7.255200</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>353</td>\n",
       "      <td>0.460413</td>\n",
       "      <td>0.591112</td>\n",
       "      <td>1.055974</td>\n",
       "      <td>06:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>6.857666</td>\n",
       "      <td>7.302848</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>320</td>\n",
       "      <td>0.494875</td>\n",
       "      <td>0.564011</td>\n",
       "      <td>1.029000</td>\n",
       "      <td>06:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>7.206811</td>\n",
       "      <td>7.330715</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>302</td>\n",
       "      <td>0.474241</td>\n",
       "      <td>0.530332</td>\n",
       "      <td>1.036158</td>\n",
       "      <td>06:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>7.223862</td>\n",
       "      <td>7.479609</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>254</td>\n",
       "      <td>0.489047</td>\n",
       "      <td>0.551509</td>\n",
       "      <td>1.047360</td>\n",
       "      <td>06:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>7.182760</td>\n",
       "      <td>7.396008</td>\n",
       "      <td>0.806000</td>\n",
       "      <td>296</td>\n",
       "      <td>0.458471</td>\n",
       "      <td>0.559650</td>\n",
       "      <td>1.032845</td>\n",
       "      <td>06:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>6.803107</td>\n",
       "      <td>7.169368</td>\n",
       "      <td>0.807000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.431901</td>\n",
       "      <td>0.561382</td>\n",
       "      <td>1.014942</td>\n",
       "      <td>06:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>6.978804</td>\n",
       "      <td>7.225801</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>339</td>\n",
       "      <td>0.421733</td>\n",
       "      <td>0.571108</td>\n",
       "      <td>1.041617</td>\n",
       "      <td>06:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>7.203639</td>\n",
       "      <td>7.373685</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>218</td>\n",
       "      <td>0.460439</td>\n",
       "      <td>0.541942</td>\n",
       "      <td>1.028788</td>\n",
       "      <td>06:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>6.930016</td>\n",
       "      <td>7.463852</td>\n",
       "      <td>0.813000</td>\n",
       "      <td>230</td>\n",
       "      <td>0.439889</td>\n",
       "      <td>0.585974</td>\n",
       "      <td>1.037642</td>\n",
       "      <td>06:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>7.107800</td>\n",
       "      <td>7.298958</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>342</td>\n",
       "      <td>0.421783</td>\n",
       "      <td>0.567807</td>\n",
       "      <td>1.029023</td>\n",
       "      <td>06:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>7.092650</td>\n",
       "      <td>7.187764</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>310</td>\n",
       "      <td>0.415407</td>\n",
       "      <td>0.534874</td>\n",
       "      <td>1.019342</td>\n",
       "      <td>06:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>7.160784</td>\n",
       "      <td>7.220438</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>251</td>\n",
       "      <td>0.417334</td>\n",
       "      <td>0.554528</td>\n",
       "      <td>1.019611</td>\n",
       "      <td>06:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>7.084028</td>\n",
       "      <td>7.457995</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>295</td>\n",
       "      <td>0.433696</td>\n",
       "      <td>0.572940</td>\n",
       "      <td>1.016508</td>\n",
       "      <td>06:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>7.111885</td>\n",
       "      <td>7.311893</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>252</td>\n",
       "      <td>0.415513</td>\n",
       "      <td>0.549317</td>\n",
       "      <td>0.984272</td>\n",
       "      <td>06:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>7.409523</td>\n",
       "      <td>7.608353</td>\n",
       "      <td>0.796000</td>\n",
       "      <td>188</td>\n",
       "      <td>0.445354</td>\n",
       "      <td>0.512529</td>\n",
       "      <td>1.016854</td>\n",
       "      <td>06:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>7.040844</td>\n",
       "      <td>7.784067</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>290</td>\n",
       "      <td>0.448408</td>\n",
       "      <td>0.545565</td>\n",
       "      <td>1.013701</td>\n",
       "      <td>06:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>6.940284</td>\n",
       "      <td>7.675137</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>287</td>\n",
       "      <td>0.421424</td>\n",
       "      <td>0.539271</td>\n",
       "      <td>1.004332</td>\n",
       "      <td>06:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>7.500206</td>\n",
       "      <td>7.690696</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>242</td>\n",
       "      <td>0.411265</td>\n",
       "      <td>0.548570</td>\n",
       "      <td>1.024690</td>\n",
       "      <td>06:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>7.072247</td>\n",
       "      <td>7.526117</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>247</td>\n",
       "      <td>0.394145</td>\n",
       "      <td>0.557755</td>\n",
       "      <td>1.012921</td>\n",
       "      <td>06:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>7.488975</td>\n",
       "      <td>7.582329</td>\n",
       "      <td>0.827000</td>\n",
       "      <td>224</td>\n",
       "      <td>0.394603</td>\n",
       "      <td>0.562059</td>\n",
       "      <td>1.025053</td>\n",
       "      <td>06:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>7.154264</td>\n",
       "      <td>7.706627</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>282</td>\n",
       "      <td>0.400157</td>\n",
       "      <td>0.573492</td>\n",
       "      <td>1.012983</td>\n",
       "      <td>06:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>7.501768</td>\n",
       "      <td>7.900072</td>\n",
       "      <td>0.827000</td>\n",
       "      <td>194</td>\n",
       "      <td>0.394923</td>\n",
       "      <td>0.592671</td>\n",
       "      <td>1.037004</td>\n",
       "      <td>06:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>7.437985</td>\n",
       "      <td>7.821165</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>235</td>\n",
       "      <td>0.392654</td>\n",
       "      <td>0.576312</td>\n",
       "      <td>1.025905</td>\n",
       "      <td>06:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>7.502294</td>\n",
       "      <td>7.633538</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>215</td>\n",
       "      <td>0.385903</td>\n",
       "      <td>0.559635</td>\n",
       "      <td>0.996670</td>\n",
       "      <td>06:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>7.423113</td>\n",
       "      <td>7.429757</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>213</td>\n",
       "      <td>0.367160</td>\n",
       "      <td>0.549880</td>\n",
       "      <td>0.985649</td>\n",
       "      <td>06:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>7.150782</td>\n",
       "      <td>7.680622</td>\n",
       "      <td>0.827000</td>\n",
       "      <td>246</td>\n",
       "      <td>0.384413</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>1.012559</td>\n",
       "      <td>06:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>7.191915</td>\n",
       "      <td>7.686968</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>225</td>\n",
       "      <td>0.362245</td>\n",
       "      <td>0.551715</td>\n",
       "      <td>1.032156</td>\n",
       "      <td>06:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>7.514558</td>\n",
       "      <td>7.761970</td>\n",
       "      <td>0.831000</td>\n",
       "      <td>207</td>\n",
       "      <td>0.377490</td>\n",
       "      <td>0.547800</td>\n",
       "      <td>1.020250</td>\n",
       "      <td>06:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>7.305919</td>\n",
       "      <td>7.823450</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>260</td>\n",
       "      <td>0.386408</td>\n",
       "      <td>0.541589</td>\n",
       "      <td>0.989687</td>\n",
       "      <td>06:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>7.856470</td>\n",
       "      <td>7.866414</td>\n",
       "      <td>0.831000</td>\n",
       "      <td>231</td>\n",
       "      <td>0.376026</td>\n",
       "      <td>0.517696</td>\n",
       "      <td>1.000522</td>\n",
       "      <td>06:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>7.565911</td>\n",
       "      <td>7.990373</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>193</td>\n",
       "      <td>0.384362</td>\n",
       "      <td>0.535085</td>\n",
       "      <td>1.008616</td>\n",
       "      <td>06:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>7.437556</td>\n",
       "      <td>7.974693</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>281</td>\n",
       "      <td>0.366182</td>\n",
       "      <td>0.564257</td>\n",
       "      <td>1.010391</td>\n",
       "      <td>06:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>7.449656</td>\n",
       "      <td>7.895270</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>291</td>\n",
       "      <td>0.360526</td>\n",
       "      <td>0.526179</td>\n",
       "      <td>1.013063</td>\n",
       "      <td>06:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>7.846672</td>\n",
       "      <td>7.705013</td>\n",
       "      <td>0.839000</td>\n",
       "      <td>248</td>\n",
       "      <td>0.339369</td>\n",
       "      <td>0.543618</td>\n",
       "      <td>1.009801</td>\n",
       "      <td>06:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>7.461768</td>\n",
       "      <td>7.937788</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>193</td>\n",
       "      <td>0.373295</td>\n",
       "      <td>0.531356</td>\n",
       "      <td>0.993029</td>\n",
       "      <td>06:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>7.361337</td>\n",
       "      <td>7.797472</td>\n",
       "      <td>0.853000</td>\n",
       "      <td>197</td>\n",
       "      <td>0.350733</td>\n",
       "      <td>0.537585</td>\n",
       "      <td>0.981982</td>\n",
       "      <td>06:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>7.267618</td>\n",
       "      <td>7.829104</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>204</td>\n",
       "      <td>0.359453</td>\n",
       "      <td>0.497064</td>\n",
       "      <td>0.979308</td>\n",
       "      <td>06:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>7.891550</td>\n",
       "      <td>8.042407</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.363440</td>\n",
       "      <td>0.518085</td>\n",
       "      <td>0.990653</td>\n",
       "      <td>06:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>7.433103</td>\n",
       "      <td>8.047579</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>223</td>\n",
       "      <td>0.350842</td>\n",
       "      <td>0.511921</td>\n",
       "      <td>0.998039</td>\n",
       "      <td>06:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>7.505778</td>\n",
       "      <td>8.183823</td>\n",
       "      <td>0.828000</td>\n",
       "      <td>228</td>\n",
       "      <td>0.371616</td>\n",
       "      <td>0.503210</td>\n",
       "      <td>0.981824</td>\n",
       "      <td>06:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>7.855963</td>\n",
       "      <td>7.914273</td>\n",
       "      <td>0.857000</td>\n",
       "      <td>174</td>\n",
       "      <td>0.340830</td>\n",
       "      <td>0.505971</td>\n",
       "      <td>0.965918</td>\n",
       "      <td>06:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>7.523208</td>\n",
       "      <td>7.953307</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>255</td>\n",
       "      <td>0.343613</td>\n",
       "      <td>0.486491</td>\n",
       "      <td>0.973311</td>\n",
       "      <td>06:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>7.899317</td>\n",
       "      <td>8.321835</td>\n",
       "      <td>0.838000</td>\n",
       "      <td>225</td>\n",
       "      <td>0.360773</td>\n",
       "      <td>0.517920</td>\n",
       "      <td>0.985910</td>\n",
       "      <td>06:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>7.613638</td>\n",
       "      <td>8.105870</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>189</td>\n",
       "      <td>0.331059</td>\n",
       "      <td>0.514546</td>\n",
       "      <td>0.987418</td>\n",
       "      <td>06:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>8.013172</td>\n",
       "      <td>8.325083</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>193</td>\n",
       "      <td>0.360851</td>\n",
       "      <td>0.509935</td>\n",
       "      <td>0.961446</td>\n",
       "      <td>06:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>7.379839</td>\n",
       "      <td>8.233780</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>181</td>\n",
       "      <td>0.336062</td>\n",
       "      <td>0.506054</td>\n",
       "      <td>0.982557</td>\n",
       "      <td>06:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>7.789823</td>\n",
       "      <td>8.180332</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>246</td>\n",
       "      <td>0.330596</td>\n",
       "      <td>0.508660</td>\n",
       "      <td>0.983983</td>\n",
       "      <td>06:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>8.325166</td>\n",
       "      <td>8.248142</td>\n",
       "      <td>0.853000</td>\n",
       "      <td>172</td>\n",
       "      <td>0.342126</td>\n",
       "      <td>0.500989</td>\n",
       "      <td>0.970283</td>\n",
       "      <td>06:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>8.224268</td>\n",
       "      <td>8.173350</td>\n",
       "      <td>0.863000</td>\n",
       "      <td>196</td>\n",
       "      <td>0.317064</td>\n",
       "      <td>0.518302</td>\n",
       "      <td>0.994278</td>\n",
       "      <td>06:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>7.816323</td>\n",
       "      <td>8.244683</td>\n",
       "      <td>0.847000</td>\n",
       "      <td>192</td>\n",
       "      <td>0.321632</td>\n",
       "      <td>0.514146</td>\n",
       "      <td>0.999902</td>\n",
       "      <td>06:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>7.715928</td>\n",
       "      <td>8.634556</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>192</td>\n",
       "      <td>0.354886</td>\n",
       "      <td>0.496399</td>\n",
       "      <td>0.978755</td>\n",
       "      <td>06:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[9.0500e-01],\n",
      "        [2.5375e-01],\n",
      "        [4.1698e-01],\n",
      "        [9.9908e-01],\n",
      "        [9.9810e-01],\n",
      "        [7.1139e-04],\n",
      "        [9.3242e-01],\n",
      "        [9.3849e-06],\n",
      "        [3.3856e-02],\n",
      "        [2.4297e-03],\n",
      "        [5.4059e-01],\n",
      "        [2.4539e-01],\n",
      "        [9.9103e-01],\n",
      "        [8.3814e-02],\n",
      "        [4.8209e-03],\n",
      "        [7.6875e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.6566529273986816: \n",
      "func:cos_distance, ap_dist: -0.9865201115608215, an_dist: -0.7569881677627563\n",
      "target probs tensor([[9.8036e-01],\n",
      "        [5.1371e-03],\n",
      "        [3.4188e-02],\n",
      "        [8.7611e-01],\n",
      "        [8.6065e-05],\n",
      "        [7.3840e-03],\n",
      "        [8.6962e-01],\n",
      "        [2.2176e-01],\n",
      "        [2.5227e-04],\n",
      "        [8.0990e-05],\n",
      "        [1.2912e-03],\n",
      "        [2.3571e-02],\n",
      "        [9.7971e-01],\n",
      "        [6.7360e-01],\n",
      "        [5.2691e-01],\n",
      "        [3.2785e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.8842581510543823: \n",
      "func:cos_distance, ap_dist: -0.987865686416626, an_dist: -0.5924427509307861\n",
      "target probs tensor([[3.8184e-01],\n",
      "        [1.0284e-03],\n",
      "        [1.1394e-02],\n",
      "        [6.5348e-01],\n",
      "        [3.9984e-03],\n",
      "        [9.8166e-01],\n",
      "        [3.2605e-03],\n",
      "        [7.2505e-03],\n",
      "        [1.7895e-05],\n",
      "        [8.2658e-04],\n",
      "        [5.8473e-03],\n",
      "        [9.9383e-02],\n",
      "        [9.8657e-01],\n",
      "        [6.4719e-04],\n",
      "        [1.0383e-05],\n",
      "        [9.9632e-01]], device='cuda:0'), loss: 0.9746476411819458: \n",
      "func:cos_distance, ap_dist: -0.9917562007904053, an_dist: -0.6082901954650879\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 0 with validation value: 0.6610000133514404.\n",
      "target probs tensor([[6.9952e-03],\n",
      "        [5.7373e-03],\n",
      "        [3.2091e-07],\n",
      "        [2.6534e-01],\n",
      "        [9.8169e-01],\n",
      "        [8.8675e-01],\n",
      "        [2.8277e-02],\n",
      "        [2.2983e-02],\n",
      "        [9.9892e-01],\n",
      "        [2.8070e-06],\n",
      "        [5.9815e-04],\n",
      "        [5.1340e-01],\n",
      "        [3.1563e-01],\n",
      "        [1.7428e-05],\n",
      "        [1.1297e-07],\n",
      "        [6.5784e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.9722590446472168: \n",
      "func:cos_distance, ap_dist: -0.9850161075592041, an_dist: -0.7760490775108337\n",
      "target probs tensor([[4.7328e-01],\n",
      "        [2.0105e-01],\n",
      "        [5.0480e-01],\n",
      "        [7.6387e-03],\n",
      "        [1.0576e-01],\n",
      "        [7.8034e-01],\n",
      "        [3.8966e-01],\n",
      "        [3.0097e-06],\n",
      "        [2.0861e-02],\n",
      "        [6.1064e-01],\n",
      "        [4.2000e-02],\n",
      "        [1.9821e-03],\n",
      "        [7.1681e-02],\n",
      "        [7.4559e-09],\n",
      "        [1.0774e-06],\n",
      "        [3.0882e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.3218844532966614: \n",
      "func:cos_distance, ap_dist: -0.9748835563659668, an_dist: -0.7832502126693726\n",
      "target probs tensor([[2.2350e-05],\n",
      "        [7.9184e-01],\n",
      "        [1.6985e-02],\n",
      "        [3.7956e-02],\n",
      "        [2.7744e-02],\n",
      "        [1.3601e-05],\n",
      "        [2.7170e-02],\n",
      "        [1.9392e-04],\n",
      "        [8.0059e-01],\n",
      "        [2.6625e-02],\n",
      "        [2.9318e-03],\n",
      "        [1.7166e-02],\n",
      "        [2.5977e-06],\n",
      "        [1.3413e-05],\n",
      "        [2.2220e-01],\n",
      "        [2.2853e-04]], device='cuda:0'), loss: 0.2245190143585205: \n",
      "func:cos_distance, ap_dist: -0.9849103689193726, an_dist: -0.8158559799194336\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 1 with validation value: 0.7160000205039978.\n",
      "target probs tensor([[2.5968e-07],\n",
      "        [1.9252e-06],\n",
      "        [1.7485e-03],\n",
      "        [1.7158e-02],\n",
      "        [1.0597e-05],\n",
      "        [6.2082e-01],\n",
      "        [2.4554e-05],\n",
      "        [7.7466e-05],\n",
      "        [2.6432e-02],\n",
      "        [2.0159e-02],\n",
      "        [6.6440e-01],\n",
      "        [1.1226e-02],\n",
      "        [5.3374e-01],\n",
      "        [9.5762e-01],\n",
      "        [2.0533e-02],\n",
      "        [9.0815e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5294751524925232: \n",
      "func:cos_distance, ap_dist: -0.9852771759033203, an_dist: -0.6489840745925903\n",
      "target probs tensor([[7.9454e-01],\n",
      "        [5.1842e-02],\n",
      "        [9.2945e-05],\n",
      "        [9.6306e-03],\n",
      "        [7.2990e-08],\n",
      "        [6.7799e-02],\n",
      "        [2.8727e-05],\n",
      "        [1.0541e-03],\n",
      "        [8.4677e-04],\n",
      "        [1.6203e-02],\n",
      "        [5.4223e-08],\n",
      "        [2.6856e-01],\n",
      "        [1.3204e-01],\n",
      "        [3.3236e-06],\n",
      "        [3.5500e-03],\n",
      "        [7.1366e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.13699868321418762: \n",
      "func:cos_distance, ap_dist: -0.9586589336395264, an_dist: -0.750836193561554\n",
      "target probs tensor([[8.6686e-01],\n",
      "        [4.8515e-01],\n",
      "        [4.9602e-01],\n",
      "        [5.1587e-03],\n",
      "        [2.0127e-03],\n",
      "        [2.3098e-01],\n",
      "        [2.1401e-03],\n",
      "        [1.8229e-02],\n",
      "        [4.4483e-02],\n",
      "        [3.2062e-03],\n",
      "        [9.7961e-01],\n",
      "        [2.2179e-04],\n",
      "        [1.5726e-06],\n",
      "        [5.4224e-03],\n",
      "        [9.6240e-01],\n",
      "        [3.5369e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.6804662942886353: \n",
      "func:cos_distance, ap_dist: -0.9476004242897034, an_dist: -0.6402167081832886\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 2.3 at the end of epoch 2\n",
      "target probs tensor([[1.4589e-01],\n",
      "        [2.1523e-03],\n",
      "        [6.6876e-04],\n",
      "        [1.1871e-01],\n",
      "        [9.8901e-01],\n",
      "        [9.5986e-01],\n",
      "        [9.6539e-01],\n",
      "        [2.7208e-01],\n",
      "        [6.6736e-01],\n",
      "        [5.4788e-05],\n",
      "        [7.9216e-03],\n",
      "        [1.2221e-04],\n",
      "        [9.2413e-01],\n",
      "        [7.6888e-04],\n",
      "        [2.9274e-02],\n",
      "        [1.2416e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.9632862210273743: \n",
      "func:cos_distance, ap_dist: -0.9896076321601868, an_dist: -0.6494756937026978\n",
      "target probs tensor([[1.2049e-05],\n",
      "        [3.0956e-05],\n",
      "        [1.5345e-05],\n",
      "        [2.1029e-05],\n",
      "        [8.0778e-01],\n",
      "        [9.1809e-05],\n",
      "        [9.7479e-03],\n",
      "        [4.5947e-02],\n",
      "        [4.8282e-01],\n",
      "        [2.4064e-02],\n",
      "        [2.9557e-07],\n",
      "        [5.4839e-06],\n",
      "        [1.1189e-01],\n",
      "        [6.1480e-02],\n",
      "        [7.1972e-03],\n",
      "        [4.7700e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.16149790585041046: \n",
      "func:cos_distance, ap_dist: -0.9846649169921875, an_dist: -0.5792582035064697\n",
      "target probs tensor([[5.8153e-01],\n",
      "        [9.9305e-01],\n",
      "        [1.9618e-05],\n",
      "        [9.4778e-06],\n",
      "        [2.4815e-06],\n",
      "        [4.3016e-06],\n",
      "        [4.8471e-03],\n",
      "        [1.8742e-03],\n",
      "        [1.1386e-05],\n",
      "        [5.0260e-02],\n",
      "        [6.9056e-02],\n",
      "        [9.9551e-01],\n",
      "        [3.8110e-01],\n",
      "        [2.3058e-04],\n",
      "        [6.3925e-06],\n",
      "        [3.1298e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.764403223991394: \n",
      "func:cos_distance, ap_dist: -0.9771573543548584, an_dist: -0.6690731048583984\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "target probs tensor([[7.1698e-01],\n",
      "        [1.6715e-03],\n",
      "        [1.8371e-02],\n",
      "        [2.0837e-03],\n",
      "        [1.0954e-05],\n",
      "        [6.1962e-02],\n",
      "        [9.9997e-01],\n",
      "        [8.8788e-02],\n",
      "        [1.3676e-01],\n",
      "        [1.5693e-02],\n",
      "        [9.5799e-02],\n",
      "        [2.3697e-03],\n",
      "        [9.9998e-01],\n",
      "        [4.3645e-02],\n",
      "        [1.0143e-04],\n",
      "        [2.2240e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.4449597597122192: \n",
      "func:cos_distance, ap_dist: -0.9952405691146851, an_dist: -0.6787550449371338\n",
      "target probs tensor([[8.3515e-01],\n",
      "        [8.7920e-02],\n",
      "        [2.9282e-01],\n",
      "        [2.8889e-01],\n",
      "        [9.9595e-01],\n",
      "        [6.7782e-04],\n",
      "        [1.7557e-05],\n",
      "        [9.9499e-01],\n",
      "        [5.1056e-04],\n",
      "        [4.7557e-01],\n",
      "        [3.2469e-01],\n",
      "        [1.3301e-06],\n",
      "        [9.0885e-05],\n",
      "        [8.1053e-01],\n",
      "        [2.5958e-03],\n",
      "        [1.2679e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.0057804584503174: \n",
      "func:cos_distance, ap_dist: -0.9523485898971558, an_dist: -0.773135781288147\n",
      "target probs tensor([[1.9431e-06],\n",
      "        [6.2364e-06],\n",
      "        [5.2054e-04],\n",
      "        [4.1645e-02],\n",
      "        [3.1923e-01],\n",
      "        [1.9094e-04],\n",
      "        [9.9500e-01],\n",
      "        [2.1180e-05],\n",
      "        [1.0430e-01],\n",
      "        [5.6637e-01],\n",
      "        [7.5247e-01],\n",
      "        [9.9936e-01],\n",
      "        [1.8502e-02],\n",
      "        [1.5076e-06],\n",
      "        [6.9792e-01],\n",
      "        [2.1399e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.0404093265533447: \n",
      "func:cos_distance, ap_dist: -0.9885974526405334, an_dist: -0.6225591897964478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 4 with validation value: 0.7329999804496765.\n",
      "target probs tensor([[9.9083e-01],\n",
      "        [1.2852e-03],\n",
      "        [3.1049e-01],\n",
      "        [7.0633e-05],\n",
      "        [2.9777e-02],\n",
      "        [6.9781e-06],\n",
      "        [3.4766e-02],\n",
      "        [1.6990e-05],\n",
      "        [8.8718e-01],\n",
      "        [5.6807e-02],\n",
      "        [9.9526e-07],\n",
      "        [1.5272e-03],\n",
      "        [3.1412e-06],\n",
      "        [2.1080e-03],\n",
      "        [9.9955e-01],\n",
      "        [8.8177e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.075446367263794: \n",
      "func:cos_distance, ap_dist: -0.9674974679946899, an_dist: -0.7517962455749512\n",
      "target probs tensor([[1.6052e-04],\n",
      "        [1.0733e-04],\n",
      "        [6.1354e-01],\n",
      "        [3.4025e-01],\n",
      "        [2.8885e-04],\n",
      "        [7.9689e-01],\n",
      "        [4.3051e-03],\n",
      "        [7.8891e-07],\n",
      "        [1.3576e-04],\n",
      "        [5.9965e-04],\n",
      "        [3.0562e-02],\n",
      "        [9.9054e-01],\n",
      "        [2.4393e-02],\n",
      "        [2.3328e-03],\n",
      "        [1.6974e-06],\n",
      "        [1.3842e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.4803304970264435: \n",
      "func:cos_distance, ap_dist: -0.989916980266571, an_dist: -0.7217401266098022\n",
      "target probs tensor([[1.0029e-04],\n",
      "        [8.8943e-01],\n",
      "        [8.1339e-06],\n",
      "        [9.6695e-02],\n",
      "        [6.2182e-01],\n",
      "        [9.9501e-01],\n",
      "        [2.3154e-01],\n",
      "        [1.4073e-05],\n",
      "        [1.3536e-04],\n",
      "        [5.6391e-01],\n",
      "        [8.3584e-02],\n",
      "        [1.9706e-03],\n",
      "        [5.9907e-05],\n",
      "        [5.9452e-01],\n",
      "        [4.2651e-01],\n",
      "        [1.1010e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.7011702060699463: \n",
      "func:cos_distance, ap_dist: -0.9902193546295166, an_dist: -0.7532156109809875\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "target probs tensor([[9.9938e-01],\n",
      "        [3.1493e-05],\n",
      "        [5.1442e-01],\n",
      "        [5.0214e-03],\n",
      "        [8.9709e-01],\n",
      "        [1.5279e-03],\n",
      "        [1.7748e-03],\n",
      "        [7.8981e-05],\n",
      "        [3.8218e-02],\n",
      "        [9.9902e-01],\n",
      "        [3.3547e-03],\n",
      "        [4.3689e-03],\n",
      "        [1.3561e-06],\n",
      "        [1.3658e-01],\n",
      "        [3.3514e-05],\n",
      "        [1.0696e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.094974160194397: \n",
      "func:cos_distance, ap_dist: -0.9534668922424316, an_dist: -0.7435362339019775\n",
      "target probs tensor([[2.3407e-02],\n",
      "        [3.2375e-04],\n",
      "        [1.5691e-06],\n",
      "        [7.8504e-10],\n",
      "        [9.4398e-05],\n",
      "        [8.6848e-01],\n",
      "        [9.6584e-01],\n",
      "        [1.9129e-01],\n",
      "        [1.1846e-02],\n",
      "        [2.7493e-03],\n",
      "        [2.7295e-05],\n",
      "        [4.3412e-02],\n",
      "        [5.0824e-02],\n",
      "        [4.3152e-02],\n",
      "        [4.1692e-05],\n",
      "        [5.5808e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.3623201549053192: \n",
      "func:cos_distance, ap_dist: -0.9962986707687378, an_dist: -0.8098809719085693\n",
      "target probs tensor([[1.6831e-03],\n",
      "        [4.8230e-04],\n",
      "        [8.6088e-02],\n",
      "        [6.7526e-03],\n",
      "        [2.8326e-01],\n",
      "        [7.4873e-06],\n",
      "        [9.9932e-01],\n",
      "        [3.9220e-04],\n",
      "        [7.1271e-01],\n",
      "        [3.2797e-07],\n",
      "        [9.7460e-01],\n",
      "        [6.1649e-01],\n",
      "        [2.2834e-03],\n",
      "        [7.7806e-05],\n",
      "        [1.3109e-07],\n",
      "        [3.7457e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.8800725936889648: \n",
      "func:cos_distance, ap_dist: -0.9921160340309143, an_dist: -0.4924904704093933\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "target probs tensor([[8.9235e-02],\n",
      "        [1.0524e-04],\n",
      "        [5.3246e-01],\n",
      "        [7.5646e-03],\n",
      "        [4.1520e-09],\n",
      "        [2.9265e-05],\n",
      "        [5.8736e-01],\n",
      "        [2.7301e-05],\n",
      "        [7.8845e-05],\n",
      "        [2.8881e-04],\n",
      "        [5.5661e-05],\n",
      "        [5.4982e-01],\n",
      "        [1.0526e-04],\n",
      "        [2.1957e-02],\n",
      "        [1.2465e-01],\n",
      "        [3.9773e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.16879087686538696: \n",
      "func:cos_distance, ap_dist: -0.9734796285629272, an_dist: -0.6452313661575317\n",
      "target probs tensor([[4.5597e-03],\n",
      "        [3.4333e-04],\n",
      "        [1.9155e-04],\n",
      "        [5.5213e-04],\n",
      "        [6.8265e-01],\n",
      "        [9.8638e-01],\n",
      "        [4.5110e-02],\n",
      "        [3.9455e-06],\n",
      "        [3.4544e-02],\n",
      "        [9.4527e-03],\n",
      "        [2.1233e-05],\n",
      "        [5.8226e-01],\n",
      "        [6.4492e-01],\n",
      "        [7.8797e-01],\n",
      "        [6.2080e-01],\n",
      "        [4.8928e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.6231043338775635: \n",
      "func:cos_distance, ap_dist: -0.9866657257080078, an_dist: -0.7724901437759399\n",
      "target probs tensor([[8.1731e-04],\n",
      "        [6.4071e-03],\n",
      "        [8.8171e-01],\n",
      "        [9.9810e-01],\n",
      "        [1.3597e-04],\n",
      "        [9.8786e-01],\n",
      "        [1.2468e-04],\n",
      "        [9.4203e-06],\n",
      "        [6.6409e-01],\n",
      "        [1.3272e-05],\n",
      "        [9.8650e-01],\n",
      "        [4.9140e-03],\n",
      "        [2.8488e-02],\n",
      "        [9.7144e-03],\n",
      "        [4.2930e-01],\n",
      "        [7.8628e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.1814146041870117: \n",
      "func:cos_distance, ap_dist: -0.9835336208343506, an_dist: -0.7941297888755798\n",
      "target probs tensor([[1.3000e-05],\n",
      "        [1.2673e-04],\n",
      "        [1.7774e-07],\n",
      "        [4.7485e-01],\n",
      "        [2.1229e-03],\n",
      "        [2.6297e-04],\n",
      "        [2.1240e-07],\n",
      "        [9.6933e-01]], device='cuda:0'), loss: 0.5163971185684204: \n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "func:cos_distance, ap_dist: -0.9963256120681763, an_dist: -0.5563345551490784\n",
      "Better model found at epoch 7 with validation value: 0.7360000014305115.\n",
      "fooling weight increased to 2.5999999999999996 at the end of epoch 7\n",
      "target probs tensor([[1.0851e-02],\n",
      "        [5.6774e-02],\n",
      "        [1.3631e-03],\n",
      "        [1.4767e-02],\n",
      "        [1.2680e-03],\n",
      "        [3.6497e-03],\n",
      "        [9.0673e-03],\n",
      "        [7.5425e-08],\n",
      "        [2.4117e-06],\n",
      "        [6.2336e-02],\n",
      "        [9.3067e-01],\n",
      "        [2.0357e-05],\n",
      "        [8.5709e-07],\n",
      "        [4.1223e-05],\n",
      "        [1.6911e-03],\n",
      "        [6.2010e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.17754897475242615: \n",
      "func:cos_distance, ap_dist: -0.9703577756881714, an_dist: -0.6083434820175171\n",
      "target probs tensor([[4.5967e-02],\n",
      "        [7.8585e-01],\n",
      "        [1.5553e-02],\n",
      "        [1.7131e-01],\n",
      "        [6.5040e-02],\n",
      "        [1.0218e-03],\n",
      "        [5.1414e-07],\n",
      "        [1.3789e-03],\n",
      "        [2.9701e-01],\n",
      "        [8.8118e-01],\n",
      "        [5.3215e-03],\n",
      "        [1.4435e-04],\n",
      "        [1.6961e-03],\n",
      "        [6.1671e-01],\n",
      "        [1.6367e-07],\n",
      "        [5.0333e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.3318837881088257: \n",
      "func:cos_distance, ap_dist: -0.9859272241592407, an_dist: -0.7946890592575073\n",
      "target probs tensor([[1.4791e-06],\n",
      "        [3.5899e-04],\n",
      "        [2.2140e-02],\n",
      "        [4.9497e-02],\n",
      "        [2.1734e-03],\n",
      "        [9.9878e-01],\n",
      "        [2.7504e-08],\n",
      "        [1.8025e-02],\n",
      "        [3.0407e-05],\n",
      "        [7.7358e-01],\n",
      "        [5.0393e-05],\n",
      "        [1.2564e-01],\n",
      "        [2.2605e-01],\n",
      "        [5.0049e-05],\n",
      "        [8.3592e-05],\n",
      "        [9.9771e-01]], device='cuda:0'), loss: 0.9221291542053223: \n",
      "func:cos_distance, ap_dist: -0.9817042350769043, an_dist: -0.6275762319564819\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 2.8999999999999995 at the end of epoch 8\n",
      "target probs tensor([[5.9406e-07],\n",
      "        [2.1910e-01],\n",
      "        [1.9887e-07],\n",
      "        [4.9009e-06],\n",
      "        [1.0898e-07],\n",
      "        [9.9115e-01],\n",
      "        [7.0715e-03],\n",
      "        [5.2136e-03],\n",
      "        [2.2517e-03],\n",
      "        [7.7664e-02],\n",
      "        [6.5273e-03],\n",
      "        [3.8887e-07],\n",
      "        [2.8038e-02],\n",
      "        [9.9993e-01],\n",
      "        [5.6138e-08],\n",
      "        [2.1020e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.9152522087097168: \n",
      "func:cos_distance, ap_dist: -0.9946640729904175, an_dist: -0.766040563583374\n",
      "target probs tensor([[1.5449e-04],\n",
      "        [6.6166e-02],\n",
      "        [3.1990e-04],\n",
      "        [2.0571e-07],\n",
      "        [1.8132e-06],\n",
      "        [3.3278e-04],\n",
      "        [7.7869e-01],\n",
      "        [7.9460e-06],\n",
      "        [1.5427e-06],\n",
      "        [1.0807e-01],\n",
      "        [9.6325e-01],\n",
      "        [7.4530e-04],\n",
      "        [2.4147e-01],\n",
      "        [9.9669e-01],\n",
      "        [5.4047e-01],\n",
      "        [3.7879e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.7350609302520752: \n",
      "func:cos_distance, ap_dist: -0.9941205382347107, an_dist: -0.6711983680725098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[6.5800e-06],\n",
      "        [8.1190e-01],\n",
      "        [1.7805e-02],\n",
      "        [5.8503e-02],\n",
      "        [2.2764e-03],\n",
      "        [2.5608e-04],\n",
      "        [9.3976e-01],\n",
      "        [2.4749e-06],\n",
      "        [4.7164e-01],\n",
      "        [3.3811e-04],\n",
      "        [2.2935e-02],\n",
      "        [7.1955e-01],\n",
      "        [1.9989e-04],\n",
      "        [3.3854e-06],\n",
      "        [2.1666e-01],\n",
      "        [3.3585e-01]], device='cuda:0'), loss: 0.44671890139579773: \n",
      "func:cos_distance, ap_dist: -0.9964731931686401, an_dist: -0.57158362865448\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 9 with validation value: 0.753000020980835.\n",
      "target probs tensor([[3.0468e-01],\n",
      "        [1.8603e-02],\n",
      "        [7.8928e-01],\n",
      "        [1.0193e-04],\n",
      "        [9.7419e-04],\n",
      "        [1.3997e-02],\n",
      "        [1.7208e-03],\n",
      "        [7.1243e-04],\n",
      "        [1.2227e-03],\n",
      "        [9.4930e-02],\n",
      "        [1.8020e-04],\n",
      "        [6.5479e-01],\n",
      "        [9.3427e-01],\n",
      "        [7.5224e-01],\n",
      "        [2.9705e-04],\n",
      "        [5.2606e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.4558477997779846: \n",
      "func:cos_distance, ap_dist: -0.9946109652519226, an_dist: -0.588943600654602\n",
      "target probs tensor([[1.5216e-01],\n",
      "        [9.9922e-01],\n",
      "        [5.7086e-03],\n",
      "        [1.6950e-06],\n",
      "        [1.4220e-05],\n",
      "        [8.1955e-01],\n",
      "        [3.1601e-05],\n",
      "        [9.9237e-06],\n",
      "        [9.2428e-04],\n",
      "        [1.5658e-03],\n",
      "        [8.0992e-03],\n",
      "        [7.7648e-01],\n",
      "        [7.6481e-01],\n",
      "        [1.5501e-05],\n",
      "        [1.4307e-06],\n",
      "        [1.2280e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.7494124174118042: \n",
      "func:cos_distance, ap_dist: -0.9780570268630981, an_dist: -0.5440624952316284\n",
      "target probs tensor([[3.3912e-08],\n",
      "        [1.4494e-07],\n",
      "        [4.6503e-08],\n",
      "        [2.6015e-02],\n",
      "        [1.6827e-06],\n",
      "        [4.4676e-04],\n",
      "        [7.7846e-01],\n",
      "        [1.3531e-06],\n",
      "        [4.8306e-03],\n",
      "        [4.5462e-07],\n",
      "        [5.1985e-01],\n",
      "        [6.7407e-02],\n",
      "        [5.2724e-06],\n",
      "        [2.0919e-04],\n",
      "        [1.4014e-08],\n",
      "        [1.7509e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.1464160531759262: \n",
      "func:cos_distance, ap_dist: -0.9594226479530334, an_dist: -0.5083837509155273\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 3.1999999999999993 at the end of epoch 10\n",
      "target probs tensor([[8.9102e-02],\n",
      "        [4.2866e-01],\n",
      "        [1.4577e-01],\n",
      "        [2.6788e-04],\n",
      "        [3.9963e-04],\n",
      "        [3.7690e-09],\n",
      "        [1.8806e-07],\n",
      "        [7.0815e-01],\n",
      "        [4.5776e-01],\n",
      "        [1.4298e-01],\n",
      "        [2.8584e-01],\n",
      "        [2.7763e-03],\n",
      "        [1.7845e-07],\n",
      "        [1.5457e-07],\n",
      "        [9.3199e-03],\n",
      "        [9.8546e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.19799157977104187: \n",
      "func:cos_distance, ap_dist: -0.9915133714675903, an_dist: -0.6580803990364075\n",
      "target probs tensor([[5.0943e-01],\n",
      "        [8.9146e-02],\n",
      "        [1.7079e-02],\n",
      "        [1.6992e-07],\n",
      "        [4.6198e-07],\n",
      "        [9.9828e-01],\n",
      "        [7.7426e-05],\n",
      "        [2.1319e-01],\n",
      "        [6.1968e-05],\n",
      "        [3.1784e-03],\n",
      "        [7.8337e-05],\n",
      "        [8.8851e-04],\n",
      "        [1.3562e-07],\n",
      "        [1.2852e-02],\n",
      "        [3.1622e-07],\n",
      "        [7.5346e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.4703950881958008: \n",
      "func:cos_distance, ap_dist: -0.9649285078048706, an_dist: -0.6574788689613342\n",
      "target probs tensor([[8.8514e-03],\n",
      "        [4.7990e-02],\n",
      "        [1.6412e-01],\n",
      "        [9.9178e-01],\n",
      "        [8.0259e-04],\n",
      "        [9.1088e-02],\n",
      "        [1.7911e-02],\n",
      "        [1.2607e-04],\n",
      "        [1.7108e-01],\n",
      "        [6.9651e-01],\n",
      "        [7.1916e-01],\n",
      "        [5.1859e-09],\n",
      "        [3.1054e-04],\n",
      "        [2.7743e-01],\n",
      "        [2.6690e-03],\n",
      "        [9.2698e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5087385773658752: \n",
      "func:cos_distance, ap_dist: -0.938971757888794, an_dist: -0.5973469018936157\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 11 with validation value: 0.7599999904632568.\n",
      "target probs tensor([[6.9916e-02],\n",
      "        [4.4008e-03],\n",
      "        [2.7266e-01],\n",
      "        [4.0333e-01],\n",
      "        [4.6618e-01],\n",
      "        [7.7843e-03],\n",
      "        [4.2641e-04],\n",
      "        [7.7810e-02],\n",
      "        [8.8278e-01],\n",
      "        [3.8409e-04],\n",
      "        [6.4365e-01],\n",
      "        [8.6694e-01],\n",
      "        [4.0876e-02],\n",
      "        [3.9087e-03],\n",
      "        [2.6424e-03],\n",
      "        [4.8386e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.4293655753135681: \n",
      "func:cos_distance, ap_dist: -0.9736058712005615, an_dist: -0.6991877555847168\n",
      "target probs tensor([[8.5023e-05],\n",
      "        [9.0836e-01],\n",
      "        [9.4309e-06],\n",
      "        [1.0048e-04],\n",
      "        [5.7832e-05],\n",
      "        [1.6266e-05],\n",
      "        [3.8208e-01],\n",
      "        [8.6835e-09],\n",
      "        [1.6247e-01],\n",
      "        [4.6429e-06],\n",
      "        [7.1342e-02],\n",
      "        [2.0919e-04],\n",
      "        [3.5677e-07],\n",
      "        [4.2030e-04],\n",
      "        [1.4231e-04],\n",
      "        [4.3798e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.19523128867149353: \n",
      "func:cos_distance, ap_dist: -0.9714374542236328, an_dist: -0.7113608121871948\n",
      "target probs tensor([[2.6936e-03],\n",
      "        [7.7754e-04],\n",
      "        [2.5085e-01],\n",
      "        [1.4521e-03],\n",
      "        [5.5858e-07],\n",
      "        [3.2124e-04],\n",
      "        [1.9556e-05],\n",
      "        [5.6043e-03],\n",
      "        [4.5414e-06],\n",
      "        [3.0446e-01],\n",
      "        [3.1855e-03],\n",
      "        [5.0017e-01],\n",
      "        [4.9405e-07],\n",
      "        [2.4356e-03],\n",
      "        [1.0313e-02],\n",
      "        [1.7554e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.09783082455396652: \n",
      "func:cos_distance, ap_dist: -0.9932185411453247, an_dist: -0.5426183938980103\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 12 with validation value: 0.7670000195503235.\n",
      "target probs tensor([[4.2934e-04],\n",
      "        [1.1092e-06],\n",
      "        [5.0888e-04],\n",
      "        [5.4065e-07],\n",
      "        [9.5004e-01],\n",
      "        [9.9518e-06],\n",
      "        [1.9445e-04],\n",
      "        [5.8443e-01],\n",
      "        [1.2118e-02],\n",
      "        [5.3853e-07],\n",
      "        [4.9323e-02],\n",
      "        [4.9383e-07],\n",
      "        [2.0777e-05],\n",
      "        [6.1643e-01],\n",
      "        [4.2645e-02],\n",
      "        [3.8408e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.3087978959083557: \n",
      "func:cos_distance, ap_dist: -0.9933810234069824, an_dist: -0.8427251577377319\n",
      "target probs tensor([[1.3302e-06],\n",
      "        [6.3222e-01],\n",
      "        [7.0530e-07],\n",
      "        [2.7833e-03],\n",
      "        [2.1163e-06],\n",
      "        [3.1020e-03],\n",
      "        [3.2921e-07],\n",
      "        [1.1224e-03],\n",
      "        [1.0833e-04],\n",
      "        [6.5268e-07],\n",
      "        [3.2550e-01],\n",
      "        [6.8436e-01],\n",
      "        [4.8363e-01],\n",
      "        [1.1619e-01],\n",
      "        [1.4552e-03],\n",
      "        [6.4767e-09]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.20876528322696686: \n",
      "func:cos_distance, ap_dist: -0.9850037693977356, an_dist: -0.7523149847984314\n",
      "target probs tensor([[9.5405e-01],\n",
      "        [8.2768e-02],\n",
      "        [2.8287e-02],\n",
      "        [3.1768e-06],\n",
      "        [1.4392e-06],\n",
      "        [6.8712e-04],\n",
      "        [4.2293e-01],\n",
      "        [9.9351e-04],\n",
      "        [7.1534e-01],\n",
      "        [4.4076e-03],\n",
      "        [2.6721e-01],\n",
      "        [3.8572e-02],\n",
      "        [5.2117e-05],\n",
      "        [1.8567e-04],\n",
      "        [8.4144e-06],\n",
      "        [7.1431e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.33493274450302124: \n",
      "func:cos_distance, ap_dist: -0.9329823851585388, an_dist: -0.5773323774337769\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 3.499999999999999 at the end of epoch 13\n",
      "target probs tensor([[4.8953e-02],\n",
      "        [3.1914e-01],\n",
      "        [6.5200e-04],\n",
      "        [9.3612e-01],\n",
      "        [2.1677e-01],\n",
      "        [2.4759e-01],\n",
      "        [8.4845e-02],\n",
      "        [8.6074e-03],\n",
      "        [3.9907e-03],\n",
      "        [2.6002e-02],\n",
      "        [8.5295e-01],\n",
      "        [9.9695e-01],\n",
      "        [5.0177e-01],\n",
      "        [1.2304e-02],\n",
      "        [1.2717e-07],\n",
      "        [8.6059e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.766419529914856: \n",
      "func:cos_distance, ap_dist: -0.9782509803771973, an_dist: -0.7885456085205078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[1.9528e-04],\n",
      "        [5.2822e-02],\n",
      "        [5.9093e-02],\n",
      "        [2.5824e-04],\n",
      "        [2.8988e-02],\n",
      "        [6.3210e-09],\n",
      "        [6.0961e-01],\n",
      "        [1.9566e-02],\n",
      "        [9.7132e-01],\n",
      "        [8.7950e-01],\n",
      "        [6.9911e-01],\n",
      "        [2.7031e-02],\n",
      "        [5.9835e-02],\n",
      "        [1.8883e-05],\n",
      "        [3.6530e-02],\n",
      "        [8.2925e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.6167532801628113: \n",
      "func:cos_distance, ap_dist: -0.9941242337226868, an_dist: -0.8833317756652832\n",
      "target probs tensor([[4.2321e-05],\n",
      "        [7.2377e-01],\n",
      "        [3.7932e-04],\n",
      "        [3.0942e-03],\n",
      "        [2.7758e-03],\n",
      "        [9.8969e-01],\n",
      "        [1.3498e-01],\n",
      "        [1.7172e-03],\n",
      "        [1.3332e-05],\n",
      "        [8.7014e-07],\n",
      "        [7.6085e-02],\n",
      "        [5.3007e-03],\n",
      "        [9.1182e-02],\n",
      "        [1.6144e-09],\n",
      "        [2.6283e-03],\n",
      "        [9.2921e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.3933945596218109: \n",
      "func:cos_distance, ap_dist: -0.9555869698524475, an_dist: -0.6011694073677063\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "target probs tensor([[3.0969e-02],\n",
      "        [1.3948e-06],\n",
      "        [6.0320e-01],\n",
      "        [7.7124e-01],\n",
      "        [7.4576e-04],\n",
      "        [7.6824e-01],\n",
      "        [1.1497e-07],\n",
      "        [3.7072e-04],\n",
      "        [2.5496e-07],\n",
      "        [2.6310e-01],\n",
      "        [9.8464e-06],\n",
      "        [8.7928e-01],\n",
      "        [4.2754e-03],\n",
      "        [1.1853e-07],\n",
      "        [9.3891e-01],\n",
      "        [2.5316e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5695860385894775: \n",
      "func:cos_distance, ap_dist: -0.9546576738357544, an_dist: -0.6231343746185303\n",
      "target probs tensor([[9.1767e-07],\n",
      "        [9.4472e-01],\n",
      "        [8.4130e-01],\n",
      "        [1.8364e-06],\n",
      "        [7.2626e-01],\n",
      "        [3.4859e-06],\n",
      "        [1.2060e-04],\n",
      "        [5.2400e-01],\n",
      "        [7.0775e-06],\n",
      "        [3.0939e-07],\n",
      "        [2.8388e-02],\n",
      "        [3.8075e-01],\n",
      "        [3.0960e-04],\n",
      "        [7.7256e-01],\n",
      "        [1.0221e-02],\n",
      "        [3.2821e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5504335165023804: \n",
      "func:cos_distance, ap_dist: -0.9912009239196777, an_dist: -0.5851568579673767\n",
      "target probs tensor([[2.9994e-01],\n",
      "        [6.1386e-03],\n",
      "        [2.3680e-02],\n",
      "        [4.4826e-02],\n",
      "        [1.0468e-03],\n",
      "        [1.9692e-07],\n",
      "        [2.1971e-03],\n",
      "        [4.0331e-05],\n",
      "        [5.8297e-04],\n",
      "        [4.6729e-04],\n",
      "        [3.2148e-01],\n",
      "        [4.6330e-05],\n",
      "        [5.1199e-05],\n",
      "        [7.4328e-04],\n",
      "        [9.0032e-02],\n",
      "        [2.0362e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.07172700017690659: \n",
      "func:cos_distance, ap_dist: -0.9794594645500183, an_dist: -0.5153844952583313\n",
      "target probs tensor([[6.7052e-07],\n",
      "        [2.0129e-05],\n",
      "        [8.4684e-08],\n",
      "        [1.9477e-01],\n",
      "        [5.2484e-03],\n",
      "        [1.9459e-04],\n",
      "        [8.9648e-06],\n",
      "        [1.4873e-04]], device='cuda:0'), loss: 0.027783561497926712: \n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "func:cos_distance, ap_dist: -0.9827872514724731, an_dist: -0.6092022061347961\n",
      "Better model found at epoch 15 with validation value: 0.7730000019073486.\n",
      "target probs tensor([[1.1965e-05],\n",
      "        [8.3285e-05],\n",
      "        [1.2537e-01],\n",
      "        [2.5976e-03],\n",
      "        [1.0841e-04],\n",
      "        [4.5454e-04],\n",
      "        [1.6840e-03],\n",
      "        [8.7406e-01],\n",
      "        [2.8461e-05],\n",
      "        [1.3064e-01],\n",
      "        [1.7822e-02],\n",
      "        [9.9787e-01],\n",
      "        [4.6148e-01],\n",
      "        [1.6190e-03],\n",
      "        [1.2427e-06],\n",
      "        [1.9106e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.571285605430603: \n",
      "func:cos_distance, ap_dist: -0.996680736541748, an_dist: -0.7188658714294434\n",
      "target probs tensor([[1.8576e-03],\n",
      "        [2.7856e-01],\n",
      "        [2.7117e-02],\n",
      "        [9.6815e-01],\n",
      "        [8.7273e-01],\n",
      "        [5.5762e-07],\n",
      "        [1.0363e-04],\n",
      "        [2.9866e-02],\n",
      "        [8.4822e-01],\n",
      "        [8.4795e-04],\n",
      "        [4.0798e-03],\n",
      "        [1.3008e-01],\n",
      "        [3.1740e-02],\n",
      "        [4.5725e-01],\n",
      "        [8.2907e-02],\n",
      "        [1.8708e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5538206100463867: \n",
      "func:cos_distance, ap_dist: -0.9860345125198364, an_dist: -0.7897427678108215\n",
      "target probs tensor([[1.9957e-02],\n",
      "        [2.6178e-04],\n",
      "        [2.0421e-02],\n",
      "        [7.5032e-01],\n",
      "        [2.5436e-03],\n",
      "        [9.9542e-01],\n",
      "        [6.6787e-07],\n",
      "        [7.0211e-03],\n",
      "        [7.9854e-04],\n",
      "        [7.1719e-01],\n",
      "        [1.6528e-04],\n",
      "        [6.4448e-02],\n",
      "        [3.6849e-01],\n",
      "        [2.6843e-05],\n",
      "        [6.9219e-06],\n",
      "        [9.9719e-01]], device='cuda:0'), loss: 0.9055629968643188: \n",
      "func:cos_distance, ap_dist: -0.9924533367156982, an_dist: -0.5911285877227783\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 3.799999999999999 at the end of epoch 16\n",
      "target probs tensor([[5.6104e-06],\n",
      "        [8.4814e-03],\n",
      "        [5.6442e-02],\n",
      "        [9.7824e-01],\n",
      "        [1.2929e-04],\n",
      "        [2.0538e-02],\n",
      "        [7.3465e-06],\n",
      "        [5.1581e-05],\n",
      "        [2.9567e-01],\n",
      "        [1.3053e-02],\n",
      "        [9.8943e-01],\n",
      "        [1.2686e-04],\n",
      "        [6.3217e-01],\n",
      "        [4.9912e-06],\n",
      "        [5.7030e-06],\n",
      "        [1.8644e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.6154671907424927: \n",
      "func:cos_distance, ap_dist: -0.9792047739028931, an_dist: -0.5420130491256714\n",
      "target probs tensor([[2.1905e-02],\n",
      "        [5.0763e-04],\n",
      "        [9.2738e-01],\n",
      "        [6.8466e-07],\n",
      "        [6.1019e-07],\n",
      "        [4.4834e-04],\n",
      "        [6.3243e-09],\n",
      "        [5.8039e-01],\n",
      "        [1.1047e-02],\n",
      "        [4.3276e-06],\n",
      "        [1.7188e-06],\n",
      "        [7.5677e-01],\n",
      "        [1.6273e-02],\n",
      "        [2.8851e-05],\n",
      "        [2.7643e-04],\n",
      "        [2.3839e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.309877872467041: \n",
      "func:cos_distance, ap_dist: -0.9878647327423096, an_dist: -0.43324679136276245\n",
      "target probs tensor([[6.6509e-05],\n",
      "        [6.5954e-01],\n",
      "        [2.5718e-03],\n",
      "        [4.5153e-02],\n",
      "        [7.9014e-03],\n",
      "        [2.4267e-03],\n",
      "        [4.1758e-03],\n",
      "        [1.7000e-06],\n",
      "        [8.5291e-01],\n",
      "        [5.6360e-03],\n",
      "        [4.0806e-04],\n",
      "        [8.7437e-01],\n",
      "        [3.8825e-04],\n",
      "        [5.7107e-08],\n",
      "        [2.7596e-02],\n",
      "        [1.1268e-07]], device='cuda:0'), loss: 0.3229009211063385: \n",
      "func:cos_distance, ap_dist: -0.9968485832214355, an_dist: -0.6788018345832825\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "target probs tensor([[8.0358e-01],\n",
      "        [3.0912e-05],\n",
      "        [5.3098e-01],\n",
      "        [6.3785e-07],\n",
      "        [5.5062e-01],\n",
      "        [3.2259e-04],\n",
      "        [2.0304e-02],\n",
      "        [8.9291e-02],\n",
      "        [1.4695e-01],\n",
      "        [8.1017e-03],\n",
      "        [3.8630e-04],\n",
      "        [2.1405e-06],\n",
      "        [3.0107e-04],\n",
      "        [3.9349e-06],\n",
      "        [2.4467e-06],\n",
      "        [1.1583e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.21667245030403137: \n",
      "func:cos_distance, ap_dist: -0.9914392232894897, an_dist: -0.7189781665802002\n",
      "target probs tensor([[9.5110e-10],\n",
      "        [6.2484e-02],\n",
      "        [2.9379e-03],\n",
      "        [6.1558e-04],\n",
      "        [1.8730e-07],\n",
      "        [3.8545e-04],\n",
      "        [6.8211e-04],\n",
      "        [4.5358e-03],\n",
      "        [1.0000e+00],\n",
      "        [9.2954e-07],\n",
      "        [2.4701e-02],\n",
      "        [1.6989e-07],\n",
      "        [1.7202e-02],\n",
      "        [7.4528e-06],\n",
      "        [8.1676e-02],\n",
      "        [1.4160e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.009817361831665: \n",
      "func:cos_distance, ap_dist: -0.9743567109107971, an_dist: -0.6078556776046753\n",
      "target probs tensor([[9.9961e-01],\n",
      "        [1.1145e-06],\n",
      "        [1.6456e-04],\n",
      "        [1.9127e-02],\n",
      "        [2.4904e-04],\n",
      "        [1.3761e-05],\n",
      "        [1.4137e-03],\n",
      "        [6.5481e-05],\n",
      "        [3.4283e-02],\n",
      "        [1.3881e-05],\n",
      "        [1.0110e-05],\n",
      "        [1.3774e-01],\n",
      "        [4.4222e-06],\n",
      "        [4.0785e-05],\n",
      "        [2.0420e-03],\n",
      "        [4.8565e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5036962628364563: \n",
      "func:cos_distance, ap_dist: -0.989896297454834, an_dist: -0.8056960105895996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "target probs tensor([[1.1589e-02],\n",
      "        [3.5752e-02],\n",
      "        [1.7476e-01],\n",
      "        [1.6219e-02],\n",
      "        [9.6793e-03],\n",
      "        [4.3238e-03],\n",
      "        [2.8700e-05],\n",
      "        [6.3563e-02],\n",
      "        [5.2677e-05],\n",
      "        [6.5855e-03],\n",
      "        [2.6216e-01],\n",
      "        [1.1070e-01],\n",
      "        [5.9772e-02],\n",
      "        [1.0442e-03],\n",
      "        [3.0242e-06],\n",
      "        [5.3841e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.05202215909957886: \n",
      "func:cos_distance, ap_dist: -0.9829228520393372, an_dist: -0.5228154063224792\n",
      "target probs tensor([[1.7541e-05],\n",
      "        [8.6637e-06],\n",
      "        [5.3357e-04],\n",
      "        [1.5098e-02],\n",
      "        [3.9322e-03],\n",
      "        [8.3079e-03],\n",
      "        [1.5543e-04],\n",
      "        [4.4447e-03],\n",
      "        [1.0347e-05],\n",
      "        [1.8103e-01],\n",
      "        [8.5637e-01],\n",
      "        [8.2648e-04],\n",
      "        [9.5530e-02],\n",
      "        [9.5051e-01],\n",
      "        [1.4269e-03],\n",
      "        [5.3432e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.3778551518917084: \n",
      "func:cos_distance, ap_dist: -0.9942512512207031, an_dist: -0.6227149963378906\n",
      "target probs tensor([[6.4961e-06],\n",
      "        [5.1545e-03],\n",
      "        [8.2556e-02],\n",
      "        [1.7858e-05],\n",
      "        [3.2601e-07],\n",
      "        [6.5818e-03],\n",
      "        [1.5828e-08],\n",
      "        [9.2486e-01],\n",
      "        [9.8392e-01],\n",
      "        [1.8605e-05],\n",
      "        [5.4907e-04],\n",
      "        [1.5166e-01],\n",
      "        [3.2720e-01],\n",
      "        [8.0805e-01],\n",
      "        [3.6116e-07],\n",
      "        [3.0089e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5644750595092773: \n",
      "func:cos_distance, ap_dist: -0.9952092170715332, an_dist: -0.7367345094680786\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 19 with validation value: 0.7829999923706055.\n",
      "target probs tensor([[1.4074e-03],\n",
      "        [5.1692e-06],\n",
      "        [2.3208e-07],\n",
      "        [5.0618e-02],\n",
      "        [6.3531e-04],\n",
      "        [2.0448e-01],\n",
      "        [7.0629e-07],\n",
      "        [9.6243e-01],\n",
      "        [2.5768e-01],\n",
      "        [3.7963e-04],\n",
      "        [3.6173e-05],\n",
      "        [1.0913e-04],\n",
      "        [3.5472e-02],\n",
      "        [2.3147e-08],\n",
      "        [9.9993e-01],\n",
      "        [2.3526e-08]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.8378219604492188: \n",
      "func:cos_distance, ap_dist: -0.9453912377357483, an_dist: -0.5924314856529236\n",
      "target probs tensor([[9.8609e-01],\n",
      "        [7.5233e-01],\n",
      "        [7.7600e-07],\n",
      "        [2.7159e-03],\n",
      "        [7.8528e-09],\n",
      "        [7.3205e-01],\n",
      "        [9.5918e-01],\n",
      "        [2.4286e-04],\n",
      "        [7.5683e-01],\n",
      "        [3.1713e-05],\n",
      "        [1.0608e-06],\n",
      "        [6.8248e-04],\n",
      "        [9.1001e-02],\n",
      "        [1.5435e-07],\n",
      "        [5.4784e-01],\n",
      "        [8.4809e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.7808327674865723: \n",
      "func:cos_distance, ap_dist: -0.996146559715271, an_dist: -0.7326828837394714\n",
      "target probs tensor([[7.1314e-04],\n",
      "        [1.5251e-01],\n",
      "        [1.7079e-05],\n",
      "        [5.7278e-01],\n",
      "        [1.7748e-02],\n",
      "        [1.1635e-03],\n",
      "        [1.5177e-02],\n",
      "        [2.8756e-03],\n",
      "        [4.6543e-07],\n",
      "        [2.2009e-05],\n",
      "        [2.3684e-05],\n",
      "        [3.0424e-04],\n",
      "        [5.3468e-05],\n",
      "        [1.2616e-02],\n",
      "        [2.4017e-03],\n",
      "        [7.2464e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.06683866679668427: \n",
      "func:cos_distance, ap_dist: -0.9822203516960144, an_dist: -0.7375875115394592\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 4.099999999999999 at the end of epoch 20\n",
      "target probs tensor([[2.8338e-01],\n",
      "        [7.8753e-02],\n",
      "        [2.7975e-04],\n",
      "        [9.9029e-01],\n",
      "        [3.6687e-01],\n",
      "        [2.5473e-03],\n",
      "        [1.7956e-04],\n",
      "        [5.1564e-09],\n",
      "        [3.4086e-01],\n",
      "        [1.2156e-04],\n",
      "        [6.6306e-01],\n",
      "        [6.6338e-05],\n",
      "        [3.1488e-04],\n",
      "        [1.9408e-02],\n",
      "        [2.7879e-07],\n",
      "        [5.7525e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.44001835584640503: \n",
      "func:cos_distance, ap_dist: -0.9932664036750793, an_dist: -0.48710504174232483\n",
      "target probs tensor([[3.6785e-01],\n",
      "        [4.7235e-05],\n",
      "        [1.1135e-02],\n",
      "        [1.1959e-02],\n",
      "        [5.1361e-04],\n",
      "        [1.2613e-01],\n",
      "        [4.3907e-01],\n",
      "        [3.9565e-05],\n",
      "        [1.1265e-01],\n",
      "        [1.6078e-03],\n",
      "        [3.3413e-02],\n",
      "        [1.1340e-01],\n",
      "        [6.7830e-04],\n",
      "        [9.8839e-04],\n",
      "        [7.0457e-01],\n",
      "        [5.4024e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.16858156025409698: \n",
      "func:cos_distance, ap_dist: -0.9702447056770325, an_dist: -0.5876278281211853\n",
      "target probs tensor([[5.3284e-01],\n",
      "        [3.5311e-03],\n",
      "        [1.0000e+00],\n",
      "        [2.5847e-03],\n",
      "        [5.5621e-03],\n",
      "        [9.2621e-09],\n",
      "        [2.6524e-05],\n",
      "        [1.1693e-08],\n",
      "        [9.4238e-02],\n",
      "        [2.1152e-02],\n",
      "        [3.1421e-02],\n",
      "        [1.2204e-01],\n",
      "        [1.5992e-04],\n",
      "        [2.3555e-03],\n",
      "        [3.7922e-05],\n",
      "        [1.3362e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.5053112506866455: \n",
      "func:cos_distance, ap_dist: -0.9964295029640198, an_dist: -0.6307308673858643\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 4.399999999999999 at the end of epoch 21\n",
      "target probs tensor([[5.9944e-01],\n",
      "        [5.1226e-04],\n",
      "        [4.8946e-01],\n",
      "        [3.0726e-08],\n",
      "        [6.2501e-03],\n",
      "        [1.2103e-06],\n",
      "        [2.7100e-08],\n",
      "        [8.7678e-01],\n",
      "        [9.8624e-01],\n",
      "        [2.9631e-01],\n",
      "        [1.1097e-04],\n",
      "        [9.3666e-02],\n",
      "        [5.7459e-07],\n",
      "        [8.9558e-08],\n",
      "        [6.4982e-01],\n",
      "        [7.7149e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.6843298077583313: \n",
      "func:cos_distance, ap_dist: -0.9989050626754761, an_dist: -0.7326560020446777\n",
      "target probs tensor([[4.0326e-05],\n",
      "        [8.8171e-01],\n",
      "        [1.2275e-02],\n",
      "        [1.0649e-01],\n",
      "        [2.0005e-06],\n",
      "        [1.9390e-05],\n",
      "        [1.8768e-02],\n",
      "        [9.6149e-01],\n",
      "        [1.2180e-08],\n",
      "        [3.1923e-06],\n",
      "        [7.6845e-01],\n",
      "        [7.3696e-09],\n",
      "        [7.5788e-01],\n",
      "        [8.1231e-04],\n",
      "        [2.3533e-04],\n",
      "        [4.0479e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5261328816413879: \n",
      "func:cos_distance, ap_dist: -0.9546536803245544, an_dist: -0.7210884094238281\n",
      "target probs tensor([[2.6900e-02],\n",
      "        [3.0372e-01],\n",
      "        [9.7911e-01],\n",
      "        [9.1308e-07],\n",
      "        [1.7763e-06],\n",
      "        [6.7271e-07],\n",
      "        [6.7497e-07],\n",
      "        [2.4204e-04],\n",
      "        [1.2289e-02],\n",
      "        [9.5749e-01],\n",
      "        [1.5317e-03],\n",
      "        [6.2511e-01],\n",
      "        [1.2258e-02],\n",
      "        [1.2618e-03],\n",
      "        [6.4395e-02],\n",
      "        [1.5261e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5307056903839111: \n",
      "func:cos_distance, ap_dist: -0.9912046194076538, an_dist: -0.46205228567123413\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 22 with validation value: 0.7879999876022339.\n",
      "fooling weight increased to 4.699999999999998 at the end of epoch 22\n",
      "target probs tensor([[2.1226e-04],\n",
      "        [2.4726e-02],\n",
      "        [3.5074e-03],\n",
      "        [5.3067e-04],\n",
      "        [1.1906e-01],\n",
      "        [2.3465e-01],\n",
      "        [1.3751e-02],\n",
      "        [6.8170e-04],\n",
      "        [1.2822e-05],\n",
      "        [2.3340e-09],\n",
      "        [2.0641e-02],\n",
      "        [9.5375e-01],\n",
      "        [1.0137e-06],\n",
      "        [2.9084e-02],\n",
      "        [1.2170e-02],\n",
      "        [3.1954e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.2234138548374176: \n",
      "func:cos_distance, ap_dist: -0.9943451881408691, an_dist: -0.7128840684890747\n",
      "target probs tensor([[1.5708e-05],\n",
      "        [4.8376e-08],\n",
      "        [2.1884e-06],\n",
      "        [4.0307e-03],\n",
      "        [7.0468e-02],\n",
      "        [1.4157e-10],\n",
      "        [1.5401e-01],\n",
      "        [3.3902e-06],\n",
      "        [8.5008e-01],\n",
      "        [1.5280e-01],\n",
      "        [1.1138e-05],\n",
      "        [1.4728e-02],\n",
      "        [6.6825e-01],\n",
      "        [2.9375e-05],\n",
      "        [2.5746e-06],\n",
      "        [3.0431e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.21412988007068634: \n",
      "func:cos_distance, ap_dist: -0.9922804832458496, an_dist: -0.5933118462562561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[2.4839e-01],\n",
      "        [5.6823e-03],\n",
      "        [5.5319e-02],\n",
      "        [1.7041e-04],\n",
      "        [7.8709e-07],\n",
      "        [2.3910e-04],\n",
      "        [3.7921e-04],\n",
      "        [3.4259e-06],\n",
      "        [2.9009e-05],\n",
      "        [1.1160e-08],\n",
      "        [3.0797e-05],\n",
      "        [4.1314e-05],\n",
      "        [1.9248e-03],\n",
      "        [9.9896e-01],\n",
      "        [3.2674e-01],\n",
      "        [2.2343e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.4759030044078827: \n",
      "func:cos_distance, ap_dist: -0.9854564666748047, an_dist: -0.6579990386962891\n",
      "target probs tensor([[1.0789e-08],\n",
      "        [9.4061e-05],\n",
      "        [4.6343e-07],\n",
      "        [2.4191e-01],\n",
      "        [3.3469e-02],\n",
      "        [1.0371e-05],\n",
      "        [4.1323e-07],\n",
      "        [7.3008e-06]], device='cuda:0'), loss: 0.0388878732919693: \n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "func:cos_distance, ap_dist: -0.9794269800186157, an_dist: -0.3970475494861603\n",
      "target probs tensor([[1.1621e-09],\n",
      "        [9.9492e-01],\n",
      "        [1.8425e-02],\n",
      "        [1.7597e-01],\n",
      "        [1.1788e-01],\n",
      "        [1.1922e-01],\n",
      "        [8.0451e-01],\n",
      "        [2.9142e-02],\n",
      "        [8.0997e-01],\n",
      "        [5.5224e-01],\n",
      "        [3.8906e-02],\n",
      "        [5.1457e-02],\n",
      "        [6.3771e-05],\n",
      "        [9.4899e-04],\n",
      "        [3.7568e-01],\n",
      "        [2.6667e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.6540617346763611: \n",
      "func:cos_distance, ap_dist: -0.9910305738449097, an_dist: -0.6377936005592346\n",
      "target probs tensor([[4.9087e-06],\n",
      "        [2.8146e-06],\n",
      "        [3.8657e-05],\n",
      "        [3.3147e-05],\n",
      "        [2.2228e-04],\n",
      "        [8.9254e-04],\n",
      "        [5.3467e-05],\n",
      "        [8.0394e-03],\n",
      "        [3.4890e-05],\n",
      "        [4.4424e-01],\n",
      "        [1.2213e-09],\n",
      "        [3.6463e-03],\n",
      "        [7.0263e-02],\n",
      "        [5.7726e-02],\n",
      "        [6.7308e-01],\n",
      "        [9.9387e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.43410763144493103: \n",
      "func:cos_distance, ap_dist: -0.9969651699066162, an_dist: -0.549738347530365\n",
      "target probs tensor([[4.3242e-06],\n",
      "        [2.5042e-04],\n",
      "        [5.9237e-01],\n",
      "        [3.6057e-02],\n",
      "        [1.8648e-03],\n",
      "        [9.9811e-01],\n",
      "        [5.6953e-10],\n",
      "        [8.7991e-04],\n",
      "        [1.2158e-05],\n",
      "        [2.6958e-01],\n",
      "        [2.0638e-05],\n",
      "        [1.1064e-01],\n",
      "        [4.6320e-04],\n",
      "        [2.9126e-03],\n",
      "        [8.8641e-05],\n",
      "        [9.9620e-01]], device='cuda:0'), loss: 0.8258664011955261: \n",
      "func:cos_distance, ap_dist: -0.9849730730056763, an_dist: -0.5629967451095581\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "target probs tensor([[9.4235e-01],\n",
      "        [2.9124e-01],\n",
      "        [4.3294e-05],\n",
      "        [7.2054e-07],\n",
      "        [4.3184e-08],\n",
      "        [1.5713e-07],\n",
      "        [1.4301e-03],\n",
      "        [4.8817e-04],\n",
      "        [1.1526e-10],\n",
      "        [3.3188e-03],\n",
      "        [8.6933e-03],\n",
      "        [5.7807e-03],\n",
      "        [9.0935e-07],\n",
      "        [1.0306e-04],\n",
      "        [3.7387e-04],\n",
      "        [1.8700e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.20229345560073853: \n",
      "func:cos_distance, ap_dist: -0.9777612090110779, an_dist: -0.5912651419639587\n",
      "target probs tensor([[2.0005e-01],\n",
      "        [7.6366e-01],\n",
      "        [1.3358e-04],\n",
      "        [6.0291e-03],\n",
      "        [6.6097e-06],\n",
      "        [9.2069e-01],\n",
      "        [6.0474e-01],\n",
      "        [1.9109e-05],\n",
      "        [6.0490e-02],\n",
      "        [9.6505e-01],\n",
      "        [8.3391e-05],\n",
      "        [8.5544e-01],\n",
      "        [2.7004e-01],\n",
      "        [2.9862e-06],\n",
      "        [7.2030e-04],\n",
      "        [2.5847e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.675014853477478: \n",
      "func:cos_distance, ap_dist: -0.9854887127876282, an_dist: -0.6708972454071045\n",
      "target probs tensor([[3.7335e-05],\n",
      "        [4.5277e-01],\n",
      "        [5.1080e-03],\n",
      "        [3.1120e-02],\n",
      "        [1.0804e-02],\n",
      "        [4.1012e-04],\n",
      "        [2.1957e-02],\n",
      "        [8.9084e-07],\n",
      "        [7.7666e-01],\n",
      "        [8.0218e-04],\n",
      "        [4.4533e-03],\n",
      "        [8.3444e-01],\n",
      "        [1.5306e-05],\n",
      "        [2.8569e-09],\n",
      "        [6.9137e-01],\n",
      "        [3.3372e-03]], device='cuda:0'), loss: 0.32217925786972046: \n",
      "func:cos_distance, ap_dist: -0.9946141242980957, an_dist: -0.7729608416557312\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 4.999999999999998 at the end of epoch 25\n",
      "target probs tensor([[8.2104e-01],\n",
      "        [2.8543e-03],\n",
      "        [6.8935e-01],\n",
      "        [7.0268e-05],\n",
      "        [2.2546e-01],\n",
      "        [4.3976e-01],\n",
      "        [5.1962e-05],\n",
      "        [2.6261e-02],\n",
      "        [8.7459e-05],\n",
      "        [2.1300e-04],\n",
      "        [1.1114e-06],\n",
      "        [9.7820e-07],\n",
      "        [4.7391e-04],\n",
      "        [4.8730e-01],\n",
      "        [1.0767e-09],\n",
      "        [7.5120e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.2769084572792053: \n",
      "func:cos_distance, ap_dist: -0.9944064617156982, an_dist: -0.7934445142745972\n",
      "target probs tensor([[9.4392e-01],\n",
      "        [2.0804e-08],\n",
      "        [5.9820e-07],\n",
      "        [6.3935e-04],\n",
      "        [9.1697e-05],\n",
      "        [6.6279e-10],\n",
      "        [1.1400e-02],\n",
      "        [3.9987e-04],\n",
      "        [7.7403e-05],\n",
      "        [9.9878e-01],\n",
      "        [8.7561e-05],\n",
      "        [2.0458e-01],\n",
      "        [9.0419e-01],\n",
      "        [2.9351e-02],\n",
      "        [1.4656e-10],\n",
      "        [9.9258e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.0693289041519165: \n",
      "func:cos_distance, ap_dist: -0.9644449949264526, an_dist: -0.8522942662239075\n",
      "target probs tensor([[1.2639e-01],\n",
      "        [9.9740e-01],\n",
      "        [1.0241e-02],\n",
      "        [5.3732e-09],\n",
      "        [7.6141e-06],\n",
      "        [4.4392e-05],\n",
      "        [1.8939e-06],\n",
      "        [5.1544e-04],\n",
      "        [1.0524e-03],\n",
      "        [1.9043e-08],\n",
      "        [1.2381e-05],\n",
      "        [9.1544e-01],\n",
      "        [1.1245e-03],\n",
      "        [8.0156e-07],\n",
      "        [2.6075e-07],\n",
      "        [4.4921e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.538662314414978: \n",
      "func:cos_distance, ap_dist: -0.9944782853126526, an_dist: -0.5434697270393372\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 26 with validation value: 0.796999990940094.\n",
      "target probs tensor([[9.6798e-04],\n",
      "        [9.7776e-01],\n",
      "        [3.8770e-03],\n",
      "        [8.7209e-01],\n",
      "        [1.5115e-01],\n",
      "        [2.0242e-08],\n",
      "        [1.2713e-03],\n",
      "        [3.6235e-03],\n",
      "        [1.8396e-01],\n",
      "        [9.5574e-01],\n",
      "        [9.3664e-05],\n",
      "        [6.5647e-07],\n",
      "        [7.2403e-01],\n",
      "        [3.0627e-04],\n",
      "        [2.5275e-03],\n",
      "        [6.3246e-10]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.665463924407959: \n",
      "func:cos_distance, ap_dist: -0.9911257028579712, an_dist: -0.6753391027450562\n",
      "target probs tensor([[3.9331e-08],\n",
      "        [5.7458e-04],\n",
      "        [9.0559e-06],\n",
      "        [1.6830e-05],\n",
      "        [1.8275e-02],\n",
      "        [6.0201e-05],\n",
      "        [7.3058e-05],\n",
      "        [6.4056e-01],\n",
      "        [1.7976e-07],\n",
      "        [7.0610e-06],\n",
      "        [1.2640e-07],\n",
      "        [1.9857e-05],\n",
      "        [7.0837e-04],\n",
      "        [6.3948e-02],\n",
      "        [1.0244e-03],\n",
      "        [5.3148e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.07280249148607254: \n",
      "func:cos_distance, ap_dist: -0.9909231066703796, an_dist: -0.6006662845611572\n",
      "target probs tensor([[9.2531e-05],\n",
      "        [2.2542e-07],\n",
      "        [2.7681e-03],\n",
      "        [5.6089e-06],\n",
      "        [2.9545e-07],\n",
      "        [4.9426e-02],\n",
      "        [1.4938e-06],\n",
      "        [1.2067e-04],\n",
      "        [2.1157e-04],\n",
      "        [5.6555e-01],\n",
      "        [2.0265e-06],\n",
      "        [8.3119e-05],\n",
      "        [5.9039e-01],\n",
      "        [1.7035e-02],\n",
      "        [1.5313e-02],\n",
      "        [6.1501e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.1729602962732315: \n",
      "func:cos_distance, ap_dist: -0.9636372327804565, an_dist: -0.5064433813095093\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "target probs tensor([[1.0344e-01],\n",
      "        [1.1687e-05],\n",
      "        [2.0364e-05],\n",
      "        [4.8904e-01],\n",
      "        [2.4599e-01],\n",
      "        [9.9760e-01],\n",
      "        [6.6777e-07],\n",
      "        [2.7451e-07],\n",
      "        [9.7881e-03],\n",
      "        [3.6594e-02],\n",
      "        [1.0856e-05],\n",
      "        [1.5806e-03],\n",
      "        [1.8252e-02],\n",
      "        [2.8160e-01],\n",
      "        [3.9916e-01],\n",
      "        [5.3284e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5002221465110779: \n",
      "func:cos_distance, ap_dist: -0.9937975406646729, an_dist: -0.6303669214248657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[6.2614e-02],\n",
      "        [1.1898e-02],\n",
      "        [1.3015e-03],\n",
      "        [7.9686e-09],\n",
      "        [5.1209e-03],\n",
      "        [8.9626e-05],\n",
      "        [2.1598e-01],\n",
      "        [9.3216e-06],\n",
      "        [9.0859e-05],\n",
      "        [4.3711e-08],\n",
      "        [2.3874e-01],\n",
      "        [2.2123e-01],\n",
      "        [1.6405e-05],\n",
      "        [5.3307e-01],\n",
      "        [6.8427e-05],\n",
      "        [7.2698e-09]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.10069146007299423: \n",
      "func:cos_distance, ap_dist: -0.9866300821304321, an_dist: -0.6475640535354614\n",
      "target probs tensor([[7.5889e-01],\n",
      "        [6.1609e-07],\n",
      "        [2.6549e-06],\n",
      "        [3.8815e-01],\n",
      "        [1.3905e-01],\n",
      "        [2.7370e-01],\n",
      "        [7.3242e-04],\n",
      "        [4.8963e-01],\n",
      "        [5.3715e-06],\n",
      "        [1.6330e-02],\n",
      "        [5.9590e-08],\n",
      "        [1.8272e-06],\n",
      "        [1.4723e-03],\n",
      "        [1.5454e-01],\n",
      "        [5.3049e-07],\n",
      "        [4.8861e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.2026553601026535: \n",
      "func:cos_distance, ap_dist: -0.9967705607414246, an_dist: -0.4682345688343048\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 28 with validation value: 0.7990000247955322.\n",
      "fooling weight increased to 5.299999999999998 at the end of epoch 28\n",
      "target probs tensor([[5.5725e-04],\n",
      "        [4.5439e-01],\n",
      "        [9.1185e-06],\n",
      "        [9.9329e-06],\n",
      "        [1.6396e-02],\n",
      "        [9.6543e-01],\n",
      "        [2.5751e-07],\n",
      "        [3.2126e-01],\n",
      "        [7.8038e-02],\n",
      "        [2.5421e-04],\n",
      "        [7.7147e-05],\n",
      "        [2.7296e-05],\n",
      "        [9.9146e-05],\n",
      "        [7.4957e-05],\n",
      "        [2.8727e-05],\n",
      "        [3.5672e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.2785574495792389: \n",
      "func:cos_distance, ap_dist: -0.991942286491394, an_dist: -0.7232179641723633\n",
      "target probs tensor([[3.0851e-04],\n",
      "        [2.1756e-11],\n",
      "        [4.0146e-02],\n",
      "        [3.9269e-08],\n",
      "        [6.4632e-04],\n",
      "        [5.5989e-02],\n",
      "        [4.6437e-02],\n",
      "        [1.5609e-04],\n",
      "        [4.2022e-05],\n",
      "        [1.0489e-02],\n",
      "        [3.9782e-04],\n",
      "        [6.5481e-02],\n",
      "        [2.6728e-03],\n",
      "        [1.8216e-03],\n",
      "        [1.5440e-08],\n",
      "        [5.8418e-09]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.014403654262423515: \n",
      "func:cos_distance, ap_dist: -0.995877742767334, an_dist: -0.4822816848754883\n",
      "target probs tensor([[1.9671e-07],\n",
      "        [4.7840e-03],\n",
      "        [1.5798e-02],\n",
      "        [3.4336e-06],\n",
      "        [3.0004e-11],\n",
      "        [8.3224e-03],\n",
      "        [2.3790e-01],\n",
      "        [1.5241e-03],\n",
      "        [2.5881e-04],\n",
      "        [4.7367e-06],\n",
      "        [2.6053e-06],\n",
      "        [1.9196e-02],\n",
      "        [6.9105e-01],\n",
      "        [9.4675e-03],\n",
      "        [9.6781e-01],\n",
      "        [1.0787e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.3160238564014435: \n",
      "func:cos_distance, ap_dist: -0.9944788217544556, an_dist: -0.6636199951171875\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 29 with validation value: 0.8009999990463257.\n",
      "target probs tensor([[4.2343e-03],\n",
      "        [2.3128e-02],\n",
      "        [2.2880e-02],\n",
      "        [2.3477e-07],\n",
      "        [1.9401e-05],\n",
      "        [3.3836e-01],\n",
      "        [3.1481e-01],\n",
      "        [4.7544e-03],\n",
      "        [3.1965e-02],\n",
      "        [1.7745e-03],\n",
      "        [8.8017e-01],\n",
      "        [6.9985e-07],\n",
      "        [7.5722e-05],\n",
      "        [9.4304e-01],\n",
      "        [6.5963e-01],\n",
      "        [1.4503e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.4341185688972473: \n",
      "func:cos_distance, ap_dist: -0.9904823303222656, an_dist: -0.8868838548660278\n",
      "target probs tensor([[1.4438e-02],\n",
      "        [1.0504e-03],\n",
      "        [5.3716e-05],\n",
      "        [1.5821e-01],\n",
      "        [9.8024e-01],\n",
      "        [3.5716e-02],\n",
      "        [1.1801e-02],\n",
      "        [1.3249e-04],\n",
      "        [4.1248e-05],\n",
      "        [1.4068e-01],\n",
      "        [1.3343e-06],\n",
      "        [1.8759e-05],\n",
      "        [2.5012e-01],\n",
      "        [9.5232e-01],\n",
      "        [7.7465e-02],\n",
      "        [4.9226e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.4827384352684021: \n",
      "func:cos_distance, ap_dist: -0.9880958199501038, an_dist: -0.535066545009613\n",
      "target probs tensor([[2.6467e-11],\n",
      "        [6.9789e-03],\n",
      "        [5.5193e-05],\n",
      "        [3.8866e-01],\n",
      "        [1.6009e-03],\n",
      "        [4.7985e-02],\n",
      "        [7.9485e-03],\n",
      "        [1.4476e-05],\n",
      "        [2.3540e-03],\n",
      "        [2.8663e-02],\n",
      "        [4.4254e-05],\n",
      "        [3.7686e-05],\n",
      "        [2.4922e-06],\n",
      "        [2.1871e-07],\n",
      "        [4.1726e-03],\n",
      "        [5.7178e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.03710616007447243: \n",
      "func:cos_distance, ap_dist: -0.9898018836975098, an_dist: -0.6531027555465698\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 5.599999999999998 at the end of epoch 30\n",
      "target probs tensor([[3.4751e-01],\n",
      "        [1.4246e-04],\n",
      "        [1.1083e-06],\n",
      "        [1.4706e-01],\n",
      "        [2.9736e-08],\n",
      "        [8.6435e-01],\n",
      "        [5.0185e-06],\n",
      "        [6.2115e-06],\n",
      "        [5.6122e-03],\n",
      "        [2.2810e-06],\n",
      "        [6.0276e-01],\n",
      "        [4.1824e-02],\n",
      "        [3.9929e-02],\n",
      "        [1.7326e-11],\n",
      "        [8.0243e-05],\n",
      "        [1.2888e-08]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.22476747632026672: \n",
      "func:cos_distance, ap_dist: -0.9818078279495239, an_dist: -0.6423448324203491\n",
      "target probs tensor([[2.2274e-01],\n",
      "        [9.4318e-06],\n",
      "        [1.8018e-02],\n",
      "        [8.8440e-06],\n",
      "        [1.5080e-04],\n",
      "        [2.7362e-05],\n",
      "        [1.9517e-06],\n",
      "        [6.8760e-01],\n",
      "        [2.0422e-04],\n",
      "        [4.3208e-01],\n",
      "        [1.2058e-02],\n",
      "        [4.7873e-05],\n",
      "        [3.9713e-04],\n",
      "        [9.4393e-05],\n",
      "        [3.4954e-04],\n",
      "        [2.6020e-08]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.12580271065235138: \n",
      "func:cos_distance, ap_dist: -0.9899517297744751, an_dist: -0.8151041269302368\n",
      "target probs tensor([[1.2160e-08],\n",
      "        [1.2605e-05],\n",
      "        [9.7305e-01],\n",
      "        [3.6534e-02],\n",
      "        [4.5801e-04],\n",
      "        [4.6106e-05],\n",
      "        [1.1779e-04],\n",
      "        [2.4617e-02],\n",
      "        [2.1152e-02],\n",
      "        [5.3414e-06],\n",
      "        [1.3200e-09],\n",
      "        [4.6593e-04],\n",
      "        [1.2277e-04],\n",
      "        [2.9297e-04],\n",
      "        [6.6262e-01],\n",
      "        [7.3182e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.3813367486000061: \n",
      "func:cos_distance, ap_dist: -0.9973043203353882, an_dist: -0.782204270362854\n",
      "target probs tensor([[2.6453e-07],\n",
      "        [1.2785e-08],\n",
      "        [3.0772e-08],\n",
      "        [2.1404e-01],\n",
      "        [6.2503e-02],\n",
      "        [4.3954e-05],\n",
      "        [7.6635e-06],\n",
      "        [6.2151e-05]], device='cuda:0'), loss: 0.03818847984075546: \n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "func:cos_distance, ap_dist: -0.9978845119476318, an_dist: -0.5638121962547302\n",
      "target probs tensor([[3.8797e-02],\n",
      "        [1.0710e-05],\n",
      "        [9.9739e-01],\n",
      "        [1.8650e-06],\n",
      "        [1.0827e-06],\n",
      "        [1.4155e-02],\n",
      "        [1.0911e-02],\n",
      "        [1.5177e-05],\n",
      "        [5.8421e-01],\n",
      "        [1.0643e-03],\n",
      "        [5.6911e-02],\n",
      "        [2.8848e-05],\n",
      "        [1.8051e-08],\n",
      "        [1.9170e-04],\n",
      "        [3.6089e-06],\n",
      "        [1.0529e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.4345046281814575: \n",
      "func:cos_distance, ap_dist: -0.9939117431640625, an_dist: -0.4305506646633148\n",
      "target probs tensor([[3.7113e-07],\n",
      "        [5.2818e-10],\n",
      "        [5.8535e-01],\n",
      "        [2.0550e-06],\n",
      "        [1.6636e-04],\n",
      "        [6.1772e-06],\n",
      "        [1.5196e-01],\n",
      "        [2.4332e-01],\n",
      "        [2.3019e-01],\n",
      "        [5.5282e-06],\n",
      "        [1.2647e-06],\n",
      "        [9.1897e-01],\n",
      "        [1.2912e-01],\n",
      "        [3.4702e-01],\n",
      "        [4.6611e-09],\n",
      "        [2.5520e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.2914656400680542: \n",
      "func:cos_distance, ap_dist: -0.996563196182251, an_dist: -0.6554428339004517\n",
      "target probs tensor([[4.2043e-08],\n",
      "        [1.9336e-04],\n",
      "        [5.3174e-03],\n",
      "        [4.8177e-02],\n",
      "        [4.8498e-03],\n",
      "        [9.3252e-01],\n",
      "        [6.1252e-10],\n",
      "        [6.9343e-03],\n",
      "        [7.8149e-05],\n",
      "        [4.8974e-06],\n",
      "        [2.7544e-05],\n",
      "        [4.6771e-01],\n",
      "        [1.3226e-02],\n",
      "        [5.6133e-04],\n",
      "        [1.5835e-05],\n",
      "        [7.7278e-01]], device='cuda:0'), loss: 0.3055659532546997: \n",
      "func:cos_distance, ap_dist: -0.9882215261459351, an_dist: -0.5311447978019714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 5.899999999999998 at the end of epoch 32\n",
      "target probs tensor([[4.2453e-02],\n",
      "        [9.7305e-01],\n",
      "        [4.0159e-03],\n",
      "        [1.3239e-05],\n",
      "        [5.2774e-04],\n",
      "        [3.1017e-05],\n",
      "        [3.1961e-03],\n",
      "        [2.6102e-04],\n",
      "        [7.2318e-07],\n",
      "        [2.0899e-06],\n",
      "        [5.5535e-01],\n",
      "        [2.0326e-05],\n",
      "        [7.1902e-04],\n",
      "        [6.0210e-06],\n",
      "        [3.0713e-02],\n",
      "        [3.1260e-08]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.2817176580429077: \n",
      "func:cos_distance, ap_dist: -0.9900106191635132, an_dist: -0.5127688646316528\n",
      "target probs tensor([[1.8078e-09],\n",
      "        [2.9989e-04],\n",
      "        [8.4468e-01],\n",
      "        [6.2788e-02],\n",
      "        [1.2051e-05],\n",
      "        [4.8506e-06],\n",
      "        [8.0656e-07],\n",
      "        [7.4614e-02],\n",
      "        [1.7034e-09],\n",
      "        [8.8147e-03],\n",
      "        [7.0550e-01],\n",
      "        [8.3572e-01],\n",
      "        [1.1134e-03],\n",
      "        [9.8886e-01],\n",
      "        [6.9460e-07],\n",
      "        [1.1856e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5963916182518005: \n",
      "func:cos_distance, ap_dist: -0.9823793172836304, an_dist: -0.7317168712615967\n",
      "target probs tensor([[2.8396e-08],\n",
      "        [9.7966e-01],\n",
      "        [1.7409e-03],\n",
      "        [5.0276e-02],\n",
      "        [1.9450e-03],\n",
      "        [1.5596e-05],\n",
      "        [6.7662e-04],\n",
      "        [4.9322e-06],\n",
      "        [8.7939e-01],\n",
      "        [6.7765e-04],\n",
      "        [3.6989e-03],\n",
      "        [5.8633e-01],\n",
      "        [8.2816e-06],\n",
      "        [9.8544e-07],\n",
      "        [3.1781e-02],\n",
      "        [3.9165e-03]], device='cuda:0'), loss: 0.4368607699871063: \n",
      "func:cos_distance, ap_dist: -0.9940789341926575, an_dist: -0.7287003993988037\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 33 with validation value: 0.8059999942779541.\n",
      "target probs tensor([[5.7127e-03],\n",
      "        [1.8224e-04],\n",
      "        [1.2043e-07],\n",
      "        [2.1058e-09],\n",
      "        [3.8394e-06],\n",
      "        [3.2391e-04],\n",
      "        [1.8326e-02],\n",
      "        [9.9836e-01],\n",
      "        [2.0703e-05],\n",
      "        [1.9766e-05],\n",
      "        [8.0168e-01],\n",
      "        [5.8932e-03],\n",
      "        [2.5644e-03],\n",
      "        [2.7610e-08],\n",
      "        [1.8559e-02],\n",
      "        [1.2444e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5061150789260864: \n",
      "func:cos_distance, ap_dist: -0.9859013557434082, an_dist: -0.5061088800430298\n",
      "target probs tensor([[1.0167e-07],\n",
      "        [1.1950e-04],\n",
      "        [9.9881e-01],\n",
      "        [3.0061e-10],\n",
      "        [1.2786e-02],\n",
      "        [5.1306e-01],\n",
      "        [2.5619e-07],\n",
      "        [8.0138e-03],\n",
      "        [2.2535e-01],\n",
      "        [3.0617e-07],\n",
      "        [7.0246e-04],\n",
      "        [1.1475e-04],\n",
      "        [9.9401e-01],\n",
      "        [1.6014e-01],\n",
      "        [3.7256e-01],\n",
      "        [1.5409e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.8533083200454712: \n",
      "func:cos_distance, ap_dist: -0.9758365154266357, an_dist: -0.5348016023635864\n",
      "target probs tensor([[3.8201e-10],\n",
      "        [8.1924e-01],\n",
      "        [2.3658e-06],\n",
      "        [5.9145e-09],\n",
      "        [5.7820e-03],\n",
      "        [2.9696e-02],\n",
      "        [7.4990e-02],\n",
      "        [4.7008e-04],\n",
      "        [1.3979e-03],\n",
      "        [4.0737e-05],\n",
      "        [1.3145e-04],\n",
      "        [1.7993e-04],\n",
      "        [7.9159e-01],\n",
      "        [4.9209e-05],\n",
      "        [3.3124e-01],\n",
      "        [1.6727e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.24877342581748962: \n",
      "func:cos_distance, ap_dist: -0.9897199869155884, an_dist: -0.6361598968505859\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 34 with validation value: 0.8069999814033508.\n",
      "target probs tensor([[2.9148e-06],\n",
      "        [5.5892e-02],\n",
      "        [5.2686e-06],\n",
      "        [3.3598e-08],\n",
      "        [4.8015e-01],\n",
      "        [3.4565e-05],\n",
      "        [6.1534e-01],\n",
      "        [6.8010e-02],\n",
      "        [1.4666e-10],\n",
      "        [3.5795e-04],\n",
      "        [7.7582e-01],\n",
      "        [6.0285e-05],\n",
      "        [2.0339e-06],\n",
      "        [8.3220e-02],\n",
      "        [4.8450e-01],\n",
      "        [2.1760e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.24894145131111145: \n",
      "func:cos_distance, ap_dist: -0.9918675422668457, an_dist: -0.5331319570541382\n",
      "target probs tensor([[2.7210e-05],\n",
      "        [1.4085e-05],\n",
      "        [6.8354e-01],\n",
      "        [6.7308e-02],\n",
      "        [5.2904e-01],\n",
      "        [2.3211e-01],\n",
      "        [7.3808e-03],\n",
      "        [5.7807e-04],\n",
      "        [4.8261e-07],\n",
      "        [1.7889e-05],\n",
      "        [2.0883e-06],\n",
      "        [5.1931e-05],\n",
      "        [2.2596e-03],\n",
      "        [2.5346e-01],\n",
      "        [3.7131e-06],\n",
      "        [3.6231e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.15874911844730377: \n",
      "func:cos_distance, ap_dist: -0.9965119957923889, an_dist: -0.7369143962860107\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 35 with validation value: 0.8149999976158142.\n",
      "target probs tensor([[1.3728e-07],\n",
      "        [4.8054e-01],\n",
      "        [1.3397e-06],\n",
      "        [2.4000e-04],\n",
      "        [3.3265e-05],\n",
      "        [6.5975e-06],\n",
      "        [2.1758e-04],\n",
      "        [1.7095e-07],\n",
      "        [8.9167e-03],\n",
      "        [1.5184e-01],\n",
      "        [1.5570e-08],\n",
      "        [2.8178e-04],\n",
      "        [4.0695e-03],\n",
      "        [3.5801e-02],\n",
      "        [6.9890e-09],\n",
      "        [1.3779e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.05445659160614014: \n",
      "func:cos_distance, ap_dist: -0.9918389320373535, an_dist: -0.7404156923294067\n",
      "target probs tensor([[8.4595e-03],\n",
      "        [2.0352e-02],\n",
      "        [9.2964e-03],\n",
      "        [1.0738e-06],\n",
      "        [4.2044e-04],\n",
      "        [1.1801e-01],\n",
      "        [3.6833e-09],\n",
      "        [6.4932e-02],\n",
      "        [1.9949e-04],\n",
      "        [1.3952e-02],\n",
      "        [6.8699e-04],\n",
      "        [8.3014e-05],\n",
      "        [6.8342e-06],\n",
      "        [9.8126e-05],\n",
      "        [7.2650e-03],\n",
      "        [2.6780e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.015871815383434296: \n",
      "func:cos_distance, ap_dist: -0.996493399143219, an_dist: -0.7876070737838745\n",
      "target probs tensor([[1.1290e-08],\n",
      "        [1.3524e-08],\n",
      "        [4.7659e-04],\n",
      "        [3.7072e-01],\n",
      "        [9.4637e-01],\n",
      "        [1.1261e-04],\n",
      "        [2.7101e-05],\n",
      "        [2.3688e-04],\n",
      "        [3.1506e-06],\n",
      "        [5.9481e-08],\n",
      "        [2.2956e-02],\n",
      "        [7.1841e-03],\n",
      "        [6.0001e-04],\n",
      "        [7.3673e-02],\n",
      "        [5.7552e-02],\n",
      "        [1.2619e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.2222812920808792: \n",
      "func:cos_distance, ap_dist: -0.9946643710136414, an_dist: -0.7145348787307739\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 6.1999999999999975 at the end of epoch 36\n",
      "target probs tensor([[1.5096e-01],\n",
      "        [2.3589e-05],\n",
      "        [1.8604e-04],\n",
      "        [3.8140e-04],\n",
      "        [5.8793e-06],\n",
      "        [4.4157e-02],\n",
      "        [1.4674e-05],\n",
      "        [1.2172e-05],\n",
      "        [6.4189e-04],\n",
      "        [3.3779e-01],\n",
      "        [2.9043e-07],\n",
      "        [1.8161e-04],\n",
      "        [4.8174e-01],\n",
      "        [1.2224e-04],\n",
      "        [2.9763e-04],\n",
      "        [4.6084e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.08029591292142868: \n",
      "func:cos_distance, ap_dist: -0.9871823787689209, an_dist: -0.6798301935195923\n",
      "target probs tensor([[3.4931e-06],\n",
      "        [8.3878e-01],\n",
      "        [1.9337e-02],\n",
      "        [2.8843e-03],\n",
      "        [1.7241e-05],\n",
      "        [1.7607e-01],\n",
      "        [9.0920e-03],\n",
      "        [1.5199e-02],\n",
      "        [4.7562e-05],\n",
      "        [1.3231e-01],\n",
      "        [1.1696e-04],\n",
      "        [6.8125e-03],\n",
      "        [2.5486e-03],\n",
      "        [4.5386e-04],\n",
      "        [1.8006e-07],\n",
      "        [1.0000e+00]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.091641902923584: \n",
      "func:cos_distance, ap_dist: -0.9728652238845825, an_dist: -0.8027462959289551\n",
      "target probs tensor([[1.6661e-06],\n",
      "        [8.6691e-05],\n",
      "        [3.2210e-02],\n",
      "        [3.8015e-04],\n",
      "        [2.6352e-08],\n",
      "        [1.9239e-04],\n",
      "        [8.2135e-07],\n",
      "        [1.6098e-08],\n",
      "        [1.2306e-07],\n",
      "        [1.4693e-01],\n",
      "        [1.4912e-03],\n",
      "        [1.9248e-06],\n",
      "        [4.1657e-02],\n",
      "        [1.1298e-10],\n",
      "        [6.2911e-02],\n",
      "        [9.9428e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.3415684998035431: \n",
      "func:cos_distance, ap_dist: -0.9833630919456482, an_dist: -0.764639139175415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "target probs tensor([[9.8890e-05],\n",
      "        [9.9734e-01],\n",
      "        [1.0326e-03],\n",
      "        [9.9213e-01],\n",
      "        [4.6530e-05],\n",
      "        [4.4895e-04],\n",
      "        [1.0200e-04],\n",
      "        [2.2999e-03],\n",
      "        [9.5093e-01],\n",
      "        [2.2539e-03],\n",
      "        [2.5595e-03],\n",
      "        [3.8432e-01],\n",
      "        [1.9395e-07],\n",
      "        [3.0013e-05],\n",
      "        [1.3179e-07],\n",
      "        [3.0874e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.915827751159668: \n",
      "func:cos_distance, ap_dist: -0.9929287433624268, an_dist: -0.7930121421813965\n",
      "target probs tensor([[4.9826e-05],\n",
      "        [1.7481e-01],\n",
      "        [1.1159e-05],\n",
      "        [3.3480e-01],\n",
      "        [9.4434e-01],\n",
      "        [5.9518e-01],\n",
      "        [1.2493e-02],\n",
      "        [1.9618e-06],\n",
      "        [8.0509e-04],\n",
      "        [3.4606e-01],\n",
      "        [1.4232e-06],\n",
      "        [2.8985e-08],\n",
      "        [4.4790e-06],\n",
      "        [9.5230e-03],\n",
      "        [2.2021e-06],\n",
      "        [4.3209e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.30279088020324707: \n",
      "func:cos_distance, ap_dist: -0.9898800849914551, an_dist: -0.48520272970199585\n",
      "target probs tensor([[2.8112e-02],\n",
      "        [2.6383e-05],\n",
      "        [1.4626e-01],\n",
      "        [2.1221e-04],\n",
      "        [9.3583e-08],\n",
      "        [1.1391e-05],\n",
      "        [1.3492e-01],\n",
      "        [6.9649e-02],\n",
      "        [4.6560e-05],\n",
      "        [1.1283e-01],\n",
      "        [9.4036e-01],\n",
      "        [9.9952e-01],\n",
      "        [2.8962e-08],\n",
      "        [5.2354e-08],\n",
      "        [1.2795e-01],\n",
      "        [3.0528e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.7177750468254089: \n",
      "func:cos_distance, ap_dist: -0.9873740673065186, an_dist: -0.6890270709991455\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 38 with validation value: 0.8169999718666077.\n",
      "target probs tensor([[9.9664e-01],\n",
      "        [5.5223e-03],\n",
      "        [1.7120e-06],\n",
      "        [4.7825e-01],\n",
      "        [9.6348e-01],\n",
      "        [7.7629e-01],\n",
      "        [1.0426e-06],\n",
      "        [3.8227e-07],\n",
      "        [5.7317e-05],\n",
      "        [7.5398e-05],\n",
      "        [9.0351e-01],\n",
      "        [2.6368e-05],\n",
      "        [1.5495e-01],\n",
      "        [9.9520e-01],\n",
      "        [1.1672e-04],\n",
      "        [5.8151e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.1878106594085693: \n",
      "func:cos_distance, ap_dist: -0.9894587397575378, an_dist: -0.6949069499969482\n",
      "target probs tensor([[1.4490e-05],\n",
      "        [5.0537e-02],\n",
      "        [1.2244e-03],\n",
      "        [2.3515e-04],\n",
      "        [7.1125e-09],\n",
      "        [6.2520e-01],\n",
      "        [5.8028e-01],\n",
      "        [1.0290e-01],\n",
      "        [4.1207e-07],\n",
      "        [1.3850e-08],\n",
      "        [2.0816e-06],\n",
      "        [1.7308e-01],\n",
      "        [9.9584e-08],\n",
      "        [7.0282e-01],\n",
      "        [3.3867e-02],\n",
      "        [3.0837e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.21577873826026917: \n",
      "func:cos_distance, ap_dist: -0.9625787734985352, an_dist: -0.6920791864395142\n",
      "target probs tensor([[6.0129e-05],\n",
      "        [2.1060e-01],\n",
      "        [2.6801e-03],\n",
      "        [5.5283e-02],\n",
      "        [4.7545e-05],\n",
      "        [7.4902e-01],\n",
      "        [1.8706e-08],\n",
      "        [8.9270e-02],\n",
      "        [5.9845e-09],\n",
      "        [9.3769e-04],\n",
      "        [9.9504e-04],\n",
      "        [7.4833e-01],\n",
      "        [7.7750e-03],\n",
      "        [7.3214e-04],\n",
      "        [9.9073e-01],\n",
      "        [1.3754e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.4903049170970917: \n",
      "func:cos_distance, ap_dist: -0.9946381449699402, an_dist: -0.7574113607406616\n",
      "target probs tensor([[4.6137e-08],\n",
      "        [4.0480e-07],\n",
      "        [3.6532e-06],\n",
      "        [2.4296e-01],\n",
      "        [2.0677e-02],\n",
      "        [6.8479e-06],\n",
      "        [5.5919e-08],\n",
      "        [2.2087e-03]], device='cuda:0'), loss: 0.03768210485577583: \n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "func:cos_distance, ap_dist: -0.9738818407058716, an_dist: -0.6174930334091187\n",
      "target probs tensor([[1.3167e-01],\n",
      "        [5.8558e-08],\n",
      "        [5.3813e-09],\n",
      "        [4.3521e-05],\n",
      "        [2.2840e-03],\n",
      "        [3.7563e-02],\n",
      "        [9.3443e-02],\n",
      "        [4.1241e-03],\n",
      "        [2.0177e-01],\n",
      "        [1.0695e-06],\n",
      "        [1.7800e-01],\n",
      "        [3.8042e-06],\n",
      "        [4.9170e-05],\n",
      "        [1.0779e-06],\n",
      "        [4.7150e-06],\n",
      "        [4.9313e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.04412263631820679: \n",
      "func:cos_distance, ap_dist: -0.9856276512145996, an_dist: -0.5769418478012085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[9.0279e-05],\n",
      "        [2.4707e-05],\n",
      "        [8.1624e-01],\n",
      "        [6.2449e-04],\n",
      "        [6.9241e-03],\n",
      "        [1.9310e-02],\n",
      "        [8.2394e-05],\n",
      "        [3.0327e-02],\n",
      "        [1.5347e-06],\n",
      "        [1.0447e-01],\n",
      "        [2.6695e-02],\n",
      "        [1.6786e-07],\n",
      "        [2.4344e-06],\n",
      "        [1.5458e-06],\n",
      "        [9.9525e-01],\n",
      "        [6.0964e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5112132430076599: \n",
      "func:cos_distance, ap_dist: -0.9917570948600769, an_dist: -0.7254085540771484\n",
      "target probs tensor([[1.2379e-04],\n",
      "        [2.1373e-01],\n",
      "        [3.0284e-03],\n",
      "        [9.7300e-02],\n",
      "        [1.0405e-04],\n",
      "        [6.0132e-05],\n",
      "        [3.7148e-01],\n",
      "        [6.9360e-05],\n",
      "        [4.3428e-07],\n",
      "        [8.0161e-08],\n",
      "        [1.6618e-04],\n",
      "        [8.5644e-01],\n",
      "        [1.0730e-03],\n",
      "        [6.1318e-03],\n",
      "        [1.6642e-05],\n",
      "        [9.9330e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.48525622487068176: \n",
      "func:cos_distance, ap_dist: -0.9879052639007568, an_dist: -0.5612223148345947\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 7.099999999999997 at the end of epoch 43\n",
      "target probs tensor([[2.1268e-05],\n",
      "        [2.1562e-04],\n",
      "        [1.9582e-04],\n",
      "        [7.6640e-01],\n",
      "        [9.9429e-01],\n",
      "        [6.5643e-06],\n",
      "        [2.9297e-09],\n",
      "        [9.8939e-01],\n",
      "        [5.6453e-02],\n",
      "        [1.4546e-02],\n",
      "        [2.5997e-09],\n",
      "        [7.6304e-01],\n",
      "        [9.6540e-01],\n",
      "        [5.1611e-07],\n",
      "        [1.8649e-07],\n",
      "        [1.5488e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.0026328563690186: \n",
      "func:cos_distance, ap_dist: -0.9740242958068848, an_dist: -0.7766449451446533\n",
      "target probs tensor([[8.0699e-04],\n",
      "        [9.3079e-04],\n",
      "        [3.6035e-06],\n",
      "        [1.8691e-06],\n",
      "        [1.3076e-04],\n",
      "        [3.1552e-04],\n",
      "        [2.8163e-05],\n",
      "        [8.1350e-06],\n",
      "        [6.6644e-08],\n",
      "        [1.5578e-06],\n",
      "        [6.7837e-02],\n",
      "        [1.0826e-06],\n",
      "        [4.0734e-03],\n",
      "        [3.1702e-03],\n",
      "        [9.3527e-05],\n",
      "        [9.0512e-10]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.004989201668649912: \n",
      "func:cos_distance, ap_dist: -0.9960786700248718, an_dist: -0.6720262765884399\n",
      "target probs tensor([[1.2951e-03],\n",
      "        [1.0644e-02],\n",
      "        [2.2656e-07],\n",
      "        [5.6271e-06],\n",
      "        [7.7813e-01],\n",
      "        [1.2143e-02],\n",
      "        [6.7817e-07],\n",
      "        [1.7717e-05],\n",
      "        [1.5884e-05],\n",
      "        [5.3406e-01],\n",
      "        [1.8467e-04],\n",
      "        [2.2916e-08],\n",
      "        [3.1507e-02],\n",
      "        [1.0354e-04],\n",
      "        [4.2314e-02],\n",
      "        [1.0104e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.14807896316051483: \n",
      "func:cos_distance, ap_dist: -0.9921957850456238, an_dist: -0.5456193685531616\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 7.399999999999997 at the end of epoch 44\n",
      "target probs tensor([[5.6105e-03],\n",
      "        [1.4558e-01],\n",
      "        [5.4440e-01],\n",
      "        [1.2439e-02],\n",
      "        [1.5295e-02],\n",
      "        [2.2252e-02],\n",
      "        [4.5261e-03],\n",
      "        [7.9615e-01],\n",
      "        [2.4085e-03],\n",
      "        [4.7195e-05],\n",
      "        [4.2758e-07],\n",
      "        [4.1351e-06],\n",
      "        [5.8629e-03],\n",
      "        [6.2202e-01],\n",
      "        [1.3433e-03],\n",
      "        [9.9564e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5632569789886475: \n",
      "func:cos_distance, ap_dist: -0.9928856492042542, an_dist: -0.7114239931106567\n",
      "target probs tensor([[9.9587e-01],\n",
      "        [8.0242e-06],\n",
      "        [4.3995e-03],\n",
      "        [4.3197e-05],\n",
      "        [6.5298e-07],\n",
      "        [1.7061e-04],\n",
      "        [9.9577e-01],\n",
      "        [1.6694e-08],\n",
      "        [6.3647e-01],\n",
      "        [1.6429e-04],\n",
      "        [2.0245e-07],\n",
      "        [1.3073e-01],\n",
      "        [7.7601e-01],\n",
      "        [3.6579e-05],\n",
      "        [2.0977e-01],\n",
      "        [8.5127e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.8652534484863281: \n",
      "func:cos_distance, ap_dist: -0.9900306463241577, an_dist: -0.6458271145820618\n",
      "target probs tensor([[4.2319e-07],\n",
      "        [2.8179e-02],\n",
      "        [1.5374e-04],\n",
      "        [4.2957e-03],\n",
      "        [2.2990e-04],\n",
      "        [4.3125e-07],\n",
      "        [3.2465e-05],\n",
      "        [5.6811e-04],\n",
      "        [5.6377e-05],\n",
      "        [3.9915e-04],\n",
      "        [2.7644e-02],\n",
      "        [1.9900e-04],\n",
      "        [2.7373e-05],\n",
      "        [1.3495e-01],\n",
      "        [1.0428e-02],\n",
      "        [1.2599e-08]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.013627535663545132: \n",
      "func:cos_distance, ap_dist: -0.9817657470703125, an_dist: -0.5444779396057129\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "target probs tensor([[3.3322e-03],\n",
      "        [1.5356e-01],\n",
      "        [1.8373e-05],\n",
      "        [3.0112e-02],\n",
      "        [4.8764e-01],\n",
      "        [2.8925e-05],\n",
      "        [1.8877e-05],\n",
      "        [2.0470e-06],\n",
      "        [6.6448e-05],\n",
      "        [1.0791e-08],\n",
      "        [5.9024e-01],\n",
      "        [7.8239e-01],\n",
      "        [3.8586e-06],\n",
      "        [4.2631e-03],\n",
      "        [1.4296e-01],\n",
      "        [2.6692e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.21702197194099426: \n",
      "func:cos_distance, ap_dist: -0.9920332431793213, an_dist: -0.7270515561103821\n",
      "target probs tensor([[9.8469e-04],\n",
      "        [3.0763e-01],\n",
      "        [2.4919e-01],\n",
      "        [2.9269e-06],\n",
      "        [1.0779e-01],\n",
      "        [5.5475e-01],\n",
      "        [1.5706e-02],\n",
      "        [5.3724e-06],\n",
      "        [2.1111e-02],\n",
      "        [3.8516e-05],\n",
      "        [2.9041e-01],\n",
      "        [3.3725e-07],\n",
      "        [1.5565e-05],\n",
      "        [3.0255e-03],\n",
      "        [3.2924e-01],\n",
      "        [2.3020e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.14756648242473602: \n",
      "func:cos_distance, ap_dist: -0.9859662652015686, an_dist: -0.6540470719337463\n",
      "target probs tensor([[9.6625e-01],\n",
      "        [1.9204e-04],\n",
      "        [3.9485e-04],\n",
      "        [3.9445e-02],\n",
      "        [1.7705e-07],\n",
      "        [1.1305e-02],\n",
      "        [6.3119e-02],\n",
      "        [2.4143e-05],\n",
      "        [9.1387e-04],\n",
      "        [2.8216e-01],\n",
      "        [1.0524e-06],\n",
      "        [8.6028e-01],\n",
      "        [4.9056e-06],\n",
      "        [6.8339e-01],\n",
      "        [2.0669e-02],\n",
      "        [3.0580e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.4363001883029938: \n",
      "func:cos_distance, ap_dist: -0.9943062663078308, an_dist: -0.5401769876480103\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 46 with validation value: 0.8199999928474426.\n",
      "target probs tensor([[1.3989e-04],\n",
      "        [2.4178e-05],\n",
      "        [2.1915e-02],\n",
      "        [6.2749e-06],\n",
      "        [9.3959e-05],\n",
      "        [9.6140e-01],\n",
      "        [1.7088e-07],\n",
      "        [4.9420e-01],\n",
      "        [8.7413e-02],\n",
      "        [4.6735e-08],\n",
      "        [6.1540e-07],\n",
      "        [5.6913e-07],\n",
      "        [4.0784e-01],\n",
      "        [1.7280e-03],\n",
      "        [9.1007e-04],\n",
      "        [3.2973e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.2860405743122101: \n",
      "func:cos_distance, ap_dist: -0.9969680309295654, an_dist: -0.7316164970397949\n",
      "target probs tensor([[3.7355e-01],\n",
      "        [9.2762e-01],\n",
      "        [5.6677e-07],\n",
      "        [1.7278e-03],\n",
      "        [8.2326e-07],\n",
      "        [9.7952e-02],\n",
      "        [9.9549e-05],\n",
      "        [1.6804e-03],\n",
      "        [4.9375e-01],\n",
      "        [1.2121e-04],\n",
      "        [9.5536e-05],\n",
      "        [1.4517e-01],\n",
      "        [9.6447e-01],\n",
      "        [1.6025e-02],\n",
      "        [3.9438e-01],\n",
      "        [7.0647e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.4937557578086853: \n",
      "func:cos_distance, ap_dist: -0.9951071739196777, an_dist: -0.7150307297706604\n",
      "target probs tensor([[8.4619e-03],\n",
      "        [5.5528e-02],\n",
      "        [1.5997e-02],\n",
      "        [5.2455e-08],\n",
      "        [5.6857e-06],\n",
      "        [3.7706e-02],\n",
      "        [1.2707e-03],\n",
      "        [6.3194e-01],\n",
      "        [5.6587e-01],\n",
      "        [8.8089e-07],\n",
      "        [6.1737e-01],\n",
      "        [2.9987e-03],\n",
      "        [4.9414e-04],\n",
      "        [9.4035e-01],\n",
      "        [7.3934e-01],\n",
      "        [6.2345e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.44673359394073486: \n",
      "func:cos_distance, ap_dist: -0.9944074153900146, an_dist: -0.7212038636207581\n",
      "target probs tensor([[5.4445e-07],\n",
      "        [4.7833e-07],\n",
      "        [2.6976e-07],\n",
      "        [4.1597e-01],\n",
      "        [2.7538e-05],\n",
      "        [3.1646e-05],\n",
      "        [5.3355e-08],\n",
      "        [8.0035e-02]], device='cuda:0'), loss: 0.07766078412532806: \n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "func:cos_distance, ap_dist: -0.996536135673523, an_dist: -0.6266657114028931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[1.4603e-08],\n",
      "        [9.5958e-01],\n",
      "        [3.0261e-04],\n",
      "        [8.0027e-06],\n",
      "        [1.6801e-01],\n",
      "        [1.0021e-03],\n",
      "        [4.6075e-03],\n",
      "        [1.2948e-06],\n",
      "        [9.5377e-10],\n",
      "        [2.9841e-08],\n",
      "        [2.5784e-03],\n",
      "        [7.5995e-05],\n",
      "        [7.7700e-01],\n",
      "        [6.1641e-02],\n",
      "        [3.0872e-05],\n",
      "        [3.3636e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.3103298544883728: \n",
      "func:cos_distance, ap_dist: -0.9946689009666443, an_dist: -0.7768344879150391\n",
      "target probs tensor([[1.0303e-05],\n",
      "        [1.2749e-04],\n",
      "        [8.5661e-05],\n",
      "        [8.0589e-10],\n",
      "        [3.0005e-03],\n",
      "        [8.5087e-01],\n",
      "        [1.1067e-06],\n",
      "        [1.3612e-06],\n",
      "        [9.8966e-03],\n",
      "        [7.4129e-05],\n",
      "        [2.4803e-04],\n",
      "        [1.4077e-04],\n",
      "        [3.9373e-04],\n",
      "        [1.3502e-02],\n",
      "        [7.4858e-02],\n",
      "        [1.5325e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.12552277743816376: \n",
      "func:cos_distance, ap_dist: -0.9909242391586304, an_dist: -0.6130086183547974\n",
      "target probs tensor([[8.0039e-08],\n",
      "        [1.6224e-04],\n",
      "        [7.7468e-02],\n",
      "        [1.0546e-02],\n",
      "        [7.4957e-04],\n",
      "        [8.2400e-01],\n",
      "        [8.1652e-09],\n",
      "        [9.3902e-03],\n",
      "        [4.9126e-05],\n",
      "        [5.6960e-02],\n",
      "        [1.6895e-05],\n",
      "        [6.7181e-02],\n",
      "        [1.8862e-02],\n",
      "        [3.6385e-02],\n",
      "        [1.8391e-05],\n",
      "        [6.8197e-01]], device='cuda:0'), loss: 0.19805210828781128: \n",
      "func:cos_distance, ap_dist: -0.9945041537284851, an_dist: -0.6261964440345764\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 48 with validation value: 0.8270000219345093.\n",
      "fooling weight increased to 7.699999999999997 at the end of epoch 48\n",
      "target probs tensor([[1.9935e-06],\n",
      "        [1.4241e-09],\n",
      "        [1.5692e-02],\n",
      "        [7.3884e-02],\n",
      "        [9.3014e-04],\n",
      "        [1.4895e-01],\n",
      "        [4.6876e-05],\n",
      "        [1.2846e-09],\n",
      "        [2.7795e-03],\n",
      "        [7.4823e-06],\n",
      "        [8.2283e-03],\n",
      "        [9.6916e-01],\n",
      "        [1.0394e-07],\n",
      "        [6.6415e-01],\n",
      "        [2.7526e-05],\n",
      "        [1.8266e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.3148517608642578: \n",
      "func:cos_distance, ap_dist: -0.9867767095565796, an_dist: -0.597248375415802\n",
      "target probs tensor([[2.7125e-01],\n",
      "        [4.4535e-02],\n",
      "        [2.0832e-05],\n",
      "        [3.7299e-03],\n",
      "        [8.2598e-03],\n",
      "        [5.4146e-06],\n",
      "        [4.2964e-09],\n",
      "        [7.6555e-01],\n",
      "        [7.9940e-01],\n",
      "        [1.2603e-07],\n",
      "        [2.7420e-05],\n",
      "        [1.9929e-03],\n",
      "        [3.8441e-01],\n",
      "        [2.1000e-01],\n",
      "        [1.6177e-06],\n",
      "        [3.7976e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.2598590850830078: \n",
      "func:cos_distance, ap_dist: -0.9739317893981934, an_dist: -0.6856821179389954\n",
      "target probs tensor([[4.6605e-05],\n",
      "        [3.2038e-01],\n",
      "        [2.9419e-03],\n",
      "        [2.5456e-02],\n",
      "        [1.5478e-02],\n",
      "        [1.6441e-04],\n",
      "        [1.1832e-04],\n",
      "        [1.0718e-03],\n",
      "        [6.8607e-01],\n",
      "        [3.8125e-04],\n",
      "        [6.7323e-04],\n",
      "        [4.6229e-01],\n",
      "        [3.0697e-07],\n",
      "        [2.5664e-09],\n",
      "        [2.5236e-02],\n",
      "        [3.1705e-06]], device='cuda:0'), loss: 0.1398482769727707: \n",
      "func:cos_distance, ap_dist: -0.9958317279815674, an_dist: -0.5501472353935242\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 7.9999999999999964 at the end of epoch 49\n",
      "target probs tensor([[4.0625e-02],\n",
      "        [2.1316e-04],\n",
      "        [9.6531e-01],\n",
      "        [6.0501e-04],\n",
      "        [8.6253e-10],\n",
      "        [1.4465e-01],\n",
      "        [4.9099e-02],\n",
      "        [4.6002e-02],\n",
      "        [1.8246e-03],\n",
      "        [6.3225e-09],\n",
      "        [8.7952e-01],\n",
      "        [4.0110e-07],\n",
      "        [5.9799e-03],\n",
      "        [9.1958e-01],\n",
      "        [1.7292e-03],\n",
      "        [1.6902e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5189782381057739: \n",
      "func:cos_distance, ap_dist: -0.9875979423522949, an_dist: -0.8453255295753479\n",
      "target probs tensor([[1.5311e-03],\n",
      "        [7.8625e-04],\n",
      "        [1.6886e-03],\n",
      "        [1.7153e-03],\n",
      "        [1.4620e-03],\n",
      "        [1.2174e-02],\n",
      "        [3.1744e-06],\n",
      "        [4.6090e-02],\n",
      "        [5.0158e-01],\n",
      "        [2.9328e-05],\n",
      "        [2.0897e-02],\n",
      "        [2.2362e-04],\n",
      "        [2.4254e-02],\n",
      "        [9.6750e-10],\n",
      "        [1.6567e-03],\n",
      "        [1.8055e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.05179611220955849: \n",
      "func:cos_distance, ap_dist: -0.9789528846740723, an_dist: -0.5340614318847656\n",
      "target probs tensor([[6.5110e-02],\n",
      "        [9.2870e-05],\n",
      "        [1.0654e-02],\n",
      "        [3.6854e-08],\n",
      "        [1.7838e-08],\n",
      "        [2.9809e-05],\n",
      "        [3.5278e-05],\n",
      "        [4.0477e-02],\n",
      "        [9.4906e-03],\n",
      "        [6.0616e-01],\n",
      "        [9.9973e-01],\n",
      "        [3.6246e-05],\n",
      "        [9.9491e-01],\n",
      "        [6.1521e-01],\n",
      "        [1.6765e-06],\n",
      "        [7.5027e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.0555437803268433: \n",
      "func:cos_distance, ap_dist: -0.9945680499076843, an_dist: -0.7019611597061157\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "target probs tensor([[1.9562e-01],\n",
      "        [3.9693e-07],\n",
      "        [1.8834e-04],\n",
      "        [9.2895e-01],\n",
      "        [8.2532e-04],\n",
      "        [3.1637e-08],\n",
      "        [4.4343e-06],\n",
      "        [5.8199e-01],\n",
      "        [1.8975e-03],\n",
      "        [8.0599e-05],\n",
      "        [9.9221e-05],\n",
      "        [4.3791e-06],\n",
      "        [2.6288e-01],\n",
      "        [1.0842e-04],\n",
      "        [1.1901e-08],\n",
      "        [6.2583e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.31409579515457153: \n",
      "func:cos_distance, ap_dist: -0.9776611328125, an_dist: -0.4008404612541199\n",
      "target probs tensor([[3.5626e-05],\n",
      "        [1.5820e-05],\n",
      "        [6.6271e-03],\n",
      "        [2.7398e-05],\n",
      "        [3.6045e-06],\n",
      "        [5.3344e-02],\n",
      "        [2.7678e-04],\n",
      "        [9.9988e-01],\n",
      "        [3.8639e-05],\n",
      "        [4.8856e-06],\n",
      "        [5.5323e-02],\n",
      "        [3.2999e-05],\n",
      "        [2.2328e-05],\n",
      "        [9.2374e-01],\n",
      "        [7.9068e-01],\n",
      "        [1.6287e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.8330162763595581: \n",
      "func:cos_distance, ap_dist: -0.992301344871521, an_dist: -0.5164464116096497\n",
      "target probs tensor([[7.2715e-01],\n",
      "        [7.1204e-07],\n",
      "        [2.6945e-03],\n",
      "        [4.9510e-05],\n",
      "        [8.7518e-01],\n",
      "        [2.0784e-01],\n",
      "        [5.0371e-08],\n",
      "        [9.1144e-07],\n",
      "        [1.4018e-06],\n",
      "        [3.6235e-04],\n",
      "        [8.0011e-01],\n",
      "        [4.8117e-05],\n",
      "        [6.6299e-07],\n",
      "        [3.5405e-04],\n",
      "        [1.2402e-04],\n",
      "        [4.1237e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.326648473739624: \n",
      "func:cos_distance, ap_dist: -0.9959986209869385, an_dist: -0.4939161539077759\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "target probs tensor([[3.0753e-04],\n",
      "        [8.5020e-04],\n",
      "        [1.7393e-04],\n",
      "        [1.6474e-01],\n",
      "        [6.5979e-03],\n",
      "        [1.4465e-02],\n",
      "        [2.2077e-07],\n",
      "        [1.2049e-06],\n",
      "        [7.3068e-04],\n",
      "        [3.2118e-01],\n",
      "        [1.1405e-02],\n",
      "        [2.3978e-07],\n",
      "        [1.4360e-07],\n",
      "        [3.3736e-01],\n",
      "        [7.6811e-05],\n",
      "        [1.4694e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.0732918456196785: \n",
      "func:cos_distance, ap_dist: -0.9859168529510498, an_dist: -0.6814568042755127\n",
      "target probs tensor([[2.5675e-04],\n",
      "        [2.1783e-02],\n",
      "        [1.2768e-06],\n",
      "        [7.3871e-03],\n",
      "        [2.0968e-02],\n",
      "        [7.0333e-01],\n",
      "        [3.1689e-04],\n",
      "        [1.0363e-05],\n",
      "        [1.2391e-03],\n",
      "        [3.7665e-09],\n",
      "        [5.8840e-04],\n",
      "        [1.1341e-02],\n",
      "        [2.7319e-02],\n",
      "        [2.1453e-07],\n",
      "        [4.8411e-01],\n",
      "        [2.0044e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.12308377027511597: \n",
      "func:cos_distance, ap_dist: -0.9920355677604675, an_dist: -0.7208865880966187\n",
      "target probs tensor([[2.3923e-02],\n",
      "        [3.1554e-03],\n",
      "        [1.5298e-05],\n",
      "        [3.5737e-03],\n",
      "        [8.9266e-05],\n",
      "        [3.9135e-07],\n",
      "        [9.3741e-01],\n",
      "        [1.1105e-02],\n",
      "        [4.3459e-01],\n",
      "        [2.6820e-05],\n",
      "        [4.5920e-06],\n",
      "        [7.5492e-01],\n",
      "        [8.1091e-01],\n",
      "        [8.0420e-03],\n",
      "        [1.0529e-02],\n",
      "        [4.2158e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.40462353825569153: \n",
      "func:cos_distance, ap_dist: -0.9894987344741821, an_dist: -0.6629831194877625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "target probs tensor([[1.4933e-01],\n",
      "        [2.5462e-01],\n",
      "        [3.7671e-06],\n",
      "        [5.9484e-03],\n",
      "        [8.5814e-02],\n",
      "        [6.0484e-08],\n",
      "        [2.3714e-04],\n",
      "        [2.4367e-09],\n",
      "        [1.0538e-04],\n",
      "        [3.1521e-05],\n",
      "        [1.7084e-06],\n",
      "        [1.5037e-08],\n",
      "        [2.7151e-01],\n",
      "        [2.0032e-02],\n",
      "        [3.5374e-06],\n",
      "        [9.9489e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.3852963447570801: \n",
      "func:cos_distance, ap_dist: -0.9931195378303528, an_dist: -0.5948768854141235\n",
      "target probs tensor([[1.1513e-07],\n",
      "        [5.6027e-04],\n",
      "        [3.7840e-04],\n",
      "        [3.6830e-06],\n",
      "        [8.9675e-01],\n",
      "        [9.9990e-02],\n",
      "        [6.0938e-06],\n",
      "        [3.0389e-08],\n",
      "        [8.4458e-08],\n",
      "        [2.7196e-04],\n",
      "        [5.3730e-08],\n",
      "        [1.7000e-02],\n",
      "        [4.1033e-01],\n",
      "        [9.6610e-06],\n",
      "        [4.1489e-02],\n",
      "        [8.7234e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.18530425429344177: \n",
      "func:cos_distance, ap_dist: -0.9655768871307373, an_dist: -0.6058385372161865\n",
      "target probs tensor([[2.8928e-05],\n",
      "        [3.4528e-08],\n",
      "        [5.8461e-05],\n",
      "        [3.8496e-07],\n",
      "        [6.6119e-05],\n",
      "        [3.8704e-07],\n",
      "        [5.5670e-02],\n",
      "        [2.4575e-01],\n",
      "        [4.4995e-05],\n",
      "        [9.0216e-01],\n",
      "        [1.7765e-04],\n",
      "        [1.4293e-02],\n",
      "        [1.5561e-05],\n",
      "        [3.8848e-01],\n",
      "        [5.4511e-04],\n",
      "        [6.4812e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.2634563148021698: \n",
      "func:cos_distance, ap_dist: -0.9943549633026123, an_dist: -0.5421872138977051\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 53 with validation value: 0.8349999785423279.\n",
      "target probs tensor([[3.0033e-08],\n",
      "        [4.4818e-07],\n",
      "        [3.2682e-01],\n",
      "        [9.3221e-07],\n",
      "        [1.0811e-03],\n",
      "        [1.1549e-04],\n",
      "        [9.7196e-03],\n",
      "        [7.6569e-02],\n",
      "        [2.4859e-01],\n",
      "        [1.6315e-03],\n",
      "        [1.5514e-03],\n",
      "        [8.2650e-01],\n",
      "        [2.4883e-01],\n",
      "        [1.4391e-04],\n",
      "        [1.0015e-04],\n",
      "        [1.1166e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.17653514444828033: \n",
      "func:cos_distance, ap_dist: -0.9638079404830933, an_dist: -0.5339365005493164\n",
      "target probs tensor([[8.8467e-02],\n",
      "        [4.2838e-03],\n",
      "        [1.5683e-04],\n",
      "        [9.8720e-01],\n",
      "        [4.1520e-03],\n",
      "        [2.4257e-05],\n",
      "        [8.0010e-01],\n",
      "        [1.9067e-05],\n",
      "        [7.8210e-04],\n",
      "        [8.7984e-02],\n",
      "        [5.2810e-07],\n",
      "        [9.9724e-01],\n",
      "        [8.7508e-06],\n",
      "        [5.5650e-05],\n",
      "        [9.1042e-07],\n",
      "        [1.0977e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.7535359859466553: \n",
      "func:cos_distance, ap_dist: -0.9913411140441895, an_dist: -0.6327170133590698\n",
      "target probs tensor([[9.8761e-09],\n",
      "        [1.2040e-03],\n",
      "        [8.8485e-03],\n",
      "        [6.3709e-03],\n",
      "        [7.2658e-03],\n",
      "        [7.5772e-06],\n",
      "        [4.0463e-02],\n",
      "        [3.1803e-01],\n",
      "        [2.1948e-03],\n",
      "        [3.4013e-07],\n",
      "        [3.4279e-01],\n",
      "        [1.0754e-03],\n",
      "        [1.6412e-03],\n",
      "        [3.6771e-05],\n",
      "        [3.2234e-04],\n",
      "        [1.0083e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.05461884289979935: \n",
      "func:cos_distance, ap_dist: -0.9932773113250732, an_dist: -0.415790855884552\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 8.299999999999997 at the end of epoch 54\n",
      "target probs tensor([[1.2004e-02],\n",
      "        [2.1741e-07],\n",
      "        [1.1471e-02],\n",
      "        [2.8845e-01],\n",
      "        [2.3831e-05],\n",
      "        [5.3404e-08],\n",
      "        [7.1312e-01],\n",
      "        [1.2872e-06],\n",
      "        [2.0819e-03],\n",
      "        [2.3963e-01],\n",
      "        [7.5835e-01],\n",
      "        [7.2753e-04],\n",
      "        [3.2273e-03],\n",
      "        [3.7079e-01],\n",
      "        [4.2184e-02],\n",
      "        [2.8722e-09]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.23870506882667542: \n",
      "func:cos_distance, ap_dist: -0.9974570274353027, an_dist: -0.6851849555969238\n",
      "target probs tensor([[5.8145e-03],\n",
      "        [3.3942e-04],\n",
      "        [1.2097e-03],\n",
      "        [9.5051e-03],\n",
      "        [8.7643e-01],\n",
      "        [8.5785e-08],\n",
      "        [4.5845e-04],\n",
      "        [2.1754e-05],\n",
      "        [6.8206e-05],\n",
      "        [1.5778e-04],\n",
      "        [1.8023e-04],\n",
      "        [1.0318e-03],\n",
      "        [4.8655e-02],\n",
      "        [5.3068e-02],\n",
      "        [6.0963e-04],\n",
      "        [1.8502e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.13842642307281494: \n",
      "func:cos_distance, ap_dist: -0.9967184066772461, an_dist: -0.56458580493927\n",
      "target probs tensor([[9.3330e-05],\n",
      "        [1.6867e-01],\n",
      "        [2.4026e-04],\n",
      "        [1.1115e-05],\n",
      "        [1.4385e-02],\n",
      "        [9.0863e-03],\n",
      "        [8.8000e-06],\n",
      "        [7.4653e-01],\n",
      "        [4.5960e-05],\n",
      "        [7.5173e-08],\n",
      "        [9.4025e-01],\n",
      "        [2.6791e-07],\n",
      "        [1.0289e-04],\n",
      "        [1.0912e-03],\n",
      "        [1.9082e-06],\n",
      "        [1.0845e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.2756808400154114: \n",
      "func:cos_distance, ap_dist: -0.987602710723877, an_dist: -0.6403188705444336\n",
      "target probs tensor([[1.1076e-07],\n",
      "        [1.0542e-04],\n",
      "        [3.8314e-08],\n",
      "        [4.3621e-01],\n",
      "        [1.5642e-02],\n",
      "        [8.7931e-05],\n",
      "        [4.8082e-08],\n",
      "        [1.2280e-03]], device='cuda:0'), loss: 0.07378260046243668: \n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "func:cos_distance, ap_dist: -0.998441219329834, an_dist: -0.3676775097846985\n",
      "target probs tensor([[1.2003e-01],\n",
      "        [1.0830e-03],\n",
      "        [2.2536e-06],\n",
      "        [1.7763e-02],\n",
      "        [2.1355e-04],\n",
      "        [2.2184e-09],\n",
      "        [7.9976e-03],\n",
      "        [2.8176e-08],\n",
      "        [3.3298e-05],\n",
      "        [5.9174e-03],\n",
      "        [1.4427e-06],\n",
      "        [6.4738e-07],\n",
      "        [1.9632e-06],\n",
      "        [2.6553e-03],\n",
      "        [9.9100e-05],\n",
      "        [6.8468e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.010241009294986725: \n",
      "func:cos_distance, ap_dist: -0.9754211902618408, an_dist: -0.4356926679611206\n",
      "target probs tensor([[9.7628e-02],\n",
      "        [3.7860e-07],\n",
      "        [9.9730e-01],\n",
      "        [1.3533e-04],\n",
      "        [2.1074e-03],\n",
      "        [1.7157e-04],\n",
      "        [1.8389e-02],\n",
      "        [6.1515e-01],\n",
      "        [8.5083e-04],\n",
      "        [1.8270e-08],\n",
      "        [4.4813e-03],\n",
      "        [2.2413e-05],\n",
      "        [3.2090e-01],\n",
      "        [2.9561e-03],\n",
      "        [4.4777e-04],\n",
      "        [2.5686e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.46198955178260803: \n",
      "func:cos_distance, ap_dist: -0.9847581386566162, an_dist: -0.6936495304107666\n",
      "target probs tensor([[1.1841e-06],\n",
      "        [2.4681e-03],\n",
      "        [2.6036e-03],\n",
      "        [9.1615e-05],\n",
      "        [4.9838e-04],\n",
      "        [7.8322e-01],\n",
      "        [1.0501e-10],\n",
      "        [1.1394e-02],\n",
      "        [6.2359e-05],\n",
      "        [7.8103e-06],\n",
      "        [2.8817e-05],\n",
      "        [7.7239e-02],\n",
      "        [6.6287e-03],\n",
      "        [3.7967e-06],\n",
      "        [1.7967e-05],\n",
      "        [9.9924e-01]], device='cuda:0'), loss: 0.5511109232902527: \n",
      "func:cos_distance, ap_dist: -0.9955039620399475, an_dist: -0.7026073932647705\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 8.599999999999998 at the end of epoch 56\n",
      "target probs tensor([[1.4685e-04],\n",
      "        [2.5463e-05],\n",
      "        [7.4049e-07],\n",
      "        [8.7722e-01],\n",
      "        [4.3371e-05],\n",
      "        [7.7783e-07],\n",
      "        [2.0661e-05],\n",
      "        [5.9331e-07],\n",
      "        [6.9352e-11],\n",
      "        [4.0503e-07],\n",
      "        [1.9106e-02],\n",
      "        [2.0133e-03],\n",
      "        [8.4383e-06],\n",
      "        [4.3468e-06],\n",
      "        [3.6316e-04],\n",
      "        [2.3908e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.13260303437709808: \n",
      "func:cos_distance, ap_dist: -0.9840258955955505, an_dist: -0.5787822008132935\n",
      "target probs tensor([[1.2483e-03],\n",
      "        [9.6653e-01],\n",
      "        [6.3126e-07],\n",
      "        [8.9691e-07],\n",
      "        [4.0483e-04],\n",
      "        [2.0858e-05],\n",
      "        [8.6781e-01],\n",
      "        [3.9493e-03],\n",
      "        [3.5682e-06],\n",
      "        [4.5050e-07],\n",
      "        [2.0836e-03],\n",
      "        [9.7365e-01],\n",
      "        [1.8483e-06],\n",
      "        [2.3200e-02],\n",
      "        [1.0470e-06],\n",
      "        [8.9000e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5680199265480042: \n",
      "func:cos_distance, ap_dist: -0.9865646362304688, an_dist: -0.7186559438705444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[9.0104e-07],\n",
      "        [3.4298e-01],\n",
      "        [6.1120e-04],\n",
      "        [1.5701e-02],\n",
      "        [3.3775e-04],\n",
      "        [7.0105e-04],\n",
      "        [1.7639e-03],\n",
      "        [3.1914e-05],\n",
      "        [6.4892e-01],\n",
      "        [9.6438e-05],\n",
      "        [4.7838e-04],\n",
      "        [1.1533e-01],\n",
      "        [5.9957e-07],\n",
      "        [1.5332e-05],\n",
      "        [1.1683e-02],\n",
      "        [4.1847e-10]], device='cuda:0'), loss: 0.10130953788757324: \n",
      "func:cos_distance, ap_dist: -0.9752010703086853, an_dist: -0.5523987412452698\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 57 with validation value: 0.8429999947547913.\n",
      "fooling weight increased to 8.899999999999999 at the end of epoch 57\n",
      "target probs tensor([[1.0949e-05],\n",
      "        [1.2874e-03],\n",
      "        [8.4470e-03],\n",
      "        [2.1186e-02],\n",
      "        [8.0904e-07],\n",
      "        [7.1500e-04],\n",
      "        [8.5159e-03],\n",
      "        [1.1226e-03],\n",
      "        [4.0611e-01],\n",
      "        [9.7479e-01],\n",
      "        [2.5983e-02],\n",
      "        [6.1443e-02],\n",
      "        [2.9420e-06],\n",
      "        [8.1169e-11],\n",
      "        [1.1205e-02],\n",
      "        [1.6558e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.27151915431022644: \n",
      "func:cos_distance, ap_dist: -0.9860954284667969, an_dist: -0.7390234470367432\n",
      "target probs tensor([[4.6220e-06],\n",
      "        [6.7784e-04],\n",
      "        [8.8897e-02],\n",
      "        [5.5611e-09],\n",
      "        [4.1609e-05],\n",
      "        [8.9322e-08],\n",
      "        [7.2044e-03],\n",
      "        [8.9868e-09],\n",
      "        [4.8035e-03],\n",
      "        [6.5235e-04],\n",
      "        [7.2581e-06],\n",
      "        [1.0491e-06],\n",
      "        [1.3658e-05],\n",
      "        [8.7448e-01],\n",
      "        [9.4661e-01],\n",
      "        [4.1142e-09]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.31949421763420105: \n",
      "func:cos_distance, ap_dist: -0.9896979331970215, an_dist: -0.5979254245758057\n",
      "target probs tensor([[7.3551e-08],\n",
      "        [1.4081e-03],\n",
      "        [8.1078e-02],\n",
      "        [3.2061e-06],\n",
      "        [7.4427e-02],\n",
      "        [4.2154e-06],\n",
      "        [1.2221e-05],\n",
      "        [4.0307e-01],\n",
      "        [2.6372e-05],\n",
      "        [7.4714e-05],\n",
      "        [2.1026e-01],\n",
      "        [2.8172e-10],\n",
      "        [5.9259e-07],\n",
      "        [2.0642e-04],\n",
      "        [1.8481e-01],\n",
      "        [3.7640e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.07023452967405319: \n",
      "func:cos_distance, ap_dist: -0.9922193288803101, an_dist: -0.6196405291557312\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "target probs tensor([[1.5461e-08],\n",
      "        [2.5199e-08],\n",
      "        [7.2642e-01],\n",
      "        [1.7018e-02],\n",
      "        [9.7149e-08],\n",
      "        [6.3613e-04],\n",
      "        [3.5147e-01],\n",
      "        [7.4198e-02],\n",
      "        [9.5166e-01],\n",
      "        [2.0927e-06],\n",
      "        [4.9201e-06],\n",
      "        [7.9006e-04],\n",
      "        [6.9832e-03],\n",
      "        [2.4022e-07],\n",
      "        [5.7447e-06],\n",
      "        [5.5885e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.3074376881122589: \n",
      "func:cos_distance, ap_dist: -0.9883508682250977, an_dist: -0.598049521446228\n",
      "target probs tensor([[9.4680e-05],\n",
      "        [1.3031e-05],\n",
      "        [1.4192e-04],\n",
      "        [7.5300e-04],\n",
      "        [1.8129e-02],\n",
      "        [1.6456e-02],\n",
      "        [1.2587e-07],\n",
      "        [1.2149e-06],\n",
      "        [8.4591e-06],\n",
      "        [4.6237e-04],\n",
      "        [7.2120e-03],\n",
      "        [9.9182e-01],\n",
      "        [7.0377e-04],\n",
      "        [1.3296e-05],\n",
      "        [4.8524e-04],\n",
      "        [4.4634e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.30315226316452026: \n",
      "func:cos_distance, ap_dist: -0.991651177406311, an_dist: -0.5655567049980164\n",
      "target probs tensor([[1.9674e-10],\n",
      "        [2.0290e-05],\n",
      "        [5.0934e-01],\n",
      "        [6.3633e-09],\n",
      "        [7.0278e-06],\n",
      "        [4.4697e-08],\n",
      "        [2.9416e-04],\n",
      "        [8.0078e-05],\n",
      "        [2.7561e-08],\n",
      "        [7.5547e-01],\n",
      "        [2.4159e-01],\n",
      "        [2.9334e-09],\n",
      "        [1.1035e-07],\n",
      "        [2.9676e-06],\n",
      "        [1.1694e-05],\n",
      "        [5.3433e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.19760406017303467: \n",
      "func:cos_distance, ap_dist: -0.9809743165969849, an_dist: -0.5522509813308716\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 9.2 at the end of epoch 59\n",
      "target probs tensor([[4.6795e-08],\n",
      "        [3.7748e-05],\n",
      "        [5.7754e-04],\n",
      "        [4.7874e-06],\n",
      "        [1.9748e-05],\n",
      "        [1.0041e-05],\n",
      "        [6.6212e-01],\n",
      "        [9.6895e-05],\n",
      "        [2.4122e-01],\n",
      "        [2.3992e-02],\n",
      "        [7.1586e-07],\n",
      "        [2.5244e-01],\n",
      "        [6.8775e-06],\n",
      "        [1.7685e-01],\n",
      "        [2.2792e-06],\n",
      "        [2.7073e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.11698349565267563: \n",
      "func:cos_distance, ap_dist: -0.9867406487464905, an_dist: -0.6190650463104248\n",
      "target probs tensor([[6.2969e-04],\n",
      "        [9.9254e-01],\n",
      "        [5.2046e-05],\n",
      "        [9.5932e-01],\n",
      "        [9.8609e-01],\n",
      "        [9.9967e-01],\n",
      "        [1.5515e-03],\n",
      "        [3.6675e-03],\n",
      "        [1.8186e-05],\n",
      "        [4.1174e-02],\n",
      "        [1.3098e-05],\n",
      "        [1.5860e-07],\n",
      "        [6.5470e-07],\n",
      "        [9.7605e-04],\n",
      "        [1.0298e-02],\n",
      "        [3.4594e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.2775962352752686: \n",
      "func:cos_distance, ap_dist: -0.9873279929161072, an_dist: -0.7023909091949463\n",
      "target probs tensor([[4.2428e-07],\n",
      "        [1.5896e-05],\n",
      "        [2.4226e-06],\n",
      "        [4.7224e-02],\n",
      "        [2.0101e-05],\n",
      "        [2.0943e-03],\n",
      "        [6.4302e-06],\n",
      "        [3.6280e-04],\n",
      "        [1.0693e-07],\n",
      "        [1.4791e-05],\n",
      "        [1.0823e-02],\n",
      "        [5.1776e-04],\n",
      "        [9.7413e-07],\n",
      "        [2.2215e-05],\n",
      "        [3.3734e-08],\n",
      "        [4.5813e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.04218991845846176: \n",
      "func:cos_distance, ap_dist: -0.9904859066009521, an_dist: -0.5004311203956604\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "target probs tensor([[3.2726e-02],\n",
      "        [2.8942e-06],\n",
      "        [5.7492e-08],\n",
      "        [2.4965e-06],\n",
      "        [2.1050e-03],\n",
      "        [4.3453e-05],\n",
      "        [5.8124e-05],\n",
      "        [2.9055e-04],\n",
      "        [1.6398e-06],\n",
      "        [1.6627e-04],\n",
      "        [2.5687e-03],\n",
      "        [8.5026e-01],\n",
      "        [1.6960e-02],\n",
      "        [2.4262e-02],\n",
      "        [2.7645e-04],\n",
      "        [4.4891e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.1237352043390274: \n",
      "func:cos_distance, ap_dist: -0.9899590611457825, an_dist: -0.6114751100540161\n",
      "target probs tensor([[7.3557e-06],\n",
      "        [9.2526e-04],\n",
      "        [2.2085e-02],\n",
      "        [2.4199e-02],\n",
      "        [8.9178e-01],\n",
      "        [7.4960e-01],\n",
      "        [4.5633e-04],\n",
      "        [6.0500e-03],\n",
      "        [4.1092e-01],\n",
      "        [5.3691e-03],\n",
      "        [1.2636e-04],\n",
      "        [5.0900e-08],\n",
      "        [5.1310e-07],\n",
      "        [1.0745e-02],\n",
      "        [1.0603e-02],\n",
      "        [9.3797e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.4374324679374695: \n",
      "func:cos_distance, ap_dist: -0.9981342554092407, an_dist: -0.8132673501968384\n",
      "target probs tensor([[1.7417e-08],\n",
      "        [2.7264e-03],\n",
      "        [4.6390e-10],\n",
      "        [3.0565e-08],\n",
      "        [2.8692e-03],\n",
      "        [4.5960e-06],\n",
      "        [3.1555e-08],\n",
      "        [8.6719e-01],\n",
      "        [1.1587e-04],\n",
      "        [3.1458e-04],\n",
      "        [6.6355e-10],\n",
      "        [1.7984e-02],\n",
      "        [8.6145e-03],\n",
      "        [9.0515e-01],\n",
      "        [5.9927e-01],\n",
      "        [5.2825e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.3329327702522278: \n",
      "func:cos_distance, ap_dist: -0.9813344478607178, an_dist: -0.6730839014053345\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "target probs tensor([[5.4517e-03],\n",
      "        [3.7418e-03],\n",
      "        [1.7833e-04],\n",
      "        [4.3062e-02],\n",
      "        [1.7070e-08],\n",
      "        [1.9915e-06],\n",
      "        [7.7667e-01],\n",
      "        [6.4738e-07],\n",
      "        [1.3647e-03],\n",
      "        [9.9137e-01],\n",
      "        [4.2154e-11],\n",
      "        [4.4833e-03],\n",
      "        [1.4172e-06],\n",
      "        [6.2492e-09],\n",
      "        [1.2999e-04],\n",
      "        [8.6706e-09]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.39447322487831116: \n",
      "func:cos_distance, ap_dist: -0.954384446144104, an_dist: -0.49782153964042664\n",
      "target probs tensor([[4.0022e-02],\n",
      "        [5.6780e-05],\n",
      "        [2.6427e-06],\n",
      "        [1.4391e-04],\n",
      "        [6.5681e-02],\n",
      "        [9.9823e-01],\n",
      "        [6.0437e-01],\n",
      "        [4.9583e-04],\n",
      "        [9.5171e-01],\n",
      "        [1.5983e-05],\n",
      "        [7.4416e-07],\n",
      "        [1.1794e-05],\n",
      "        [5.1963e-08],\n",
      "        [1.2678e-02],\n",
      "        [2.8042e-06],\n",
      "        [3.6073e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.6511087417602539: \n",
      "func:cos_distance, ap_dist: -0.9901020526885986, an_dist: -0.5969582796096802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[2.8202e-05],\n",
      "        [4.4999e-10],\n",
      "        [2.8969e-06],\n",
      "        [1.4850e-04],\n",
      "        [2.3257e-01],\n",
      "        [2.8077e-03],\n",
      "        [8.2506e-01],\n",
      "        [9.2717e-05],\n",
      "        [1.0128e-03],\n",
      "        [3.1146e-02],\n",
      "        [1.9241e-08],\n",
      "        [1.5273e-03],\n",
      "        [2.5108e-05],\n",
      "        [1.0516e-02],\n",
      "        [2.2591e-03],\n",
      "        [1.8839e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.1286339908838272: \n",
      "func:cos_distance, ap_dist: -0.9910693168640137, an_dist: -0.6221508979797363\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "target probs tensor([[1.2135e-05],\n",
      "        [8.9481e-04],\n",
      "        [8.7691e-02],\n",
      "        [9.7033e-01],\n",
      "        [9.9482e-01],\n",
      "        [3.5920e-02],\n",
      "        [8.4378e-04],\n",
      "        [1.5014e-02],\n",
      "        [1.2140e-02],\n",
      "        [2.5853e-03],\n",
      "        [4.8258e-02],\n",
      "        [9.3679e-08],\n",
      "        [9.0052e-04],\n",
      "        [2.9714e-05],\n",
      "        [4.9154e-02],\n",
      "        [5.3156e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5651090145111084: \n",
      "func:cos_distance, ap_dist: -0.9948809146881104, an_dist: -0.5644352436065674\n",
      "target probs tensor([[9.2340e-06],\n",
      "        [3.5682e-05],\n",
      "        [1.4558e-01],\n",
      "        [4.1289e-02],\n",
      "        [2.1026e-10],\n",
      "        [6.3079e-03],\n",
      "        [3.0568e-03],\n",
      "        [1.0301e-01],\n",
      "        [1.3987e-02],\n",
      "        [2.5002e-03],\n",
      "        [5.3497e-03],\n",
      "        [1.4067e-08],\n",
      "        [7.2686e-01],\n",
      "        [2.7734e-03],\n",
      "        [5.4032e-07],\n",
      "        [5.9764e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.15940898656845093: \n",
      "func:cos_distance, ap_dist: -0.9962450265884399, an_dist: -0.512549877166748\n",
      "target probs tensor([[1.3751e-07],\n",
      "        [6.5725e-01],\n",
      "        [8.7167e-01],\n",
      "        [2.2130e-04],\n",
      "        [4.1982e-01],\n",
      "        [3.4163e-05],\n",
      "        [2.1420e-11],\n",
      "        [9.8582e-07],\n",
      "        [1.9641e-04],\n",
      "        [4.9744e-05],\n",
      "        [2.4177e-03],\n",
      "        [4.2309e-03],\n",
      "        [2.3715e-05],\n",
      "        [5.7474e-07],\n",
      "        [1.5820e-02],\n",
      "        [1.4148e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.2307155281305313: \n",
      "func:cos_distance, ap_dist: -0.983858585357666, an_dist: -0.6242473125457764\n",
      "target probs tensor([[1.2839e-05],\n",
      "        [2.6349e-09],\n",
      "        [4.3199e-08],\n",
      "        [9.7393e-02],\n",
      "        [3.1624e-03],\n",
      "        [8.1059e-05],\n",
      "        [3.0080e-06],\n",
      "        [8.9577e-03]], device='cuda:0'), loss: 0.014341337606310844: \n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "func:cos_distance, ap_dist: -0.9687888026237488, an_dist: -0.600250244140625\n",
      "fooling weight increased to 9.5 at the end of epoch 63\n",
      "target probs tensor([[1.6346e-01],\n",
      "        [4.6550e-05],\n",
      "        [2.8249e-04],\n",
      "        [5.7467e-05],\n",
      "        [3.5027e-10],\n",
      "        [1.0753e-08],\n",
      "        [1.1598e-03],\n",
      "        [3.1058e-08],\n",
      "        [1.1250e-05],\n",
      "        [9.4154e-08],\n",
      "        [5.5268e-06],\n",
      "        [2.4496e-02],\n",
      "        [8.8902e-09],\n",
      "        [6.8230e-03],\n",
      "        [1.3907e-01],\n",
      "        [1.1133e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.0299663282930851: \n",
      "func:cos_distance, ap_dist: -0.9742609262466431, an_dist: -0.43637320399284363\n",
      "target probs tensor([[4.0768e-03],\n",
      "        [1.5801e-07],\n",
      "        [4.5507e-01],\n",
      "        [6.8118e-06],\n",
      "        [2.9635e-04],\n",
      "        [9.4066e-03],\n",
      "        [3.0875e-04],\n",
      "        [4.0244e-03],\n",
      "        [2.8395e-07],\n",
      "        [4.5911e-03],\n",
      "        [2.6205e-03],\n",
      "        [9.7036e-03],\n",
      "        [3.0054e-06],\n",
      "        [8.9632e-07],\n",
      "        [3.3058e-01],\n",
      "        [4.6048e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.06522504985332489: \n",
      "func:cos_distance, ap_dist: -0.9936113357543945, an_dist: -0.5410643815994263\n",
      "target probs tensor([[1.0833e-08],\n",
      "        [5.8945e-05],\n",
      "        [1.3105e-03],\n",
      "        [2.2723e-03],\n",
      "        [3.6518e-03],\n",
      "        [8.6351e-01],\n",
      "        [4.2212e-11],\n",
      "        [4.7493e-03],\n",
      "        [6.3109e-05],\n",
      "        [4.7842e-04],\n",
      "        [2.2672e-05],\n",
      "        [6.3222e-02],\n",
      "        [2.0043e-07],\n",
      "        [1.1893e-04],\n",
      "        [6.3578e-06],\n",
      "        [8.9047e-01]], device='cuda:0'), loss: 0.26757192611694336: \n",
      "func:cos_distance, ap_dist: -0.9940672516822815, an_dist: -0.5432535409927368\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 64 with validation value: 0.8529999852180481.\n",
      "target probs tensor([[4.2298e-04],\n",
      "        [3.1581e-04],\n",
      "        [1.2045e-01],\n",
      "        [3.1180e-02],\n",
      "        [2.0700e-04],\n",
      "        [2.3006e-02],\n",
      "        [9.7863e-06],\n",
      "        [5.7839e-06],\n",
      "        [2.7602e-01],\n",
      "        [2.8474e-05],\n",
      "        [2.2590e-01],\n",
      "        [8.3876e-05],\n",
      "        [3.6003e-02],\n",
      "        [2.1240e-05],\n",
      "        [2.1242e-06],\n",
      "        [6.9071e-10]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.050006620585918427: \n",
      "func:cos_distance, ap_dist: -0.9857807159423828, an_dist: -0.8016929030418396\n",
      "target probs tensor([[7.3045e-06],\n",
      "        [9.5367e-08],\n",
      "        [3.6670e-02],\n",
      "        [3.0197e-03],\n",
      "        [7.1582e-03],\n",
      "        [2.1624e-08],\n",
      "        [1.6818e-04],\n",
      "        [2.4848e-04],\n",
      "        [9.1992e-08],\n",
      "        [8.6846e-04],\n",
      "        [8.9823e-06],\n",
      "        [5.6787e-05],\n",
      "        [6.3716e-06],\n",
      "        [3.4404e-09],\n",
      "        [2.6965e-07],\n",
      "        [1.0149e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.0030583785846829414: \n",
      "func:cos_distance, ap_dist: -0.9858120679855347, an_dist: -0.5588699579238892\n",
      "target probs tensor([[9.8324e-09],\n",
      "        [5.1705e-01],\n",
      "        [2.8768e-03],\n",
      "        [6.2055e-03],\n",
      "        [8.0324e-03],\n",
      "        [1.5010e-04],\n",
      "        [1.5318e-04],\n",
      "        [2.6525e-05],\n",
      "        [4.4372e-01],\n",
      "        [4.2141e-04],\n",
      "        [2.1793e-03],\n",
      "        [2.4389e-02],\n",
      "        [3.7131e-05],\n",
      "        [1.7431e-06],\n",
      "        [1.6264e-02],\n",
      "        [2.9987e-11]], device='cuda:0'), loss: 0.08597216755151749: \n",
      "func:cos_distance, ap_dist: -0.9917550683021545, an_dist: -0.5116645097732544\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 9.8 at the end of epoch 65\n",
      "target probs tensor([[9.3804e-01],\n",
      "        [3.5689e-06],\n",
      "        [8.0302e-01],\n",
      "        [4.0313e-09],\n",
      "        [1.0939e-05],\n",
      "        [9.8446e-01],\n",
      "        [7.6394e-05],\n",
      "        [5.2985e-05],\n",
      "        [6.2153e-05],\n",
      "        [1.9828e-02],\n",
      "        [3.3831e-01],\n",
      "        [9.9583e-01],\n",
      "        [1.9118e-07],\n",
      "        [6.9922e-07],\n",
      "        [3.5358e-01],\n",
      "        [6.1988e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.9929128885269165: \n",
      "func:cos_distance, ap_dist: -0.97856605052948, an_dist: -0.785580575466156\n",
      "target probs tensor([[1.1190e-06],\n",
      "        [7.0418e-04],\n",
      "        [1.7184e-07],\n",
      "        [3.8148e-02],\n",
      "        [1.6581e-03],\n",
      "        [4.5086e-02],\n",
      "        [8.0258e-09],\n",
      "        [5.8100e-08],\n",
      "        [1.6185e-05],\n",
      "        [1.9108e-07],\n",
      "        [1.3171e-02],\n",
      "        [5.7408e-06],\n",
      "        [1.2231e-01],\n",
      "        [1.3200e-05],\n",
      "        [2.4174e-04],\n",
      "        [4.7132e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.014757304452359676: \n",
      "func:cos_distance, ap_dist: -0.9816573858261108, an_dist: -0.5859971046447754\n",
      "target probs tensor([[7.4093e-05],\n",
      "        [4.2861e-07],\n",
      "        [1.4273e-06],\n",
      "        [9.4369e-02],\n",
      "        [6.9365e-04],\n",
      "        [9.8039e-02],\n",
      "        [7.0653e-05],\n",
      "        [5.3278e-07],\n",
      "        [2.9588e-07],\n",
      "        [5.9056e-07],\n",
      "        [2.5986e-05],\n",
      "        [2.6792e-04],\n",
      "        [9.0670e-03],\n",
      "        [4.4363e-05],\n",
      "        [1.9181e-01],\n",
      "        [1.8713e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.02659694105386734: \n",
      "func:cos_distance, ap_dist: -0.9915258884429932, an_dist: -0.6283881664276123\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 10.100000000000001 at the end of epoch 66\n",
      "target probs tensor([[3.0421e-01],\n",
      "        [5.2040e-04],\n",
      "        [7.5057e-04],\n",
      "        [1.1575e-06],\n",
      "        [8.5501e-04],\n",
      "        [1.1532e-02],\n",
      "        [4.3068e-02],\n",
      "        [3.4317e-08],\n",
      "        [1.6477e-06],\n",
      "        [2.6158e-03],\n",
      "        [6.5169e-02],\n",
      "        [2.1961e-05],\n",
      "        [8.8959e-01],\n",
      "        [1.1181e-02],\n",
      "        [3.4618e-06],\n",
      "        [8.5057e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.16961200535297394: \n",
      "func:cos_distance, ap_dist: -0.9879083037376404, an_dist: -0.6297750473022461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[3.4467e-01],\n",
      "        [4.8399e-06],\n",
      "        [1.3930e-03],\n",
      "        [8.0586e-01],\n",
      "        [8.5425e-01],\n",
      "        [3.6632e-04],\n",
      "        [1.6409e-04],\n",
      "        [7.5488e-07],\n",
      "        [1.2281e-01],\n",
      "        [1.4849e-05],\n",
      "        [1.5909e-05],\n",
      "        [8.4130e-09],\n",
      "        [2.6481e-07],\n",
      "        [7.1981e-08],\n",
      "        [2.3176e-06],\n",
      "        [1.3768e-08]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.2575434744358063: \n",
      "func:cos_distance, ap_dist: -0.9882267713546753, an_dist: -0.5880415439605713\n",
      "target probs tensor([[6.1655e-09],\n",
      "        [2.8152e-02],\n",
      "        [9.5917e-06],\n",
      "        [4.0978e-01],\n",
      "        [1.2927e-06],\n",
      "        [6.7870e-07],\n",
      "        [9.0631e-06],\n",
      "        [1.6521e-12],\n",
      "        [2.3674e-04],\n",
      "        [2.2123e-02],\n",
      "        [3.3101e-03],\n",
      "        [2.4271e-07],\n",
      "        [5.2106e-09],\n",
      "        [1.3877e-04],\n",
      "        [2.1000e-07],\n",
      "        [6.7257e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.03637337312102318: \n",
      "func:cos_distance, ap_dist: -0.9974700808525085, an_dist: -0.39455753564834595\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "target probs tensor([[7.6419e-04],\n",
      "        [7.5433e-01],\n",
      "        [1.6285e-05],\n",
      "        [8.9200e-01],\n",
      "        [5.8926e-05],\n",
      "        [1.8796e-04],\n",
      "        [2.1813e-06],\n",
      "        [1.0034e-04],\n",
      "        [5.7240e-07],\n",
      "        [5.4744e-07],\n",
      "        [1.7573e-06],\n",
      "        [3.3554e-04],\n",
      "        [7.4720e-02],\n",
      "        [7.7377e-07],\n",
      "        [9.3239e-01],\n",
      "        [1.2742e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.4001566171646118: \n",
      "func:cos_distance, ap_dist: -0.9868555665016174, an_dist: -0.4915560185909271\n",
      "target probs tensor([[2.9380e-01],\n",
      "        [1.3468e-04],\n",
      "        [2.0491e-05],\n",
      "        [1.3822e-04],\n",
      "        [2.3585e-05],\n",
      "        [5.2930e-06],\n",
      "        [1.3256e-06],\n",
      "        [2.3616e-04],\n",
      "        [5.2176e-02],\n",
      "        [3.2105e-01],\n",
      "        [3.4307e-03],\n",
      "        [4.1526e-01],\n",
      "        [1.0722e-03],\n",
      "        [4.7384e-05],\n",
      "        [8.9195e-03],\n",
      "        [6.5857e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.08796537667512894: \n",
      "func:cos_distance, ap_dist: -0.9656602740287781, an_dist: -0.7424821257591248\n",
      "target probs tensor([[5.5970e-08],\n",
      "        [1.1590e-03],\n",
      "        [8.7316e-01],\n",
      "        [3.8663e-02],\n",
      "        [5.6962e-04],\n",
      "        [6.9655e-01],\n",
      "        [7.7268e-04],\n",
      "        [1.1111e-03],\n",
      "        [2.3408e-05],\n",
      "        [2.1597e-08],\n",
      "        [1.7647e-04],\n",
      "        [1.2031e-03],\n",
      "        [9.7765e-01],\n",
      "        [2.5271e-03],\n",
      "        [1.1189e-07],\n",
      "        [2.2075e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.4440883696079254: \n",
      "func:cos_distance, ap_dist: -0.9919384717941284, an_dist: -0.7905932664871216\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 10.400000000000002 at the end of epoch 68\n",
      "target probs tensor([[3.5661e-05],\n",
      "        [1.7586e-01],\n",
      "        [9.9605e-01],\n",
      "        [6.6613e-04],\n",
      "        [3.7795e-08],\n",
      "        [8.8644e-01],\n",
      "        [1.9372e-08],\n",
      "        [4.9193e-01],\n",
      "        [7.5711e-06],\n",
      "        [1.3220e-03],\n",
      "        [1.1300e-04],\n",
      "        [3.5665e-07],\n",
      "        [3.2624e-04],\n",
      "        [2.3444e-07],\n",
      "        [5.3321e-04],\n",
      "        [3.7103e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5363806486129761: \n",
      "func:cos_distance, ap_dist: -0.9882228374481201, an_dist: -0.6546460390090942\n",
      "target probs tensor([[2.3971e-02],\n",
      "        [8.3548e-08],\n",
      "        [1.1394e-02],\n",
      "        [3.8848e-04],\n",
      "        [8.6841e-01],\n",
      "        [4.2427e-01],\n",
      "        [2.8373e-02],\n",
      "        [1.1956e-05],\n",
      "        [5.6381e-04],\n",
      "        [8.4631e-02],\n",
      "        [9.0205e-06],\n",
      "        [2.8396e-11],\n",
      "        [1.3726e-06],\n",
      "        [2.4331e-05],\n",
      "        [7.6109e-07],\n",
      "        [9.5764e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.3684768080711365: \n",
      "func:cos_distance, ap_dist: -0.9889431595802307, an_dist: -0.604831337928772\n",
      "target probs tensor([[3.9359e-08],\n",
      "        [1.7154e-03],\n",
      "        [3.6376e-05],\n",
      "        [1.8797e-05],\n",
      "        [4.6013e-05],\n",
      "        [1.4938e-07],\n",
      "        [1.8325e-01],\n",
      "        [2.6218e-05],\n",
      "        [6.5113e-04],\n",
      "        [9.1637e-01],\n",
      "        [2.1692e-08],\n",
      "        [1.1103e-05],\n",
      "        [8.1084e-11],\n",
      "        [3.4942e-03],\n",
      "        [1.0387e-03],\n",
      "        [4.8096e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.20916619896888733: \n",
      "func:cos_distance, ap_dist: -0.9801347255706787, an_dist: -0.3994016647338867\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 69 with validation value: 0.8569999933242798.\n",
      "target probs tensor([[7.9185e-06],\n",
      "        [1.2107e-04],\n",
      "        [1.6898e-02],\n",
      "        [1.0285e-07],\n",
      "        [1.2782e-02],\n",
      "        [5.5228e-04],\n",
      "        [5.7697e-06],\n",
      "        [1.1224e-03],\n",
      "        [1.8198e-02],\n",
      "        [9.4071e-04],\n",
      "        [2.3423e-06],\n",
      "        [5.5681e-10],\n",
      "        [6.1235e-07],\n",
      "        [1.0169e-11],\n",
      "        [2.6926e-04],\n",
      "        [8.3118e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.003211202099919319: \n",
      "func:cos_distance, ap_dist: -0.9814328551292419, an_dist: -0.47684890031814575\n",
      "target probs tensor([[9.6155e-01],\n",
      "        [9.7991e-02],\n",
      "        [3.7055e-08],\n",
      "        [4.1338e-07],\n",
      "        [2.0678e-03],\n",
      "        [2.8105e-07],\n",
      "        [5.6902e-02],\n",
      "        [2.9598e-04],\n",
      "        [6.9128e-08],\n",
      "        [9.8381e-08],\n",
      "        [2.0014e-06],\n",
      "        [6.1704e-04],\n",
      "        [8.1817e-04],\n",
      "        [8.5889e-04],\n",
      "        [1.1127e-06],\n",
      "        [1.9110e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.2152508944272995: \n",
      "func:cos_distance, ap_dist: -0.9958065748214722, an_dist: -0.5063151121139526\n",
      "target probs tensor([[4.6533e-06],\n",
      "        [1.9300e-04],\n",
      "        [3.4770e-04],\n",
      "        [4.1979e-01],\n",
      "        [9.4341e-03],\n",
      "        [3.0364e-02],\n",
      "        [5.5626e-04],\n",
      "        [3.7116e-01],\n",
      "        [7.5669e-01],\n",
      "        [8.5507e-01],\n",
      "        [4.1493e-02],\n",
      "        [1.5918e-02],\n",
      "        [6.8919e-06],\n",
      "        [8.3228e-03],\n",
      "        [5.0966e-04],\n",
      "        [1.1381e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.27958357334136963: \n",
      "func:cos_distance, ap_dist: -0.990432858467102, an_dist: -0.5379235744476318\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 10.700000000000003 at the end of epoch 70\n",
      "target probs tensor([[7.0114e-01],\n",
      "        [3.1889e-01],\n",
      "        [1.1443e-03],\n",
      "        [7.4853e-07],\n",
      "        [2.5893e-02],\n",
      "        [3.4809e-05],\n",
      "        [6.2787e-07],\n",
      "        [6.5289e-07],\n",
      "        [3.2140e-06],\n",
      "        [2.8237e-05],\n",
      "        [9.7939e-01],\n",
      "        [4.6909e-03],\n",
      "        [4.8321e-06],\n",
      "        [5.4796e-04],\n",
      "        [2.7139e-03],\n",
      "        [8.4132e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.3443237543106079: \n",
      "func:cos_distance, ap_dist: -0.9899768233299255, an_dist: -0.4588538110256195\n",
      "target probs tensor([[7.4878e-01],\n",
      "        [1.5316e-06],\n",
      "        [1.1287e-08],\n",
      "        [9.7810e-01],\n",
      "        [2.2277e-08],\n",
      "        [8.6008e-09],\n",
      "        [2.4796e-12],\n",
      "        [2.8189e-05],\n",
      "        [2.4341e-02],\n",
      "        [7.7031e-07],\n",
      "        [2.4197e-06],\n",
      "        [2.7118e-07],\n",
      "        [9.5742e-01],\n",
      "        [1.1756e-05],\n",
      "        [1.8875e-03],\n",
      "        [5.6494e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5241053104400635: \n",
      "func:cos_distance, ap_dist: -0.9812417030334473, an_dist: -0.5604874491691589\n",
      "target probs tensor([[6.2993e-07],\n",
      "        [8.9956e-01],\n",
      "        [8.1184e-06],\n",
      "        [1.2730e-01],\n",
      "        [3.7600e-04],\n",
      "        [8.6014e-02],\n",
      "        [6.2453e-09],\n",
      "        [2.5932e-03],\n",
      "        [9.0884e-01],\n",
      "        [1.8402e-07],\n",
      "        [2.7943e-09],\n",
      "        [6.5846e-03],\n",
      "        [4.4622e-03],\n",
      "        [4.4199e-04],\n",
      "        [6.3602e-05],\n",
      "        [2.5028e-08]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.308374285697937: \n",
      "func:cos_distance, ap_dist: -0.9398131966590881, an_dist: -0.5070153474807739\n",
      "target probs tensor([[2.3655e-08],\n",
      "        [5.5867e-07],\n",
      "        [1.7687e-07],\n",
      "        [1.9196e-01],\n",
      "        [3.0309e-05],\n",
      "        [1.1828e-04],\n",
      "        [4.2444e-08],\n",
      "        [7.7640e-04]], device='cuda:0'), loss: 0.026759037747979164: \n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "func:cos_distance, ap_dist: -0.9067287445068359, an_dist: -0.34912580251693726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fooling weight increased to 11.000000000000004 at the end of epoch 71\n",
      "target probs tensor([[7.9762e-08],\n",
      "        [1.1904e-05],\n",
      "        [1.3349e-04],\n",
      "        [8.7323e-03],\n",
      "        [6.8335e-07],\n",
      "        [3.6073e-05],\n",
      "        [3.4347e-03],\n",
      "        [1.3319e-06],\n",
      "        [5.4643e-01],\n",
      "        [1.6342e-01],\n",
      "        [7.3876e-03],\n",
      "        [8.3223e-02],\n",
      "        [8.6578e-05],\n",
      "        [4.8286e-03],\n",
      "        [9.4566e-02],\n",
      "        [3.0348e-08]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.07375022023916245: \n",
      "func:cos_distance, ap_dist: -0.9927043914794922, an_dist: -0.5918279886245728\n",
      "target probs tensor([[2.4666e-05],\n",
      "        [1.0859e-03],\n",
      "        [7.8564e-03],\n",
      "        [9.5292e-09],\n",
      "        [3.9871e-06],\n",
      "        [3.6853e-04],\n",
      "        [3.0097e-02],\n",
      "        [1.9633e-04],\n",
      "        [4.2199e-08],\n",
      "        [2.2021e-08],\n",
      "        [7.5725e-06],\n",
      "        [7.8527e-01],\n",
      "        [8.2758e-08],\n",
      "        [9.9992e-01],\n",
      "        [9.9320e-01],\n",
      "        [2.2165e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.0018783807754517: \n",
      "func:cos_distance, ap_dist: -0.970278799533844, an_dist: -0.6612421274185181\n",
      "target probs tensor([[3.2355e-07],\n",
      "        [1.0946e-04],\n",
      "        [6.6697e-04],\n",
      "        [3.0960e-08],\n",
      "        [1.1201e-03],\n",
      "        [7.2107e-01],\n",
      "        [8.2264e-09],\n",
      "        [1.5243e-02],\n",
      "        [3.4745e-05],\n",
      "        [4.1771e-06],\n",
      "        [2.8571e-04],\n",
      "        [5.9218e-02],\n",
      "        [3.3427e-04],\n",
      "        [1.5138e-04],\n",
      "        [1.3246e-05],\n",
      "        [5.7470e-01]], device='cuda:0'), loss: 0.13818056881427765: \n",
      "func:cos_distance, ap_dist: -0.9843043088912964, an_dist: -0.6147035360336304\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "target probs tensor([[2.9142e-05],\n",
      "        [1.2797e-03],\n",
      "        [6.2835e-08],\n",
      "        [6.2138e-01],\n",
      "        [3.7829e-03],\n",
      "        [7.8594e-01],\n",
      "        [9.2802e-04],\n",
      "        [1.8686e-03],\n",
      "        [5.6160e-05],\n",
      "        [8.9381e-08],\n",
      "        [3.6381e-06],\n",
      "        [1.1076e-01],\n",
      "        [1.6453e-04],\n",
      "        [1.1296e-04],\n",
      "        [2.8377e-04],\n",
      "        [2.8097e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.1855301558971405: \n",
      "func:cos_distance, ap_dist: -0.995279848575592, an_dist: -0.6275087594985962\n",
      "target probs tensor([[2.0693e-01],\n",
      "        [3.6669e-05],\n",
      "        [1.0631e-02],\n",
      "        [3.6610e-07],\n",
      "        [1.6659e-06],\n",
      "        [3.0752e-03],\n",
      "        [2.3979e-07],\n",
      "        [9.3173e-04],\n",
      "        [2.5907e-04],\n",
      "        [7.6414e-05],\n",
      "        [2.9912e-01],\n",
      "        [9.9451e-01],\n",
      "        [9.9828e-03],\n",
      "        [1.3044e-03],\n",
      "        [8.9828e-02],\n",
      "        [1.7110e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.3695720434188843: \n",
      "func:cos_distance, ap_dist: -0.9541285037994385, an_dist: -0.553483247756958\n",
      "target probs tensor([[7.2219e-09],\n",
      "        [1.8360e-03],\n",
      "        [4.4376e-04],\n",
      "        [1.8032e-02],\n",
      "        [5.4876e-03],\n",
      "        [1.4052e-04],\n",
      "        [2.0323e-04],\n",
      "        [2.3148e-05],\n",
      "        [7.3798e-01],\n",
      "        [4.3333e-04],\n",
      "        [8.8532e-05],\n",
      "        [2.5697e-02],\n",
      "        [6.3760e-07],\n",
      "        [1.0380e-08],\n",
      "        [1.0998e-02],\n",
      "        [8.5229e-07]], device='cuda:0'), loss: 0.08770520240068436: \n",
      "func:cos_distance, ap_dist: -0.9850904941558838, an_dist: -0.4785395562648773\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 11.300000000000004 at the end of epoch 73\n",
      "target probs tensor([[3.2660e-10],\n",
      "        [7.1526e-07],\n",
      "        [1.4361e-08],\n",
      "        [5.8871e-07],\n",
      "        [1.2942e-06],\n",
      "        [1.9477e-07],\n",
      "        [3.4902e-03],\n",
      "        [1.4345e-04],\n",
      "        [2.8220e-06],\n",
      "        [3.1149e-05],\n",
      "        [9.9970e-01],\n",
      "        [1.1122e-03],\n",
      "        [8.6983e-03],\n",
      "        [8.4150e-05],\n",
      "        [3.8074e-06],\n",
      "        [9.7144e-09]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.5080848336219788: \n",
      "func:cos_distance, ap_dist: -0.9855625033378601, an_dist: -0.542650580406189\n",
      "target probs tensor([[2.0895e-08],\n",
      "        [7.2134e-06],\n",
      "        [7.3817e-02],\n",
      "        [6.0145e-05],\n",
      "        [3.9671e-06],\n",
      "        [7.8947e-08],\n",
      "        [1.0981e-05],\n",
      "        [8.1547e-02],\n",
      "        [5.6222e-02],\n",
      "        [3.1314e-02],\n",
      "        [2.8313e-06],\n",
      "        [6.0879e-07],\n",
      "        [1.2841e-05],\n",
      "        [4.2802e-04],\n",
      "        [4.1866e-04],\n",
      "        [1.2786e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.015853285789489746: \n",
      "func:cos_distance, ap_dist: -0.9912246465682983, an_dist: -0.44956159591674805\n",
      "target probs tensor([[5.8078e-01],\n",
      "        [6.5557e-08],\n",
      "        [7.7283e-10],\n",
      "        [1.1577e-07],\n",
      "        [2.6602e-06],\n",
      "        [1.1960e-04],\n",
      "        [1.8041e-05],\n",
      "        [2.8071e-02],\n",
      "        [6.7430e-06],\n",
      "        [1.0638e-02],\n",
      "        [8.6337e-05],\n",
      "        [5.5961e-02],\n",
      "        [9.3989e-01],\n",
      "        [5.7374e-10],\n",
      "        [5.6606e-07],\n",
      "        [9.6709e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.2367255985736847: \n",
      "func:cos_distance, ap_dist: -0.9965108036994934, an_dist: -0.42241907119750977\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "target probs tensor([[6.0137e-07],\n",
      "        [2.5031e-02],\n",
      "        [3.4276e-04],\n",
      "        [9.2005e-01],\n",
      "        [1.6340e-05],\n",
      "        [2.0022e-05],\n",
      "        [2.8950e-05],\n",
      "        [6.9759e-04],\n",
      "        [6.1643e-04],\n",
      "        [3.0274e-08],\n",
      "        [2.2932e-01],\n",
      "        [9.0329e-09],\n",
      "        [3.8190e-08],\n",
      "        [6.9956e-09],\n",
      "        [4.9003e-02],\n",
      "        [9.1819e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.17906777560710907: \n",
      "func:cos_distance, ap_dist: -0.9909051656723022, an_dist: -0.6085209846496582\n",
      "target probs tensor([[3.7464e-02],\n",
      "        [7.2737e-01],\n",
      "        [3.3181e-08],\n",
      "        [1.1034e-01],\n",
      "        [7.9571e-04],\n",
      "        [2.2776e-02],\n",
      "        [3.2076e-04],\n",
      "        [1.3640e-03],\n",
      "        [1.3654e-04],\n",
      "        [6.0955e-08],\n",
      "        [3.1819e-04],\n",
      "        [5.9090e-06],\n",
      "        [2.8418e-01],\n",
      "        [2.5144e-02],\n",
      "        [8.7066e-01],\n",
      "        [8.8601e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.3785938024520874: \n",
      "func:cos_distance, ap_dist: -0.9916015267372131, an_dist: -0.732012152671814\n",
      "target probs tensor([[1.9516e-04],\n",
      "        [1.2612e-09],\n",
      "        [7.2698e-08],\n",
      "        [9.9803e-01],\n",
      "        [8.5568e-05],\n",
      "        [1.8400e-02],\n",
      "        [1.6807e-01],\n",
      "        [3.0588e-06],\n",
      "        [3.7297e-05],\n",
      "        [5.7798e-03],\n",
      "        [6.5099e-03],\n",
      "        [2.6775e-05],\n",
      "        [3.6265e-07],\n",
      "        [1.9501e-04],\n",
      "        [3.2065e-03],\n",
      "        [2.7485e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.40303945541381836: \n",
      "func:cos_distance, ap_dist: -0.9736077785491943, an_dist: -0.7677101492881775\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "target probs tensor([[6.5511e-05],\n",
      "        [1.2463e-06],\n",
      "        [1.6225e-06],\n",
      "        [5.7203e-09],\n",
      "        [2.4673e-05],\n",
      "        [1.6366e-08],\n",
      "        [6.6239e-02],\n",
      "        [1.2044e-02],\n",
      "        [8.8265e-08],\n",
      "        [2.4796e-05],\n",
      "        [5.2012e-01],\n",
      "        [6.1057e-03],\n",
      "        [4.5821e-06],\n",
      "        [2.7480e-02],\n",
      "        [4.9378e-02],\n",
      "        [5.5393e-08]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.056225962936878204: \n",
      "func:cos_distance, ap_dist: -0.9957876205444336, an_dist: -0.5439527630805969\n",
      "target probs tensor([[8.4414e-01],\n",
      "        [1.7748e-03],\n",
      "        [1.6868e-08],\n",
      "        [4.5762e-04],\n",
      "        [7.3296e-05],\n",
      "        [1.6878e-03],\n",
      "        [1.0645e-08],\n",
      "        [2.2471e-03],\n",
      "        [6.7745e-07],\n",
      "        [9.1544e-09],\n",
      "        [1.6487e-07],\n",
      "        [2.0845e-01],\n",
      "        [1.4580e-03],\n",
      "        [3.7034e-03],\n",
      "        [3.8039e-08],\n",
      "        [5.3887e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.1798793375492096: \n",
      "func:cos_distance, ap_dist: -0.9957513809204102, an_dist: -0.7438987493515015\n",
      "target probs tensor([[2.1807e-04],\n",
      "        [9.4998e-01],\n",
      "        [4.8945e-02],\n",
      "        [4.0410e-06],\n",
      "        [7.6250e-04],\n",
      "        [1.1469e-02],\n",
      "        [6.2273e-07],\n",
      "        [6.3629e-06],\n",
      "        [3.3702e-05],\n",
      "        [4.6691e-09],\n",
      "        [1.0000e+00],\n",
      "        [7.5872e-05],\n",
      "        [1.4266e-05],\n",
      "        [1.0663e-02],\n",
      "        [5.3615e-01],\n",
      "        [4.3397e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.0988833904266357: \n",
      "func:cos_distance, ap_dist: -0.9899520874023438, an_dist: -0.5989537239074707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 11.600000000000005 at the end of epoch 76\n",
      "target probs tensor([[3.2998e-03],\n",
      "        [5.4000e-03],\n",
      "        [4.7967e-02],\n",
      "        [5.3937e-08],\n",
      "        [8.2828e-01],\n",
      "        [4.3772e-05],\n",
      "        [2.6781e-06],\n",
      "        [1.0264e-04],\n",
      "        [5.5025e-07],\n",
      "        [1.6445e-06],\n",
      "        [9.9995e-01],\n",
      "        [7.1266e-03],\n",
      "        [4.6925e-01],\n",
      "        [7.2404e-07],\n",
      "        [3.4525e-04],\n",
      "        [1.0217e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.7683773040771484: \n",
      "func:cos_distance, ap_dist: -0.9957360625267029, an_dist: -0.651700496673584\n",
      "target probs tensor([[2.7275e-02],\n",
      "        [5.2723e-03],\n",
      "        [7.0407e-02],\n",
      "        [9.0124e-01],\n",
      "        [9.2942e-06],\n",
      "        [8.1911e-03],\n",
      "        [3.7563e-04],\n",
      "        [6.7328e-05],\n",
      "        [1.0705e-05],\n",
      "        [1.3412e-05],\n",
      "        [4.1344e-07],\n",
      "        [5.5637e-03],\n",
      "        [7.3595e-01],\n",
      "        [5.0540e-10],\n",
      "        [4.3611e-05],\n",
      "        [1.8656e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.23543572425842285: \n",
      "func:cos_distance, ap_dist: -0.992824912071228, an_dist: -0.6953587532043457\n",
      "target probs tensor([[1.7567e-01],\n",
      "        [6.6258e-04],\n",
      "        [1.3998e-08],\n",
      "        [6.3075e-01],\n",
      "        [4.9458e-01],\n",
      "        [1.5029e-07],\n",
      "        [8.3813e-08],\n",
      "        [1.9748e-03],\n",
      "        [4.8243e-04],\n",
      "        [2.8757e-03],\n",
      "        [6.2182e-01],\n",
      "        [2.0119e-08],\n",
      "        [9.2180e-05],\n",
      "        [1.9810e-06],\n",
      "        [1.3847e-05],\n",
      "        [8.5940e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.3007572293281555: \n",
      "func:cos_distance, ap_dist: -0.9876086711883545, an_dist: -0.7426397800445557\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 77 with validation value: 0.8629999756813049.\n",
      "target probs tensor([[1.0822e-04],\n",
      "        [1.0052e-04],\n",
      "        [1.4505e-03],\n",
      "        [5.3067e-06],\n",
      "        [6.6025e-08],\n",
      "        [2.1336e-06],\n",
      "        [2.2221e-03],\n",
      "        [3.1876e-01],\n",
      "        [1.7740e-05],\n",
      "        [2.1570e-05],\n",
      "        [1.2203e-04],\n",
      "        [4.4624e-03],\n",
      "        [3.2557e-05],\n",
      "        [1.1385e-06],\n",
      "        [3.0916e-08],\n",
      "        [5.9245e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.028342362493276596: \n",
      "func:cos_distance, ap_dist: -0.9775465726852417, an_dist: -0.6886076331138611\n",
      "target probs tensor([[4.5077e-08],\n",
      "        [1.3392e-04],\n",
      "        [1.4775e-02],\n",
      "        [9.6432e-01],\n",
      "        [5.0206e-01],\n",
      "        [1.8943e-03],\n",
      "        [1.9867e-05],\n",
      "        [9.7479e-04],\n",
      "        [2.7748e-04],\n",
      "        [2.5627e-03],\n",
      "        [7.6586e-07],\n",
      "        [8.9846e-08],\n",
      "        [6.8095e-02],\n",
      "        [8.8963e-09],\n",
      "        [2.7220e-02],\n",
      "        [7.9415e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.25933346152305603: \n",
      "func:cos_distance, ap_dist: -0.9905226230621338, an_dist: -0.5544123649597168\n",
      "target probs tensor([[1.1275e-06],\n",
      "        [9.9992e-01],\n",
      "        [6.3025e-04],\n",
      "        [1.0421e-01],\n",
      "        [3.1285e-01],\n",
      "        [9.2314e-04],\n",
      "        [2.3241e-04],\n",
      "        [2.5147e-06],\n",
      "        [9.8672e-01],\n",
      "        [1.2712e-06],\n",
      "        [2.4915e-06],\n",
      "        [1.4715e-05],\n",
      "        [1.6631e-01],\n",
      "        [4.0297e-03],\n",
      "        [4.7023e-04],\n",
      "        [5.5416e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.9032062292098999: \n",
      "func:cos_distance, ap_dist: -0.9968517422676086, an_dist: -0.6959195733070374\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "fooling weight increased to 11.900000000000006 at the end of epoch 78\n",
      "target probs tensor([[2.5759e-02],\n",
      "        [6.4770e-07],\n",
      "        [8.9529e-09],\n",
      "        [1.1106e-03],\n",
      "        [1.3216e-02],\n",
      "        [9.4316e-01],\n",
      "        [4.8380e-07],\n",
      "        [2.5529e-04],\n",
      "        [3.1417e-02],\n",
      "        [1.2666e-09],\n",
      "        [1.1368e-07],\n",
      "        [6.7824e-11],\n",
      "        [7.1536e-07],\n",
      "        [1.9774e-07],\n",
      "        [8.8000e-01],\n",
      "        [1.6631e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.3276435136795044: \n",
      "func:cos_distance, ap_dist: -0.9953570365905762, an_dist: -0.7206647992134094\n",
      "target probs tensor([[7.2587e-09],\n",
      "        [2.0135e-07],\n",
      "        [1.6199e-05],\n",
      "        [2.5963e-04],\n",
      "        [7.3880e-04],\n",
      "        [4.5477e-11],\n",
      "        [6.3588e-01],\n",
      "        [1.7939e-01],\n",
      "        [7.1842e-05],\n",
      "        [2.1735e-07],\n",
      "        [8.2694e-02],\n",
      "        [4.2791e-09],\n",
      "        [1.1895e-06],\n",
      "        [4.4477e-08],\n",
      "        [8.9715e-06],\n",
      "        [9.9879e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.500689685344696: \n",
      "func:cos_distance, ap_dist: -0.987330973148346, an_dist: -0.47277766466140747\n",
      "target probs tensor([[2.5805e-01],\n",
      "        [6.5683e-06],\n",
      "        [1.0322e-06],\n",
      "        [2.9573e-08],\n",
      "        [4.7114e-03],\n",
      "        [1.1989e-05],\n",
      "        [5.4734e-01],\n",
      "        [5.7177e-05],\n",
      "        [3.6187e-03],\n",
      "        [8.0494e-04],\n",
      "        [9.8327e-09],\n",
      "        [3.1530e-06],\n",
      "        [4.4566e-06],\n",
      "        [8.2307e-05],\n",
      "        [2.6475e-02],\n",
      "        [2.8396e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 0.07063032686710358: \n",
      "func:cos_distance, ap_dist: -0.9850234985351562, an_dist: -0.7070553302764893\n",
      "target probs tensor([[6.5019e-09],\n",
      "        [4.7165e-08],\n",
      "        [4.4804e-10],\n",
      "        [7.1233e-02],\n",
      "        [5.6363e-04],\n",
      "        [3.9607e-05],\n",
      "        [3.7973e-08],\n",
      "        [1.3005e-03]], device='cuda:0'), loss: 0.009475289843976498: \n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "func:cos_distance, ap_dist: -0.9936302304267883, an_dist: -0.6720917224884033\n",
      "fooling weight increased to 12.200000000000006 at the end of epoch 79\n",
      "models_directory returned is:  models/145\n",
      "models_directory returned is:  models/145\n"
     ]
    }
   ],
   "source": [
    "# RUN SITE\n",
    "\n",
    "if mode == \"sanity_check\":\n",
    "  print(\"\\n\\n\\nWARNING: you are training on a sanity_check dataset.\\n\\n\\n\\n\")\n",
    "\n",
    "saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=env.save_filename + \"-best\")\n",
    "saver_every_epoch = SaveModelCallback(learn, every='epoch', name=env.save_filename)\n",
    "fooling_weight_scheduler = FoolingWeightScheduler(learn, fooling_loss_index = 3)\n",
    "\n",
    "# with Hooks(gen, append_stats_normal) as hooks:\n",
    "#   learn.fit(1, lr=5e-03, wd = 0., callbacks=[saver_best, saver_every_epoch])\n",
    "  \n",
    "learn.fit(80, lr=1e-3, wd = 0., callbacks=[saver_best, saver_every_epoch, fooling_weight_scheduler])\n",
    "\n",
    "# # learn.fit(70, lr=1e-02, wd = 0.001, callbacks=[saver_best, saver_every_epoch])\n",
    "\n",
    "# learn.fit(60, lr=1e-2, wd = 0.001, callbacks=[saver_best, saver_every_epoch])\n",
    "\n",
    "# for i in range(10):\n",
    "#   learn.fit_one_cycle(7, wd = 0.,max_lr=1., div_factor = 1000.) \n",
    "  \n",
    "# learn.fit_one_cycle(5, max_lr=2e-2) #used for vgg-19-bn\n",
    "# learn.fit_one_cycle(5, max_lr=3e-3) # used for resnet50\n",
    "\n",
    "shutil.copyfile(env.temp_csv_path + '/' + env.save_filename + \".csv\", env.get_csv_path() + '.csv')\n",
    "shutil.copytree(env.data_path/env.get_learner_models_dir(), env.get_models_path())\n",
    "shutil.rmtree(env.data_path/env.get_learner_models_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram\n",
    "fig, axes = plt.subplots(len(hooks),1, figsize=(30,12))\n",
    "for ax,h in zip(axes.flatten(), hooks):\n",
    "  ax.imshow(get_hist(h), origin='lower')\n",
    "  ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mean and std\n",
    "fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "for h in hooks:\n",
    "  ms, ss, _ = h.stats\n",
    "  ax0.plot(ms[:100])\n",
    "  ax1.plot(ss[:100])\n",
    "plt.legend(range(len(hooks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "for h in hooks:\n",
    "  ms, ss, _ = h.stats\n",
    "  ax0.plot(ms)\n",
    "  ax1.plot(ss)\n",
    "plt.legend(range(len(hooks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero precentage:\n",
    "fig,axes = plt.subplots(len(hooks),1, figsize=(30,30))\n",
    "for ax,h in zip(axes.flatten(), hooks):\n",
    "    ax.plot(get_min(h))\n",
    "    ax.set_ylim(0,1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MO2fZ-hSSUzJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# z1 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "# z2 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "z1 = torch.tensor([0.8, -0.5] * 5).cuda()\n",
    "z2 = torch.tensor([-1.] * 10).cuda()\n",
    "print(\"z1: \", z1)\n",
    "print(\"z2: \", z2)\n",
    "print(\"distance: \", torch.norm(z1-z2,p=2))\n",
    "model = learn.model.eval()\n",
    "\n",
    "z_s = interpolate(z1, z2, 0.1)\n",
    "print(len(z_s))\n",
    "\n",
    "for i,z in enumerate(z_s):\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n",
    "  #img.save('./pics/' + str(i) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea : have 200 noises (1 for each class), then start iterating the dataset, and for each image, randomly apply one noise and record the result\n",
    "def targeted_diversity(learn, n_perturbations = 200, percentage = 95):\n",
    "  model = learn.model.eval()\n",
    "\n",
    "  one_hot_conditions = [torch.empty(z_dim).uniform_(0,1).cuda().detach() for _ in range(n_perturbations)]\n",
    "#   for i in range(z_dim):\n",
    "#     one_hot_conditions[i][i] = 1.\n",
    "\n",
    "  perturbations = [model.forward_single_z(z) for z in one_hot_conditions]\n",
    "\n",
    "  hist = [0.] * z_dim\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 : print(\"at batch_no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbations[np.random.randint(0,len(perturbations))][None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      hist[pred] += 1\n",
    "\n",
    "  pred_histogram_sum = np.sum(hist)\n",
    "  indexed_pred_histogram = [(i, hist_element) for i,hist_element in  \n",
    "                            enumerate(hist)]\n",
    "\n",
    "  indexed_pred_histogram.sort(key=lambda x: x[1], reverse = True)\n",
    "\n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = indexed_pred_histogram[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / pred_histogram_sum) * 100.\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, indexed_pred_histogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 5\n",
      "at batch no 10\n",
      "at batch no 15\n",
      "at batch no 20\n",
      "at batch no 25\n",
      "at batch no 30\n",
      "at batch no 35\n",
      "at batch no 40\n",
      "at batch no 45\n",
      "at batch no 50\n",
      "at batch no 55\n",
      "at batch no 60\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(364,\n",
       " [(721, 143.20),\n",
       "  (750, 49.40),\n",
       "  (971, 48.40),\n",
       "  (794, 47.10),\n",
       "  (431, 35.10),\n",
       "  (669, 35.10),\n",
       "  (414, 30.60),\n",
       "  (588, 28.40),\n",
       "  (520, 24.90),\n",
       "  (61, 24.20),\n",
       "  (904, 18.30),\n",
       "  (411, 14.30),\n",
       "  (828, 13.50),\n",
       "  (39, 11.70),\n",
       "  (556, 11.40),\n",
       "  (581, 9.20),\n",
       "  (651, 8.30),\n",
       "  (489, 7.80),\n",
       "  (599, 6.60),\n",
       "  (84, 5.50),\n",
       "  (572, 4.80),\n",
       "  (907, 4.70),\n",
       "  (987, 4.40),\n",
       "  (401, 4.30),\n",
       "  (490, 4.30),\n",
       "  (614, 4.30),\n",
       "  (60, 4.00),\n",
       "  (955, 3.50),\n",
       "  (711, 3.30),\n",
       "  (48, 3.20),\n",
       "  (691, 3.10),\n",
       "  (709, 3.00),\n",
       "  (419, 2.90),\n",
       "  (770, 2.90),\n",
       "  (815, 2.90),\n",
       "  (864, 2.80),\n",
       "  (879, 2.80),\n",
       "  (108, 2.70),\n",
       "  (441, 2.70),\n",
       "  (632, 2.70),\n",
       "  (56, 2.50),\n",
       "  (604, 2.50),\n",
       "  (55, 2.40),\n",
       "  (464, 2.30),\n",
       "  (549, 2.30),\n",
       "  (575, 2.20),\n",
       "  (893, 2.20),\n",
       "  (973, 2.20),\n",
       "  (96, 2.10),\n",
       "  (496, 2.10),\n",
       "  (518, 2.10),\n",
       "  (412, 2.00),\n",
       "  (641, 2.00),\n",
       "  (762, 2.00),\n",
       "  (801, 2.00),\n",
       "  (872, 2.00),\n",
       "  (858, 1.90),\n",
       "  (871, 1.90),\n",
       "  (580, 1.80),\n",
       "  (621, 1.80),\n",
       "  (746, 1.80),\n",
       "  (806, 1.80),\n",
       "  (46, 1.70),\n",
       "  (151, 1.70),\n",
       "  (171, 1.70),\n",
       "  (633, 1.70),\n",
       "  (781, 1.70),\n",
       "  (790, 1.70),\n",
       "  (850, 1.70),\n",
       "  (424, 1.60),\n",
       "  (443, 1.60),\n",
       "  (453, 1.60),\n",
       "  (646, 1.60),\n",
       "  (981, 1.60),\n",
       "  (128, 1.50),\n",
       "  (360, 1.50),\n",
       "  (457, 1.50),\n",
       "  (545, 1.50),\n",
       "  (680, 1.50),\n",
       "  (722, 1.50),\n",
       "  (743, 1.50),\n",
       "  (1, 1.40),\n",
       "  (94, 1.40),\n",
       "  (230, 1.40),\n",
       "  (292, 1.40),\n",
       "  (406, 1.40),\n",
       "  (410, 1.40),\n",
       "  (440, 1.40),\n",
       "  (506, 1.40),\n",
       "  (847, 1.40),\n",
       "  (897, 1.40),\n",
       "  (67, 1.30),\n",
       "  (68, 1.30),\n",
       "  (124, 1.30),\n",
       "  (417, 1.30),\n",
       "  (538, 1.30),\n",
       "  (706, 1.30),\n",
       "  (779, 1.30),\n",
       "  (791, 1.30),\n",
       "  (826, 1.30),\n",
       "  (865, 1.30),\n",
       "  (898, 1.30),\n",
       "  (937, 1.30),\n",
       "  (953, 1.30),\n",
       "  (242, 1.20),\n",
       "  (310, 1.20),\n",
       "  (408, 1.20),\n",
       "  (582, 1.20),\n",
       "  (591, 1.20),\n",
       "  (698, 1.20),\n",
       "  (857, 1.20),\n",
       "  (896, 1.20),\n",
       "  (963, 1.20),\n",
       "  (83, 1.10),\n",
       "  (90, 1.10),\n",
       "  (92, 1.10),\n",
       "  (123, 1.10),\n",
       "  (293, 1.10),\n",
       "  (300, 1.10),\n",
       "  (316, 1.10),\n",
       "  (393, 1.10),\n",
       "  (407, 1.10),\n",
       "  (468, 1.10),\n",
       "  (612, 1.10),\n",
       "  (619, 1.10),\n",
       "  (645, 1.10),\n",
       "  (656, 1.10),\n",
       "  (738, 1.10),\n",
       "  (783, 1.10),\n",
       "  (805, 1.10),\n",
       "  (817, 1.10),\n",
       "  (906, 1.10),\n",
       "  (7, 1.00),\n",
       "  (25, 1.00),\n",
       "  (31, 1.00),\n",
       "  (33, 1.00),\n",
       "  (37, 1.00),\n",
       "  (57, 1.00),\n",
       "  (75, 1.00),\n",
       "  (88, 1.00),\n",
       "  (91, 1.00),\n",
       "  (102, 1.00),\n",
       "  (115, 1.00),\n",
       "  (120, 1.00),\n",
       "  (163, 1.00),\n",
       "  (176, 1.00),\n",
       "  (186, 1.00),\n",
       "  (195, 1.00),\n",
       "  (218, 1.00),\n",
       "  (231, 1.00),\n",
       "  (247, 1.00),\n",
       "  (260, 1.00),\n",
       "  (274, 1.00),\n",
       "  (281, 1.00),\n",
       "  (290, 1.00),\n",
       "  (306, 1.00),\n",
       "  (307, 1.00),\n",
       "  (314, 1.00),\n",
       "  (318, 1.00),\n",
       "  (334, 1.00),\n",
       "  (376, 1.00),\n",
       "  (392, 1.00),\n",
       "  (429, 1.00),\n",
       "  (474, 1.00),\n",
       "  (483, 1.00),\n",
       "  (488, 1.00),\n",
       "  (492, 1.00),\n",
       "  (498, 1.00),\n",
       "  (507, 1.00),\n",
       "  (516, 1.00),\n",
       "  (528, 1.00),\n",
       "  (530, 1.00),\n",
       "  (533, 1.00),\n",
       "  (562, 1.00),\n",
       "  (566, 1.00),\n",
       "  (602, 1.00),\n",
       "  (625, 1.00),\n",
       "  (637, 1.00),\n",
       "  (661, 1.00),\n",
       "  (684, 1.00),\n",
       "  (694, 1.00),\n",
       "  (716, 1.00),\n",
       "  (725, 1.00),\n",
       "  (734, 1.00),\n",
       "  (787, 1.00),\n",
       "  (816, 1.00),\n",
       "  (822, 1.00),\n",
       "  (824, 1.00),\n",
       "  (837, 1.00),\n",
       "  (873, 1.00),\n",
       "  (923, 1.00),\n",
       "  (934, 1.00),\n",
       "  (944, 1.00),\n",
       "  (992, 1.00),\n",
       "  (997, 1.00),\n",
       "  (40, 0.90),\n",
       "  (62, 0.90),\n",
       "  (105, 0.90),\n",
       "  (164, 0.90),\n",
       "  (301, 0.90),\n",
       "  (347, 0.90),\n",
       "  (369, 0.90),\n",
       "  (397, 0.90),\n",
       "  (425, 0.90),\n",
       "  (444, 0.90),\n",
       "  (753, 0.90),\n",
       "  (819, 0.90),\n",
       "  (868, 0.90),\n",
       "  (889, 0.90),\n",
       "  (982, 0.90),\n",
       "  (47, 0.80),\n",
       "  (72, 0.80),\n",
       "  (86, 0.80),\n",
       "  (97, 0.80),\n",
       "  (118, 0.80),\n",
       "  (254, 0.80),\n",
       "  (275, 0.80),\n",
       "  (304, 0.80),\n",
       "  (331, 0.80),\n",
       "  (463, 0.80),\n",
       "  (509, 0.80),\n",
       "  (532, 0.80),\n",
       "  (576, 0.80),\n",
       "  (586, 0.80),\n",
       "  (606, 0.80),\n",
       "  (638, 0.80),\n",
       "  (703, 0.80),\n",
       "  (732, 0.80),\n",
       "  (772, 0.80),\n",
       "  (786, 0.80),\n",
       "  (803, 0.80),\n",
       "  (829, 0.80),\n",
       "  (878, 0.80),\n",
       "  (899, 0.80),\n",
       "  (905, 0.80),\n",
       "  (915, 0.80),\n",
       "  (984, 0.80),\n",
       "  (8, 0.70),\n",
       "  (15, 0.70),\n",
       "  (38, 0.70),\n",
       "  (42, 0.70),\n",
       "  (109, 0.70),\n",
       "  (116, 0.70),\n",
       "  (144, 0.70),\n",
       "  (155, 0.70),\n",
       "  (189, 0.70),\n",
       "  (235, 0.70),\n",
       "  (313, 0.70),\n",
       "  (341, 0.70),\n",
       "  (353, 0.70),\n",
       "  (355, 0.70),\n",
       "  (387, 0.70),\n",
       "  (438, 0.70),\n",
       "  (476, 0.70),\n",
       "  (497, 0.70),\n",
       "  (508, 0.70),\n",
       "  (517, 0.70),\n",
       "  (526, 0.70),\n",
       "  (547, 0.70),\n",
       "  (552, 0.70),\n",
       "  (564, 0.70),\n",
       "  (570, 0.70),\n",
       "  (579, 0.70),\n",
       "  (620, 0.70),\n",
       "  (629, 0.70),\n",
       "  (727, 0.70),\n",
       "  (741, 0.70),\n",
       "  (752, 0.70),\n",
       "  (778, 0.70),\n",
       "  (788, 0.70),\n",
       "  (814, 0.70),\n",
       "  (925, 0.70),\n",
       "  (17, 0.60),\n",
       "  (19, 0.60),\n",
       "  (87, 0.60),\n",
       "  (289, 0.60),\n",
       "  (291, 0.60),\n",
       "  (294, 0.60),\n",
       "  (327, 0.60),\n",
       "  (375, 0.60),\n",
       "  (398, 0.60),\n",
       "  (409, 0.60),\n",
       "  (433, 0.60),\n",
       "  (445, 0.60),\n",
       "  (472, 0.60),\n",
       "  (482, 0.60),\n",
       "  (535, 0.60),\n",
       "  (544, 0.60),\n",
       "  (565, 0.60),\n",
       "  (671, 0.60),\n",
       "  (679, 0.60),\n",
       "  (696, 0.60),\n",
       "  (701, 0.60),\n",
       "  (751, 0.60),\n",
       "  (760, 0.60),\n",
       "  (796, 0.60),\n",
       "  (797, 0.60),\n",
       "  (820, 0.60),\n",
       "  (867, 0.60),\n",
       "  (892, 0.60),\n",
       "  (902, 0.60),\n",
       "  (920, 0.60),\n",
       "  (998, 0.60),\n",
       "  (0, 0.50),\n",
       "  (45, 0.50),\n",
       "  (50, 0.50),\n",
       "  (52, 0.50),\n",
       "  (107, 0.50),\n",
       "  (149, 0.50),\n",
       "  (159, 0.50),\n",
       "  (336, 0.50),\n",
       "  (342, 0.50),\n",
       "  (348, 0.50),\n",
       "  (391, 0.50),\n",
       "  (495, 0.50),\n",
       "  (515, 0.50),\n",
       "  (523, 0.50),\n",
       "  (527, 0.50),\n",
       "  (539, 0.50),\n",
       "  (555, 0.50),\n",
       "  (605, 0.50),\n",
       "  (607, 0.50),\n",
       "  (609, 0.50),\n",
       "  (626, 0.50),\n",
       "  (654, 0.50),\n",
       "  (655, 0.50),\n",
       "  (664, 0.50),\n",
       "  (674, 0.50),\n",
       "  (705, 0.50),\n",
       "  (719, 0.50),\n",
       "  (745, 0.50),\n",
       "  (754, 0.50),\n",
       "  (759, 0.50),\n",
       "  (768, 0.50),\n",
       "  (823, 0.50),\n",
       "  (853, 0.50),\n",
       "  (900, 0.50),\n",
       "  (917, 0.50),\n",
       "  (985, 0.50),\n",
       "  (9, 0.40),\n",
       "  (23, 0.40),\n",
       "  (24, 0.40),\n",
       "  (28, 0.40),\n",
       "  (63, 0.40),\n",
       "  (77, 0.40),\n",
       "  (126, 0.40),\n",
       "  (134, 0.40),\n",
       "  (140, 0.40),\n",
       "  (168, 0.40),\n",
       "  (192, 0.40),\n",
       "  (197, 0.40),\n",
       "  (205, 0.40),\n",
       "  (219, 0.40),\n",
       "  (224, 0.40),\n",
       "  (236, 0.40),\n",
       "  (249, 0.40),\n",
       "  (305, 0.40),\n",
       "  (319, 0.40),\n",
       "  (321, 0.40),\n",
       "  (332, 0.40),\n",
       "  (363, 0.40),\n",
       "  (381, 0.40),\n",
       "  (383, 0.40),\n",
       "  (388, 0.40),\n",
       "  (389, 0.40),\n",
       "  (396, 0.40),\n",
       "  (473, 0.40),\n",
       "  (481, 0.40),\n",
       "  (491, 0.40),\n",
       "  (522, 0.40),\n",
       "  (546, 0.40),\n",
       "  (574, 0.40),\n",
       "  (584, 0.40),\n",
       "  (672, 0.40),\n",
       "  (692, 0.40),\n",
       "  (697, 0.40),\n",
       "  (712, 0.40),\n",
       "  (802, 0.40),\n",
       "  (843, 0.40),\n",
       "  (854, 0.40),\n",
       "  (870, 0.40),\n",
       "  (880, 0.40),\n",
       "  (882, 0.40),\n",
       "  (890, 0.40),\n",
       "  (918, 0.40),\n",
       "  (938, 0.40),\n",
       "  (991, 0.40),\n",
       "  (41, 0.30),\n",
       "  (65, 0.30),\n",
       "  (71, 0.30),\n",
       "  (74, 0.30),\n",
       "  (113, 0.30),\n",
       "  (183, 0.30),\n",
       "  (191, 0.30),\n",
       "  (196, 0.30),\n",
       "  (202, 0.30),\n",
       "  (228, 0.30),\n",
       "  (232, 0.30),\n",
       "  (253, 0.30),\n",
       "  (303, 0.30),\n",
       "  (320, 0.30),\n",
       "  (337, 0.30),\n",
       "  (350, 0.30),\n",
       "  (358, 0.30),\n",
       "  (364, 0.30),\n",
       "  (399, 0.30),\n",
       "  (452, 0.30),\n",
       "  (454, 0.30),\n",
       "  (477, 0.30),\n",
       "  (480, 0.30),\n",
       "  (512, 0.30),\n",
       "  (537, 0.30),\n",
       "  (550, 0.30),\n",
       "  (554, 0.30),\n",
       "  (560, 0.30),\n",
       "  (593, 0.30),\n",
       "  (640, 0.30),\n",
       "  (644, 0.30),\n",
       "  (683, 0.30),\n",
       "  (707, 0.30),\n",
       "  (720, 0.30),\n",
       "  (729, 0.30),\n",
       "  (748, 0.30),\n",
       "  (757, 0.30),\n",
       "  (758, 0.30),\n",
       "  (777, 0.30),\n",
       "  (811, 0.30),\n",
       "  (831, 0.30),\n",
       "  (840, 0.30),\n",
       "  (846, 0.30),\n",
       "  (875, 0.30),\n",
       "  (883, 0.30),\n",
       "  (932, 0.30),\n",
       "  (939, 0.30),\n",
       "  (951, 0.30),\n",
       "  (957, 0.30),\n",
       "  (968, 0.30),\n",
       "  (995, 0.30),\n",
       "  (58, 0.20),\n",
       "  (82, 0.20),\n",
       "  (93, 0.20),\n",
       "  (98, 0.20),\n",
       "  (99, 0.20),\n",
       "  (100, 0.20),\n",
       "  (117, 0.20),\n",
       "  (122, 0.20),\n",
       "  (131, 0.20),\n",
       "  (138, 0.20),\n",
       "  (153, 0.20),\n",
       "  (161, 0.20),\n",
       "  (178, 0.20),\n",
       "  (188, 0.20),\n",
       "  (198, 0.20),\n",
       "  (206, 0.20),\n",
       "  (238, 0.20),\n",
       "  (283, 0.20),\n",
       "  (284, 0.20),\n",
       "  (317, 0.20),\n",
       "  (323, 0.20),\n",
       "  (326, 0.20),\n",
       "  (340, 0.20),\n",
       "  (344, 0.20),\n",
       "  (379, 0.20),\n",
       "  (395, 0.20),\n",
       "  (420, 0.20),\n",
       "  (432, 0.20),\n",
       "  (435, 0.20),\n",
       "  (436, 0.20),\n",
       "  (455, 0.20),\n",
       "  (484, 0.20),\n",
       "  (485, 0.20),\n",
       "  (487, 0.20),\n",
       "  (502, 0.20),\n",
       "  (505, 0.20),\n",
       "  (514, 0.20),\n",
       "  (534, 0.20),\n",
       "  (557, 0.20),\n",
       "  (585, 0.20),\n",
       "  (595, 0.20),\n",
       "  (618, 0.20),\n",
       "  (635, 0.20),\n",
       "  (643, 0.20),\n",
       "  (681, 0.20),\n",
       "  (700, 0.20),\n",
       "  (717, 0.20),\n",
       "  (723, 0.20),\n",
       "  (728, 0.20),\n",
       "  (736, 0.20),\n",
       "  (747, 0.20),\n",
       "  (785, 0.20),\n",
       "  (800, 0.20),\n",
       "  (808, 0.20),\n",
       "  (809, 0.20),\n",
       "  (818, 0.20),\n",
       "  (821, 0.20),\n",
       "  (825, 0.20),\n",
       "  (832, 0.20),\n",
       "  (844, 0.20),\n",
       "  (852, 0.20),\n",
       "  (876, 0.20),\n",
       "  (881, 0.20),\n",
       "  (926, 0.20),\n",
       "  (962, 0.20),\n",
       "  (966, 0.20),\n",
       "  (996, 0.20),\n",
       "  (11, 0.10),\n",
       "  (18, 0.10),\n",
       "  (21, 0.10),\n",
       "  (22, 0.10),\n",
       "  (26, 0.10),\n",
       "  (66, 0.10),\n",
       "  (70, 0.10),\n",
       "  (76, 0.10),\n",
       "  (79, 0.10),\n",
       "  (95, 0.10),\n",
       "  (111, 0.10),\n",
       "  (114, 0.10),\n",
       "  (121, 0.10),\n",
       "  (125, 0.10),\n",
       "  (129, 0.10),\n",
       "  (132, 0.10),\n",
       "  (139, 0.10),\n",
       "  (141, 0.10),\n",
       "  (142, 0.10),\n",
       "  (146, 0.10),\n",
       "  (158, 0.10),\n",
       "  (173, 0.10),\n",
       "  (179, 0.10),\n",
       "  (193, 0.10),\n",
       "  (214, 0.10),\n",
       "  (222, 0.10),\n",
       "  (229, 0.10),\n",
       "  (237, 0.10),\n",
       "  (252, 0.10),\n",
       "  (256, 0.10),\n",
       "  (268, 0.10),\n",
       "  (273, 0.10),\n",
       "  (276, 0.10),\n",
       "  (278, 0.10),\n",
       "  (282, 0.10),\n",
       "  (295, 0.10),\n",
       "  (298, 0.10),\n",
       "  (308, 0.10),\n",
       "  (309, 0.10),\n",
       "  (311, 0.10),\n",
       "  (315, 0.10),\n",
       "  (330, 0.10),\n",
       "  (343, 0.10),\n",
       "  (346, 0.10),\n",
       "  (351, 0.10),\n",
       "  (352, 0.10),\n",
       "  (356, 0.10),\n",
       "  (361, 0.10),\n",
       "  (365, 0.10),\n",
       "  (366, 0.10),\n",
       "  (377, 0.10),\n",
       "  (413, 0.10),\n",
       "  (423, 0.10),\n",
       "  (427, 0.10),\n",
       "  (428, 0.10),\n",
       "  (439, 0.10),\n",
       "  (442, 0.10),\n",
       "  (447, 0.10),\n",
       "  (448, 0.10),\n",
       "  (450, 0.10),\n",
       "  (451, 0.10),\n",
       "  (459, 0.10),\n",
       "  (475, 0.10),\n",
       "  (486, 0.10),\n",
       "  (493, 0.10),\n",
       "  (504, 0.10),\n",
       "  (521, 0.10),\n",
       "  (540, 0.10),\n",
       "  (551, 0.10),\n",
       "  (563, 0.10),\n",
       "  (569, 0.10),\n",
       "  (577, 0.10),\n",
       "  (603, 0.10),\n",
       "  (611, 0.10),\n",
       "  (613, 0.10),\n",
       "  (615, 0.10),\n",
       "  (616, 0.10),\n",
       "  (636, 0.10),\n",
       "  (639, 0.10),\n",
       "  (650, 0.10),\n",
       "  (665, 0.10),\n",
       "  (668, 0.10),\n",
       "  (673, 0.10),\n",
       "  (730, 0.10),\n",
       "  (735, 0.10),\n",
       "  (755, 0.10),\n",
       "  (756, 0.10),\n",
       "  (761, 0.10),\n",
       "  (763, 0.10),\n",
       "  (764, 0.10),\n",
       "  (765, 0.10),\n",
       "  (775, 0.10),\n",
       "  (792, 0.10),\n",
       "  (804, 0.10),\n",
       "  (813, 0.10),\n",
       "  (830, 0.10),\n",
       "  (833, 0.10),\n",
       "  (834, 0.10),\n",
       "  (836, 0.10),\n",
       "  (839, 0.10),\n",
       "  (842, 0.10),\n",
       "  (855, 0.10),\n",
       "  (856, 0.10),\n",
       "  (863, 0.10),\n",
       "  (866, 0.10),\n",
       "  (884, 0.10),\n",
       "  (885, 0.10),\n",
       "  (901, 0.10),\n",
       "  (927, 0.10),\n",
       "  (946, 0.10),\n",
       "  (950, 0.10),\n",
       "  (952, 0.10),\n",
       "  (954, 0.10),\n",
       "  (978, 0.10),\n",
       "  (983, 0.10),\n",
       "  (988, 0.10),\n",
       "  (999, 0.10),\n",
       "  (2, 0.00),\n",
       "  (3, 0.00),\n",
       "  (4, 0.00),\n",
       "  (5, 0.00),\n",
       "  (6, 0.00),\n",
       "  (10, 0.00),\n",
       "  (12, 0.00),\n",
       "  (13, 0.00),\n",
       "  (14, 0.00),\n",
       "  (16, 0.00),\n",
       "  (20, 0.00),\n",
       "  (27, 0.00),\n",
       "  (29, 0.00),\n",
       "  (30, 0.00),\n",
       "  (32, 0.00),\n",
       "  (34, 0.00),\n",
       "  (35, 0.00),\n",
       "  (36, 0.00),\n",
       "  (43, 0.00),\n",
       "  (44, 0.00),\n",
       "  (49, 0.00),\n",
       "  (51, 0.00),\n",
       "  (53, 0.00),\n",
       "  (54, 0.00),\n",
       "  (59, 0.00),\n",
       "  (64, 0.00),\n",
       "  (69, 0.00),\n",
       "  (73, 0.00),\n",
       "  (78, 0.00),\n",
       "  (80, 0.00),\n",
       "  (81, 0.00),\n",
       "  (85, 0.00),\n",
       "  (89, 0.00),\n",
       "  (101, 0.00),\n",
       "  (103, 0.00),\n",
       "  (104, 0.00),\n",
       "  (106, 0.00),\n",
       "  (110, 0.00),\n",
       "  (112, 0.00),\n",
       "  (119, 0.00),\n",
       "  (127, 0.00),\n",
       "  (130, 0.00),\n",
       "  (133, 0.00),\n",
       "  (135, 0.00),\n",
       "  (136, 0.00),\n",
       "  (137, 0.00),\n",
       "  (143, 0.00),\n",
       "  (145, 0.00),\n",
       "  (147, 0.00),\n",
       "  (148, 0.00),\n",
       "  (150, 0.00),\n",
       "  (152, 0.00),\n",
       "  (154, 0.00),\n",
       "  (156, 0.00),\n",
       "  (157, 0.00),\n",
       "  (160, 0.00),\n",
       "  (162, 0.00),\n",
       "  (165, 0.00),\n",
       "  (166, 0.00),\n",
       "  (167, 0.00),\n",
       "  (169, 0.00),\n",
       "  (170, 0.00),\n",
       "  (172, 0.00),\n",
       "  (174, 0.00),\n",
       "  (175, 0.00),\n",
       "  (177, 0.00),\n",
       "  (180, 0.00),\n",
       "  (181, 0.00),\n",
       "  (182, 0.00),\n",
       "  (184, 0.00),\n",
       "  (185, 0.00),\n",
       "  (187, 0.00),\n",
       "  (190, 0.00),\n",
       "  (194, 0.00),\n",
       "  (199, 0.00),\n",
       "  (200, 0.00),\n",
       "  (201, 0.00),\n",
       "  (203, 0.00),\n",
       "  (204, 0.00),\n",
       "  (207, 0.00),\n",
       "  (208, 0.00),\n",
       "  (209, 0.00),\n",
       "  (210, 0.00),\n",
       "  (211, 0.00),\n",
       "  (212, 0.00),\n",
       "  (213, 0.00),\n",
       "  (215, 0.00),\n",
       "  (216, 0.00),\n",
       "  (217, 0.00),\n",
       "  (220, 0.00),\n",
       "  (221, 0.00),\n",
       "  (223, 0.00),\n",
       "  (225, 0.00),\n",
       "  (226, 0.00),\n",
       "  (227, 0.00),\n",
       "  (233, 0.00),\n",
       "  (234, 0.00),\n",
       "  (239, 0.00),\n",
       "  (240, 0.00),\n",
       "  (241, 0.00),\n",
       "  (243, 0.00),\n",
       "  (244, 0.00),\n",
       "  (245, 0.00),\n",
       "  (246, 0.00),\n",
       "  (248, 0.00),\n",
       "  (250, 0.00),\n",
       "  (251, 0.00),\n",
       "  (255, 0.00),\n",
       "  (257, 0.00),\n",
       "  (258, 0.00),\n",
       "  (259, 0.00),\n",
       "  (261, 0.00),\n",
       "  (262, 0.00),\n",
       "  (263, 0.00),\n",
       "  (264, 0.00),\n",
       "  (265, 0.00),\n",
       "  (266, 0.00),\n",
       "  (267, 0.00),\n",
       "  (269, 0.00),\n",
       "  (270, 0.00),\n",
       "  (271, 0.00),\n",
       "  (272, 0.00),\n",
       "  (277, 0.00),\n",
       "  (279, 0.00),\n",
       "  (280, 0.00),\n",
       "  (285, 0.00),\n",
       "  (286, 0.00),\n",
       "  (287, 0.00),\n",
       "  (288, 0.00),\n",
       "  (296, 0.00),\n",
       "  (297, 0.00),\n",
       "  (299, 0.00),\n",
       "  (302, 0.00),\n",
       "  (312, 0.00),\n",
       "  (322, 0.00),\n",
       "  (324, 0.00),\n",
       "  (325, 0.00),\n",
       "  (328, 0.00),\n",
       "  (329, 0.00),\n",
       "  (333, 0.00),\n",
       "  (335, 0.00),\n",
       "  (338, 0.00),\n",
       "  (339, 0.00),\n",
       "  (345, 0.00),\n",
       "  (349, 0.00),\n",
       "  (354, 0.00),\n",
       "  (357, 0.00),\n",
       "  (359, 0.00),\n",
       "  (362, 0.00),\n",
       "  (367, 0.00),\n",
       "  (368, 0.00),\n",
       "  (370, 0.00),\n",
       "  (371, 0.00),\n",
       "  (372, 0.00),\n",
       "  (373, 0.00),\n",
       "  (374, 0.00),\n",
       "  (378, 0.00),\n",
       "  (380, 0.00),\n",
       "  (382, 0.00),\n",
       "  (384, 0.00),\n",
       "  (385, 0.00),\n",
       "  (386, 0.00),\n",
       "  (390, 0.00),\n",
       "  (394, 0.00),\n",
       "  (400, 0.00),\n",
       "  (402, 0.00),\n",
       "  (403, 0.00),\n",
       "  (404, 0.00),\n",
       "  (405, 0.00),\n",
       "  (415, 0.00),\n",
       "  (416, 0.00),\n",
       "  (418, 0.00),\n",
       "  (421, 0.00),\n",
       "  (422, 0.00),\n",
       "  (426, 0.00),\n",
       "  (430, 0.00),\n",
       "  (434, 0.00),\n",
       "  (437, 0.00),\n",
       "  (446, 0.00),\n",
       "  (449, 0.00),\n",
       "  (456, 0.00),\n",
       "  (458, 0.00),\n",
       "  (460, 0.00),\n",
       "  (461, 0.00),\n",
       "  (462, 0.00),\n",
       "  (465, 0.00),\n",
       "  (466, 0.00),\n",
       "  (467, 0.00),\n",
       "  (469, 0.00),\n",
       "  (470, 0.00),\n",
       "  (471, 0.00),\n",
       "  (478, 0.00),\n",
       "  (479, 0.00),\n",
       "  (494, 0.00),\n",
       "  (499, 0.00),\n",
       "  (500, 0.00),\n",
       "  (501, 0.00),\n",
       "  (503, 0.00),\n",
       "  (510, 0.00),\n",
       "  (511, 0.00),\n",
       "  (513, 0.00),\n",
       "  (519, 0.00),\n",
       "  (524, 0.00),\n",
       "  (525, 0.00),\n",
       "  (529, 0.00),\n",
       "  (531, 0.00),\n",
       "  (536, 0.00),\n",
       "  (541, 0.00),\n",
       "  (542, 0.00),\n",
       "  (543, 0.00),\n",
       "  (548, 0.00),\n",
       "  (553, 0.00),\n",
       "  (558, 0.00),\n",
       "  (559, 0.00),\n",
       "  (561, 0.00),\n",
       "  (567, 0.00),\n",
       "  (568, 0.00),\n",
       "  (571, 0.00),\n",
       "  (573, 0.00),\n",
       "  (578, 0.00),\n",
       "  (583, 0.00),\n",
       "  (587, 0.00),\n",
       "  (589, 0.00),\n",
       "  (590, 0.00),\n",
       "  (592, 0.00),\n",
       "  (594, 0.00),\n",
       "  (596, 0.00),\n",
       "  (597, 0.00),\n",
       "  (598, 0.00),\n",
       "  (600, 0.00),\n",
       "  (601, 0.00),\n",
       "  (608, 0.00),\n",
       "  (610, 0.00),\n",
       "  (617, 0.00),\n",
       "  (622, 0.00),\n",
       "  (623, 0.00),\n",
       "  (624, 0.00),\n",
       "  (627, 0.00),\n",
       "  (628, 0.00),\n",
       "  (630, 0.00),\n",
       "  (631, 0.00),\n",
       "  (634, 0.00),\n",
       "  (642, 0.00),\n",
       "  (647, 0.00),\n",
       "  (648, 0.00),\n",
       "  (649, 0.00),\n",
       "  (652, 0.00),\n",
       "  (653, 0.00),\n",
       "  (657, 0.00),\n",
       "  (658, 0.00),\n",
       "  (659, 0.00),\n",
       "  (660, 0.00),\n",
       "  (662, 0.00),\n",
       "  (663, 0.00),\n",
       "  (666, 0.00),\n",
       "  (667, 0.00),\n",
       "  (670, 0.00),\n",
       "  (675, 0.00),\n",
       "  (676, 0.00),\n",
       "  (677, 0.00),\n",
       "  (678, 0.00),\n",
       "  (682, 0.00),\n",
       "  (685, 0.00),\n",
       "  (686, 0.00),\n",
       "  (687, 0.00),\n",
       "  (688, 0.00),\n",
       "  (689, 0.00),\n",
       "  (690, 0.00),\n",
       "  (693, 0.00),\n",
       "  (695, 0.00),\n",
       "  (699, 0.00),\n",
       "  (702, 0.00),\n",
       "  (704, 0.00),\n",
       "  (708, 0.00),\n",
       "  (710, 0.00),\n",
       "  (713, 0.00),\n",
       "  (714, 0.00),\n",
       "  (715, 0.00),\n",
       "  (718, 0.00),\n",
       "  (724, 0.00),\n",
       "  (726, 0.00),\n",
       "  (731, 0.00),\n",
       "  (733, 0.00),\n",
       "  (737, 0.00),\n",
       "  (739, 0.00),\n",
       "  (740, 0.00),\n",
       "  (742, 0.00),\n",
       "  (744, 0.00),\n",
       "  (749, 0.00),\n",
       "  (766, 0.00),\n",
       "  (767, 0.00),\n",
       "  (769, 0.00),\n",
       "  (771, 0.00),\n",
       "  (773, 0.00),\n",
       "  (774, 0.00),\n",
       "  (776, 0.00),\n",
       "  (780, 0.00),\n",
       "  (782, 0.00),\n",
       "  (784, 0.00),\n",
       "  (789, 0.00),\n",
       "  (793, 0.00),\n",
       "  (795, 0.00),\n",
       "  (798, 0.00),\n",
       "  (799, 0.00),\n",
       "  (807, 0.00),\n",
       "  (810, 0.00),\n",
       "  (812, 0.00),\n",
       "  (827, 0.00),\n",
       "  (835, 0.00),\n",
       "  (838, 0.00),\n",
       "  (841, 0.00),\n",
       "  (845, 0.00),\n",
       "  (848, 0.00),\n",
       "  (849, 0.00),\n",
       "  (851, 0.00),\n",
       "  (859, 0.00),\n",
       "  (860, 0.00),\n",
       "  (861, 0.00),\n",
       "  (862, 0.00),\n",
       "  (869, 0.00),\n",
       "  (874, 0.00),\n",
       "  (877, 0.00),\n",
       "  (886, 0.00),\n",
       "  (887, 0.00),\n",
       "  (888, 0.00),\n",
       "  (891, 0.00),\n",
       "  (894, 0.00),\n",
       "  (895, 0.00),\n",
       "  (903, 0.00),\n",
       "  (908, 0.00),\n",
       "  (909, 0.00),\n",
       "  (910, 0.00),\n",
       "  (911, 0.00),\n",
       "  (912, 0.00),\n",
       "  (913, 0.00),\n",
       "  (914, 0.00),\n",
       "  (916, 0.00),\n",
       "  (919, 0.00),\n",
       "  (921, 0.00),\n",
       "  (922, 0.00),\n",
       "  (924, 0.00),\n",
       "  (928, 0.00),\n",
       "  (929, 0.00),\n",
       "  (930, 0.00),\n",
       "  (931, 0.00),\n",
       "  (933, 0.00),\n",
       "  (935, 0.00),\n",
       "  (936, 0.00),\n",
       "  (940, 0.00),\n",
       "  (941, 0.00),\n",
       "  (942, 0.00),\n",
       "  (943, 0.00),\n",
       "  (945, 0.00),\n",
       "  (947, 0.00),\n",
       "  (948, 0.00),\n",
       "  (949, 0.00),\n",
       "  (956, 0.00),\n",
       "  (958, 0.00),\n",
       "  (959, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (964, 0.00),\n",
       "  (965, 0.00),\n",
       "  (967, 0.00),\n",
       "  (969, 0.00),\n",
       "  (970, 0.00),\n",
       "  (972, 0.00),\n",
       "  (974, 0.00),\n",
       "  (975, 0.00),\n",
       "  (976, 0.00),\n",
       "  (977, 0.00),\n",
       "  (979, 0.00),\n",
       "  (980, 0.00),\n",
       "  (986, 0.00),\n",
       "  (989, 0.00),\n",
       "  (990, 0.00),\n",
       "  (993, 0.00),\n",
       "  (994, 0.00)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5f9012c6a0>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_hist = sorted(hist, key=lambda x: x[0], reverse = False)\n",
    "values = [elem[1] for elem in sorted_hist]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.8413)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 5\n",
      "at batch no 10\n",
      "at batch no 15\n",
      "at batch no 20\n",
      "at batch no 25\n",
      "at batch no 30\n",
      "at batch no 35\n",
      "at batch no 40\n",
      "at batch no 45\n",
      "at batch no 50\n",
      "at batch no 55\n",
      "at batch no 60\n",
      "at batch no 65\n",
      "at batch no 70\n",
      "at batch no 75\n",
      "at batch no 80\n",
      "at batch no 85\n",
      "at batch no 90\n",
      "at batch no 95\n",
      "at batch no 100\n",
      "at batch no 105\n",
      "at batch no 110\n",
      "at batch no 115\n",
      "at batch no 120\n",
      "at batch no 125\n",
      "at batch no 130\n",
      "at batch no 135\n",
      "at batch no 140\n",
      "at batch no 145\n",
      "at batch no 150\n",
      "at batch no 155\n",
      "at batch no 160\n",
      "at batch no 165\n",
      "at batch no 170\n",
      "at batch no 175\n",
      "at batch no 180\n",
      "at batch no 185\n",
      "at batch no 190\n",
      "at batch no 195\n",
      "at batch no 200\n",
      "at batch no 205\n",
      "at batch no 210\n",
      "at batch no 215\n",
      "at batch no 220\n",
      "at batch no 225\n",
      "at batch no 230\n",
      "at batch no 235\n",
      "at batch no 240\n",
      "at batch no 245\n",
      "at batch no 250\n",
      "at batch no 255\n",
      "at batch no 260\n",
      "at batch no 265\n",
      "at batch no 270\n",
      "at batch no 275\n",
      "at batch no 280\n",
      "at batch no 285\n",
      "at batch no 290\n",
      "at batch no 295\n",
      "at batch no 300\n",
      "at batch no 305\n",
      "at batch no 310\n",
      "at batch no 315\n",
      "at batch no 320\n",
      "at batch no 325\n",
      "at batch no 330\n",
      "at batch no 335\n",
      "at batch no 340\n",
      "at batch no 345\n",
      "at batch no 350\n",
      "at batch no 355\n",
      "at batch no 360\n",
      "at batch no 365\n",
      "at batch no 370\n",
      "at batch no 375\n",
      "at batch no 380\n",
      "at batch no 385\n",
      "at batch no 390\n",
      "at batch no 395\n",
      "at batch no 400\n",
      "at batch no 405\n",
      "at batch no 410\n",
      "at batch no 415\n",
      "at batch no 420\n",
      "at batch no 425\n",
      "at batch no 430\n",
      "at batch no 435\n",
      "at batch no 440\n",
      "at batch no 445\n",
      "at batch no 450\n",
      "at batch no 455\n",
      "at batch no 460\n",
      "at batch no 465\n",
      "at batch no 470\n",
      "at batch no 475\n",
      "at batch no 480\n",
      "at batch no 485\n",
      "at batch no 490\n",
      "at batch no 495\n",
      "at batch no 500\n",
      "at batch no 505\n",
      "at batch no 510\n",
      "at batch no 515\n",
      "at batch no 520\n",
      "at batch no 525\n",
      "at batch no 530\n",
      "at batch no 535\n",
      "at batch no 540\n",
      "at batch no 545\n",
      "at batch no 550\n",
      "at batch no 555\n",
      "at batch no 560\n",
      "at batch no 565\n",
      "at batch no 570\n",
      "at batch no 575\n",
      "at batch no 580\n",
      "at batch no 585\n",
      "at batch no 590\n",
      "at batch no 595\n",
      "at batch no 600\n",
      "at batch no 605\n",
      "at batch no 610\n",
      "at batch no 615\n",
      "at batch no 620\n",
      "at batch no 625\n",
      "at batch no 630\n",
      "at batch no 635\n",
      "at batch no 640\n",
      "at batch no 645\n",
      "at batch no 650\n",
      "at batch no 655\n",
      "at batch no 660\n",
      "at batch no 665\n",
      "at batch no 670\n",
      "at batch no 675\n",
      "at batch no 680\n",
      "at batch no 685\n",
      "at batch no 690\n",
      "at batch no 695\n",
      "at batch no 700\n",
      "at batch no 705\n",
      "at batch no 710\n",
      "at batch no 715\n",
      "at batch no 720\n",
      "at batch no 725\n",
      "at batch no 730\n",
      "at batch no 735\n",
      "at batch no 740\n",
      "at batch no 745\n",
      "at batch no 750\n",
      "at batch no 755\n",
      "at batch no 760\n",
      "at batch no 765\n",
      "at batch no 770\n",
      "at batch no 775\n",
      "at batch no 780\n",
      "at batch no 785\n",
      "at batch no 790\n",
      "at batch no 795\n",
      "at batch no 800\n",
      "at batch no 805\n",
      "at batch no 810\n",
      "at batch no 815\n",
      "at batch no 820\n",
      "at batch no 825\n",
      "at batch no 830\n",
      "at batch no 835\n",
      "at batch no 840\n",
      "at batch no 845\n",
      "at batch no 850\n",
      "at batch no 855\n",
      "at batch no 860\n",
      "at batch no 865\n",
      "at batch no 870\n",
      "at batch no 875\n",
      "at batch no 880\n",
      "at batch no 885\n",
      "at batch no 890\n",
      "at batch no 895\n",
      "at batch no 900\n",
      "at batch no 905\n",
      "at batch no 910\n",
      "at batch no 915\n",
      "at batch no 920\n",
      "at batch no 925\n",
      "at batch no 930\n",
      "at batch no 935\n",
      "at batch no 940\n",
      "at batch no 945\n",
      "at batch no 950\n",
      "at batch no 955\n",
      "at batch no 960\n",
      "at batch no 965\n",
      "at batch no 970\n",
      "at batch no 975\n",
      "at batch no 980\n",
      "at batch no 985\n",
      "at batch no 990\n",
      "at batch no 995\n",
      "at batch no 1000\n",
      "at batch no 1005\n",
      "at batch no 1010\n",
      "at batch no 1015\n",
      "at batch no 1020\n",
      "at batch no 1025\n",
      "at batch no 1030\n",
      "at batch no 1035\n",
      "at batch no 1040\n",
      "at batch no 1045\n",
      "at batch no 1050\n",
      "at batch no 1055\n",
      "at batch no 1060\n",
      "at batch no 1065\n",
      "at batch no 1070\n",
      "at batch no 1075\n",
      "at batch no 1080\n",
      "at batch no 1085\n",
      "at batch no 1090\n",
      "at batch no 1095\n",
      "at batch no 1100\n",
      "at batch no 1105\n",
      "at batch no 1110\n",
      "at batch no 1115\n",
      "at batch no 1120\n",
      "at batch no 1125\n",
      "at batch no 1130\n",
      "at batch no 1135\n",
      "at batch no 1140\n",
      "at batch no 1145\n",
      "at batch no 1150\n",
      "at batch no 1155\n",
      "at batch no 1160\n",
      "at batch no 1165\n",
      "at batch no 1170\n",
      "at batch no 1175\n",
      "at batch no 1180\n",
      "at batch no 1185\n",
      "at batch no 1190\n",
      "at batch no 1195\n",
      "at batch no 1200\n",
      "at batch no 1205\n",
      "at batch no 1210\n",
      "at batch no 1215\n",
      "at batch no 1220\n",
      "at batch no 1225\n",
      "at batch no 1230\n",
      "at batch no 1235\n",
      "at batch no 1240\n",
      "at batch no 1245\n",
      "at batch no 1250\n",
      "at batch no 1255\n",
      "at batch no 1260\n",
      "at batch no 1265\n",
      "at batch no 1270\n",
      "at batch no 1275\n",
      "at batch no 1280\n",
      "at batch no 1285\n",
      "at batch no 1290\n",
      "at batch no 1295\n",
      "at batch no 1300\n",
      "at batch no 1305\n",
      "at batch no 1310\n",
      "at batch no 1315\n",
      "at batch no 1320\n",
      "at batch no 1325\n",
      "at batch no 1330\n",
      "at batch no 1335\n",
      "at batch no 1340\n",
      "at batch no 1345\n",
      "at batch no 1350\n",
      "at batch no 1355\n",
      "at batch no 1360\n",
      "at batch no 1365\n",
      "at batch no 1370\n",
      "at batch no 1375\n",
      "at batch no 1380\n",
      "at batch no 1385\n",
      "at batch no 1390\n",
      "at batch no 1395\n",
      "at batch no 1400\n",
      "at batch no 1405\n",
      "at batch no 1410\n",
      "at batch no 1415\n",
      "at batch no 1420\n",
      "at batch no 1425\n",
      "at batch no 1430\n",
      "at batch no 1435\n",
      "at batch no 1440\n",
      "at batch no 1445\n",
      "at batch no 1450\n",
      "at batch no 1455\n",
      "at batch no 1460\n",
      "at batch no 1465\n",
      "at batch no 1470\n",
      "at batch no 1475\n",
      "at batch no 1480\n",
      "at batch no 1485\n",
      "at batch no 1490\n",
      "at batch no 1495\n",
      "at batch no 1500\n",
      "at batch no 1505\n",
      "at batch no 1510\n",
      "at batch no 1515\n",
      "at batch no 1520\n",
      "at batch no 1525\n",
      "at batch no 1530\n",
      "at batch no 1535\n",
      "at batch no 1540\n",
      "at batch no 1545\n",
      "at batch no 1550\n",
      "at batch no 1555\n",
      "at batch no 1560\n",
      "at batch no 1565\n",
      "at batch no 1570\n",
      "at batch no 1575\n",
      "at batch no 1580\n",
      "at batch no 1585\n",
      "at batch no 1590\n",
      "at batch no 1595\n",
      "at batch no 1600\n",
      "at batch no 1605\n",
      "at batch no 1610\n",
      "at batch no 1615\n",
      "at batch no 1620\n",
      "at batch no 1625\n",
      "at batch no 1630\n",
      "at batch no 1635\n",
      "at batch no 1640\n",
      "at batch no 1645\n",
      "at batch no 1650\n",
      "at batch no 1655\n",
      "at batch no 1660\n",
      "at batch no 1665\n",
      "at batch no 1670\n",
      "at batch no 1675\n",
      "at batch no 1680\n",
      "at batch no 1685\n",
      "at batch no 1690\n",
      "at batch no 1695\n",
      "at batch no 1700\n",
      "at batch no 1705\n",
      "at batch no 1710\n",
      "at batch no 1715\n",
      "at batch no 1720\n",
      "at batch no 1725\n",
      "at batch no 1730\n",
      "at batch no 1735\n",
      "at batch no 1740\n",
      "at batch no 1745\n",
      "at batch no 1750\n",
      "at batch no 1755\n",
      "at batch no 1760\n",
      "at batch no 1765\n",
      "at batch no 1770\n",
      "at batch no 1775\n",
      "at batch no 1780\n",
      "at batch no 1785\n",
      "at batch no 1790\n",
      "at batch no 1795\n",
      "at batch no 1800\n",
      "at batch no 1805\n",
      "at batch no 1810\n",
      "at batch no 1815\n",
      "at batch no 1820\n",
      "at batch no 1825\n",
      "at batch no 1830\n",
      "at batch no 1835\n",
      "at batch no 1840\n",
      "at batch no 1845\n",
      "at batch no 1850\n",
      "at batch no 1855\n",
      "at batch no 1860\n",
      "at batch no 1865\n",
      "at batch no 1870\n",
      "at batch no 1875\n",
      "at batch no 1880\n",
      "at batch no 1885\n",
      "at batch no 1890\n",
      "at batch no 1895\n",
      "at batch no 1900\n",
      "at batch no 1905\n",
      "at batch no 1910\n",
      "at batch no 1915\n",
      "at batch no 1920\n",
      "at batch no 1925\n",
      "at batch no 1930\n",
      "at batch no 1935\n",
      "at batch no 1940\n",
      "at batch no 1945\n",
      "at batch no 1950\n",
      "at batch no 1955\n",
      "at batch no 1960\n",
      "at batch no 1965\n",
      "at batch no 1970\n",
      "at batch no 1975\n",
      "at batch no 1980\n",
      "at batch no 1985\n",
      "at batch no 1990\n",
      "at batch no 1995\n",
      "at batch no 2000\n",
      "at batch no 2005\n",
      "at batch no 2010\n",
      "at batch no 2015\n",
      "at batch no 2020\n",
      "at batch no 2025\n",
      "at batch no 2030\n",
      "at batch no 2035\n",
      "at batch no 2040\n",
      "at batch no 2045\n",
      "at batch no 2050\n",
      "at batch no 2055\n",
      "at batch no 2060\n",
      "at batch no 2065\n",
      "at batch no 2070\n",
      "at batch no 2075\n",
      "at batch no 2080\n",
      "at batch no 2085\n",
      "at batch no 2090\n",
      "at batch no 2095\n",
      "at batch no 2100\n",
      "at batch no 2105\n",
      "at batch no 2110\n",
      "at batch no 2115\n",
      "at batch no 2120\n",
      "at batch no 2125\n",
      "at batch no 2130\n",
      "at batch no 2135\n",
      "at batch no 2140\n",
      "at batch no 2145\n",
      "at batch no 2150\n",
      "at batch no 2155\n",
      "at batch no 2160\n",
      "at batch no 2165\n",
      "at batch no 2170\n",
      "at batch no 2175\n",
      "at batch no 2180\n",
      "at batch no 2185\n",
      "at batch no 2190\n",
      "at batch no 2195\n",
      "at batch no 2200\n",
      "at batch no 2205\n",
      "at batch no 2210\n",
      "at batch no 2215\n",
      "at batch no 2220\n",
      "at batch no 2225\n",
      "at batch no 2230\n",
      "at batch no 2235\n",
      "at batch no 2240\n",
      "at batch no 2245\n",
      "at batch no 2250\n",
      "at batch no 2255\n",
      "at batch no 2260\n",
      "at batch no 2265\n",
      "at batch no 2270\n",
      "at batch no 2275\n",
      "at batch no 2280\n",
      "at batch no 2285\n",
      "at batch no 2290\n",
      "at batch no 2295\n",
      "at batch no 2300\n",
      "at batch no 2305\n",
      "at batch no 2310\n",
      "at batch no 2315\n",
      "at batch no 2320\n",
      "at batch no 2325\n",
      "at batch no 2330\n",
      "at batch no 2335\n",
      "at batch no 2340\n",
      "at batch no 2345\n",
      "at batch no 2350\n",
      "at batch no 2355\n",
      "at batch no 2360\n",
      "at batch no 2365\n",
      "at batch no 2370\n",
      "at batch no 2375\n",
      "at batch no 2380\n",
      "at batch no 2385\n",
      "at batch no 2390\n",
      "at batch no 2395\n",
      "at batch no 2400\n",
      "at batch no 2405\n",
      "at batch no 2410\n",
      "at batch no 2415\n",
      "at batch no 2420\n",
      "at batch no 2425\n",
      "at batch no 2430\n",
      "at batch no 2435\n",
      "at batch no 2440\n",
      "at batch no 2445\n",
      "at batch no 2450\n",
      "at batch no 2455\n",
      "at batch no 2460\n",
      "at batch no 2465\n",
      "at batch no 2470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 2475\n",
      "at batch no 2480\n",
      "at batch no 2485\n",
      "at batch no 2490\n",
      "at batch no 2495\n",
      "at batch no 2500\n",
      "at batch no 2505\n",
      "at batch no 2510\n",
      "at batch no 2515\n",
      "at batch no 2520\n",
      "at batch no 2525\n",
      "at batch no 2530\n",
      "at batch no 2535\n",
      "at batch no 2540\n",
      "at batch no 2545\n",
      "at batch no 2550\n",
      "at batch no 2555\n",
      "at batch no 2560\n",
      "at batch no 2565\n",
      "at batch no 2570\n",
      "at batch no 2575\n",
      "at batch no 2580\n",
      "at batch no 2585\n",
      "at batch no 2590\n",
      "at batch no 2595\n",
      "at batch no 2600\n",
      "at batch no 2605\n",
      "at batch no 2610\n",
      "at batch no 2615\n",
      "at batch no 2620\n",
      "at batch no 2625\n",
      "at batch no 2630\n",
      "at batch no 2635\n",
      "at batch no 2640\n",
      "at batch no 2645\n",
      "at batch no 2650\n",
      "at batch no 2655\n",
      "at batch no 2660\n",
      "at batch no 2665\n",
      "at batch no 2670\n",
      "at batch no 2675\n",
      "at batch no 2680\n",
      "at batch no 2685\n",
      "at batch no 2690\n",
      "at batch no 2695\n",
      "at batch no 2700\n",
      "at batch no 2705\n",
      "at batch no 2710\n",
      "at batch no 2715\n",
      "at batch no 2720\n",
      "at batch no 2725\n",
      "at batch no 2730\n",
      "at batch no 2735\n"
     ]
    }
   ],
   "source": [
    "#on test\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 5\n",
      "at batch no 10\n",
      "at batch no 15\n",
      "at batch no 20\n",
      "at batch no 25\n",
      "at batch no 30\n",
      "at batch no 35\n",
      "at batch no 40\n",
      "at batch no 45\n",
      "at batch no 50\n",
      "at batch no 55\n",
      "at batch no 60\n",
      "at batch no 65\n",
      "at batch no 70\n",
      "at batch no 75\n",
      "at batch no 80\n",
      "at batch no 85\n",
      "at batch no 90\n",
      "at batch no 95\n",
      "at batch no 100\n",
      "at batch no 105\n",
      "at batch no 110\n",
      "at batch no 115\n",
      "at batch no 120\n",
      "at batch no 125\n",
      "at batch no 130\n",
      "at batch no 135\n",
      "at batch no 140\n",
      "at batch no 145\n",
      "at batch no 150\n",
      "at batch no 155\n",
      "at batch no 160\n",
      "at batch no 165\n",
      "at batch no 170\n",
      "at batch no 175\n",
      "at batch no 180\n",
      "at batch no 185\n",
      "at batch no 190\n",
      "at batch no 195\n",
      "at batch no 200\n",
      "at batch no 205\n",
      "at batch no 210\n",
      "at batch no 215\n",
      "at batch no 220\n",
      "at batch no 225\n",
      "at batch no 230\n",
      "at batch no 235\n",
      "at batch no 240\n",
      "at batch no 245\n",
      "at batch no 250\n",
      "at batch no 255\n",
      "at batch no 260\n",
      "at batch no 265\n",
      "at batch no 270\n",
      "at batch no 275\n",
      "at batch no 280\n",
      "at batch no 285\n",
      "at batch no 290\n",
      "at batch no 295\n",
      "at batch no 300\n",
      "at batch no 305\n",
      "at batch no 310\n",
      "at batch no 315\n",
      "at batch no 320\n",
      "at batch no 325\n",
      "at batch no 330\n",
      "at batch no 335\n",
      "at batch no 340\n",
      "at batch no 345\n",
      "at batch no 350\n",
      "at batch no 355\n",
      "at batch no 360\n",
      "at batch no 365\n",
      "at batch no 370\n",
      "at batch no 375\n",
      "at batch no 380\n",
      "at batch no 385\n",
      "at batch no 390\n",
      "at batch no 395\n",
      "at batch no 400\n",
      "at batch no 405\n",
      "at batch no 410\n",
      "at batch no 415\n",
      "at batch no 420\n",
      "at batch no 425\n",
      "at batch no 430\n",
      "at batch no 435\n",
      "at batch no 440\n",
      "at batch no 445\n",
      "at batch no 450\n",
      "at batch no 455\n",
      "at batch no 460\n",
      "at batch no 465\n",
      "at batch no 470\n",
      "at batch no 475\n",
      "at batch no 480\n",
      "at batch no 485\n",
      "at batch no 490\n",
      "at batch no 495\n",
      "at batch no 500\n",
      "at batch no 505\n",
      "at batch no 510\n",
      "at batch no 515\n",
      "at batch no 520\n",
      "at batch no 525\n",
      "at batch no 530\n",
      "at batch no 535\n",
      "at batch no 540\n",
      "at batch no 545\n",
      "at batch no 550\n",
      "at batch no 555\n",
      "at batch no 560\n",
      "at batch no 565\n",
      "at batch no 570\n",
      "at batch no 575\n",
      "at batch no 580\n",
      "at batch no 585\n",
      "at batch no 590\n",
      "at batch no 595\n",
      "at batch no 600\n",
      "at batch no 605\n",
      "at batch no 610\n",
      "at batch no 615\n",
      "at batch no 620\n",
      "at batch no 625\n",
      "at batch no 630\n",
      "at batch no 635\n",
      "at batch no 640\n",
      "at batch no 645\n",
      "at batch no 650\n",
      "at batch no 655\n",
      "at batch no 660\n",
      "at batch no 665\n",
      "at batch no 670\n",
      "at batch no 675\n",
      "at batch no 680\n",
      "at batch no 685\n",
      "at batch no 690\n",
      "at batch no 695\n",
      "at batch no 700\n",
      "at batch no 705\n",
      "at batch no 710\n",
      "at batch no 715\n",
      "at batch no 720\n",
      "at batch no 725\n",
      "at batch no 730\n",
      "at batch no 735\n",
      "at batch no 740\n",
      "at batch no 745\n",
      "at batch no 750\n",
      "at batch no 755\n",
      "at batch no 760\n",
      "at batch no 765\n",
      "at batch no 770\n",
      "at batch no 775\n",
      "at batch no 780\n",
      "at batch no 785\n",
      "at batch no 790\n",
      "at batch no 795\n",
      "at batch no 800\n",
      "at batch no 805\n",
      "at batch no 810\n",
      "at batch no 815\n",
      "at batch no 820\n",
      "at batch no 825\n",
      "at batch no 830\n",
      "at batch no 835\n",
      "at batch no 840\n",
      "at batch no 845\n",
      "at batch no 850\n",
      "at batch no 855\n",
      "at batch no 860\n",
      "at batch no 865\n",
      "at batch no 870\n",
      "at batch no 875\n",
      "at batch no 880\n",
      "at batch no 885\n",
      "at batch no 890\n",
      "at batch no 895\n",
      "at batch no 900\n",
      "at batch no 905\n",
      "at batch no 910\n",
      "at batch no 915\n",
      "at batch no 920\n",
      "at batch no 925\n",
      "at batch no 930\n",
      "at batch no 935\n",
      "at batch no 940\n",
      "at batch no 945\n",
      "at batch no 950\n",
      "at batch no 955\n",
      "at batch no 960\n",
      "at batch no 965\n",
      "at batch no 970\n",
      "at batch no 975\n",
      "at batch no 980\n",
      "at batch no 985\n",
      "at batch no 990\n",
      "at batch no 995\n",
      "at batch no 1000\n",
      "at batch no 1005\n",
      "at batch no 1010\n",
      "at batch no 1015\n",
      "at batch no 1020\n",
      "at batch no 1025\n",
      "at batch no 1030\n",
      "at batch no 1035\n",
      "at batch no 1040\n",
      "at batch no 1045\n",
      "at batch no 1050\n",
      "at batch no 1055\n",
      "at batch no 1060\n",
      "at batch no 1065\n",
      "at batch no 1070\n",
      "at batch no 1075\n",
      "at batch no 1080\n",
      "at batch no 1085\n",
      "at batch no 1090\n",
      "at batch no 1095\n",
      "at batch no 1100\n",
      "at batch no 1105\n",
      "at batch no 1110\n",
      "at batch no 1115\n",
      "at batch no 1120\n",
      "at batch no 1125\n",
      "at batch no 1130\n",
      "at batch no 1135\n",
      "at batch no 1140\n",
      "at batch no 1145\n",
      "at batch no 1150\n",
      "at batch no 1155\n",
      "at batch no 1160\n",
      "at batch no 1165\n",
      "at batch no 1170\n",
      "at batch no 1175\n",
      "at batch no 1180\n",
      "at batch no 1185\n",
      "at batch no 1190\n",
      "at batch no 1195\n",
      "at batch no 1200\n",
      "at batch no 1205\n",
      "at batch no 1210\n",
      "at batch no 1215\n",
      "at batch no 1220\n",
      "at batch no 1225\n",
      "at batch no 1230\n",
      "at batch no 1235\n",
      "at batch no 1240\n",
      "at batch no 1245\n",
      "at batch no 1250\n",
      "at batch no 1255\n",
      "at batch no 1260\n",
      "at batch no 1265\n",
      "at batch no 1270\n",
      "at batch no 1275\n",
      "at batch no 1280\n",
      "at batch no 1285\n",
      "at batch no 1290\n",
      "at batch no 1295\n",
      "at batch no 1300\n",
      "at batch no 1305\n",
      "at batch no 1310\n",
      "at batch no 1315\n",
      "at batch no 1320\n",
      "at batch no 1325\n",
      "at batch no 1330\n",
      "at batch no 1335\n",
      "at batch no 1340\n",
      "at batch no 1345\n",
      "at batch no 1350\n",
      "at batch no 1355\n",
      "at batch no 1360\n",
      "at batch no 1365\n",
      "at batch no 1370\n",
      "at batch no 1375\n",
      "at batch no 1380\n",
      "at batch no 1385\n",
      "at batch no 1390\n",
      "at batch no 1395\n",
      "at batch no 1400\n",
      "at batch no 1405\n",
      "at batch no 1410\n",
      "at batch no 1415\n",
      "at batch no 1420\n",
      "at batch no 1425\n",
      "at batch no 1430\n",
      "at batch no 1435\n",
      "at batch no 1440\n",
      "at batch no 1445\n",
      "at batch no 1450\n",
      "at batch no 1455\n",
      "at batch no 1460\n",
      "at batch no 1465\n",
      "at batch no 1470\n",
      "at batch no 1475\n",
      "at batch no 1480\n",
      "at batch no 1485\n",
      "at batch no 1490\n",
      "at batch no 1495\n",
      "at batch no 1500\n",
      "at batch no 1505\n",
      "at batch no 1510\n",
      "at batch no 1515\n",
      "at batch no 1520\n",
      "at batch no 1525\n",
      "at batch no 1530\n",
      "at batch no 1535\n",
      "at batch no 1540\n",
      "at batch no 1545\n",
      "at batch no 1550\n",
      "at batch no 1555\n",
      "at batch no 1560\n",
      "at batch no 1565\n",
      "at batch no 1570\n",
      "at batch no 1575\n",
      "at batch no 1580\n",
      "at batch no 1585\n",
      "at batch no 1590\n",
      "at batch no 1595\n",
      "at batch no 1600\n",
      "at batch no 1605\n",
      "at batch no 1610\n",
      "at batch no 1615\n",
      "at batch no 1620\n",
      "at batch no 1625\n",
      "at batch no 1630\n",
      "at batch no 1635\n",
      "at batch no 1640\n",
      "at batch no 1645\n",
      "at batch no 1650\n",
      "at batch no 1655\n",
      "at batch no 1660\n",
      "at batch no 1665\n",
      "at batch no 1670\n",
      "at batch no 1675\n",
      "at batch no 1680\n",
      "at batch no 1685\n",
      "at batch no 1690\n",
      "at batch no 1695\n",
      "at batch no 1700\n",
      "at batch no 1705\n",
      "at batch no 1710\n",
      "at batch no 1715\n",
      "at batch no 1720\n",
      "at batch no 1725\n",
      "at batch no 1730\n",
      "at batch no 1735\n",
      "at batch no 1740\n",
      "at batch no 1745\n",
      "at batch no 1750\n",
      "at batch no 1755\n",
      "at batch no 1760\n",
      "at batch no 1765\n",
      "at batch no 1770\n",
      "at batch no 1775\n",
      "at batch no 1780\n",
      "at batch no 1785\n",
      "at batch no 1790\n",
      "at batch no 1795\n",
      "at batch no 1800\n",
      "at batch no 1805\n",
      "at batch no 1810\n",
      "at batch no 1815\n",
      "at batch no 1820\n",
      "at batch no 1825\n",
      "at batch no 1830\n",
      "at batch no 1835\n",
      "at batch no 1840\n",
      "at batch no 1845\n",
      "at batch no 1850\n",
      "at batch no 1855\n",
      "at batch no 1860\n",
      "at batch no 1865\n",
      "at batch no 1870\n",
      "at batch no 1875\n",
      "at batch no 1880\n",
      "at batch no 1885\n",
      "at batch no 1890\n",
      "at batch no 1895\n",
      "at batch no 1900\n",
      "at batch no 1905\n",
      "at batch no 1910\n",
      "at batch no 1915\n",
      "at batch no 1920\n",
      "at batch no 1925\n",
      "at batch no 1930\n",
      "at batch no 1935\n",
      "at batch no 1940\n",
      "at batch no 1945\n",
      "at batch no 1950\n",
      "at batch no 1955\n",
      "at batch no 1960\n",
      "at batch no 1965\n",
      "at batch no 1970\n",
      "at batch no 1975\n",
      "at batch no 1980\n",
      "at batch no 1985\n",
      "at batch no 1990\n",
      "at batch no 1995\n",
      "at batch no 2000\n",
      "at batch no 2005\n",
      "at batch no 2010\n",
      "at batch no 2015\n",
      "at batch no 2020\n",
      "at batch no 2025\n",
      "at batch no 2030\n",
      "at batch no 2035\n",
      "at batch no 2040\n",
      "at batch no 2045\n",
      "at batch no 2050\n",
      "at batch no 2055\n",
      "at batch no 2060\n",
      "at batch no 2065\n",
      "at batch no 2070\n",
      "at batch no 2075\n",
      "at batch no 2080\n",
      "at batch no 2085\n",
      "at batch no 2090\n",
      "at batch no 2095\n",
      "at batch no 2100\n",
      "at batch no 2105\n",
      "at batch no 2110\n",
      "at batch no 2115\n",
      "at batch no 2120\n",
      "at batch no 2125\n",
      "at batch no 2130\n",
      "at batch no 2135\n",
      "at batch no 2140\n",
      "at batch no 2145\n",
      "at batch no 2150\n",
      "at batch no 2155\n",
      "at batch no 2160\n",
      "at batch no 2165\n",
      "at batch no 2170\n",
      "at batch no 2175\n",
      "at batch no 2180\n",
      "at batch no 2185\n",
      "at batch no 2190\n",
      "at batch no 2195\n",
      "at batch no 2200\n",
      "at batch no 2205\n",
      "at batch no 2210\n",
      "at batch no 2215\n",
      "at batch no 2220\n",
      "at batch no 2225\n",
      "at batch no 2230\n",
      "at batch no 2235\n",
      "at batch no 2240\n",
      "at batch no 2245\n",
      "at batch no 2250\n",
      "at batch no 2255\n",
      "at batch no 2260\n",
      "at batch no 2265\n",
      "at batch no 2270\n",
      "at batch no 2275\n",
      "at batch no 2280\n",
      "at batch no 2285\n",
      "at batch no 2290\n",
      "at batch no 2295\n",
      "at batch no 2300\n",
      "at batch no 2305\n",
      "at batch no 2310\n",
      "at batch no 2315\n",
      "at batch no 2320\n",
      "at batch no 2325\n",
      "at batch no 2330\n",
      "at batch no 2335\n",
      "at batch no 2340\n",
      "at batch no 2345\n",
      "at batch no 2350\n",
      "at batch no 2355\n",
      "at batch no 2360\n",
      "at batch no 2365\n",
      "at batch no 2370\n",
      "at batch no 2375\n",
      "at batch no 2380\n",
      "at batch no 2385\n",
      "at batch no 2390\n",
      "at batch no 2395\n",
      "at batch no 2400\n",
      "at batch no 2405\n",
      "at batch no 2410\n",
      "at batch no 2415\n",
      "at batch no 2420\n",
      "at batch no 2425\n",
      "at batch no 2430\n",
      "at batch no 2435\n",
      "at batch no 2440\n",
      "at batch no 2445\n",
      "at batch no 2450\n",
      "at batch no 2455\n",
      "at batch no 2460\n",
      "at batch no 2465\n",
      "at batch no 2470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 2475\n",
      "at batch no 2480\n",
      "at batch no 2485\n",
      "at batch no 2490\n",
      "at batch no 2495\n",
      "at batch no 2500\n",
      "at batch no 2505\n",
      "at batch no 2510\n",
      "at batch no 2515\n",
      "at batch no 2520\n",
      "at batch no 2525\n",
      "at batch no 2530\n",
      "at batch no 2535\n",
      "at batch no 2540\n",
      "at batch no 2545\n",
      "at batch no 2550\n",
      "at batch no 2555\n",
      "at batch no 2560\n",
      "at batch no 2565\n",
      "at batch no 2570\n",
      "at batch no 2575\n",
      "at batch no 2580\n",
      "at batch no 2585\n",
      "at batch no 2590\n",
      "at batch no 2595\n",
      "at batch no 2600\n",
      "at batch no 2605\n",
      "at batch no 2610\n",
      "at batch no 2615\n",
      "at batch no 2620\n",
      "at batch no 2625\n",
      "at batch no 2630\n",
      "at batch no 2635\n",
      "at batch no 2640\n",
      "at batch no 2645\n",
      "at batch no 2650\n",
      "at batch no 2655\n",
      "at batch no 2660\n",
      "at batch no 2665\n",
      "at batch no 2670\n",
      "at batch no 2675\n",
      "at batch no 2680\n",
      "at batch no 2685\n",
      "at batch no 2690\n",
      "at batch no 2695\n",
      "at batch no 2700\n",
      "at batch no 2705\n",
      "at batch no 2710\n",
      "at batch no 2715\n",
      "at batch no 2720\n",
      "at batch no 2725\n",
      "at batch no 2730\n",
      "at batch no 2735\n",
      "at batch no 2740\n",
      "at batch no 2745\n",
      "at batch no 2750\n",
      "at batch no 2755\n",
      "at batch no 2760\n",
      "at batch no 2765\n",
      "at batch no 2770\n",
      "at batch no 2775\n",
      "at batch no 2780\n",
      "at batch no 2785\n",
      "at batch no 2790\n",
      "at batch no 2795\n",
      "at batch no 2800\n",
      "at batch no 2805\n",
      "at batch no 2810\n",
      "at batch no 2815\n",
      "at batch no 2820\n",
      "at batch no 2825\n",
      "at batch no 2830\n",
      "at batch no 2835\n",
      "at batch no 2840\n",
      "at batch no 2845\n",
      "at batch no 2850\n",
      "at batch no 2855\n",
      "at batch no 2860\n",
      "at batch no 2865\n",
      "at batch no 2870\n",
      "at batch no 2875\n",
      "at batch no 2880\n",
      "at batch no 2885\n",
      "at batch no 2890\n",
      "at batch no 2895\n",
      "at batch no 2900\n",
      "at batch no 2905\n",
      "at batch no 2910\n",
      "at batch no 2915\n",
      "at batch no 2920\n",
      "at batch no 2925\n",
      "at batch no 2930\n",
      "at batch no 2935\n",
      "at batch no 2940\n",
      "at batch no 2945\n",
      "at batch no 2950\n",
      "at batch no 2955\n",
      "at batch no 2960\n",
      "at batch no 2965\n",
      "at batch no 2970\n",
      "at batch no 2975\n",
      "at batch no 2980\n",
      "at batch no 2985\n",
      "at batch no 2990\n",
      "at batch no 2995\n",
      "at batch no 3000\n",
      "at batch no 3005\n",
      "at batch no 3010\n",
      "at batch no 3015\n",
      "at batch no 3020\n",
      "at batch no 3025\n",
      "at batch no 3030\n",
      "at batch no 3035\n",
      "at batch no 3040\n",
      "at batch no 3045\n",
      "at batch no 3050\n",
      "at batch no 3055\n",
      "at batch no 3060\n",
      "at batch no 3065\n",
      "at batch no 3070\n",
      "at batch no 3075\n",
      "at batch no 3080\n",
      "at batch no 3085\n",
      "at batch no 3090\n",
      "at batch no 3095\n",
      "at batch no 3100\n",
      "at batch no 3105\n",
      "at batch no 3110\n",
      "at batch no 3115\n",
      "at batch no 3120\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(355,\n",
       " [(971, 8352.30),\n",
       "  (721, 6808.50),\n",
       "  (669, 3768.10),\n",
       "  (556, 2503.90),\n",
       "  (588, 1898.60),\n",
       "  (520, 1807.60),\n",
       "  (414, 1464.90),\n",
       "  (431, 1427.50),\n",
       "  (828, 1418.70),\n",
       "  (750, 1379.70),\n",
       "  (61, 1116.90),\n",
       "  (904, 753.10),\n",
       "  (411, 668.20),\n",
       "  (39, 639.00),\n",
       "  (794, 601.60),\n",
       "  (599, 433.40),\n",
       "  (489, 395.90),\n",
       "  (490, 297.90),\n",
       "  (709, 295.30),\n",
       "  (604, 281.30),\n",
       "  (955, 277.30),\n",
       "  (84, 264.70),\n",
       "  (581, 237.50),\n",
       "  (790, 231.00),\n",
       "  (614, 226.40),\n",
       "  (770, 214.60),\n",
       "  (48, 173.10),\n",
       "  (837, 171.30),\n",
       "  (893, 162.60),\n",
       "  (907, 159.60),\n",
       "  (509, 133.00),\n",
       "  (572, 126.10),\n",
       "  (973, 106.00),\n",
       "  (570, 105.00),\n",
       "  (419, 98.10),\n",
       "  (824, 97.60),\n",
       "  (591, 97.20),\n",
       "  (67, 91.00),\n",
       "  (46, 90.80),\n",
       "  (651, 90.40),\n",
       "  (401, 89.00),\n",
       "  (748, 88.10),\n",
       "  (60, 81.20),\n",
       "  (741, 79.90),\n",
       "  (56, 78.00),\n",
       "  (805, 76.90),\n",
       "  (412, 76.60),\n",
       "  (580, 76.60),\n",
       "  (443, 74.40),\n",
       "  (801, 73.00),\n",
       "  (762, 72.50),\n",
       "  (151, 69.60),\n",
       "  (711, 68.40),\n",
       "  (737, 68.10),\n",
       "  (55, 66.60),\n",
       "  (516, 65.90),\n",
       "  (406, 65.40),\n",
       "  (455, 64.40),\n",
       "  (621, 62.90),\n",
       "  (582, 62.50),\n",
       "  (319, 61.60),\n",
       "  (788, 60.90),\n",
       "  (815, 60.20),\n",
       "  (646, 59.90),\n",
       "  (485, 58.80),\n",
       "  (468, 57.70),\n",
       "  (879, 56.10),\n",
       "  (987, 56.10),\n",
       "  (441, 55.80),\n",
       "  (464, 55.60),\n",
       "  (800, 54.00),\n",
       "  (791, 51.40),\n",
       "  (754, 50.60),\n",
       "  (703, 49.50),\n",
       "  (706, 47.70),\n",
       "  (97, 45.90),\n",
       "  (575, 45.90),\n",
       "  (340, 45.60),\n",
       "  (992, 44.40),\n",
       "  (116, 44.20),\n",
       "  (555, 44.10),\n",
       "  (40, 43.40),\n",
       "  (138, 40.90),\n",
       "  (0, 40.80),\n",
       "  (94, 40.60),\n",
       "  (410, 40.50),\n",
       "  (533, 40.40),\n",
       "  (698, 40.30),\n",
       "  (476, 39.80),\n",
       "  (562, 39.70),\n",
       "  (691, 39.70),\n",
       "  (300, 39.40),\n",
       "  (746, 39.10),\n",
       "  (734, 38.70),\n",
       "  (953, 38.10),\n",
       "  (407, 37.80),\n",
       "  (155, 37.70),\n",
       "  (293, 37.70),\n",
       "  (643, 37.40),\n",
       "  (878, 36.60),\n",
       "  (796, 36.40),\n",
       "  (806, 36.40),\n",
       "  (171, 35.90),\n",
       "  (109, 34.80),\n",
       "  (611, 34.80),\n",
       "  (661, 34.50),\n",
       "  (645, 34.30),\n",
       "  (779, 33.60),\n",
       "  (323, 33.50),\n",
       "  (518, 32.30),\n",
       "  (195, 31.70),\n",
       "  (62, 31.50),\n",
       "  (858, 31.50),\n",
       "  (692, 31.40),\n",
       "  (497, 31.00),\n",
       "  (868, 30.80),\n",
       "  (304, 30.60),\n",
       "  (635, 30.30),\n",
       "  (118, 29.50),\n",
       "  (916, 29.40),\n",
       "  (850, 29.20),\n",
       "  (636, 28.90),\n",
       "  (396, 28.80),\n",
       "  (547, 28.80),\n",
       "  (752, 28.60),\n",
       "  (981, 28.10),\n",
       "  (363, 28.00),\n",
       "  (619, 27.80),\n",
       "  (829, 27.40),\n",
       "  (917, 27.20),\n",
       "  (162, 27.10),\n",
       "  (654, 27.10),\n",
       "  (436, 26.80),\n",
       "  (870, 26.70),\n",
       "  (897, 26.70),\n",
       "  (496, 26.60),\n",
       "  (676, 26.40),\n",
       "  (738, 26.30),\n",
       "  (96, 26.20),\n",
       "  (108, 26.00),\n",
       "  (124, 26.00),\n",
       "  (47, 25.90),\n",
       "  (768, 25.60),\n",
       "  (506, 25.40),\n",
       "  (308, 25.30),\n",
       "  (612, 25.20),\n",
       "  (238, 25.10),\n",
       "  (90, 25.00),\n",
       "  (25, 24.90),\n",
       "  (854, 24.90),\n",
       "  (237, 24.80),\n",
       "  (819, 24.80),\n",
       "  (515, 24.70),\n",
       "  (110, 24.40),\n",
       "  (290, 24.30),\n",
       "  (440, 24.20),\n",
       "  (353, 24.10),\n",
       "  (679, 23.80),\n",
       "  (886, 23.60),\n",
       "  (898, 23.60),\n",
       "  (254, 23.30),\n",
       "  (640, 23.10),\n",
       "  (128, 23.00),\n",
       "  (655, 22.90),\n",
       "  (946, 22.80),\n",
       "  (178, 22.60),\n",
       "  (808, 22.40),\n",
       "  (459, 22.10),\n",
       "  (944, 22.10),\n",
       "  (545, 22.00),\n",
       "  (722, 21.80),\n",
       "  (735, 21.70),\n",
       "  (781, 21.70),\n",
       "  (82, 21.50),\n",
       "  (632, 21.40),\n",
       "  (641, 21.40),\n",
       "  (522, 21.10),\n",
       "  (576, 21.10),\n",
       "  (843, 21.10),\n",
       "  (444, 20.90),\n",
       "  (565, 20.90),\n",
       "  (866, 20.90),\n",
       "  (863, 20.80),\n",
       "  (159, 20.70),\n",
       "  (705, 20.70),\n",
       "  (482, 20.60),\n",
       "  (457, 20.50),\n",
       "  (327, 20.30),\n",
       "  (560, 20.30),\n",
       "  (135, 20.10),\n",
       "  (288, 20.10),\n",
       "  (892, 20.10),\n",
       "  (65, 19.80),\n",
       "  (725, 19.80),\n",
       "  (241, 19.60),\n",
       "  (253, 19.60),\n",
       "  (564, 19.60),\n",
       "  (687, 19.50),\n",
       "  (7, 19.00),\n",
       "  (532, 18.90),\n",
       "  (826, 18.90),\n",
       "  (251, 18.70),\n",
       "  (8, 18.60),\n",
       "  (88, 18.60),\n",
       "  (275, 18.60),\n",
       "  (282, 18.60),\n",
       "  (292, 18.60),\n",
       "  (526, 18.50),\n",
       "  (918, 18.50),\n",
       "  (76, 18.40),\n",
       "  (603, 18.30),\n",
       "  (63, 18.20),\n",
       "  (915, 18.20),\n",
       "  (772, 18.10),\n",
       "  (199, 18.00),\n",
       "  (133, 17.90),\n",
       "  (538, 17.90),\n",
       "  (880, 17.90),\n",
       "  (716, 17.80),\n",
       "  (817, 17.80),\n",
       "  (474, 17.60),\n",
       "  (453, 17.50),\n",
       "  (430, 17.30),\n",
       "  (31, 17.20),\n",
       "  (566, 17.20),\n",
       "  (136, 17.10),\n",
       "  (937, 17.10),\n",
       "  (561, 16.90),\n",
       "  (102, 16.80),\n",
       "  (627, 16.80),\n",
       "  (685, 16.80),\n",
       "  (857, 16.80),\n",
       "  (670, 16.70),\n",
       "  (867, 16.60),\n",
       "  (765, 16.50),\n",
       "  (173, 16.40),\n",
       "  (274, 16.40),\n",
       "  (671, 16.40),\n",
       "  (398, 16.30),\n",
       "  (123, 16.20),\n",
       "  (352, 16.10),\n",
       "  (822, 16.00),\n",
       "  (447, 15.90),\n",
       "  (874, 15.90),\n",
       "  (783, 15.80),\n",
       "  (354, 15.70),\n",
       "  (938, 15.70),\n",
       "  (328, 15.50),\n",
       "  (949, 15.50),\n",
       "  (552, 15.40),\n",
       "  (129, 15.30),\n",
       "  (231, 15.10),\n",
       "  (563, 15.10),\n",
       "  (601, 15.10),\n",
       "  (208, 15.00),\n",
       "  (757, 15.00),\n",
       "  (985, 15.00),\n",
       "  (45, 14.90),\n",
       "  (409, 14.80),\n",
       "  (847, 14.80),\n",
       "  (85, 14.60),\n",
       "  (523, 14.60),\n",
       "  (680, 14.50),\n",
       "  (852, 14.50),\n",
       "  (950, 14.50),\n",
       "  (982, 14.50),\n",
       "  (242, 14.40),\n",
       "  (594, 14.40),\n",
       "  (836, 14.40),\n",
       "  (730, 14.30),\n",
       "  (865, 14.30),\n",
       "  (37, 14.20),\n",
       "  (530, 14.20),\n",
       "  (423, 14.10),\n",
       "  (723, 14.10),\n",
       "  (966, 14.10),\n",
       "  (625, 14.00),\n",
       "  (417, 13.90),\n",
       "  (472, 13.90),\n",
       "  (539, 13.80),\n",
       "  (579, 13.80),\n",
       "  (656, 13.80),\n",
       "  (899, 13.70),\n",
       "  (263, 13.60),\n",
       "  (642, 13.50),\n",
       "  (119, 13.30),\n",
       "  (665, 13.30),\n",
       "  (872, 13.30),\n",
       "  (620, 13.20),\n",
       "  (855, 13.10),\n",
       "  (936, 13.10),\n",
       "  (30, 13.00),\n",
       "  (393, 13.00),\n",
       "  (849, 13.00),\n",
       "  (609, 12.90),\n",
       "  (130, 12.80),\n",
       "  (192, 12.70),\n",
       "  (778, 12.70),\n",
       "  (98, 12.60),\n",
       "  (273, 12.60),\n",
       "  (986, 12.60),\n",
       "  (751, 12.50),\n",
       "  (927, 12.50),\n",
       "  (968, 12.50),\n",
       "  (586, 12.40),\n",
       "  (595, 12.40),\n",
       "  (864, 12.40),\n",
       "  (330, 12.30),\n",
       "  (438, 12.30),\n",
       "  (541, 12.30),\n",
       "  (549, 12.20),\n",
       "  (956, 12.20),\n",
       "  (912, 12.00),\n",
       "  (984, 12.00),\n",
       "  (57, 11.80),\n",
       "  (77, 11.80),\n",
       "  (387, 11.70),\n",
       "  (717, 11.70),\n",
       "  (425, 11.60),\n",
       "  (616, 11.60),\n",
       "  (9, 11.50),\n",
       "  (211, 11.50),\n",
       "  (392, 11.50),\n",
       "  (439, 11.50),\n",
       "  (724, 11.50),\n",
       "  (774, 11.50),\n",
       "  (91, 11.40),\n",
       "  (144, 11.40),\n",
       "  (668, 11.40),\n",
       "  (832, 11.40),\n",
       "  (181, 11.30),\n",
       "  (760, 11.30),\n",
       "  (727, 11.20),\n",
       "  (954, 11.20),\n",
       "  (189, 11.10),\n",
       "  (885, 11.10),\n",
       "  (887, 11.10),\n",
       "  (605, 11.00),\n",
       "  (693, 11.00),\n",
       "  (201, 10.90),\n",
       "  (215, 10.90),\n",
       "  (492, 10.90),\n",
       "  (571, 10.90),\n",
       "  (311, 10.80),\n",
       "  (50, 10.70),\n",
       "  (107, 10.70),\n",
       "  (230, 10.70),\n",
       "  (424, 10.70),\n",
       "  (445, 10.70),\n",
       "  (694, 10.70),\n",
       "  (695, 10.70),\n",
       "  (28, 10.60),\n",
       "  (487, 10.60),\n",
       "  (559, 10.60),\n",
       "  (388, 10.50),\n",
       "  (639, 10.50),\n",
       "  (6, 10.40),\n",
       "  (216, 10.40),\n",
       "  (602, 10.30),\n",
       "  (811, 10.20),\n",
       "  (821, 10.20),\n",
       "  (23, 10.10),\n",
       "  (554, 10.00),\n",
       "  (820, 10.00),\n",
       "  (41, 9.90),\n",
       "  (180, 9.90),\n",
       "  (217, 9.90),\n",
       "  (121, 9.80),\n",
       "  (301, 9.80),\n",
       "  (637, 9.80),\n",
       "  (24, 9.70),\n",
       "  (316, 9.70),\n",
       "  (365, 9.70),\n",
       "  (86, 9.60),\n",
       "  (303, 9.50),\n",
       "  (306, 9.50),\n",
       "  (667, 9.50),\n",
       "  (888, 9.50),\n",
       "  (911, 9.50),\n",
       "  (27, 9.40),\n",
       "  (99, 9.40),\n",
       "  (250, 9.40),\n",
       "  (458, 9.40),\n",
       "  (527, 9.40),\n",
       "  (104, 9.30),\n",
       "  (321, 9.30),\n",
       "  (343, 9.30),\n",
       "  (624, 9.30),\n",
       "  (784, 9.30),\n",
       "  (161, 9.20),\n",
       "  (182, 9.20),\n",
       "  (225, 9.20),\n",
       "  (243, 9.20),\n",
       "  (429, 9.20),\n",
       "  (448, 9.20),\n",
       "  (573, 9.20),\n",
       "  (839, 9.20),\n",
       "  (1, 9.10),\n",
       "  (658, 9.10),\n",
       "  (766, 9.10),\n",
       "  (825, 9.10),\n",
       "  (260, 9.00),\n",
       "  (531, 9.00),\n",
       "  (470, 8.90),\n",
       "  (633, 8.90),\n",
       "  (951, 8.90),\n",
       "  (712, 8.80),\n",
       "  (382, 8.70),\n",
       "  (732, 8.70),\n",
       "  (157, 8.60),\n",
       "  (219, 8.60),\n",
       "  (278, 8.60),\n",
       "  (214, 8.50),\n",
       "  (239, 8.50),\n",
       "  (281, 8.50),\n",
       "  (361, 8.50),\n",
       "  (466, 8.50),\n",
       "  (42, 8.40),\n",
       "  (134, 8.40),\n",
       "  (249, 8.40),\n",
       "  (375, 8.40),\n",
       "  (920, 8.40),\n",
       "  (83, 8.30),\n",
       "  (234, 8.30),\n",
       "  (704, 8.30),\n",
       "  (830, 8.30),\n",
       "  (875, 8.30),\n",
       "  (298, 8.20),\n",
       "  (467, 8.20),\n",
       "  (193, 8.10),\n",
       "  (498, 8.10),\n",
       "  (763, 8.10),\n",
       "  (823, 8.10),\n",
       "  (905, 8.10),\n",
       "  (236, 8.00),\n",
       "  (334, 8.00),\n",
       "  (383, 8.00),\n",
       "  (397, 8.00),\n",
       "  (759, 8.00),\n",
       "  (100, 7.90),\n",
       "  (115, 7.90),\n",
       "  (186, 7.80),\n",
       "  (218, 7.80),\n",
       "  (248, 7.80),\n",
       "  (638, 7.80),\n",
       "  (652, 7.80),\n",
       "  (814, 7.80),\n",
       "  (963, 7.80),\n",
       "  (127, 7.70),\n",
       "  (206, 7.70),\n",
       "  (235, 7.70),\n",
       "  (359, 7.70),\n",
       "  (465, 7.70),\n",
       "  (537, 7.70),\n",
       "  (481, 7.60),\n",
       "  (141, 7.50),\n",
       "  (786, 7.50),\n",
       "  (408, 7.40),\n",
       "  (15, 7.30),\n",
       "  (198, 7.30),\n",
       "  (758, 7.30),\n",
       "  (873, 7.30),\n",
       "  (164, 7.20),\n",
       "  (684, 7.20),\n",
       "  (688, 7.20),\n",
       "  (853, 7.20),\n",
       "  (882, 7.20),\n",
       "  (71, 7.10),\n",
       "  (172, 7.10),\n",
       "  (196, 7.10),\n",
       "  (508, 7.10),\n",
       "  (221, 7.00),\n",
       "  (247, 7.00),\n",
       "  (584, 7.00),\n",
       "  (782, 7.00),\n",
       "  (11, 6.90),\n",
       "  (38, 6.90),\n",
       "  (902, 6.90),\n",
       "  (148, 6.80),\n",
       "  (337, 6.80),\n",
       "  (348, 6.80),\n",
       "  (479, 6.80),\n",
       "  (483, 6.80),\n",
       "  (495, 6.80),\n",
       "  (777, 6.80),\n",
       "  (957, 6.80),\n",
       "  (153, 6.70),\n",
       "  (271, 6.70),\n",
       "  (618, 6.70),\n",
       "  (205, 6.60),\n",
       "  (339, 6.60),\n",
       "  (922, 6.60),\n",
       "  (939, 6.60),\n",
       "  (997, 6.60),\n",
       "  (264, 6.50),\n",
       "  (310, 6.50),\n",
       "  (358, 6.50),\n",
       "  (477, 6.50),\n",
       "  (707, 6.50),\n",
       "  (156, 6.40),\n",
       "  (433, 6.40),\n",
       "  (529, 6.40),\n",
       "  (699, 6.40),\n",
       "  (72, 6.30),\n",
       "  (75, 6.30),\n",
       "  (390, 6.30),\n",
       "  (471, 6.30),\n",
       "  (881, 6.30),\n",
       "  (355, 6.20),\n",
       "  (776, 6.20),\n",
       "  (840, 6.20),\n",
       "  (932, 6.20),\n",
       "  (948, 6.20),\n",
       "  (280, 6.10),\n",
       "  (360, 6.10),\n",
       "  (775, 6.10),\n",
       "  (889, 6.10),\n",
       "  (890, 6.10),\n",
       "  (74, 6.00),\n",
       "  (93, 6.00),\n",
       "  (255, 6.00),\n",
       "  (257, 6.00),\n",
       "  (283, 6.00),\n",
       "  (79, 5.90),\n",
       "  (276, 5.90),\n",
       "  (317, 5.90),\n",
       "  (491, 5.90),\n",
       "  (626, 5.90),\n",
       "  (70, 5.80),\n",
       "  (158, 5.80),\n",
       "  (302, 5.80),\n",
       "  (313, 5.80),\n",
       "  (792, 5.80),\n",
       "  (307, 5.70),\n",
       "  (325, 5.70),\n",
       "  (454, 5.70),\n",
       "  (921, 5.70),\n",
       "  (131, 5.60),\n",
       "  (660, 5.60),\n",
       "  (696, 5.60),\n",
       "  (710, 5.60),\n",
       "  (764, 5.60),\n",
       "  (113, 5.50),\n",
       "  (125, 5.50),\n",
       "  (309, 5.50),\n",
       "  (391, 5.50),\n",
       "  (512, 5.50),\n",
       "  (659, 5.50),\n",
       "  (117, 5.40),\n",
       "  (210, 5.40),\n",
       "  (259, 5.40),\n",
       "  (267, 5.40),\n",
       "  (622, 5.40),\n",
       "  (983, 5.40),\n",
       "  (142, 5.30),\n",
       "  (209, 5.30),\n",
       "  (318, 5.20),\n",
       "  (462, 5.20),\n",
       "  (488, 5.20),\n",
       "  (528, 5.20),\n",
       "  (535, 5.20),\n",
       "  (816, 5.20),\n",
       "  (991, 5.20),\n",
       "  (92, 5.10),\n",
       "  (203, 5.10),\n",
       "  (451, 5.10),\n",
       "  (14, 5.00),\n",
       "  (44, 5.00),\n",
       "  (224, 5.00),\n",
       "  (286, 5.00),\n",
       "  (418, 5.00),\n",
       "  (449, 5.00),\n",
       "  (574, 5.00),\n",
       "  (664, 5.00),\n",
       "  (700, 5.00),\n",
       "  (701, 5.00),\n",
       "  (990, 5.00),\n",
       "  (80, 4.90),\n",
       "  (295, 4.90),\n",
       "  (514, 4.90),\n",
       "  (544, 4.90),\n",
       "  (761, 4.90),\n",
       "  (894, 4.90),\n",
       "  (18, 4.80),\n",
       "  (58, 4.80),\n",
       "  (884, 4.80),\n",
       "  (910, 4.80),\n",
       "  (919, 4.80),\n",
       "  (270, 4.70),\n",
       "  (402, 4.70),\n",
       "  (463, 4.70),\n",
       "  (519, 4.70),\n",
       "  (606, 4.70),\n",
       "  (629, 4.70),\n",
       "  (650, 4.70),\n",
       "  (733, 4.70),\n",
       "  (851, 4.70),\n",
       "  (900, 4.70),\n",
       "  (332, 4.60),\n",
       "  (592, 4.60),\n",
       "  (630, 4.60),\n",
       "  (931, 4.60),\n",
       "  (988, 4.60),\n",
       "  (998, 4.60),\n",
       "  (176, 4.50),\n",
       "  (294, 4.50),\n",
       "  (569, 4.50),\n",
       "  (756, 4.50),\n",
       "  (132, 4.40),\n",
       "  (351, 4.40),\n",
       "  (507, 4.40),\n",
       "  (577, 4.40),\n",
       "  (608, 4.40),\n",
       "  (653, 4.40),\n",
       "  (170, 4.30),\n",
       "  (165, 4.20),\n",
       "  (284, 4.20),\n",
       "  (344, 4.20),\n",
       "  (585, 4.20),\n",
       "  (672, 4.20),\n",
       "  (166, 4.10),\n",
       "  (191, 4.10),\n",
       "  (265, 4.10),\n",
       "  (320, 4.10),\n",
       "  (342, 4.10),\n",
       "  (428, 4.10),\n",
       "  (557, 4.10),\n",
       "  (597, 4.10),\n",
       "  (848, 4.10),\n",
       "  (331, 4.00),\n",
       "  (347, 4.00),\n",
       "  (389, 4.00),\n",
       "  (543, 4.00),\n",
       "  (628, 4.00),\n",
       "  (185, 3.90),\n",
       "  (223, 3.90),\n",
       "  (269, 3.90),\n",
       "  (314, 3.90),\n",
       "  (613, 3.90),\n",
       "  (678, 3.90),\n",
       "  (838, 3.90),\n",
       "  (952, 3.90),\n",
       "  (995, 3.90),\n",
       "  (51, 3.80),\n",
       "  (322, 3.80),\n",
       "  (376, 3.80),\n",
       "  (607, 3.80),\n",
       "  (663, 3.80),\n",
       "  (17, 3.70),\n",
       "  (36, 3.70),\n",
       "  (377, 3.70),\n",
       "  (399, 3.70),\n",
       "  (797, 3.70),\n",
       "  (859, 3.70),\n",
       "  (877, 3.70),\n",
       "  (934, 3.70),\n",
       "  (120, 3.60),\n",
       "  (336, 3.60),\n",
       "  (511, 3.60),\n",
       "  (891, 3.60),\n",
       "  (959, 3.60),\n",
       "  (188, 3.50),\n",
       "  (315, 3.50),\n",
       "  (385, 3.50),\n",
       "  (395, 3.50),\n",
       "  (456, 3.50),\n",
       "  (546, 3.50),\n",
       "  (631, 3.50),\n",
       "  (674, 3.50),\n",
       "  (714, 3.50),\n",
       "  (802, 3.50),\n",
       "  (860, 3.50),\n",
       "  (68, 3.40),\n",
       "  (101, 3.40),\n",
       "  (163, 3.40),\n",
       "  (232, 3.40),\n",
       "  (272, 3.40),\n",
       "  (345, 3.40),\n",
       "  (513, 3.40),\n",
       "  (517, 3.40),\n",
       "  (994, 3.40),\n",
       "  (52, 3.30),\n",
       "  (329, 3.30),\n",
       "  (420, 3.30),\n",
       "  (906, 3.30),\n",
       "  (924, 3.30),\n",
       "  (989, 3.30),\n",
       "  (105, 3.20),\n",
       "  (486, 3.20),\n",
       "  (502, 3.20),\n",
       "  (675, 3.20),\n",
       "  (697, 3.20),\n",
       "  (43, 3.10),\n",
       "  (226, 3.10),\n",
       "  (245, 3.10),\n",
       "  (256, 3.10),\n",
       "  (291, 3.10),\n",
       "  (510, 3.10),\n",
       "  (736, 3.10),\n",
       "  (803, 3.10),\n",
       "  (49, 3.00),\n",
       "  (154, 3.00),\n",
       "  (174, 3.00),\n",
       "  (202, 3.00),\n",
       "  (222, 3.00),\n",
       "  (258, 3.00),\n",
       "  (542, 3.00),\n",
       "  (861, 3.00),\n",
       "  (197, 2.90),\n",
       "  (356, 2.90),\n",
       "  (446, 2.90),\n",
       "  (593, 2.90),\n",
       "  (767, 2.90),\n",
       "  (835, 2.90),\n",
       "  (941, 2.90),\n",
       "  (12, 2.80),\n",
       "  (194, 2.80),\n",
       "  (220, 2.80),\n",
       "  (404, 2.80),\n",
       "  (534, 2.80),\n",
       "  (558, 2.80),\n",
       "  (876, 2.80),\n",
       "  (35, 2.70),\n",
       "  (89, 2.70),\n",
       "  (183, 2.70),\n",
       "  (228, 2.70),\n",
       "  (362, 2.70),\n",
       "  (400, 2.70),\n",
       "  (505, 2.70),\n",
       "  (587, 2.70),\n",
       "  (787, 2.70),\n",
       "  (833, 2.70),\n",
       "  (958, 2.70),\n",
       "  (177, 2.60),\n",
       "  (207, 2.60),\n",
       "  (305, 2.60),\n",
       "  (367, 2.60),\n",
       "  (370, 2.60),\n",
       "  (683, 2.60),\n",
       "  (145, 2.50),\n",
       "  (371, 2.50),\n",
       "  (475, 2.50),\n",
       "  (503, 2.50),\n",
       "  (551, 2.50),\n",
       "  (666, 2.50),\n",
       "  (812, 2.50),\n",
       "  (16, 2.40),\n",
       "  (122, 2.40),\n",
       "  (213, 2.40),\n",
       "  (227, 2.40),\n",
       "  (268, 2.40),\n",
       "  (297, 2.40),\n",
       "  (426, 2.40),\n",
       "  (647, 2.40),\n",
       "  (720, 2.40),\n",
       "  (747, 2.40),\n",
       "  (753, 2.40),\n",
       "  (769, 2.40),\n",
       "  (831, 2.40),\n",
       "  (845, 2.40),\n",
       "  (862, 2.40),\n",
       "  (947, 2.40),\n",
       "  (10, 2.30),\n",
       "  (364, 2.30),\n",
       "  (368, 2.30),\n",
       "  (583, 2.30),\n",
       "  (615, 2.30),\n",
       "  (728, 2.30),\n",
       "  (807, 2.30),\n",
       "  (842, 2.30),\n",
       "  (53, 2.20),\n",
       "  (54, 2.20),\n",
       "  (871, 2.20),\n",
       "  (925, 2.20),\n",
       "  (146, 2.10),\n",
       "  (160, 2.10),\n",
       "  (437, 2.10),\n",
       "  (244, 2.00),\n",
       "  (350, 2.00),\n",
       "  (432, 2.00),\n",
       "  (962, 2.00),\n",
       "  (66, 1.90),\n",
       "  (126, 1.90),\n",
       "  (212, 1.90),\n",
       "  (240, 1.90),\n",
       "  (369, 1.90),\n",
       "  (381, 1.90),\n",
       "  (521, 1.90),\n",
       "  (589, 1.90),\n",
       "  (644, 1.90),\n",
       "  (289, 1.80),\n",
       "  (346, 1.80),\n",
       "  (484, 1.80),\n",
       "  (540, 1.80),\n",
       "  (804, 1.80),\n",
       "  (846, 1.80),\n",
       "  (22, 1.70),\n",
       "  (261, 1.70),\n",
       "  (341, 1.70),\n",
       "  (379, 1.70),\n",
       "  (421, 1.70),\n",
       "  (690, 1.70),\n",
       "  (743, 1.70),\n",
       "  (795, 1.70),\n",
       "  (883, 1.70),\n",
       "  (81, 1.60),\n",
       "  (179, 1.60),\n",
       "  (312, 1.60),\n",
       "  (380, 1.60),\n",
       "  (386, 1.60),\n",
       "  (427, 1.60),\n",
       "  (452, 1.60),\n",
       "  (536, 1.60),\n",
       "  (719, 1.60),\n",
       "  (749, 1.60),\n",
       "  (789, 1.60),\n",
       "  (979, 1.60),\n",
       "  (33, 1.50),\n",
       "  (64, 1.50),\n",
       "  (140, 1.50),\n",
       "  (187, 1.50),\n",
       "  (493, 1.50),\n",
       "  (673, 1.50),\n",
       "  (841, 1.50),\n",
       "  (896, 1.50),\n",
       "  (972, 1.50),\n",
       "  (999, 1.50),\n",
       "  (5, 1.40),\n",
       "  (87, 1.40),\n",
       "  (114, 1.40),\n",
       "  (143, 1.40),\n",
       "  (374, 1.40),\n",
       "  (413, 1.40),\n",
       "  (809, 1.40),\n",
       "  (32, 1.30),\n",
       "  (262, 1.30),\n",
       "  (550, 1.30),\n",
       "  (596, 1.30),\n",
       "  (929, 1.30),\n",
       "  (69, 1.20),\n",
       "  (78, 1.20),\n",
       "  (139, 1.20),\n",
       "  (147, 1.20),\n",
       "  (175, 1.20),\n",
       "  (252, 1.20),\n",
       "  (279, 1.20),\n",
       "  (299, 1.20),\n",
       "  (372, 1.20),\n",
       "  (548, 1.20),\n",
       "  (740, 1.20),\n",
       "  (813, 1.20),\n",
       "  (59, 1.10),\n",
       "  (169, 1.10),\n",
       "  (229, 1.10),\n",
       "  (394, 1.10),\n",
       "  (657, 1.10),\n",
       "  (702, 1.10),\n",
       "  (708, 1.10),\n",
       "  (718, 1.10),\n",
       "  (856, 1.10),\n",
       "  (19, 1.00),\n",
       "  (26, 1.00),\n",
       "  (34, 1.00),\n",
       "  (95, 1.00),\n",
       "  (137, 1.00),\n",
       "  (204, 1.00),\n",
       "  (333, 1.00),\n",
       "  (357, 1.00),\n",
       "  (378, 1.00),\n",
       "  (827, 1.00),\n",
       "  (869, 1.00),\n",
       "  (923, 1.00),\n",
       "  (945, 1.00),\n",
       "  (969, 1.00),\n",
       "  (152, 0.90),\n",
       "  (200, 0.90),\n",
       "  (405, 0.90),\n",
       "  (422, 0.90),\n",
       "  (434, 0.90),\n",
       "  (478, 0.90),\n",
       "  (731, 0.90),\n",
       "  (996, 0.90),\n",
       "  (2, 0.80),\n",
       "  (73, 0.80),\n",
       "  (338, 0.80),\n",
       "  (567, 0.80),\n",
       "  (590, 0.80),\n",
       "  (677, 0.80),\n",
       "  (755, 0.80),\n",
       "  (773, 0.80),\n",
       "  (909, 0.80),\n",
       "  (975, 0.80),\n",
       "  (246, 0.70),\n",
       "  (435, 0.70),\n",
       "  (480, 0.70),\n",
       "  (745, 0.70),\n",
       "  (799, 0.70),\n",
       "  (461, 0.60),\n",
       "  (610, 0.60),\n",
       "  (926, 0.60),\n",
       "  (943, 0.60),\n",
       "  (20, 0.50),\n",
       "  (21, 0.50),\n",
       "  (285, 0.50),\n",
       "  (324, 0.50),\n",
       "  (416, 0.50),\n",
       "  (500, 0.50),\n",
       "  (578, 0.50),\n",
       "  (623, 0.50),\n",
       "  (930, 0.50),\n",
       "  (13, 0.40),\n",
       "  (167, 0.40),\n",
       "  (184, 0.40),\n",
       "  (373, 0.40),\n",
       "  (494, 0.40),\n",
       "  (662, 0.40),\n",
       "  (686, 0.40),\n",
       "  (689, 0.40),\n",
       "  (744, 0.40),\n",
       "  (834, 0.40),\n",
       "  (844, 0.40),\n",
       "  (933, 0.40),\n",
       "  (168, 0.30),\n",
       "  (266, 0.30),\n",
       "  (296, 0.30),\n",
       "  (469, 0.30),\n",
       "  (553, 0.30),\n",
       "  (681, 0.30),\n",
       "  (793, 0.30),\n",
       "  (818, 0.30),\n",
       "  (964, 0.30),\n",
       "  (111, 0.20),\n",
       "  (277, 0.20),\n",
       "  (326, 0.20),\n",
       "  (442, 0.20),\n",
       "  (568, 0.20),\n",
       "  (682, 0.20),\n",
       "  (729, 0.20),\n",
       "  (742, 0.20),\n",
       "  (785, 0.20),\n",
       "  (903, 0.20),\n",
       "  (928, 0.20),\n",
       "  (149, 0.10),\n",
       "  (190, 0.10),\n",
       "  (287, 0.10),\n",
       "  (349, 0.10),\n",
       "  (366, 0.10),\n",
       "  (415, 0.10),\n",
       "  (450, 0.10),\n",
       "  (473, 0.10),\n",
       "  (600, 0.10),\n",
       "  (649, 0.10),\n",
       "  (715, 0.10),\n",
       "  (739, 0.10),\n",
       "  (798, 0.10),\n",
       "  (913, 0.10),\n",
       "  (976, 0.10),\n",
       "  (978, 0.10),\n",
       "  (3, 0.00),\n",
       "  (4, 0.00),\n",
       "  (29, 0.00),\n",
       "  (103, 0.00),\n",
       "  (106, 0.00),\n",
       "  (112, 0.00),\n",
       "  (150, 0.00),\n",
       "  (233, 0.00),\n",
       "  (335, 0.00),\n",
       "  (384, 0.00),\n",
       "  (403, 0.00),\n",
       "  (460, 0.00),\n",
       "  (499, 0.00),\n",
       "  (501, 0.00),\n",
       "  (504, 0.00),\n",
       "  (524, 0.00),\n",
       "  (525, 0.00),\n",
       "  (598, 0.00),\n",
       "  (617, 0.00),\n",
       "  (634, 0.00),\n",
       "  (648, 0.00),\n",
       "  (713, 0.00),\n",
       "  (726, 0.00),\n",
       "  (771, 0.00),\n",
       "  (780, 0.00),\n",
       "  (810, 0.00),\n",
       "  (895, 0.00),\n",
       "  (901, 0.00),\n",
       "  (908, 0.00),\n",
       "  (914, 0.00),\n",
       "  (935, 0.00),\n",
       "  (940, 0.00),\n",
       "  (942, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (965, 0.00),\n",
       "  (967, 0.00),\n",
       "  (970, 0.00),\n",
       "  (974, 0.00),\n",
       "  (977, 0.00),\n",
       "  (980, 0.00),\n",
       "  (993, 0.00)])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f56bfdf23c8>]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# on test\n",
    "sorted_hist = sorted(hist, key=lambda x: x[0], reverse = False)\n",
    "values = [elem[1] for elem in sorted_hist]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.8474)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "entropy(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fCvFG0VMKts"
   },
   "outputs": [],
   "source": [
    "def make_triplet_samples(z, margin, r2, r3):\n",
    "  positive_sample = z + random_vector_volume(z.shape, 0, margin).cuda() \n",
    "  negative_sample = z + random_vector_volume(z.shape, r2, r3).cuda()\n",
    "  return positive_sample, negative_sample\n",
    "\n",
    "def random_vector_surface(shape, r = 1.):\n",
    "  mat = torch.randn(size=shape).cuda()\n",
    "  norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "  return (mat/norm) * r\n",
    "\n",
    "def random_vector_volume(shape, inner_r, outer_r):\n",
    "  fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "  fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "  fraction.unsqueeze_(-1)\n",
    "  return random_vector_surface(shape, 1) * fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PizmBkGqMKtu"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_frequent(x):\n",
    "  return Counter(x).most_common(1)[0]\n",
    "\n",
    "def preds_around(center, radius, n_preds, model, dummy_img):\n",
    "  z_s = random_vector_volume([n_preds, 10], radius, radius + 0.01) + center[None]\n",
    "  noises = model.forward_z(z_s)\n",
    "  perturbed_imgs = noises + dummy_img \n",
    "  return torch.argmax(arch(perturbed_imgs), 1)\n",
    "  \n",
    "def most_freq_pred_around(center, radius, n_preds, model, dummy_img):\n",
    "  preds = preds_around(center, radius, n_preds, model, dummy_img)\n",
    "  most_freq = most_frequent(preds.tolist())\n",
    "  return (class_index_to_label(most_freq[0]), most_freq[1]/n_preds)\n",
    "\n",
    "def investigate_neighborhood(z, step, model, dummy_img):\n",
    "  with torch.no_grad():\n",
    "    result = []\n",
    "    for radius in np.arange(0.1, 6., step):\n",
    "#       print(\"creating {} more preds\".format(int(10 + 5 * (radius ** 2))))\n",
    "      most_freq_pred = most_freq_pred_around(z, radius, int(10 + 5 * (radius ** 2)), model, dummy_img)\n",
    "      result.append((radius, most_freq_pred))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-btRW4qMKtw",
    "outputId": "90e81f6a-1b9e-45a4-ae82-bda370319bd9"
   },
   "outputs": [],
   "source": [
    "#experiment 1\n",
    "\n",
    "z = torch.tensor([0.5] * 10).cuda()\n",
    "# z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "# z_s = z[None]\n",
    "\n",
    "model = learn.model.eval()\n",
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "  \n",
    "for i in range(6):\n",
    "  z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "  print(\"investigation for: \", z)\n",
    "  for elem in investigate_neighborhood(z, 0.5, model, x_img):\n",
    "    print(elem)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 1-1: modified investigate_z\n",
    "z_investigate_path = '/root/Derakhshani/adversarial/textual_notes/investigate_z_{}.txt'.format(env.save_filename)\n",
    "if Path(z_investigate_path).exists(): raise FileExistsError(\"file already exists\")\n",
    "file = open(str(z_investigate_path), 'w')\n",
    "        \n",
    "for i, (z, noise) in enumerate(zip(pruned_z_s, pruned_noises)):\n",
    "  hist = compute_prediction_histogram(learn, noise)\n",
    "  indexed_hist = [(i, val) for i, val in enumerate(hist)]\n",
    "  sorted_hist = sorted(indexed_hist, key=lambda x: x[1], reverse=True)\n",
    "  labeled_hist = [(class_index_to_label(i), count) for i, count in sorted_hist]\n",
    "  print(\"result {}:\".format(i))\n",
    "  print(big_vector_to_str(z))\n",
    "  print(labeled_hist[:6])\n",
    "  print(\"\\n\\n\")\n",
    "  \n",
    "  file.write(\"result {}:\\n\".format(i))\n",
    "  file.write(big_vector_to_str(z) + \"\\n\")\n",
    "  file.write(str(labeled_hist[:6]))\n",
    "  file.write(\"\\n\\n\\n\")\n",
    "  file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bp6YOnipMKtz"
   },
   "outputs": [],
   "source": [
    "#experiment 2\n",
    "import itertools\n",
    "z_s = [torch.tensor(t).cuda() for t in itertools.product( *([[-0.33, 0.33]] * 10) )]\n",
    "model = learn.model.eval()\n",
    "noises = []\n",
    "with torch.no_grad():\n",
    "  for z in z_s:\n",
    "    noises.append(model.forward_single_z(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "55lErWDyMKt1",
    "outputId": "93d8fb71-3fd5-44a5-d3ec-8013e13f17ba"
   },
   "outputs": [],
   "source": [
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "\n",
    "preds = []\n",
    "for noise in noises:\n",
    "  perturbed_img = x_img + noise\n",
    "  preds.append(torch.argmax(arch(perturbed_img[None]), 1)[0].item())\n",
    "\n",
    "from collections import Counter\n",
    "result = [(class_index_to_label(index), count) for index, count in Counter(preds).most_common(5)]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4WSg-wBFMKt5"
   },
   "outputs": [],
   "source": [
    "#experiment 3\n",
    "import itertools\n",
    "dimension_values = [[-0.9, 0.9]] * z_dim\n",
    "for i in range(z_dim):\n",
    "  if i % 100 != 0:\n",
    "    dimension_values[i] = [0.]\n",
    "# dimension_values[0] = [0.]\n",
    "# dimension_values[3] = [0.]\n",
    "# dimension_values[6] = [0.]\n",
    "# dimension_values[9] = [0.]\n",
    "pruned_z_s = [torch.tensor(t).cuda() for t in itertools.product(*dimension_values)]\n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 3: for the targeted-attack case\n",
    "pruned_z_s = []\n",
    "for i in range(z_dim):\n",
    "  new_z = torch.zeros(z_dim).cuda()\n",
    "  new_z[i] = 1.\n",
    "  pruned_z_s.append(new_z)\n",
    "  \n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 3-1: noises for \n",
    "pruned_z_s = []\n",
    "# for i in range(z_dim):\n",
    "#   new_z = torch.empty(z_dim).uniform_(0,1).cuda().detach()\n",
    "#   pruned_z_s.append(new_z)\n",
    "\n",
    "for i in range(z_dim):\n",
    "  new_z = torch.zeros(z_dim).cuda().detach()\n",
    "  new_z[i] = 1.\n",
    "  pruned_z_s.append(new_z)\n",
    "  \n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise in pruned_noises[0:200]:\n",
    "  img = noise_to_image(noise)\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider web\n",
    "z_values = [\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33],\n",
    "  [-0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33,  0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33,  0.33,  0.33],\n",
    "  [-0.33,  0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_values = [\n",
    "  # window screen\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuAVZzmKMKt9",
    "outputId": "7e6643e0-ce55-438b-e0ae-79bde3ee4cef"
   },
   "outputs": [],
   "source": [
    "#vgg-16_12 most repeated labels:\n",
    "l = [(611, 215.0),\n",
    "  (474, 194.1),\n",
    "  (398, 120.3),\n",
    "  (721, 79.6),\n",
    "  (741, 73.5),\n",
    "  (510, 62.5)]\n",
    "\n",
    "[(class_index_to_label(index), count) for index, count in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAgk-YyWc3rG"
   },
   "outputs": [],
   "source": [
    "# learn.recorder.plot_losses()\n",
    "# learn.recorder.plot_lr()\n",
    "# learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTHG4Bt7VDYp"
   },
   "outputs": [],
   "source": [
    "fooling_rates = []\n",
    "model = learn.model.eval()\n",
    "learn.metrics = [validation_single_perturbation]\n",
    "for i in range(10):\n",
    "  global_perturbations = model(torch.rand(1, 3, 224, 244).cuda())[0]\n",
    "  nag_util.global_perturbations = global_perturbations\n",
    "  fooling_rates.append(learn.validate()[1].cpu().item())\n",
    "  print(\"%d : %f\"%(i, fooling_rates[-1]))\n",
    "\n",
    "mean = np.mean(fooling_rates)\n",
    "stddev = np.std(fooling_rates)\n",
    "print(mean, stddev); print(fooling_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "OFCjzI7UaY3C",
    "outputId": "740185b4-dd54-46f4-b0af-79ee452568e1"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[200][0]\n",
    "x = normalize(x_img.data.cuda())\n",
    "z = torch.tensor([-0.33,  0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33], dtype=torch.float32).cuda()\n",
    "# z = torch.empty(z_dim).uniform_(-1,1).cuda()\n",
    "p = model.forward_single_z(z).detach()\n",
    "\n",
    "p_x = x + p\n",
    "# print(\"img range, noise range\")\n",
    "# print_range(x); print_range(p)\n",
    "adv_label = class_index_to_label(arch(p_x[None]).argmax(1).item())\n",
    "print_big_vector(arch(p_x[None])[0])\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0., 1.])\n",
    "p_img = Image(p)\n",
    "x_img.show()\n",
    "p_img.show()\n",
    "p_x_img.show()\n",
    "\n",
    "\n",
    "# print_range(p)\n",
    "# print_range(denormalize(x))\n",
    "# print_range(p_x)\n",
    "\n",
    "benign_label = class_index_to_label(arch(x[None]).argmax(1).item())\n",
    "\n",
    "print_big_vector(arch(x[None])[0])\n",
    "print(benign_label, adv_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzwsI2P1ZANz"
   },
   "outputs": [],
   "source": [
    "z1 = torch.tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p1 = model.forward_single_z(z1)\n",
    "\n",
    "z2 = torch.tensor([1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p2 = model.forward_single_z(z2)\n",
    "\n",
    "z3 = torch.tensor([1, 1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p3 = model.forward_single_z(z3)\n",
    "\n",
    "l2_distance(p1, p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eroI82OKSnAL"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[4][0]\n",
    "x = x_img.data[None].cuda()\n",
    "p = model(x)[0].squeeze().detach() \n",
    "x = x.squeeze()\n",
    "x = normalize(x)\n",
    "\n",
    "p_x = x + p\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0.,1.])\n",
    "p_img = Image(p)\n",
    "# x_img.show()\n",
    "p_img.show()\n",
    "# p_x_img.show()\n",
    "\n",
    "print_range(p)\n",
    "print_range(x)\n",
    "print_range(p_x)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NAG-tripletLossExperiment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
