{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ahmad-PH/nag-notebooks/blob/master/NAG_tripletLossExperiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "cqeZpz16do4y",
    "outputId": "19e7e40d-3281-46ee-99b5-d8cf518ff9b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up-to-date.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "\n",
    "import os; import subprocess\n",
    "\n",
    "def detect_env():\n",
    "    if 'content' in os.listdir('/'):\n",
    "      return 'colab'\n",
    "    elif 'mlcm-deep' in os.listdir('/home'):\n",
    "      return 'mlcm'\n",
    "    else:\n",
    "      return 'IBM'\n",
    "  \n",
    "def run_shell_command(cmd):\n",
    "  p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "  print(str(p.communicate()[0], 'utf-8'))\n",
    "  \n",
    "if detect_env() == 'colab': root_folder = '/content'\n",
    "elif detect_env() == 'IBM' : root_folder = '/root/Derakhshani/adversarial'\n",
    "elif detect_env() == 'mlcm' : root_folder = '/home/mlcm-deep/AhmadPourihosseini/NAG'\n",
    "python_files_path = root_folder + '/nag-public'\n",
    "if os.path.isdir(python_files_path):\n",
    "  initial_dir = os.getcwd()\n",
    "  os.chdir(python_files_path)\n",
    "  run_shell_command('git pull')\n",
    "  os.chdir(initial_dir)\n",
    "else:\n",
    "  os.chdir(root_folder)\n",
    "  run_shell_command('git clone https://github.com/ahmad-PH/nag-public.git')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oEHu0mgxuV7e"
   },
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.imports import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import sys; import os; import shutil\n",
    "# import art #INSTALL LATER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "I9kHEmcwuV7h",
    "outputId": "617dbc71-9537-41ca-c842-21c579e8306b"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(python_files_path + '/NAG-11May-beforeDenoiser')\n",
    "\n",
    "from nag_util import *\n",
    "import nag_util\n",
    "from environment import *\n",
    "from visualization import *\n",
    "from utility import *\n",
    "from create_perturbed_dataset import *\n",
    "\n",
    "env = create_env()\n",
    "env.setup(cuda_index = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0NUgmvPSuV7k"
   },
   "outputs": [],
   "source": [
    "gen_arch = \"targeted\"\n",
    "# gen_arch = \"non-targeted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOC5e4SpuV7m"
   },
   "outputs": [],
   "source": [
    "class GeneralRelu(nn.Module):\n",
    "  def __init__(self, leak=None, sub=None, maxv=None):\n",
    "    super().__init__()\n",
    "    self.leak,self.sub,self.maxv = leak,sub,maxv\n",
    "\n",
    "  def forward(self, x): \n",
    "    x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n",
    "    if self.sub is not None: x.sub_(self.sub)\n",
    "    if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "    return x\n",
    "  \n",
    "class deconv_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k_size = (4,4), s = (2,2), pad = (1,1), b = True, activation = True):\n",
    "        super(deconv_layer, self).__init__()\n",
    "\n",
    "        self.CT2d = nn.ConvTranspose2d(in_channels = in_ch,\n",
    "                                  out_channels = out_ch,\n",
    "                                  kernel_size = k_size,\n",
    "                                  stride = s, \n",
    "                                  padding = pad,\n",
    "                                  bias = b)\n",
    "        self.BN2d = nn.BatchNorm2d(out_ch)\n",
    "        \n",
    "        self.activation = activation\n",
    "        if self.activation:\n",
    "            self.relu = GeneralRelu(0, 0.2, 5)\n",
    "        \n",
    "        self.weight_init()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.activation:\n",
    "            return self.relu(self.BN2d(self.CT2d(input)))\n",
    "        else:\n",
    "            return self.BN2d(self.CT2d(input))\n",
    "\n",
    "    def weight_init(self):\n",
    "        self.CT2d.weight.data.normal_(mean = 0, std = 0.02)\n",
    "        self.CT2d.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VxqpShBnuV7p"
   },
   "outputs": [],
   "source": [
    "class deconv_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k_size = (4,4), s = (2,2), pad = (1,1), b = True, activation = True):\n",
    "        super(deconv_layer, self).__init__()\n",
    "\n",
    "        self.CT2d = nn.ConvTranspose2d(in_channels = in_ch,\n",
    "                                  out_channels = out_ch,\n",
    "                                  kernel_size = k_size,\n",
    "                                  stride = s, \n",
    "                                  padding = pad,\n",
    "                                  bias = b)\n",
    "        self.BN2d = nn.BatchNorm2d(out_ch)\n",
    "        self.activation = activation\n",
    "\n",
    "        self.weight_init()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.activation:\n",
    "            return F.relu(self.BN2d(self.CT2d(input)), inplace=True)\n",
    "        else:\n",
    "            return self.BN2d(self.CT2d(input))\n",
    "\n",
    "    def weight_init(self):\n",
    "        self.CT2d.weight.data.normal_(mean = 0, std = 0.02)\n",
    "        self.CT2d.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFFluyiPuV7r"
   },
   "outputs": [],
   "source": [
    "if gen_arch == \"targeted\":\n",
    "  class Gen(nn.Module):\n",
    "    def __init__(self, z_dim, active_labels = (0, 1000), gf_dim=64, y_dim = None,\n",
    "                 df_dim = 64, image_shape = [3,128,128]):\n",
    "      super(Gen, self).__init__()\n",
    "      \n",
    "      self.bs = None\n",
    "      self.z_dim = z_dim\n",
    "      self.gf_dim = gf_dim\n",
    "      self.y_dim = y_dim\n",
    "      self.df_dim = df_dim\n",
    "      self.image_shape = image_shape\n",
    "      self.active_labels = active_labels\n",
    "      \n",
    "      self.n_unit_coeffs = [10, 7, 4, 2, 1, 1, 1]\n",
    "      self.n_units = [coeff * self.gf_dim for coeff in self.n_unit_coeffs]\n",
    "      \n",
    "      self.z_ = nn.Linear(self.z_dim, self.n_units[0] * 4 * 4, bias=True)\n",
    "      self.z_.bias.data.fill_(0)\n",
    "      self.BN_ = nn.BatchNorm2d(self.n_units[0])\n",
    "\n",
    "      self.half = max(self.gf_dim // 2, 1) \n",
    "      self.quarter = max(self.gf_dim // 4, 1)\n",
    "      self.eighth = max(self.gf_dim // 8, 1)\n",
    "      # sixteenth = max(self.gf_dim // 16, 1)\n",
    "\n",
    "      self.CT2d_1 = deconv_layer(self.n_units[0], self.n_units[1], k_size = (5,5), pad = (2,2))\n",
    "      self.CT2d_2 = deconv_layer(self.n_units[1], self.n_units[2])    \n",
    "      self.CT2d_3 = deconv_layer(self.n_units[2], self.n_units[3])\n",
    "      self.CT2d_4 = deconv_layer(self.n_units[3], self.n_units[4])\n",
    "      self.CT2d_5 = deconv_layer(self.n_units[4], self.n_units[5])\n",
    "      self.CT2d_6 = deconv_layer(self.n_units[5], self.n_units[6])\n",
    "      self.CT2d_7 = deconv_layer(self.n_units[6], 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "      \n",
    "    # static variables:\n",
    "    ksi = 10.0\n",
    "    output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1]))\n",
    "    \n",
    "    def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "      h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "      h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "      output = deconv_layer(h_input)\n",
    "      assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "              \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "              \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "      return output\n",
    "\n",
    "    def forward_z(self, z):\n",
    "      self.bs = z.shape[0]\n",
    "\n",
    "      h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "      assert h0.shape[2:] == (4, 4), \"Unexpected shape, it shoud be (4,4)\"\n",
    "\n",
    "      h1 = self.CT2d_1(h0)\n",
    "      h2 = self.CT2d_2(h1)\n",
    "      h3 = self.CT2d_3(h2)\n",
    "      h4 = self.CT2d_4(h3)\n",
    "      h5 = self.CT2d_5(h4)\n",
    "      h6 = self.CT2d_6(h5)\n",
    "      h7 = self.CT2d_7(h6)\n",
    "      \n",
    "#       h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "#       h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "#       h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "#       h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "#       h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "#       h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "#       h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "      # this coeff scales the output to be appropriate for images that are \n",
    "      # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "      # interval)\n",
    "      return Gen.output_coeff * torch.tanh(h7)\n",
    "\n",
    "  #   # blind-selection\n",
    "    def forward(self, inputs):\n",
    "      self.bs = inputs.shape[0]\n",
    "\n",
    "      benign_preds_onehot = arch(inputs)\n",
    "#       benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "      worst_preds = torch.argmin(benign_preds_onehot, dim = 1)\n",
    "#       second_best_preds = torch.topk(benign_preds_onehot, 2, dim=1)[1][:, 1]\n",
    "      \n",
    "      z = torch.zeros([self.bs, 1000]).cuda()\n",
    "      for i in range(self.bs):\n",
    "        random_label = worst_preds[i].item()\n",
    "        z[i][random_label] = 1.\n",
    "\n",
    "      z_out = self.forward_z(z)\n",
    "\n",
    "      return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "\n",
    "  #   #second-best selection: made validation so much worse\n",
    "  #   def forward(self, inputs):\n",
    "  #     self.bs = inputs.shape[0]\n",
    "\n",
    "  #     benign_preds_onehot = arch(inputs)\n",
    "  #     target_preds = torch.topk(benign_preds_onehot, 2, dim = 1).indices[:, 1:]\n",
    "\n",
    "  #     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "  #     for i in range(self.bs):\n",
    "  #       z[i][target_preds[i]] = 1.\n",
    "\n",
    "  #     z_out = self.forward_z(z)\n",
    "\n",
    "  #     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "\n",
    "  #    def forward(self, inputs):\n",
    "  #     self.bs = inputs.shape[0]\n",
    "\n",
    "  #     benign_preds_onehot = arch(inputs)\n",
    "  #     benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "\n",
    "  #     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "  #     random_label = self.randint(0,1000, exclude = benign_preds.tolist())\n",
    "  #     for i in range(self.bs):\n",
    "  #       z[i][random_label] = 1.\n",
    "\n",
    "  #     z_out = self.forward_z(z)\n",
    "\n",
    "  #     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "\n",
    "    @staticmethod\n",
    "    def randint(low, high, exclude):\n",
    "      if exclude >= low and exclude < high:\n",
    "        temp = np.random.randint(low, high - 1)\n",
    "        if temp >= exclude:\n",
    "          temp = temp + 1\n",
    "        return temp\n",
    "      else:\n",
    "        return np.random.randint(low, high)\n",
    "\n",
    "    def forward_single_z(self, z):\n",
    "      return self.forward_z(z[None]).squeeze()\n",
    "\n",
    "    def generate_single_noise(self):\n",
    "      z = torch.empty(self.z_dim).uniform_(-1,1).cuda()\n",
    "      return self.forward_single_z(z)         \n",
    "\n",
    "    def make_triplet_samples(self, z, margin, r2, r3):\n",
    "      positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "      negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "      return positive_sample, negative_sample\n",
    "\n",
    "    def random_vector_surface(self, shape, r = 1.):\n",
    "      mat = torch.randn(size=shape).cuda()\n",
    "      norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "      return (mat/norm) * r\n",
    "\n",
    "\n",
    "    def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "      fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "      fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "      fraction.unsqueeze_(-1)\n",
    "      return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "    def make_z(self, in_shape):\n",
    "      return torch.empty(in_shape).cuda().uniform_(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anIZD0Q4uV7u"
   },
   "outputs": [],
   "source": [
    "# g = Gen(z_dim = 1000).cuda()\n",
    "# t = torch.empty(1000).uniform_().cuda()\n",
    "# g.forward_single_z(t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tLqK8jNyuV7x"
   },
   "outputs": [],
   "source": [
    "if gen_arch == \"non-targeted\":\n",
    "  class Gen(nn.Module):\n",
    "    def __init__(self, z_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "      super(Gen, self).__init__()\n",
    "\n",
    "      self.bs = None\n",
    "      self.z_dim = z_dim\n",
    "      self.gf_dim = gf_dim\n",
    "      self.y_dim = y_dim\n",
    "      self.df_dim = df_dim\n",
    "      self.image_shape = image_shape\n",
    "\n",
    "      self.z_ = nn.Linear(self.z_dim, self.gf_dim * 7 * 4 * 4, bias=True)\n",
    "      self.z_.bias.data.fill_(0)\n",
    "      self.BN_ = nn.BatchNorm2d(self.gf_dim * 7)\n",
    "\n",
    "      self.half = max(self.gf_dim // 2, 1) \n",
    "      self.quarter = max(self.gf_dim // 4, 1)\n",
    "      self.eighth = max(self.gf_dim // 8, 1)\n",
    "      # sixteenth = max(self.gf_dim // 16, 1)\n",
    "\n",
    "      self.CT2d_1 = deconv_layer(self.gf_dim * 8, self.gf_dim * 4, k_size = (5,5), pad = (2,2))\n",
    "      self.CT2d_2 = deconv_layer(self.gf_dim * 5, self.gf_dim * 2)    \n",
    "      self.CT2d_3 = deconv_layer(self.gf_dim * 2 + self.half, self.gf_dim * 1)\n",
    "      self.CT2d_4 = deconv_layer(self.gf_dim * 1 + self.quarter, self.gf_dim * 1)\n",
    "      self.CT2d_5 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "      self.CT2d_6 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "      self.CT2d_7 = deconv_layer(self.gf_dim * 1 + self.eighth, 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "    ksi = 10.0\n",
    "    output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "      \n",
    "    def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "      h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "      h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "      output = deconv_layer(h_input)\n",
    "      assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "              \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "              \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "      return output\n",
    "\n",
    "    def forward_z(self, z):\n",
    "      self.bs = z.shape[0]\n",
    "\n",
    "      h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "      assert h0.shape[2:] == (4, 4), \"Non-expected shape, it shoud be (4,4)\"\n",
    "\n",
    "      h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "      h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "      h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "      h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "      h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "      h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "      h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "      # this coeff scales the output to be appropriate for images that are \n",
    "      # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "      # interval)\n",
    "      return Gen.output_coeff * torch.tanh(h7)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "      self.bs = inputs.shape[0]\n",
    "      z = inputs.new_empty([self.bs, self.z_dim]).uniform_(-1,1).cuda()\n",
    "      p, n = self.make_triplet_samples(z, 0.1, 0.1, 2.)\n",
    "\n",
    "      z_out = self.forward_z(z)\n",
    "#       p_out = self.forward_z(p)\n",
    "#       n_out = self.forward_z(n)\n",
    "\n",
    "#       return z_out, p_out, n_out, inputs, z\n",
    "      return z_out, None, None, inputs, z\n",
    "\n",
    "    def forward_single_z(self, z):\n",
    "      return self.forward_z(z[None]).squeeze()\n",
    "\n",
    "    def generate_single_noise(self):\n",
    "      z = torch.empty(self.z_dim).uniform_(-1,1).cuda()\n",
    "      return self.forward_single_z(z)\n",
    "\n",
    "\n",
    "    def make_triplet_samples(self, z, margin, r2, r3):\n",
    "      positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "      negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "      return positive_sample, negative_sample\n",
    "\n",
    "    def random_vector_surface(self, shape, r = 1.):\n",
    "      mat = torch.randn(size=shape).cuda()\n",
    "      norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "      return (mat/norm) * r\n",
    "\n",
    "\n",
    "    def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "      fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "      fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "      fraction.unsqueeze_(-1)\n",
    "      return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "    def make_z(self, in_shape):\n",
    "      return torch.empty(in_shape).cuda().uniform_(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jGpb4lMluV79"
   },
   "outputs": [],
   "source": [
    "def js_distance(x1, x2):\n",
    "  m = 0.5 * (x1 + x2)\n",
    "  return 0.5 * (F.kl_div(x1, m) + F.kl_div(x2, m))\n",
    "\n",
    "def kl_distance(x1, x2):\n",
    "  inp = torch.log(x1)\n",
    "  target = x2\n",
    "  return F.kl_div(inp, target, reduction='batchmean')\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  x1 = tensorify(x1)\n",
    "  x2 = tensorify(x2)\n",
    "  x1 = x1 / torch.sum(x1)\n",
    "  x2 = x2 / torch.sum(x2)\n",
    "  return kl_distance(x1[None], x2[None])\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  if not isinstance(x1, torch.Tensor): x1 = torch.tensor(x1)\n",
    "  if not isinstance(x2, torch.Tensor): x2 = torch.tensor(x2)\n",
    "  x1 = x1 * 100. / torch.sum(x1)\n",
    "  x2 = x2 * 100. / torch.sum(x2)\n",
    "  return torch.norm(x1 - x2, 2)\n",
    "\n",
    "def distance_from_uniform(x):\n",
    "  return distrib_distance(x, [1.] * len(x))\n",
    "\n",
    "def wasserstein_distance(x1, x2):\n",
    "  return torch.mean(x1 - x2)\n",
    "\n",
    "def l1_distance(x1, x2):\n",
    "  return F.l1_loss(x1, x2)\n",
    "\n",
    "def l2_distance(x1, x2):\n",
    "  return F.mse_loss(x1 * 10, x2 * 10)\n",
    "\n",
    "def mse_loss(x1, x2):\n",
    "  return F.mse_loss(x1, x2)\n",
    "\n",
    "def cos_distance(x1, x2, dim = 1):\n",
    "  return -1 * torch.mean(F.cosine_similarity(x1, x2, dim=dim))\n",
    "\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, distance_func, margin):\n",
    "  # max distance when using l1_distance is 2\n",
    "  # max distacne when using l2-distance is sqrt(2)\n",
    "  ap_dist = distance_func(anchor, positive)\n",
    "  an_dist = distance_func(anchor, negative)\n",
    "\n",
    "  triplet_loss.call_count += 1\n",
    "  if triplet_loss.call_count % 200 == 0 : #and anchor.shape[1] == 1000:\n",
    "#     print(\"a: \", end=\"\"); print_big_vector(anchor[0])\n",
    "#     print(\"p: \", end=\"\"); print_big_vector(positive[0])\n",
    "#     print(\"n: \", end=\"\"); print_big_vector(negative[0])\n",
    "    print(\"func:{}, ap_dist: {}, an_dist: {}\".format(distance_func.__name__, ap_dist, an_dist))\n",
    "    \n",
    "  return torch.mean(F.relu(ap_dist - an_dist + margin))\n",
    "\n",
    "triplet_loss.call_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZ55e3NVuV8E"
   },
   "outputs": [],
   "source": [
    "# def diversity_loss(embeddings, z_s):\n",
    "#   size = z_s.shape[0]\n",
    "#   result = 0\n",
    "#   for i in range(size):\n",
    "#     for j in range(i+1, size):\n",
    "# #       a = F.cosine_similarity(embeddings[i], embeddings[j], dim = 0)\n",
    "# #       b = torch.norm(z_s[i] - z_s[j], 2, dim = 0)\n",
    "# #       print('embeddings: ')\n",
    "# #       print_big_vector(embeddings[i])\n",
    "# #       print_big_vector(embeddings[j])\n",
    "# #       print(f'a: {a}, b:{b}, multiple: {a*b}')\n",
    "#       result += F.cosine_similarity(embeddings[i], embeddings[j], dim = 0) * \\\n",
    "#                 torch.norm(z_s[i] - z_s[j], 2, dim = 0)\n",
    "#   n_pairs = (size * (size - 1)) / 2\n",
    "#   mean = result / n_pairs \n",
    "# #   print(f'result {result}, n_pairs {n_pairs}, mean {mean}')\n",
    "#   return mean\n",
    "  \n",
    "\n",
    "# # normalized with shuffling\n",
    "# def diversity_loss(embeddings, deranged_embeddings, z_s, deranged_z_s):\n",
    "#     cos_similarity = F.cosine_similarity(embeddings, deranged_embeddings)\n",
    "#     z_distance = torch.norm(z_s - deranged_z_s, 2, dim = 1)\n",
    "#     return torch.mean(cos_similarity * z_distance)\n",
    "  \n",
    "# # normalized with shuffling\n",
    "# def diversity_loss(embeddings, z_s):\n",
    "#     deranged_embeddings, deranged_z_s = derange(embeddings, z_s)\n",
    "#     cos_similarity = F.cosine_similarity(embeddings, deranged_embeddings)\n",
    "#     z_distance = torch.norm(z_s - deranged_z_s, dim = 1)\n",
    "#     max_possible_z_distance = 6.3246\n",
    "#     return torch.mean(cos_similarity * (z_distance/max_possible_z_distance))\n",
    "\n",
    "\n",
    "def diversity_loss(input, target):\n",
    "#   return -1 * torch.mean(torch.pow(f_x_a-f_x_s,2))\n",
    "  if input.shape[0] != batch_size:\n",
    "    print(\"input shape: \", input.shape)\n",
    "    print(\"target shape: \", target.shape, \"\\n\\n\")\n",
    "  return torch.mean(F.cosine_similarity(\n",
    "    input.view([batch_size, -1]),\n",
    "    target.view([batch_size, -1]), \n",
    "  ))\n",
    "\n",
    "\n",
    "if gen_arch == 'non-targeted':\n",
    "  def fool_loss(input, target):\n",
    "    true_class = torch.argmax(target, dim=1).view(-1,1).long().cuda()\n",
    "    target_probabilities = input.gather(1, true_class)\n",
    "    epsilon = 1e-10\n",
    "    result =  torch.mean(-1 * torch.log(1 - target_probabilities + epsilon))\n",
    "\n",
    "    fool_loss.call_count += 1\n",
    "    if fool_loss.call_count % 200 == 0:\n",
    "      print(\"target probs {}, loss: {}: \".format(target_probabilities, result))\n",
    "\n",
    "    return result\n",
    "\n",
    "  fool_loss.call_count = 0\n",
    "\n",
    "if gen_arch == 'targeted':\n",
    "  def fool_loss(model_output, target_labels):\n",
    "    target_labels = target_labels.view(-1, 1).long().cuda()\n",
    "    target_probabilities = model_output.gather(1, target_labels)\n",
    "    epsilon = 1e-10\n",
    "    # highest possible fool_loss is - log(1e-10) == 23\n",
    "    result = torch.mean(-1 * torch.log(target_probabilities + epsilon))\n",
    "\n",
    "    fool_loss.call_count += 1\n",
    "    if fool_loss.call_count % 200 == 0:\n",
    "      print(\"target probs {}, loss: {}: \".format(target_probabilities, result))\n",
    "\n",
    "    return result\n",
    "\n",
    "  fool_loss.call_count = 0\n",
    "\n",
    "\n",
    "def targeted_validation(gen_output, target):\n",
    "  perturbations, _, _, clean_images, _, z = gen_output\n",
    "  perturbed_images = clean_images + perturbations\n",
    "  target_labels = torch.argmax(z, 1)\n",
    "  adversary_preds = torch.argmax(arch(perturbed_images), 1)\n",
    "#   print('adv preds: ', adversary_preds.shape, adversary_preds)\n",
    "#   print('target_labels: ', target_labels.shape, target_labels)\n",
    "#   print('eq: ', (adversary_preds == target_labels))\n",
    "  return (adversary_preds == target_labels).float().mean()\n",
    "  \n",
    "\n",
    "# # targeted \n",
    "# def validation(gen_output, target):\n",
    "#   perturbations, _, _, clean_images, _, _ = gen_output\n",
    "#   return validation_(perturbations, clean_images)\n",
    "\n",
    "# # non-targeted\n",
    "# def validation(gen_output, target):\n",
    "#   perturbations, _, _, clean_images, _ = gen_output\n",
    "#   return validation_(perturbations, clean_images)\n",
    "\n",
    "# # general\n",
    "def validation(gen_output, target):\n",
    "  perturbations = gen_output[0]\n",
    "  clean_images = gen_output[3]\n",
    "  return validation_(perturbations, clean_images, target)\n",
    "\n",
    "unfooled_histogram = np.array([0.] * 1000)\n",
    "fooled_histogram = np.array([0.] * 1000)\n",
    "valid_cnt = 0\n",
    "\n",
    "def print_hist(unfooled, fooled):\n",
    "  indexed = [(i, u) for i, u in enumerate(unfooled)]\n",
    "  summarized = list(filter(lambda x: x[1] > 0.0, indexed))\n",
    "  total = fooled + unfooled\n",
    "\n",
    "  percent_total = [(i, 100. * u / (total[i] + 1e-10), total[i]) for i, u in enumerate(unfooled)]\n",
    "  sorted_percent_total = sorted(percent_total, key =lambda x: x[1], reverse = True)\n",
    "\n",
    "  print('\\npercent_total: ')\n",
    "  print(list(filter(lambda x: x[1] > 0.0, sorted_percent_total)))\n",
    "  print('\\n')\n",
    "  \n",
    "  return sorted_percent_total\n",
    "\n",
    "def validation_(perturbations, clean_images, target):\n",
    "  # THE CLAMP BELOW IS EXPERIMENTAL, REMOVE THEM ASAP\n",
    "  # THE COMPARISON WITH TARGET IS EXPERIMENTAL TOO, REMOVE IT ASAP\n",
    "  perturbed_images = clean_images + perturbations\n",
    "#   perturbed_images = nag_util.normalize(torch.clamp(nag_util.denormalize(perturbed_images), 0., 1.))\n",
    "  benign_preds = torch.argmax(arch(clean_images), 1)\n",
    "  adversary_preds = torch.argmax(arch(perturbed_images), 1)\n",
    "\n",
    "  is_unfooled = (benign_preds == adversary_preds)\n",
    "  for i , unfooled in enumerate(is_unfooled):\n",
    "    if unfooled == 1:\n",
    "      unfooled_histogram[benign_preds[i]] += 1\n",
    "    else:\n",
    "      fooled_histogram[benign_preds[i]] += 1\n",
    "  \n",
    "#   global valid_cnt\n",
    "#   valid_cnt += 1\n",
    "#   if valid_cnt % 10 == 0:\n",
    "#     print_hist(unfooled_histogram, fooled_histogram)\n",
    "    \n",
    "  return (benign_preds != adversary_preds).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LrJhunt5uV8J"
   },
   "outputs": [],
   "source": [
    "if gen_arch == 'targeted':\n",
    "  class FeatureLoss(nn.Module):\n",
    "      def __name__(self):\n",
    "        return \"feature_loss\"\n",
    "\n",
    "      def __init__(self, dis, layers, layer_weights):\n",
    "          super().__init__()\n",
    "\n",
    "          # define generator here \n",
    "          self.dis = dis\n",
    "          self.diversity_layers = layers\n",
    "          self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "          self.weights = layer_weights\n",
    "          self.metric_names = [\"fool_loss\"] #+ [f\"div_loss_{i}\" for i in range(len(layers))] #maybe Gram\n",
    "  #         self.triplet_hooks = hook_outputs([arch.m.features[4]], detach=False)\n",
    "\n",
    "      def make_features(self, x, clone=False):\n",
    "          y = self.dis(x)\n",
    "          return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "\n",
    "      def forward(self, inp, target):\n",
    "        sigma_B, _, _, X_B, B_Y, z = inp\n",
    "\n",
    "        X_A = X_B + sigma_B\n",
    "  #       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "        A_Y, A_feat = self.make_features(X_A)\n",
    "  #       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "        chosen_labels = z.argmax(dim=1)\n",
    "        fooling_loss =  fool_loss(A_Y, chosen_labels)\n",
    "\n",
    "  #       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "  #       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "  #       self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "  #       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "        self.losses = [fooling_loss]\n",
    "        self.metrics = dict(zip(self.metric_names, [fooling_loss]))\n",
    "\n",
    "        return sum(self.losses)\n",
    "\n",
    "      def add_perturbation_shuffled(self, inp, perturbation):\n",
    "  #       j = torch.randperm(inp.shape[0])\n",
    "          j = derangement(inp.shape[0])\n",
    "          return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CMBxuBUXuV8N"
   },
   "outputs": [],
   "source": [
    "def derange(*args):\n",
    "  if len(args) == 0: raise ValueError('shuffle function needs atleast one argument')\n",
    "  deranged_indexes = derangement(args[0].shape[0])\n",
    "  if not all([args[0].shape[0] == arg.shape[0] for arg in args]): \n",
    "    raise ValueError('inputs to shuffle must all have the same 0th dimension')\n",
    "  return [arg[deranged_indexes] for arg in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nelYZyzhuV8R"
   },
   "outputs": [],
   "source": [
    "if gen_arch == 'non-targeted':\n",
    "  class FeatureLoss(nn.Module):\n",
    "      def __name__(self):\n",
    "        return \"feature_loss\"\n",
    "\n",
    "      def __init__(self, dis, layers, layer_weights):\n",
    "          super().__init__()\n",
    "\n",
    "          self.dis = dis\n",
    "          self.diversity_layers = layers\n",
    "          self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "          self.weights = layer_weights\n",
    "\n",
    "  #         self.metric_names = [\"fool_loss\"] + [f\"div_loss_{i}\" for i in range(len(layers))] + ['triplet_loss']# Maybe Gram\n",
    "  #         self.metric_names = [\"div_loss\"] + ['triplet_loss']# Maybe Gram\n",
    "          self.metric_names = [\"fool_loss\"] + ['div_loss']# Maybe Gram\n",
    "          self.triplet_weight = 4.\n",
    "          self.div_weight = 1.\n",
    "          self.fooling_weight = 1.\n",
    "\n",
    "      def make_features(self, x, clone=False):\n",
    "          y = self.dis(x)\n",
    "          return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "\n",
    "      # contrastive loss\n",
    "      def forward(self, inp, target):\n",
    "          sigma_B, sigma_pos, sigma_neg, X_B, z_B = inp\n",
    "\n",
    "          deranged_perturbations, deranged_z_s = derange(sigma_B, z_B)\n",
    "\n",
    "          X_A = X_B + sigma_B\n",
    "          X_S = X_B + deranged_perturbations\n",
    "#           X_A_pos = X_B + sigma_pos\n",
    "#           X_A_neg = X_B + sigma_neg\n",
    "\n",
    "          B_Y, _ = self.make_features(X_B)\n",
    "          A_Y, A_feat = self.make_features(X_A)\n",
    "          _, S_feat = self.make_features(X_S)\n",
    "#           pos_softmax, _ = self.make_features(X_A_pos)\n",
    "#           neg_softmax, _ = self.make_features(X_A_neg)\n",
    "\n",
    "          raw_fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "          weighted_fooling_loss = self.fooling_weight * raw_fooling_loss\n",
    "\n",
    "#           raw_diversity_loss = diversity_loss(A_feat[0], S_feat[0], z_B, deranged_z_s)\n",
    "          raw_diversity_loss = diversity_loss(A_feat[0], S_feat[0])\n",
    "          weighted_diversity_loss = raw_diversity_loss * self.div_weight\n",
    "\n",
    "#           raw_triplet_loss = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "#           weighted_triplet_loss = raw_triplet_loss * self.triplet_weight\n",
    "\n",
    "  #         self.losses = weighted_diversity_losses + [weighted_triplet_loss]\n",
    "  #         raw_losses = raw_diversity_losses + [raw_triplet_loss]\n",
    "\n",
    "          self.losses = [weighted_fooling_loss] + [weighted_diversity_loss] #+ [weighted_triplet_loss]\n",
    "          raw_losses = [raw_fooling_loss] + [raw_diversity_loss] #+ [raw_triplet_loss]\n",
    "\n",
    "  #         self.losses = [fooling_loss] + [weighted_triplet_loss]\n",
    "  #         self.metrics = dict(zip(self.metric_names, [fooling_loss] + [raw_triplet_loss]))\n",
    "\n",
    "          if len(self.metric_names) != len(raw_losses):\n",
    "            raise Exception(\"length of metric names unequals length of losses\")\n",
    "\n",
    "          self.metrics = dict(zip(self.metric_names, raw_losses))\n",
    "          return sum(self.losses)\n",
    "\n",
    "\n",
    "\n",
    "  # #     triplet loss\n",
    "  #     def forward(self, inp, target):\n",
    "  #         sigma_B, sigma_pos, sigma_neg, X_B, _ = inp\n",
    "\n",
    "  #         X_A = self.add_perturbation(X_B, sigma_B) \n",
    "  #         X_A_pos = self.add_perturbation(X_B, sigma_pos)\n",
    "  #         X_A_neg = self.add_perturbation(X_B, sigma_neg) \n",
    "  #         X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "  # #         B_Y, _ = self.make_features(X_B)\n",
    "  #         A_Y, A_feat = self.make_features(X_A)\n",
    "  # #         _, S_feat = self.make_features(X_S)\n",
    "  #         pos_softmax, _ = self.make_features(X_A_pos)\n",
    "  #         neg_softmax, _ = self.make_features(X_A_neg)\n",
    "\n",
    "  # #         raw_fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "  # #         weighted_fooling_loss = self.fooling_weight * raw_fooling_loss\n",
    "\n",
    "  #         raw_diversity_losses = [diversity_loss(a_f, s_f, sigma_B, ) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "  #         weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "  #         raw_triplet_loss = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "  #         weighted_triplet_loss = raw_triplet_loss * self.triplet_weight\n",
    "\n",
    "  #         self.losses = weighted_diversity_losses + [weighted_triplet_loss]\n",
    "  #         raw_losses = raw_diversity_losses + [raw_triplet_loss]\n",
    "\n",
    "  # #         self.losses = [weighted_fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss]\n",
    "  # #         raw_losses = [raw_fooling_loss] + raw_diversity_losses + [raw_triplet_loss]\n",
    "\n",
    "  # #         self.losses = [fooling_loss] + [weighted_triplet_loss]\n",
    "  # #         self.metrics = dict(zip(self.metric_names, [fooling_loss] + [raw_triplet_loss]))\n",
    "\n",
    "  #         if len(self.metric_names) != len(raw_losses):\n",
    "  #           raise Exception(\"length of metric names unequals length of losses\")\n",
    "\n",
    "  #         self.metrics = dict(zip(self.metric_names, raw_losses))\n",
    "  #         return sum(self.losses)\n",
    "\n",
    "\n",
    "  #     #use two types of triplet losses\n",
    "  #     def forward(self, inp, target):\n",
    "  #       sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "  #       X_A = self.add_perturbation(X_B, sigma_B) \n",
    "  #       X_A_pos = self.add_perturbation(X_B, sigma_pos)\n",
    "  #       X_A_neg = self.add_perturbation(X_B, sigma_neg) \n",
    "\n",
    "  #       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "  #       B_Y, _ = self.make_features(X_B)\n",
    "  #       A_Y, A_feat = self.make_features(X_A)\n",
    "  #       _, S_feat = self.make_features(X_S)\n",
    "  #       pos_softmax, _ = self.make_features(X_A_pos)\n",
    "  #       neg_softmax, _ = self.make_features(X_A_neg)\n",
    "\n",
    "  #       fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "  #       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "  #       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "  #       raw_triplet_loss_sm = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "  #       weighted_triplet_loss_sm = raw_triplet_loss_sm * self.triplet_weight_sm\n",
    "\n",
    "  #       raw_triplet_loss_noise = triplet_loss(sigma_B, sigma_pos, sigma_neg, l2_distance, 5.)\n",
    "  #       weighted_triplet_loss_noise = raw_triplet_loss_noise * self.triplet_weight_noise\n",
    "\n",
    "  #       self.losses = [fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss_sm, weighted_triplet_loss_noise] \n",
    "  #       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses + [weighted_triplet_loss_sm, weighted_triplet_loss_noise]))\n",
    "\n",
    "  #       return sum(self.losses)\n",
    "\n",
    "  #     # just fooling and diversity\n",
    "  #     def forward(self, inp, target):\n",
    "  #       sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "  #       X_A = self.add_perturbation(X_B, sigma_B) \n",
    "\n",
    "  #       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "  #       B_Y, _ = self.make_features(X_B)\n",
    "  #       A_Y, A_feat = self.make_features(X_A)\n",
    "  #       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "  #       fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "  #       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "  #       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "  #       self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "  #       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "  #       return sum(self.losses)\n",
    "\n",
    "\n",
    "      def add_perturbation_shuffled(self, inp, perturbation):\n",
    "        j = derangement(inp.shape[0])\n",
    "        return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WNg_ej-1uV8V"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def produce_summary(root_folder, n_files):\n",
    "  def writeline(file, values, fmt_string):\n",
    "    file.write(', '.join(fmt_string.format(v) for v in values) + '\\n')\n",
    "  \n",
    "  last_rows = []\n",
    "  for i in range(n_files):\n",
    "    prefix = '/root/Derakhshani/adversarial/textual_notes/CSVs'\n",
    "    df = pd.read_csv(\"{}/{}/{}.csv\".format(prefix, root_folder, i))\n",
    "    last_rows.append(df.iloc[-1][1:-1].values.tolist())\n",
    "  \n",
    "  last_rows = np.array(last_rows)\n",
    "  \n",
    "  labels = list(df.columns[1:-1])\n",
    "  means = np.mean(last_rows, axis=0).tolist()\n",
    "  outfile = open('{}/{}/summary.txt'.format(prefix, root_folder), 'w+')\n",
    "  outfile.write('means: \\n')\n",
    "  writeline(outfile, labels, '{: >20}')\n",
    "  writeline(outfile, means, '{: >20.3}')\n",
    "  outfile.write('\\n')\n",
    "      \n",
    "  operations = []\n",
    "  for column in df.columns[1:-1]:\n",
    "    if column in ['train_loss', 'valid_loss', 'fool_loss', 'triplet_loss'] or column[:8] == 'div_loss':\n",
    "      operations.append('min')\n",
    "    elif column in ['validation', 'targeted_validation', 'div_metric', 'entropy']:\n",
    "      operations.append('max')\n",
    "    else:\n",
    "      raise ValueError('column {} is not recognized'.format(column))\n",
    "    \n",
    "  results = []\n",
    "  indexes = []\n",
    "  \n",
    "  for i in range(len(operations)):\n",
    "    values = last_rows[:, i]\n",
    "    if operations[i] == 'max': operation = np.max\n",
    "    elif operations[i] == 'min': operation = np.min\n",
    "    result = operation(values)\n",
    "    results.append(result)\n",
    "    indexes.append(values.tolist().index(result))\n",
    "  \n",
    "  outfile.write('bests: \\n')\n",
    "  writeline(outfile, labels, '{: >20}')\n",
    "  writeline(outfile, operations, '{: >20}')\n",
    "  writeline(outfile, results, '{: >20.3}')\n",
    "  writeline(outfile, indexes, '{: >20}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuYuRqozuV8Y"
   },
   "outputs": [],
   "source": [
    "from distutils import dir_util \n",
    "\n",
    "def investigate_initial_settings(n_settings, n_epochs, lr, wd, results_dir):\n",
    "  os.mkdir(env.get_csv_dir() + results_dir)\n",
    "  os.mkdir(env.get_models_dir() + results_dir)\n",
    "  \n",
    "  for setting_ind in range(n_settings):\n",
    "    print(f\"investigation no: {setting_ind}\")\n",
    "    learn = None; gen = None; gc.collect()\n",
    "    gen = Gen(z_dim = z_dim)\n",
    "    init_cnn(gen, True)\n",
    "    \n",
    "    tmp_csv_filename =  env.temp_csv_path + '/' + results_dir + '/' + str(setting_ind)\n",
    "    csv_logger = partial(ImmediateCSVLogger, filename=tmp_csv_filename)\n",
    "\n",
    "    if gen_arch == 'non-targeted':\n",
    "      metrics = [validation]\n",
    "    elif gen_arch == 'targeted':\n",
    "      metrics = [validation, targeted_validation]\n",
    "      \n",
    "    learn = Learner(data, gen, loss_func = feat_loss, metrics=metrics, \n",
    "                    model_dir = env.get_learner_models_dir(), \n",
    "                    callback_fns=[DiversityMetric, LossMetrics, csv_logger])\n",
    "    \n",
    "    saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=model.__name__ + \"-best\")\n",
    "    saver_every_epoch = SaveModelCallback(learn, every='epoch', name=model.__name__)\n",
    "\n",
    "    learn.fit(n_epochs, lr=lr, wd = wd, callbacks=[saver_best, saver_every_epoch])\n",
    "    \n",
    "    shutil.copyfile(tmp_csv_filename + \".csv\", env.get_csv_dir() + results_dir + '/' + str(setting_ind) + '.csv')\n",
    "    \n",
    "    model_dest = env.get_models_dir() + results_dir + '/' + str(setting_ind)\n",
    "    os.mkdir(model_dest)\n",
    "    dir_util.copy_tree(env.data_path/env.get_learner_models_dir(), model_dest)\n",
    "    shutil.rmtree(env.data_path/env.get_learner_models_dir())  \n",
    "    \n",
    "  produce_summary(results_dir, n_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "97oO_lhduV8a"
   },
   "outputs": [],
   "source": [
    "def generate_perturbations(learn, n_perturbations):\n",
    "  initial_training_mode = learn.model.training\n",
    "  gen = learn.model.eval()\n",
    "  perturbations = [gen.generate_single_noise() for _ in range(n_perturbations)]\n",
    "  learn.model.train(initial_training_mode)  \n",
    "  return perturbations\n",
    "\n",
    "def compute_prediction_histogram(learn, perturbation, verbose=False):\n",
    "  pred_hist = [0] * 1000\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 and verbose: print (\"at batch no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbation[None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      pred_hist[pred] += 1\n",
    "  return pred_hist\n",
    "\n",
    "\n",
    "def compute_mean_prediction_histogram(learn, perturbations, verbose=False):\n",
    "  pred_hist = torch.tensor([0] * 1000).detach_()\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 5 == 0 and verbose: print(f\"at batch no {batch_no}\")\n",
    "    for j, perturbation in enumerate(perturbations):\n",
    "      perturbed_batch = batch + perturbation[None]\n",
    "      preds = arch(perturbed_batch).argmax(1)\n",
    "      for pred in preds:\n",
    "        pred_hist[pred] += 1\n",
    "  pred_hist = pred_hist.float() / len(perturbations)\n",
    "  return pred_hist.tolist()\n",
    "\n",
    "\n",
    "def classes_needed_to_reach(percentage, hist):\n",
    "  hist_sum = np.sum(hist)\n",
    "  indexed_hist = [(i, hist_element) for i,hist_element in  \n",
    "                          enumerate(hist)]\n",
    "  sorted_hist = sorted(indexed_hist, key=lambda x: x[1], reverse = True)\n",
    "  \n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = sorted_hist[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / hist_sum) * 100.\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, sorted_hist\n",
    "\n",
    "def diversity(learn, n_perturbations, percentage = 95, verbose = True):\n",
    "  pred_histogram = compute_mean_prediction_histogram(\n",
    "      learn, generate_perturbations(learn, n_perturbations), verbose\n",
    "  )\n",
    "  print(\"finished creating the prediction histogram\")\n",
    "\n",
    "  return classes_needed_to_reach(95, pred_histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2erFgTn8uV8d"
   },
   "outputs": [],
   "source": [
    "class DiversityMetric(LearnerCallback):\n",
    "  _order = -20 # Needs to run before the recorder\n",
    "  \n",
    "  def __init__(self, learn):\n",
    "    super().__init__(learn)\n",
    "    self.average_over = 4\n",
    "    self.n_perturbations = 10\n",
    "    self.percentage = 95\n",
    "  \n",
    "  def on_train_begin(self, **kwargs):\n",
    "    self.learn.recorder.add_metric_names(['div_metric', 'entropy'])\n",
    "    \n",
    "  def on_epoch_begin(self, **kwargs):\n",
    "    global learn\n",
    "    self.perturbations_list = [generate_perturbations(self.learn, self.n_perturbations) \\\n",
    "                          for _ in range(self.average_over)]\n",
    "    self.pred_hist_list = [torch.tensor([0] * 1000).detach_() for _ in range(self.average_over)]\n",
    "    \n",
    "  def on_batch_end(self, last_output, train, **kwargs):\n",
    "    if not train:\n",
    "      images = last_output[3]; assert(images.shape[1:] == (3,224, 224))\n",
    "      for perturbations, pred_hist in zip(self.perturbations_list, self.pred_hist_list):\n",
    "        for j, perturbation in enumerate(perturbations):\n",
    "          perturbed_batch = images + perturbation[None]\n",
    "          preds = arch(perturbed_batch).argmax(1)\n",
    "          for pred in preds:\n",
    "            pred_hist[pred] += 1\n",
    "  \n",
    "  def on_epoch_end(self, last_metrics, **kwargs):\n",
    "    for i in range(len(self.pred_hist_list)):\n",
    "      self.pred_hist_list[i] = (self.pred_hist_list[i].float() / self.n_perturbations).tolist()\n",
    "    \n",
    "    div_metric_list = [classes_needed_to_reach(self.percentage, pred_hist)[0] \\\n",
    "                          for pred_hist in self.pred_hist_list]\n",
    "    entropy_list = [entropy(pred_hist) for pred_hist in self.pred_hist_list]\n",
    "    return add_metrics(last_metrics, [np.mean(div_metric_list), np.mean(entropy_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "itHU97Y_uV8h"
   },
   "outputs": [],
   "source": [
    "class TargetedDiversityMetric(DiversityMetric):\n",
    "    def __init__(self, n_perturbations, percentage):\n",
    "      super().__init__(n_perturbations, percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xnByiCgVuV8t"
   },
   "outputs": [],
   "source": [
    "class FoolingWeightScheduler(LearnerCallback):\n",
    "  def __init__(self, learn: Learner):\n",
    "    super().__init__(learn)\n",
    "    self.weights_history = []\n",
    "    self.fooling_loss_history = []\n",
    "  \n",
    "  def get_metric_value(self, metric_name):\n",
    "    for value, name in zip(self.learn.recorder.metrics[-1],self.learn.recorder.names[3:-1]):\n",
    "      if name == metric_name:\n",
    "        return value\n",
    "    raise ValueError('Could not find {} metric.'.format(metric_name))\n",
    "  \n",
    "  def on_epoch_end(self, last_metrics, **kwargs):\n",
    "    # history keeping\n",
    "    self.weights_history.append((kwargs['epoch'], self.learn.loss_func.fooling_weight))\n",
    "    \n",
    "    # the actual functionality\n",
    "    fooling_loss = self.get_metric_value('fool_loss')\n",
    "    self.fooling_loss_history.append(fooling_loss)\n",
    "    \n",
    "    if len(self.weights_history) < 2:\n",
    "      return\n",
    "    \n",
    "    if self.fooling_loss_history[-1] > self.fooling_loss_history[-2]:\n",
    "      self.learn.loss_func.fooling_weight += 0.3    \n",
    "      print('fooling weight increased to {} at the end of epoch {}'.format(\n",
    "        self.learn.loss_func.fooling_weight, kwargs['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wElU0RSRuV80"
   },
   "outputs": [],
   "source": [
    "class CyclicalLRScheduler(LearnerCallback):\n",
    "  def __init__(self, learn, max_lr, min_lr, cycle_len):\n",
    "    super().__init__(learn)\n",
    "    self.max_lr = max_lr\n",
    "    self.min_lr = min_lr\n",
    "    self.cycle_len = cycle_len\n",
    "    \n",
    "  def on_train_begin(self, **kwargs):\n",
    "    self.n_iter_per_epoch = len(self.learn.data.train_dl)\n",
    "    self.cycle_len_iters = self.cycle_len * self.n_iter_per_epoch\n",
    "    self.learn.opt.lr = self.min_lr\n",
    "    \n",
    "    \n",
    "  def on_batch_end(self, iteration, train, **kwargs):\n",
    "    if train:\n",
    "      cycle_index = iteration % self.cycle_len_iters\n",
    "      half_cycle_len = self.cycle_len_iters / 2\n",
    "\n",
    "      if cycle_index < half_cycle_len:\n",
    "        new_lr = float(self.max_lr - self.min_lr) / half_cycle_len * cycle_index + self.min_lr\n",
    "      else:\n",
    "        new_lr = float(self.min_lr - self.max_lr) / half_cycle_len * (cycle_index - half_cycle_len) + self.max_lr\n",
    "\n",
    "#       print('iter: {}, lr: {}'.format(iteration, new_lr))\n",
    "      self.opt.lr = new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YtQh1qcsuV87"
   },
   "outputs": [],
   "source": [
    "# def get_data(src, path, bs, size):\n",
    "#     data = (src.label_from_func(lambda x: path/x.name)\n",
    "#            .databunch(bs=bs))\n",
    "#     data.c = 3\n",
    "#     return data\n",
    "\n",
    "def get_unet_model(arch):\n",
    "    data_path = env.data_path/\"train\"\n",
    "#     data_path = Path(\"/content/dataset/train\")\n",
    "#     data_path = env.data_path.parent/\"dataset_dummy_denoiser\"\n",
    "    src = ImageImageList.from_folder(data_path).split_none()\n",
    "    data = (src.label_from_func(lambda x: x)\n",
    "            .transform(get_transforms(), size=224)\n",
    "            .databunch(bs = batch_size)\n",
    "           )\n",
    "    data.c = 3\n",
    "    learn = unet_learner(data, arch, norm_type=NormType.Weight)\n",
    "    learn.unfreeze()\n",
    "    return learn.model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denoiser_type = \"online\"\n",
    "denoiser_type = \"offline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fP_fcudauV8-"
   },
   "outputs": [],
   "source": [
    "if denoiser_type == \"online\":\n",
    "  class Denoiser(nn.Module):\n",
    "    def __init__(self, generator):\n",
    "      super().__init__()\n",
    "      self.generator = generator.eval()\n",
    "      requires_grad(self.generator, False)\n",
    "      self.denoiser = get_unet_model(models.resnet18)\n",
    "\n",
    "    def forward(self, input_img):\n",
    "  #     print('defended fw call:')\n",
    "      perturbation = self.generator(input_img)[0].detach()\n",
    "      perturbed_img = (input_img + perturbation).detach()\n",
    "      # self.denoiser(perturbed_img)\n",
    "      predicted_noise = self.generator.output_coeff * torch.tanh(self.denoiser(perturbed_img))\n",
    "      restored_img = perturbed_img - predicted_noise\n",
    "  #     print(('orig label: {} \\npert label: {} \\nrest label: {}\\n' \\\n",
    "  #           'predicted_noise: ({},{}), {} \\nperturbation:({}, {}), {} \\n').format(\n",
    "  #       arch(input_img).argmax(dim=1), arch(perturbed_img).argmax(dim=1), arch(restored_img).argmax(dim=1),\n",
    "  #       predicted_noise.min(), predicted_noise.max(), predicted_noise.shape,\n",
    "  #       perturbation.min(), perturbation.max(), perturbation.shape\n",
    "  #     ))\n",
    "      return restored_img, perturbation, input_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "if denoiser_type == \"offline\":\n",
    "  class Denoiser(nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.denoiser = get_unet_model(models.resnet18)\n",
    "\n",
    "    def forward(self, perturbed_img):\n",
    "      perturbed_img.detach_()\n",
    "      predicted_noise = Gen.output_coeff * torch.tanh(self.denoiser(perturbed_img))\n",
    "      restored_img = perturbed_img - predicted_noise\n",
    "  #     print(('orig label: {} \\npert label: {} \\nrest label: {}\\n' \\\n",
    "  #           'predicted_noise: ({},{}), {} \\nperturbation:({}, {}), {} \\n').format(\n",
    "  #       arch(input_img).argmax(dim=1), arch(perturbed_img).argmax(dim=1), arch(restored_img).argmax(dim=1),\n",
    "  #       predicted_noise.min(), predicted_noise.max(), predicted_noise.shape,\n",
    "  #       perturbation.min(), perturbation.max(), perturbation.shape\n",
    "  #     ))\n",
    "      return restored_img, perturbed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWErjJ0kuV9B"
   },
   "outputs": [],
   "source": [
    "def denoiser_validation(defended_classifier_output, target):\n",
    "  restored_images, _, clean_images = defended_classifier_output\n",
    "  \n",
    "  original_preds = torch.argmax(arch(clean_images), 1)\n",
    "  restored_preds = torch.argmax(arch(restored_images), 1)\n",
    "  \n",
    "  return (original_preds == restored_preds).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pp7UY97FuV9E"
   },
   "outputs": [],
   "source": [
    "dloss_cnt = 0\n",
    "class DenoiserLoss(nn.Module):\n",
    "  def __name__(self):\n",
    "    return \"denoiser_loss\"\n",
    "  \n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.metric_names = [\"denoiser_loss\"]\n",
    "    self.hooks = hook_outputs([arch.m.fc], detach=False)\n",
    "    \n",
    "  def make_features(self, inputs):\n",
    "    y = arch(inputs)\n",
    "    return y, self.hooks.stored[0]\n",
    "       \n",
    "  def forward(self, defended_classifier_output, target):\n",
    "    restored_img, perturbations, clean_images = defended_classifier_output\n",
    "    \n",
    "    restored_pred, restored_feats = self.make_features(restored_img)\n",
    "    original_pred, original_feats = self.make_features(clean_images)\n",
    "#     print('restored: {}, orig: {}'.format(restored_feats.shape, original_feats.shape))\n",
    "#     winning_class = original_prediction_vector.argmax(dim=1)\n",
    "#     denoiser_loss = F.cross_entropy(restored_prediction_vector, winning_class)\n",
    "\n",
    "    denoiser_loss = F.l1_loss(restored_feats, original_feats)\n",
    "    # denoiser_loss = -1 * torch.mean(F.cosine_similarity(restored_pred, original_pred, dim=1))\n",
    "    \n",
    "    global dloss_cnt\n",
    "    if dloss_cnt % 10 == 0: print('denoser_loss:', denoiser_loss)\n",
    "    dloss_cnt+=1\n",
    "  \n",
    "    self.losses = [denoiser_loss]\n",
    "    self.metrics = dict(zip(self.metric_names, self.losses))\n",
    "    \n",
    "    return denoiser_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tltucTv2ep9-"
   },
   "outputs": [],
   "source": [
    "# mode = 'sanity_check'\n",
    "mode = 'normal'\n",
    "# mode = 'div_metric_calc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLbgT7gAuV9K"
   },
   "outputs": [],
   "source": [
    "model = models.resnet50\n",
    "# model = models.resnet152\n",
    "# model = models.vgg16_bn\n",
    "# model = torchvision.models.googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SO1h55obXzOv",
    "outputId": "2a8f20da-21e6-420f-de2f-16e9ec691764"
   },
   "outputs": [],
   "source": [
    "if mode == \"normal\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "elif mode == \"sanity_check\":\n",
    "  env.load_dataset('dataset_sanity_check_small', 'dataset_sanity_check_small')  \n",
    "  env.set_data_path('dataset_sanity_check_small')\n",
    "elif mode == \"div_metric_calc\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "  env.load_test_dataset(str(env.data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "id": "koaQZmjMom7w",
    "outputId": "99dc7e0d-d739-4cc2-b0dd-85041e5bc0ca"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "gpu_flag = True\n",
    "nag_util.batch_size = batch_size; nag_util.gpu_flag = gpu_flag;\n",
    "tfms = get_transforms(do_flip=False, max_rotate=0)\n",
    "data = (ImageList.from_folder(env.data_path)\n",
    "        .split_by_folder(valid=('test' if mode == 'div_metric_calc' else 'valid'))\n",
    "        .label_from_folder()\n",
    "        .transform(tfms, size=224)\n",
    "        .databunch(bs=batch_size, num_workers=1)\n",
    "        .normalize(imagenet_stats))\n",
    "\n",
    "# data.show_batch(rows=2, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_perturbed = (ImageList.from_folder('/home/mlcm-deep/AhmadPourihosseini/NAG/datasets/perturbed_resnet50_72')\n",
    "        .split_by_folder(valid=('test' if mode == 'div_metric_calc' else 'valid'))\n",
    "        .label_from_folder()\n",
    "        .transform(None, size=224)\n",
    "        .databunch(bs=batch_size, num_workers=1)\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO CONTINUE:\n",
    "import os\n",
    "\n",
    "def denoiser_label_func(x):\n",
    "  dataset_type = x.parts[-3]\n",
    "  class_name = x.parts[-2]\n",
    "  img_name = os.path.splitext(x.parts[-1])[0]\n",
    "  return os.path.join(*x.parts[:-4], 'dataset', dataset_type, class_name, img_name + '.jpg')\n",
    "\n",
    "data_denoiser = (ImageImageList.from_folder('/home/mlcm-deep/AhmadPourihosseini/NAG/datasets/perturbed_resnet50_72')\n",
    "        .split_by_folder(valid=('test' if mode == 'div_metric_calc' else 'valid'))\n",
    "        .label_from_func(denoiser_label_func)\n",
    "        .transform(None, size=224, tfm_y=True)\n",
    "        .databunch(bs=batch_size, num_workers=1)\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j = 0\n",
    "# for i, (d, t) in enumerate(data.fix_dl):\n",
    "#   print(data.fix_ds.items[i], t[0])\n",
    "#   Image(nag_util.denormalize(d)).show()\n",
    "#   j += 1\n",
    "#   if j == 20:\n",
    "#     break\n",
    "  \n",
    "\n",
    "# # for d, t in data_denoiser.train_dl:\n",
    "# #   im1 = Image(d[0])\n",
    "# #   im2 = Image(t[0])\n",
    "# #   im1.show(), im2.show()\n",
    "# #   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test site:\n",
    "# l = Learner(data_perturbed, model(True), metrics = [accuracy])\n",
    "# l.validate(data_perturbed.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = Learner(data, model(True), metrics = [accuracy])\n",
    "# l.validate(data.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rename_imagenet_folders(root_folder):\n",
    "#     name_map = {}\n",
    "#     with open('/home/mlcm-deep/AhmadPourihosseini/NAG/nag-public/NAG-11May-beforeDenoiser/imagenet_clsidx_to_id.txt', 'r') as f:\n",
    "#         for line in f:\n",
    "#             cls_idx, name_id = line.strip().split(' ')\n",
    "#             name_map[int(cls_idx)] = name_id\n",
    "\n",
    "#     for idx, name in name_map.items():\n",
    "#       if Path('{}/{}'.format(root_folder, idx)).exists():\n",
    "#         os.rename('{}/{}'.format(root_folder, idx), '{}/{}'.format(root_folder, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename_imagenet_folders('/home/mlcm-deep/AhmadPourihosseini/NAG/datasets/perturbed/valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDBkRV8yovwV"
   },
   "outputs": [],
   "source": [
    "if gen_arch == \"non-targeted\":\n",
    "  z_dim = 10\n",
    "elif gen_arch == \"targeted\":\n",
    "  z_dim = 1000\n",
    "  \n",
    "arch = SoftmaxWrapper(model(pretrained=True).cuda().eval())\n",
    "nag_util.arch = arch\n",
    "requires_grad(arch, False)\n",
    "\n",
    "# vgg:\n",
    "# layers = []\n",
    "# blocks = [i-1 for i,o in enumerate(children(arch.features)) if isinstance(o, nn.MaxPool2d)]\n",
    "# layers = [arch.features[i] for i in blocks]\n",
    "# layer_weights = [1] * len(layers)\n",
    "\n",
    "layers = [\n",
    "    arch.softmax\n",
    "]\n",
    "\n",
    "layer_weights = [1.] * len(layers)\n",
    "\n",
    "# inception:\n",
    "# layers = [\n",
    "#     arch.Conv2d_1a_3x3,\n",
    "#     arch.Mixed_6e,\n",
    "#     arch.Mixed_7a,\n",
    "#     arch.fc    \n",
    "# ]\n",
    "# layer_weights = [1.0/4.0] * len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qd9gXUy_ovww"
   },
   "outputs": [],
   "source": [
    "feat_loss = FeatureLoss(arch, layers, layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZJe5W7IfuV9n"
   },
   "outputs": [],
   "source": [
    "class LRAnneal(LearnerCallback):\n",
    "  _order = -20 # Needs to run before the recorder\n",
    "  \n",
    "  def __init__(self, learn, final_value):\n",
    "    super().__init__(learn)\n",
    "    self.final_value = final_value\n",
    "  \n",
    "  def on_train_begin(self, **kwargs):\n",
    "    self.initial_value = self.opt.lr\n",
    "    self.learn.recorder.add_metric_names(['lr'])\n",
    "  \n",
    "  def on_epoch_end(self, epoch, n_epochs, last_metrics, **kwargs):\n",
    "    self.opt.lr = annealing_linear(self.initial_value, self.final_value, float(epoch) / n_epochs)\n",
    "    return add_metrics(last_metrics, self.opt.lr)\n",
    "  \n",
    "# class LRMonitor(LearnerCallBack):\n",
    "#   def __init__(self, learn):\n",
    "#     super().__init__(learn)\n",
    "#     self.name = 'lr'\n",
    "    \n",
    "#   def on_epoch_end(self, last_metrics, **kwargs):\n",
    "#     return add_metrics(last_metrics, self.opt.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N80J714juV9q"
   },
   "outputs": [],
   "source": [
    "# env.save_filename = 'resnet50_65' #resnet50_64\n",
    "# env.save_filename = 'resnet50_17'\n",
    "env.save_filename = 'unet_resnet50_72_1'\n",
    "\n",
    "if Path(env.get_csv_path() + '.csv').exists(): raise FileExistsError(\"csv_path already exists\")\n",
    "if Path(env.get_models_path()).exists(): raise FileExistsError(\"models_path already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9J20CBLS8S9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models_directory returned is:  models/738\n"
     ]
    }
   ],
   "source": [
    "learn = None; gen = None; gc.collect()\n",
    "csv_logger = partial(ImmediateCSVLogger, filename= env.temp_csv_path + '/' + env.save_filename)\n",
    "gen = Gen(z_dim=z_dim)\n",
    "init_cnn(gen, True)\n",
    "\n",
    "if gen_arch == 'non-targeted':\n",
    "  metrics = [validation]\n",
    "elif gen_arch == 'targeted':\n",
    "  metrics = [validation, targeted_validation]\n",
    "    \n",
    "gen_learn = Learner(data, gen, loss_func = feat_loss,\n",
    "                    model_dir = env.get_learner_models_dir(),\n",
    "                    metrics=metrics, callback_fns=[DiversityMetric, LossMetrics, csv_logger])\n",
    "\n",
    "# learn = Learner(data, Gen(z_dim=10), loss_func = feat_loss, metrics=[validation], callback_fns=LossMetrics, opt_func = optim.SGD)\n",
    "# learn = Learner(data, Gen(z_dim=z_dim), loss_func = feat_loss, metrics=[validation], callback_fns=[LossMetrics, DiversityWeightsScheduler])\n",
    "\n",
    "# load_starting_point(learn, model.__name__, z_dim)\n",
    "# random_seed(42, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0wOZYzOHDEdB",
    "outputId": "7436dbf1-33ad-4c5d-a652-73f10e59a024"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (9000 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "n02643566,n02643566,n02643566,n02643566,n02643566\n",
       "Path: /home/mlcm-deep/AhmadPourihosseini/NAG/datasets/dataset;\n",
       "\n",
       "Valid: LabelList (1000 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "n02643566,n02091244,n04367480,n03877472,n03710637\n",
       "Path: /home/mlcm-deep/AhmadPourihosseini/NAG/datasets/dataset;\n",
       "\n",
       "Test: None, model=Gen(\n",
       "  (z_): Linear(in_features=1000, out_features=10240, bias=True)\n",
       "  (BN_): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (CT2d_1): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(640, 448, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (BN2d): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_2): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(448, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_3): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_4): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_5): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_6): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_7): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(64, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (BN2d): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FeatureLoss(\n",
       "  (dis): SoftmaxWrapper(\n",
       "    (m): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "), metrics=[<function validation at 0x7f67406f2ea0>, <function targeted_validation at 0x7f672c2cb048>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/mlcm-deep/AhmadPourihosseini/NAG/datasets/dataset'), model_dir='models/738', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), <class '__main__.DiversityMetric'>, <class 'fastai.callbacks.loss_metrics.LossMetrics'>, functools.partial(<class 'nag_util.ImmediateCSVLogger'>, filename='/home/mlcm-deep/AhmadPourihosseini/NAG/temp/unet_resnet50_72_1')], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): Linear(in_features=1000, out_features=10240, bias=True)\n",
       "  (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ConvTranspose2d(640, 448, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): ConvTranspose2d(448, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (8): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (12): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ConvTranspose2d(64, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (15): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !cp \"/content/gdrive/My Drive/DL/models/vgg16_12-last.pth\"  \"/content/\"\n",
    "# learn.load('/content/vgg16_12-last')\n",
    "\n",
    "# load_filename = 'resnet50-11_39'\n",
    "# load_filename = 'resnet50_startpoint_0'\n",
    "# load_filename = 'googlenet_13_attempt5/googlenet_13_attempt5_29'\n",
    "# load_filename = 'investigate_googlenet_4/1/googlenet_1'\n",
    "# load_filename = 'vgg16_30/vgg16_30_69'\n",
    "# load_filename = 'vgg16_12-last'\n",
    "# load_filename = 'googlenet_25_attempt2/googlenet_25_attempt2_399'\n",
    "# load_filename = 'googlenet_28_labelset1/googlenet_28_labelset1_59'\n",
    "# load_filename = 'googlenet_32/googlenet_32_99' # targ\n",
    "# load_filename = 'investigate_googlenet_5/1/googlenet_2'\n",
    "# load_filename = None\n",
    "\n",
    "# load_filename = 'googlenet_15/googlenet_15_89' # non-targ\n",
    "load_filename = 'resnet50_72/resnet50_72_99'\n",
    "# load_filename = 'vgg16_38/vgg16_38_99'\n",
    "\n",
    "gen_learn.load(env.get_models_dir() + '/' + load_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test/transform site:\n",
    "# gen_learn.validate(metrics=[validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_dataset(env: Env, trainOrValid: str, model: torch.nn.Module, dest_folder: str):\n",
    "  data = (ImageList.from_folder(env.data_path)\n",
    "          .split_by_folder(valid='valid')\n",
    "          .label_from_folder()\n",
    "          .transform(None, size=224, resize_method=ResizeMethod.SQUISH)\n",
    "          .databunch(bs=32, num_workers=1)\n",
    "          .normalize(imagenet_stats))\n",
    "\n",
    "  if trainOrValid == 'train':\n",
    "    dataloader = data.train_dl.new(shuffle=False)\n",
    "    items = data.train_ds.items\n",
    "  elif trainOrValid == 'valid':\n",
    "    dataloader = data.valid_dl\n",
    "    items = data.valid_ds.items\n",
    "  else:\n",
    "    raise ValueError('invalid value for trainOrValid')\n",
    "\n",
    "  for i in range(1000):\n",
    "    os.makedirs(\"{}/{}\".format(dest_folder, data.classes[i]))\n",
    "\n",
    "  model = model.eval()\n",
    "  # next_filename_per_label = [0] * 1000\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i, (batch, target_labels) in enumerate(dataloader):\n",
    "      perturbation = model(batch)[0]\n",
    "      perturbed_batch = batch + perturbation\n",
    "      # The image is denormalized with imagenet stats from the range [-2.5, 2.5] to the range [0.,1.].\n",
    "      denormalized_batch = torch.clamp(nag_util.denormalize(perturbed_batch), 0., 1.)\n",
    "      for j, image_data in enumerate(denormalized_batch):\n",
    "        image =Image(image_data)\n",
    "        target_label = target_labels[j].item()\n",
    "        print(items[i * batch_size + j])\n",
    "        image.save(\"{}/{}/{}.png\".format(dest_folder, data.classes[target_label],\n",
    "                                         os.path.splitext(items[i * batch_size + j].parts[-1])[0]))\n",
    "        # image.save(\"{}/{}/{}.png\".format(dest_folder, data.classes[target_label], next_filename_per_label[target_label]))\n",
    "        # next_filename_per_label[target_label] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perturb_dataset(env, 'valid', gen_learn.model, '/home/mlcm-deep/AhmadPourihosseini/NAG/transformed_resnet50_valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 768
    },
    "colab_type": "code",
    "id": "0m6LtLWzuV9y",
    "outputId": "6f8ebffe-9167-48e8-d216-120c36db97ea",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models_directory returned is:  models/738\n"
     ]
    }
   ],
   "source": [
    "env.set_data_path('perturbed_resnet50_72')\n",
    "denoiser_learn = Learner(data, Denoiser(), loss_func = DenoiserLoss(), \n",
    "                        model_dir = env.get_learner_models_dir(),\n",
    "                        metrics=denoiser_validation, callback_fns = [LossMetrics, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FpkcsitGuV91"
   },
   "outputs": [],
   "source": [
    "# load_filename = 'unet_3xxx/unet_3xxx_4'\n",
    "# denoiser_learn.load(env.get_models_dir() + '/' + load_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67ASjRfAuV95"
   },
   "outputs": [],
   "source": [
    "# class ArtWrapper(nn.Module):\n",
    "#   def __init__(self, wrap_around):\n",
    "#     super().__init__()\n",
    "#     self.wrap_around = wrap_around\n",
    "    \n",
    "#   def forward(self, inp):\n",
    "# #     print('passed input:', inp.shape)\n",
    "# #     print('converted: ', inp[None].shape)\n",
    "#     out = self.wrap_around(inp[None])\n",
    "# #     print('out shape: ', out[0].shape)\n",
    "#     final_out = arch(out[0]).squeeze()\n",
    "# #     print('final out shape: ', final_out.shape)\n",
    "#     return final_out\n",
    "\n",
    "# class BasicClassifierLoss(nn.Module):\n",
    "#   def __init__(self):\n",
    "#       super().__init__()\n",
    "    \n",
    "#   def forward(self, inp, target):\n",
    "# #     return -1 * torch.log(torch.gather(inp, 1, target.unsqueeze(0)))\n",
    "#     return -1 * torch.log(torch.gather(inp, 0, target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQK3R_ibuV97",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from art import metrics\n",
    "# from art.classifiers import PyTorchClassifier\n",
    "\n",
    "# x =  denoiser_learn.data.train_ds[0][0].data.numpy()\n",
    "# y =  denoiser_learn.data.train_ds[0][1]\n",
    "# one_hot = [0.] * 1000\n",
    "# one_hot[363] = 1.\n",
    "# y = np.array(one_hot)\n",
    "# print(x.shape, y.shape)\n",
    "\n",
    "# wrapped_model = ArtWrapper(denoiser_learn.model)\n",
    "# # WARNING: omitting clip values\n",
    "# art_classifier = PyTorchClassifier(model = wrapped_model, loss = BasicClassifierLoss(), #feat_loss\n",
    "#                                    optimizer = denoiser_learn.opt, input_shape = x.shape,\n",
    "#                                    nb_classes = 1000)\n",
    "\n",
    "# # metrics.empirical_robustness(art_classifier, x, 'fgsm')\n",
    "# metrics.loss_sensitivity(art_classifier, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7wNKhI_quV9_"
   },
   "outputs": [],
   "source": [
    "# learn.lr_find(1e-6, 100)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2UGfpASmuV-K"
   },
   "outputs": [],
   "source": [
    "# results_dir = 'investigate_googlenet_4'\n",
    "# investigate_initial_settings(4, 2, lr = 1e-2, wd = 0.0, results_dir = results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yEUcLde8uV-P"
   },
   "outputs": [],
   "source": [
    "# shutil.rmtree(env.get_models_dir() + results_dir)\n",
    "# shutil.rmtree(env.get_csv_dir() + results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "0375tKI1uV-X",
    "outputId": "97d6f817-8c3c-4d6e-94ef-28e580def24d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the selected settings are : \n",
      "\tmode: normal \n",
      "\tnetw-under-attack: resnet50 \n",
      "\tload filename: resnet50_72/resnet50_72_99 \n",
      "      \tsave filename: unet_resnet50_72_1\n",
      "\tmetric names: ['fool_loss']\n",
      "\tgen arch: targeted\n",
      "\n",
      "please MAKE SURE that the config is correct.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  load_filename\n",
    "except NameError:\n",
    "  load_filename = None\n",
    "\n",
    "print(\"the selected settings are : \")\n",
    "print('''\\tmode: {} \\n\\tnetw-under-attack: {} \\n\\tload filename: {} \n",
    "      \\tsave filename: {}\\n\\tmetric names: {}\\n\\tgen arch: {}\\n'''.format(\n",
    "      mode, model.__name__, load_filename , env.save_filename, feat_loss.metric_names,\n",
    "      gen_arch\n",
    "))\n",
    "print(\"please MAKE SURE that the config is correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHpw9RJkuV-d"
   },
   "outputs": [],
   "source": [
    "# learn.lr_find(1e-6, 100)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Bttj6lx7uV-g",
    "outputId": "a1380bcb-116d-42c5-b8b9-660325776041"
   },
   "outputs": [],
   "source": [
    "# RUN SITE\n",
    "if 'x' in env.save_filename and mode != 'sanity_check':\n",
    "  raise ValueError('save_filename contains x')\n",
    "\n",
    "saver_best = SaveModelCallback(denoiser_learn, every='improvement', monitor='validation', name=env.save_filename + \"-best\")\n",
    "saver_every_epoch = SaveModelCallback(denoiser_learn, every='epoch', name=env.save_filename)\n",
    "# fooling_weight_scheduler = FoolingWeightScheduler(learn)\n",
    "# lr_anneal = LRAnneal(learn, 1e-4)\n",
    "# file_ctrl = FileControl(learn, '/root/Derakhshani/adversarial/ctrl', learn.model)\n",
    "# cyclical_sched = CyclicalLRScheduler(learn, 3e-2, 6e-4, 4)\n",
    "\n",
    "callbacks = [saver_best, saver_every_epoch]\n",
    "# callbacks.append(lr_anneal)\n",
    "# callbacks.append(fooling_weight_scheduler)\n",
    "# callbacks.append(file_ctrl)\n",
    "# callbacks.append(cyclical_sched)\n",
    "\n",
    "denoiser_learn.fit(5, lr=1e-2, callbacks=callbacks)\n",
    "\n",
    "# with Hooks(gen, append_stats_normal) as hooks:\n",
    "#   learn.fit(1, lr=5e-03, wd = 0., callbacks=[saver_best, saver_every_epoch])\n",
    "\n",
    "# for i in range(10):\n",
    "#   learn.fit_one_cycle(7, wd = 0.,max_lr=1., div_factor = 1000.) \n",
    "\n",
    "shutil.copyfile(env.temp_csv_path + '/' + env.save_filename + \".csv\", env.get_csv_path() + '.csv')\n",
    "shutil.copytree(env.data_path/env.get_learner_models_dir(), env.get_models_path())\n",
    "shutil.rmtree(env.data_path/env.get_learner_models_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0KeEvBoXuV-k",
    "outputId": "96c51d41-9601-4c26-c25e-4bc440ea044f"
   },
   "outputs": [],
   "source": [
    "# denoiser_learn.recorder.plot_losses()\n",
    "# denoiser_learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "loqpojwPuV-r",
    "outputId": "e9a36e41-004f-4910-94c2-4d5b8971c391"
   },
   "outputs": [],
   "source": [
    "# cleanup cell:\n",
    "# from cleanup import cleanup_models_folder\n",
    "# cleanup_models_folder('/root/Derakhshani/adversarial/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c_DunxBouV_C"
   },
   "outputs": [],
   "source": [
    "# zero precentage:\n",
    "fig,axes = plt.subplots(len(hooks),1, figsize=(30,30))\n",
    "for ax,h in zip(axes.flatten(), hooks):\n",
    "    ax.plot(get_min(h))\n",
    "    ax.set_ylim(0,1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MO2fZ-hSSUzJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# z1 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "# z2 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "z1 = torch.tensor([0.8, -0.5] * 5).cuda()\n",
    "z2 = torch.tensor([-1.] * 10).cuda()\n",
    "print(\"z1: \", z1)\n",
    "print(\"z2: \", z2)\n",
    "print(\"distance: \", torch.norm(z1-z2,p=2))\n",
    "model = learn.model.eval()\n",
    "\n",
    "z_s = interpolate(z1, z2, 0.1)\n",
    "print(len(z_s))\n",
    "\n",
    "for i,z in enumerate(z_s):\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n",
    "  #img.save('./pics/' + str(i) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DunkwbVVuV_I"
   },
   "outputs": [],
   "source": [
    "# idea : have 200 noises (1 for each class), then start iterating the dataset, and for each image, randomly apply one noise and record the result\n",
    "def targeted_diversity(learn, n_perturbations = 200, percentage = 95):\n",
    "  model = learn.model.eval()\n",
    "\n",
    "  one_hot_conditions = [torch.empty(z_dim).uniform_(0,1).cuda().detach() for _ in range(n_perturbations)]\n",
    "#   for i in range(z_dim):\n",
    "#     one_hot_conditions[i][i] = 1.\n",
    "\n",
    "  perturbations = [model.forward_single_z(z) for z in one_hot_conditions]\n",
    "\n",
    "  hist = [0.] * 1000\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 : print(\"at batch_no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbations[np.random.randint(0,len(perturbations))][None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      hist[pred] += 1\n",
    "\n",
    "  pred_histogram_sum = np.sum(hist)\n",
    "  indexed_pred_histogram = [(i, hist_element) for i,hist_element in  \n",
    "                            enumerate(hist)]\n",
    "\n",
    "  indexed_pred_histogram.sort(key=lambda x: x[1], reverse = True)\n",
    "\n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = indexed_pred_histogram[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / pred_histogram_sum) * 100.\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, indexed_pred_histogram\n",
    "\n",
    "def targeted_diversity_average(learn, n_perturbations = 200, percentage = 95, average_over = 4):\n",
    "  results = []\n",
    "  for i in range(average_over):\n",
    "    n, _ = targeted_diversity(learn, n_perturbations, percentage)\n",
    "    print(f'done with the {i}th calculation: {n}')\n",
    "    results.append(n)\n",
    "  return np.mean(results)\n",
    "\n",
    "def diversity_average(learn, n_perturbations = 10, percentage = 95, average_over = 4):\n",
    "  results = []\n",
    "  for i in range(average_over):\n",
    "    n, _ = diversity(learn, n_perturbations, percentage, verbose = False)\n",
    "    print(f'done with the {i}th calculation: {n}')\n",
    "    results.append(n)\n",
    "  return np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8DDlxWkUuV_J",
    "outputId": "08883a1e-4f25-4560-caa7-3a442a45e3d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 5\n",
      "at batch no 10\n",
      "at batch no 15\n",
      "at batch no 20\n",
      "at batch no 25\n",
      "at batch no 30\n",
      "at batch no 35\n",
      "at batch no 40\n",
      "at batch no 45\n",
      "at batch no 50\n",
      "at batch no 55\n",
      "at batch no 60\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(566,\n",
       " [(794, 50.099998474121094),\n",
       "  (599, 21.100000381469727),\n",
       "  (668, 20.200000762939453),\n",
       "  (904, 15.0),\n",
       "  (973, 13.600000381469727),\n",
       "  (490, 12.899999618530273),\n",
       "  (39, 12.699999809265137),\n",
       "  (770, 12.600000381469727),\n",
       "  (741, 11.300000190734863),\n",
       "  (828, 11.100000381469727),\n",
       "  (109, 9.199999809265137),\n",
       "  (556, 8.600000381469727),\n",
       "  (489, 8.399999618530273),\n",
       "  (955, 8.399999618530273),\n",
       "  (887, 8.300000190734863),\n",
       "  (669, 8.100000381469727),\n",
       "  (84, 7.400000095367432),\n",
       "  (855, 7.0),\n",
       "  (538, 6.800000190734863),\n",
       "  (108, 6.599999904632568),\n",
       "  (124, 6.0),\n",
       "  (397, 5.900000095367432),\n",
       "  (48, 5.800000190734863),\n",
       "  (61, 5.5),\n",
       "  (777, 4.900000095367432),\n",
       "  (721, 4.800000190734863),\n",
       "  (401, 4.5),\n",
       "  (971, 4.400000095367432),\n",
       "  (893, 4.300000190734863),\n",
       "  (857, 4.099999904632568),\n",
       "  (591, 4.0),\n",
       "  (711, 3.9000000953674316),\n",
       "  (709, 3.799999952316284),\n",
       "  (455, 3.700000047683716),\n",
       "  (55, 3.5999999046325684),\n",
       "  (414, 3.5),\n",
       "  (906, 3.5),\n",
       "  (151, 3.4000000953674316),\n",
       "  (389, 3.4000000953674316),\n",
       "  (406, 3.4000000953674316),\n",
       "  (865, 3.299999952316284),\n",
       "  (750, 3.200000047683716),\n",
       "  (581, 3.0),\n",
       "  (0, 2.9000000953674316),\n",
       "  (1, 2.9000000953674316),\n",
       "  (476, 2.9000000953674316),\n",
       "  (915, 2.9000000953674316),\n",
       "  (363, 2.799999952316284),\n",
       "  (837, 2.799999952316284),\n",
       "  (982, 2.799999952316284),\n",
       "  (110, 2.700000047683716),\n",
       "  (46, 2.5999999046325684),\n",
       "  (62, 2.5999999046325684),\n",
       "  (593, 2.5999999046325684),\n",
       "  (640, 2.5999999046325684),\n",
       "  (735, 2.5999999046325684),\n",
       "  (819, 2.5999999046325684),\n",
       "  (94, 2.5),\n",
       "  (126, 2.5),\n",
       "  (431, 2.5),\n",
       "  (611, 2.4000000953674316),\n",
       "  (864, 2.4000000953674316),\n",
       "  (868, 2.4000000953674316),\n",
       "  (222, 2.299999952316284),\n",
       "  (558, 2.299999952316284),\n",
       "  (572, 2.299999952316284),\n",
       "  (850, 2.299999952316284),\n",
       "  (115, 2.200000047683716),\n",
       "  (410, 2.200000047683716),\n",
       "  (698, 2.200000047683716),\n",
       "  (763, 2.200000047683716),\n",
       "  (772, 2.200000047683716),\n",
       "  (779, 2.200000047683716),\n",
       "  (97, 2.0999999046325684),\n",
       "  (238, 2.0999999046325684),\n",
       "  (440, 2.0999999046325684),\n",
       "  (447, 2.0999999046325684),\n",
       "  (464, 2.0999999046325684),\n",
       "  (872, 2.0999999046325684),\n",
       "  (898, 2.0999999046325684),\n",
       "  (68, 2.0),\n",
       "  (118, 2.0),\n",
       "  (182, 2.0),\n",
       "  (242, 2.0),\n",
       "  (292, 2.0),\n",
       "  (393, 2.0),\n",
       "  (520, 2.0),\n",
       "  (621, 2.0),\n",
       "  (60, 1.899999976158142),\n",
       "  (188, 1.899999976158142),\n",
       "  (189, 1.899999976158142),\n",
       "  (192, 1.899999976158142),\n",
       "  (342, 1.899999976158142),\n",
       "  (411, 1.899999976158142),\n",
       "  (472, 1.899999976158142),\n",
       "  (570, 1.899999976158142),\n",
       "  (619, 1.899999976158142),\n",
       "  (620, 1.899999976158142),\n",
       "  (724, 1.899999976158142),\n",
       "  (791, 1.899999976158142),\n",
       "  (800, 1.899999976158142),\n",
       "  (199, 1.7999999523162842),\n",
       "  (290, 1.7999999523162842),\n",
       "  (348, 1.7999999523162842),\n",
       "  (423, 1.7999999523162842),\n",
       "  (761, 1.7999999523162842),\n",
       "  (762, 1.7999999523162842),\n",
       "  (870, 1.7999999523162842),\n",
       "  (920, 1.7999999523162842),\n",
       "  (72, 1.7000000476837158),\n",
       "  (128, 1.7000000476837158),\n",
       "  (155, 1.7000000476837158),\n",
       "  (195, 1.7000000476837158),\n",
       "  (334, 1.7000000476837158),\n",
       "  (526, 1.7000000476837158),\n",
       "  (547, 1.7000000476837158),\n",
       "  (671, 1.7000000476837158),\n",
       "  (725, 1.7000000476837158),\n",
       "  (775, 1.7000000476837158),\n",
       "  (783, 1.7000000476837158),\n",
       "  (824, 1.7000000476837158),\n",
       "  (871, 1.7000000476837158),\n",
       "  (123, 1.600000023841858),\n",
       "  (375, 1.600000023841858),\n",
       "  (457, 1.600000023841858),\n",
       "  (468, 1.600000023841858),\n",
       "  (492, 1.600000023841858),\n",
       "  (508, 1.600000023841858),\n",
       "  (550, 1.600000023841858),\n",
       "  (562, 1.600000023841858),\n",
       "  (803, 1.600000023841858),\n",
       "  (817, 1.600000023841858),\n",
       "  (953, 1.600000023841858),\n",
       "  (963, 1.600000023841858),\n",
       "  (107, 1.5),\n",
       "  (119, 1.5),\n",
       "  (202, 1.5),\n",
       "  (230, 1.5),\n",
       "  (420, 1.5),\n",
       "  (477, 1.5),\n",
       "  (564, 1.5),\n",
       "  (748, 1.5),\n",
       "  (815, 1.5),\n",
       "  (907, 1.5),\n",
       "  (76, 1.399999976158142),\n",
       "  (83, 1.399999976158142),\n",
       "  (193, 1.399999976158142),\n",
       "  (231, 1.399999976158142),\n",
       "  (274, 1.399999976158142),\n",
       "  (293, 1.399999976158142),\n",
       "  (305, 1.399999976158142),\n",
       "  (314, 1.399999976158142),\n",
       "  (336, 1.399999976158142),\n",
       "  (552, 1.399999976158142),\n",
       "  (565, 1.399999976158142),\n",
       "  (579, 1.399999976158142),\n",
       "  (597, 1.399999976158142),\n",
       "  (624, 1.399999976158142),\n",
       "  (679, 1.399999976158142),\n",
       "  (784, 1.399999976158142),\n",
       "  (786, 1.399999976158142),\n",
       "  (801, 1.399999976158142),\n",
       "  (891, 1.399999976158142),\n",
       "  (902, 1.399999976158142),\n",
       "  (33, 1.2999999523162842),\n",
       "  (57, 1.2999999523162842),\n",
       "  (96, 1.2999999523162842),\n",
       "  (120, 1.2999999523162842),\n",
       "  (247, 1.2999999523162842),\n",
       "  (275, 1.2999999523162842),\n",
       "  (328, 1.2999999523162842),\n",
       "  (355, 1.2999999523162842),\n",
       "  (409, 1.2999999523162842),\n",
       "  (441, 1.2999999523162842),\n",
       "  (505, 1.2999999523162842),\n",
       "  (586, 1.2999999523162842),\n",
       "  (588, 1.2999999523162842),\n",
       "  (633, 1.2999999523162842),\n",
       "  (638, 1.2999999523162842),\n",
       "  (821, 1.2999999523162842),\n",
       "  (842, 1.2999999523162842),\n",
       "  (892, 1.2999999523162842),\n",
       "  (58, 1.2000000476837158),\n",
       "  (65, 1.2000000476837158),\n",
       "  (171, 1.2000000476837158),\n",
       "  (204, 1.2000000476837158),\n",
       "  (205, 1.2000000476837158),\n",
       "  (219, 1.2000000476837158),\n",
       "  (307, 1.2000000476837158),\n",
       "  (308, 1.2000000476837158),\n",
       "  (327, 1.2000000476837158),\n",
       "  (331, 1.2000000476837158),\n",
       "  (353, 1.2000000476837158),\n",
       "  (366, 1.2000000476837158),\n",
       "  (443, 1.2000000476837158),\n",
       "  (454, 1.2000000476837158),\n",
       "  (474, 1.2000000476837158),\n",
       "  (495, 1.2000000476837158),\n",
       "  (563, 1.2000000476837158),\n",
       "  (574, 1.2000000476837158),\n",
       "  (602, 1.2000000476837158),\n",
       "  (641, 1.2000000476837158),\n",
       "  (654, 1.2000000476837158),\n",
       "  (658, 1.2000000476837158),\n",
       "  (781, 1.2000000476837158),\n",
       "  (823, 1.2000000476837158),\n",
       "  (848, 1.2000000476837158),\n",
       "  (854, 1.2000000476837158),\n",
       "  (858, 1.2000000476837158),\n",
       "  (883, 1.2000000476837158),\n",
       "  (24, 1.100000023841858),\n",
       "  (41, 1.100000023841858),\n",
       "  (51, 1.100000023841858),\n",
       "  (113, 1.100000023841858),\n",
       "  (116, 1.100000023841858),\n",
       "  (236, 1.100000023841858),\n",
       "  (249, 1.100000023841858),\n",
       "  (253, 1.100000023841858),\n",
       "  (271, 1.100000023841858),\n",
       "  (281, 1.100000023841858),\n",
       "  (300, 1.100000023841858),\n",
       "  (310, 1.100000023841858),\n",
       "  (317, 1.100000023841858),\n",
       "  (318, 1.100000023841858),\n",
       "  (319, 1.100000023841858),\n",
       "  (350, 1.100000023841858),\n",
       "  (381, 1.100000023841858),\n",
       "  (395, 1.100000023841858),\n",
       "  (396, 1.100000023841858),\n",
       "  (491, 1.100000023841858),\n",
       "  (496, 1.100000023841858),\n",
       "  (497, 1.100000023841858),\n",
       "  (506, 1.100000023841858),\n",
       "  (507, 1.100000023841858),\n",
       "  (527, 1.100000023841858),\n",
       "  (544, 1.100000023841858),\n",
       "  (575, 1.100000023841858),\n",
       "  (609, 1.100000023841858),\n",
       "  (626, 1.100000023841858),\n",
       "  (759, 1.100000023841858),\n",
       "  (787, 1.100000023841858),\n",
       "  (796, 1.100000023841858),\n",
       "  (806, 1.100000023841858),\n",
       "  (820, 1.100000023841858),\n",
       "  (834, 1.100000023841858),\n",
       "  (863, 1.100000023841858),\n",
       "  (882, 1.100000023841858),\n",
       "  (894, 1.100000023841858),\n",
       "  (918, 1.100000023841858),\n",
       "  (981, 1.100000023841858),\n",
       "  (996, 1.100000023841858),\n",
       "  (7, 1.0),\n",
       "  (8, 1.0),\n",
       "  (10, 1.0),\n",
       "  (15, 1.0),\n",
       "  (17, 1.0),\n",
       "  (19, 1.0),\n",
       "  (25, 1.0),\n",
       "  (28, 1.0),\n",
       "  (37, 1.0),\n",
       "  (42, 1.0),\n",
       "  (45, 1.0),\n",
       "  (49, 1.0),\n",
       "  (52, 1.0),\n",
       "  (53, 1.0),\n",
       "  (63, 1.0),\n",
       "  (70, 1.0),\n",
       "  (75, 1.0),\n",
       "  (79, 1.0),\n",
       "  (86, 1.0),\n",
       "  (87, 1.0),\n",
       "  (90, 1.0),\n",
       "  (91, 1.0),\n",
       "  (92, 1.0),\n",
       "  (98, 1.0),\n",
       "  (102, 1.0),\n",
       "  (105, 1.0),\n",
       "  (117, 1.0),\n",
       "  (134, 1.0),\n",
       "  (139, 1.0),\n",
       "  (140, 1.0),\n",
       "  (141, 1.0),\n",
       "  (144, 1.0),\n",
       "  (158, 1.0),\n",
       "  (161, 1.0),\n",
       "  (162, 1.0),\n",
       "  (163, 1.0),\n",
       "  (164, 1.0),\n",
       "  (173, 1.0),\n",
       "  (183, 1.0),\n",
       "  (186, 1.0),\n",
       "  (196, 1.0),\n",
       "  (197, 1.0),\n",
       "  (198, 1.0),\n",
       "  (206, 1.0),\n",
       "  (213, 1.0),\n",
       "  (218, 1.0),\n",
       "  (228, 1.0),\n",
       "  (235, 1.0),\n",
       "  (260, 1.0),\n",
       "  (273, 1.0),\n",
       "  (284, 1.0),\n",
       "  (289, 1.0),\n",
       "  (291, 1.0),\n",
       "  (301, 1.0),\n",
       "  (304, 1.0),\n",
       "  (306, 1.0),\n",
       "  (312, 1.0),\n",
       "  (313, 1.0),\n",
       "  (316, 1.0),\n",
       "  (321, 1.0),\n",
       "  (323, 1.0),\n",
       "  (337, 1.0),\n",
       "  (347, 1.0),\n",
       "  (360, 1.0),\n",
       "  (376, 1.0),\n",
       "  (378, 1.0),\n",
       "  (387, 1.0),\n",
       "  (392, 1.0),\n",
       "  (398, 1.0),\n",
       "  (417, 1.0),\n",
       "  (425, 1.0),\n",
       "  (428, 1.0),\n",
       "  (429, 1.0),\n",
       "  (433, 1.0),\n",
       "  (445, 1.0),\n",
       "  (451, 1.0),\n",
       "  (483, 1.0),\n",
       "  (488, 1.0),\n",
       "  (498, 1.0),\n",
       "  (518, 1.0),\n",
       "  (528, 1.0),\n",
       "  (530, 1.0),\n",
       "  (531, 1.0),\n",
       "  (533, 1.0),\n",
       "  (566, 1.0),\n",
       "  (580, 1.0),\n",
       "  (608, 1.0),\n",
       "  (612, 1.0),\n",
       "  (616, 1.0),\n",
       "  (625, 1.0),\n",
       "  (629, 1.0),\n",
       "  (637, 1.0),\n",
       "  (645, 1.0),\n",
       "  (646, 1.0),\n",
       "  (651, 1.0),\n",
       "  (655, 1.0),\n",
       "  (661, 1.0),\n",
       "  (684, 1.0),\n",
       "  (687, 1.0),\n",
       "  (691, 1.0),\n",
       "  (692, 1.0),\n",
       "  (694, 1.0),\n",
       "  (716, 1.0),\n",
       "  (719, 1.0),\n",
       "  (734, 1.0),\n",
       "  (738, 1.0),\n",
       "  (746, 1.0),\n",
       "  (753, 1.0),\n",
       "  (768, 1.0),\n",
       "  (793, 1.0),\n",
       "  (802, 1.0),\n",
       "  (816, 1.0),\n",
       "  (826, 1.0),\n",
       "  (830, 1.0),\n",
       "  (831, 1.0),\n",
       "  (847, 1.0),\n",
       "  (873, 1.0),\n",
       "  (884, 1.0),\n",
       "  (905, 1.0),\n",
       "  (923, 1.0),\n",
       "  (932, 1.0),\n",
       "  (934, 1.0),\n",
       "  (937, 1.0),\n",
       "  (939, 1.0),\n",
       "  (944, 1.0),\n",
       "  (946, 1.0),\n",
       "  (957, 1.0),\n",
       "  (959, 1.0),\n",
       "  (984, 1.0),\n",
       "  (985, 1.0),\n",
       "  (987, 1.0),\n",
       "  (989, 1.0),\n",
       "  (992, 1.0),\n",
       "  (9, 0.8999999761581421),\n",
       "  (18, 0.8999999761581421),\n",
       "  (21, 0.8999999761581421),\n",
       "  (36, 0.8999999761581421),\n",
       "  (47, 0.8999999761581421),\n",
       "  (85, 0.8999999761581421),\n",
       "  (135, 0.8999999761581421),\n",
       "  (142, 0.8999999761581421),\n",
       "  (145, 0.8999999761581421),\n",
       "  (176, 0.8999999761581421),\n",
       "  (187, 0.8999999761581421),\n",
       "  (263, 0.8999999761581421),\n",
       "  (266, 0.8999999761581421),\n",
       "  (303, 0.8999999761581421),\n",
       "  (315, 0.8999999761581421),\n",
       "  (326, 0.8999999761581421),\n",
       "  (365, 0.8999999761581421),\n",
       "  (384, 0.8999999761581421),\n",
       "  (390, 0.8999999761581421),\n",
       "  (391, 0.8999999761581421),\n",
       "  (408, 0.8999999761581421),\n",
       "  (459, 0.8999999761581421),\n",
       "  (463, 0.8999999761581421),\n",
       "  (482, 0.8999999761581421),\n",
       "  (503, 0.8999999761581421),\n",
       "  (534, 0.8999999761581421),\n",
       "  (535, 0.8999999761581421),\n",
       "  (555, 0.8999999761581421),\n",
       "  (577, 0.8999999761581421),\n",
       "  (635, 0.8999999761581421),\n",
       "  (663, 0.8999999761581421),\n",
       "  (674, 0.8999999761581421),\n",
       "  (702, 0.8999999761581421),\n",
       "  (703, 0.8999999761581421),\n",
       "  (712, 0.8999999761581421),\n",
       "  (743, 0.8999999761581421),\n",
       "  (757, 0.8999999761581421),\n",
       "  (764, 0.8999999761581421),\n",
       "  (776, 0.8999999761581421),\n",
       "  (788, 0.8999999761581421),\n",
       "  (808, 0.8999999761581421),\n",
       "  (832, 0.8999999761581421),\n",
       "  (833, 0.8999999761581421),\n",
       "  (900, 0.8999999761581421),\n",
       "  (968, 0.8999999761581421),\n",
       "  (988, 0.8999999761581421),\n",
       "  (997, 0.8999999761581421),\n",
       "  (38, 0.800000011920929),\n",
       "  (77, 0.800000011920929),\n",
       "  (93, 0.800000011920929),\n",
       "  (100, 0.800000011920929),\n",
       "  (160, 0.800000011920929),\n",
       "  (246, 0.800000011920929),\n",
       "  (254, 0.800000011920929),\n",
       "  (280, 0.800000011920929),\n",
       "  (294, 0.800000011920929),\n",
       "  (344, 0.800000011920929),\n",
       "  (372, 0.800000011920929),\n",
       "  (377, 0.800000011920929),\n",
       "  (399, 0.800000011920929),\n",
       "  (407, 0.800000011920929),\n",
       "  (415, 0.800000011920929),\n",
       "  (430, 0.800000011920929),\n",
       "  (432, 0.800000011920929),\n",
       "  (439, 0.800000011920929),\n",
       "  (458, 0.800000011920929),\n",
       "  (514, 0.800000011920929),\n",
       "  (545, 0.800000011920929),\n",
       "  (546, 0.800000011920929),\n",
       "  (595, 0.800000011920929),\n",
       "  (603, 0.800000011920929),\n",
       "  (643, 0.800000011920929),\n",
       "  (644, 0.800000011920929),\n",
       "  (672, 0.800000011920929),\n",
       "  (696, 0.800000011920929),\n",
       "  (729, 0.800000011920929),\n",
       "  (732, 0.800000011920929),\n",
       "  (809, 0.800000011920929),\n",
       "  (822, 0.800000011920929),\n",
       "  (829, 0.800000011920929),\n",
       "  (838, 0.800000011920929),\n",
       "  (843, 0.800000011920929),\n",
       "  (852, 0.800000011920929),\n",
       "  (885, 0.800000011920929),\n",
       "  (889, 0.800000011920929),\n",
       "  (901, 0.800000011920929),\n",
       "  (956, 0.800000011920929),\n",
       "  (34, 0.699999988079071),\n",
       "  (50, 0.699999988079071),\n",
       "  (99, 0.699999988079071),\n",
       "  (112, 0.699999988079071),\n",
       "  (168, 0.699999988079071),\n",
       "  (184, 0.699999988079071),\n",
       "  (214, 0.699999988079071),\n",
       "  (216, 0.699999988079071),\n",
       "  (217, 0.699999988079071),\n",
       "  (232, 0.699999988079071),\n",
       "  (270, 0.699999988079071),\n",
       "  (320, 0.699999988079071),\n",
       "  (330, 0.699999988079071),\n",
       "  (335, 0.699999988079071),\n",
       "  (345, 0.699999988079071),\n",
       "  (361, 0.699999988079071),\n",
       "  (388, 0.699999988079071),\n",
       "  (412, 0.699999988079071),\n",
       "  (512, 0.699999988079071),\n",
       "  (561, 0.699999988079071),\n",
       "  (576, 0.699999988079071),\n",
       "  (632, 0.699999988079071),\n",
       "  (695, 0.699999988079071),\n",
       "  (805, 0.699999988079071),\n",
       "  (879, 0.699999988079071),\n",
       "  (886, 0.699999988079071),\n",
       "  (952, 0.699999988079071),\n",
       "  (32, 0.6000000238418579),\n",
       "  (88, 0.6000000238418579),\n",
       "  (146, 0.6000000238418579),\n",
       "  (148, 0.6000000238418579),\n",
       "  (209, 0.6000000238418579),\n",
       "  (211, 0.6000000238418579),\n",
       "  (322, 0.6000000238418579),\n",
       "  (340, 0.6000000238418579),\n",
       "  (343, 0.6000000238418579),\n",
       "  (442, 0.6000000238418579),\n",
       "  (450, 0.6000000238418579),\n",
       "  (470, 0.6000000238418579),\n",
       "  (532, 0.6000000238418579),\n",
       "  (539, 0.6000000238418579),\n",
       "  (554, 0.6000000238418579),\n",
       "  (560, 0.6000000238418579),\n",
       "  (571, 0.6000000238418579),\n",
       "  (584, 0.6000000238418579),\n",
       "  (589, 0.6000000238418579),\n",
       "  (656, 0.6000000238418579),\n",
       "  (699, 0.6000000238418579),\n",
       "  (736, 0.6000000238418579),\n",
       "  (754, 0.6000000238418579),\n",
       "  (758, 0.6000000238418579),\n",
       "  (765, 0.6000000238418579),\n",
       "  (790, 0.6000000238418579),\n",
       "  (888, 0.6000000238418579),\n",
       "  (890, 0.6000000238418579),\n",
       "  (972, 0.6000000238418579),\n",
       "  (979, 0.6000000238418579),\n",
       "  (12, 0.5),\n",
       "  (16, 0.5),\n",
       "  (22, 0.5),\n",
       "  (143, 0.5),\n",
       "  (157, 0.5),\n",
       "  (166, 0.5),\n",
       "  (203, 0.5),\n",
       "  (212, 0.5),\n",
       "  (221, 0.5),\n",
       "  (224, 0.5),\n",
       "  (229, 0.5),\n",
       "  (234, 0.5),\n",
       "  (237, 0.5),\n",
       "  (252, 0.5),\n",
       "  (276, 0.5),\n",
       "  (282, 0.5),\n",
       "  (288, 0.5),\n",
       "  (295, 0.5),\n",
       "  (296, 0.5),\n",
       "  (309, 0.5),\n",
       "  (329, 0.5),\n",
       "  (402, 0.5),\n",
       "  (413, 0.5),\n",
       "  (436, 0.5),\n",
       "  (444, 0.5),\n",
       "  (448, 0.5),\n",
       "  (471, 0.5),\n",
       "  (540, 0.5),\n",
       "  (604, 0.5),\n",
       "  (639, 0.5),\n",
       "  (647, 0.5),\n",
       "  (683, 0.5),\n",
       "  (697, 0.5),\n",
       "  (715, 0.5),\n",
       "  (752, 0.5),\n",
       "  (755, 0.5),\n",
       "  (795, 0.5),\n",
       "  (811, 0.5),\n",
       "  (839, 0.5),\n",
       "  (853, 0.5),\n",
       "  (862, 0.5),\n",
       "  (877, 0.5),\n",
       "  (897, 0.5),\n",
       "  (910, 0.5),\n",
       "  (911, 0.5),\n",
       "  (927, 0.5),\n",
       "  (977, 0.5),\n",
       "  (20, 0.4000000059604645),\n",
       "  (29, 0.4000000059604645),\n",
       "  (69, 0.4000000059604645),\n",
       "  (122, 0.4000000059604645),\n",
       "  (125, 0.4000000059604645),\n",
       "  (130, 0.4000000059604645),\n",
       "  (132, 0.4000000059604645),\n",
       "  (178, 0.4000000059604645),\n",
       "  (180, 0.4000000059604645),\n",
       "  (256, 0.4000000059604645),\n",
       "  (264, 0.4000000059604645),\n",
       "  (298, 0.4000000059604645),\n",
       "  (370, 0.4000000059604645),\n",
       "  (419, 0.4000000059604645),\n",
       "  (424, 0.4000000059604645),\n",
       "  (480, 0.4000000059604645),\n",
       "  (484, 0.4000000059604645),\n",
       "  (509, 0.4000000059604645),\n",
       "  (511, 0.4000000059604645),\n",
       "  (537, 0.4000000059604645),\n",
       "  (582, 0.4000000059604645),\n",
       "  (587, 0.4000000059604645),\n",
       "  (590, 0.4000000059604645),\n",
       "  (606, 0.4000000059604645),\n",
       "  (615, 0.4000000059604645),\n",
       "  (652, 0.4000000059604645),\n",
       "  (670, 0.4000000059604645),\n",
       "  (689, 0.4000000059604645),\n",
       "  (700, 0.4000000059604645),\n",
       "  (707, 0.4000000059604645),\n",
       "  (720, 0.4000000059604645),\n",
       "  (727, 0.4000000059604645),\n",
       "  (745, 0.4000000059604645),\n",
       "  (804, 0.4000000059604645),\n",
       "  (844, 0.4000000059604645),\n",
       "  (866, 0.4000000059604645),\n",
       "  (878, 0.4000000059604645),\n",
       "  (881, 0.4000000059604645),\n",
       "  (896, 0.4000000059604645),\n",
       "  (925, 0.4000000059604645),\n",
       "  (2, 0.30000001192092896),\n",
       "  (5, 0.30000001192092896),\n",
       "  (74, 0.30000001192092896),\n",
       "  (78, 0.30000001192092896),\n",
       "  (81, 0.30000001192092896),\n",
       "  (89, 0.30000001192092896),\n",
       "  (101, 0.30000001192092896),\n",
       "  (136, 0.30000001192092896),\n",
       "  (138, 0.30000001192092896),\n",
       "  (169, 0.30000001192092896),\n",
       "  (190, 0.30000001192092896),\n",
       "  (207, 0.30000001192092896),\n",
       "  (208, 0.30000001192092896),\n",
       "  (215, 0.30000001192092896),\n",
       "  (243, 0.30000001192092896),\n",
       "  (250, 0.30000001192092896),\n",
       "  (285, 0.30000001192092896),\n",
       "  (286, 0.30000001192092896),\n",
       "  (302, 0.30000001192092896),\n",
       "  (332, 0.30000001192092896),\n",
       "  (341, 0.30000001192092896),\n",
       "  (357, 0.30000001192092896),\n",
       "  (364, 0.30000001192092896),\n",
       "  (369, 0.30000001192092896),\n",
       "  (386, 0.30000001192092896),\n",
       "  (403, 0.30000001192092896),\n",
       "  (438, 0.30000001192092896),\n",
       "  (456, 0.30000001192092896),\n",
       "  (461, 0.30000001192092896),\n",
       "  (487, 0.30000001192092896),\n",
       "  (523, 0.30000001192092896),\n",
       "  (541, 0.30000001192092896),\n",
       "  (542, 0.30000001192092896),\n",
       "  (607, 0.30000001192092896),\n",
       "  (613, 0.30000001192092896),\n",
       "  (636, 0.30000001192092896),\n",
       "  (650, 0.30000001192092896),\n",
       "  (704, 0.30000001192092896),\n",
       "  (740, 0.30000001192092896),\n",
       "  (818, 0.30000001192092896),\n",
       "  (867, 0.30000001192092896),\n",
       "  (6, 0.20000000298023224),\n",
       "  (11, 0.20000000298023224),\n",
       "  (14, 0.20000000298023224),\n",
       "  (23, 0.20000000298023224),\n",
       "  (35, 0.20000000298023224),\n",
       "  (40, 0.20000000298023224),\n",
       "  (44, 0.20000000298023224),\n",
       "  (56, 0.20000000298023224),\n",
       "  (71, 0.20000000298023224),\n",
       "  (114, 0.20000000298023224),\n",
       "  (159, 0.20000000298023224),\n",
       "  (201, 0.20000000298023224),\n",
       "  (223, 0.20000000298023224),\n",
       "  (241, 0.20000000298023224),\n",
       "  (248, 0.20000000298023224),\n",
       "  (261, 0.20000000298023224),\n",
       "  (265, 0.20000000298023224),\n",
       "  (269, 0.20000000298023224),\n",
       "  (272, 0.20000000298023224),\n",
       "  (352, 0.20000000298023224),\n",
       "  (394, 0.20000000298023224),\n",
       "  (453, 0.20000000298023224),\n",
       "  (486, 0.20000000298023224),\n",
       "  (502, 0.20000000298023224),\n",
       "  (513, 0.20000000298023224),\n",
       "  (516, 0.20000000298023224),\n",
       "  (524, 0.20000000298023224),\n",
       "  (529, 0.20000000298023224),\n",
       "  (567, 0.20000000298023224),\n",
       "  (592, 0.20000000298023224),\n",
       "  (601, 0.20000000298023224),\n",
       "  (614, 0.20000000298023224),\n",
       "  (634, 0.20000000298023224),\n",
       "  (642, 0.20000000298023224),\n",
       "  (649, 0.20000000298023224),\n",
       "  (662, 0.20000000298023224),\n",
       "  (664, 0.20000000298023224),\n",
       "  (667, 0.20000000298023224),\n",
       "  (678, 0.20000000298023224),\n",
       "  (680, 0.20000000298023224),\n",
       "  (688, 0.20000000298023224),\n",
       "  (706, 0.20000000298023224),\n",
       "  (751, 0.20000000298023224),\n",
       "  (766, 0.20000000298023224),\n",
       "  (773, 0.20000000298023224),\n",
       "  (799, 0.20000000298023224),\n",
       "  (827, 0.20000000298023224),\n",
       "  (859, 0.20000000298023224),\n",
       "  (962, 0.20000000298023224),\n",
       "  (976, 0.20000000298023224),\n",
       "  (978, 0.20000000298023224),\n",
       "  (994, 0.20000000298023224),\n",
       "  (4, 0.10000000149011612),\n",
       "  (31, 0.10000000149011612),\n",
       "  (67, 0.10000000149011612),\n",
       "  (95, 0.10000000149011612),\n",
       "  (121, 0.10000000149011612),\n",
       "  (150, 0.10000000149011612),\n",
       "  (170, 0.10000000149011612),\n",
       "  (181, 0.10000000149011612),\n",
       "  (226, 0.10000000149011612),\n",
       "  (227, 0.10000000149011612),\n",
       "  (245, 0.10000000149011612),\n",
       "  (257, 0.10000000149011612),\n",
       "  (259, 0.10000000149011612),\n",
       "  (267, 0.10000000149011612),\n",
       "  (279, 0.10000000149011612),\n",
       "  (297, 0.10000000149011612),\n",
       "  (311, 0.10000000149011612),\n",
       "  (324, 0.10000000149011612),\n",
       "  (354, 0.10000000149011612),\n",
       "  (358, 0.10000000149011612),\n",
       "  (359, 0.10000000149011612),\n",
       "  (362, 0.10000000149011612),\n",
       "  (379, 0.10000000149011612),\n",
       "  (380, 0.10000000149011612),\n",
       "  (382, 0.10000000149011612),\n",
       "  (383, 0.10000000149011612),\n",
       "  (385, 0.10000000149011612),\n",
       "  (400, 0.10000000149011612),\n",
       "  (422, 0.10000000149011612),\n",
       "  (434, 0.10000000149011612),\n",
       "  (452, 0.10000000149011612),\n",
       "  (501, 0.10000000149011612),\n",
       "  (517, 0.10000000149011612),\n",
       "  (568, 0.10000000149011612),\n",
       "  (578, 0.10000000149011612),\n",
       "  (585, 0.10000000149011612),\n",
       "  (605, 0.10000000149011612),\n",
       "  (618, 0.10000000149011612),\n",
       "  (628, 0.10000000149011612),\n",
       "  (657, 0.10000000149011612),\n",
       "  (665, 0.10000000149011612),\n",
       "  (676, 0.10000000149011612),\n",
       "  (685, 0.10000000149011612),\n",
       "  (690, 0.10000000149011612),\n",
       "  (701, 0.10000000149011612),\n",
       "  (710, 0.10000000149011612),\n",
       "  (749, 0.10000000149011612),\n",
       "  (771, 0.10000000149011612),\n",
       "  (774, 0.10000000149011612),\n",
       "  (778, 0.10000000149011612),\n",
       "  (782, 0.10000000149011612),\n",
       "  (797, 0.10000000149011612),\n",
       "  (814, 0.10000000149011612),\n",
       "  (825, 0.10000000149011612),\n",
       "  (849, 0.10000000149011612),\n",
       "  (869, 0.10000000149011612),\n",
       "  (874, 0.10000000149011612),\n",
       "  (875, 0.10000000149011612),\n",
       "  (903, 0.10000000149011612),\n",
       "  (919, 0.10000000149011612),\n",
       "  (921, 0.10000000149011612),\n",
       "  (933, 0.10000000149011612),\n",
       "  (938, 0.10000000149011612),\n",
       "  (943, 0.10000000149011612),\n",
       "  (949, 0.10000000149011612),\n",
       "  (983, 0.10000000149011612),\n",
       "  (990, 0.10000000149011612),\n",
       "  (3, 0.0),\n",
       "  (13, 0.0),\n",
       "  (26, 0.0),\n",
       "  (27, 0.0),\n",
       "  (30, 0.0),\n",
       "  (43, 0.0),\n",
       "  (54, 0.0),\n",
       "  (59, 0.0),\n",
       "  (64, 0.0),\n",
       "  (66, 0.0),\n",
       "  (73, 0.0),\n",
       "  (80, 0.0),\n",
       "  (82, 0.0),\n",
       "  (103, 0.0),\n",
       "  (104, 0.0),\n",
       "  (106, 0.0),\n",
       "  (111, 0.0),\n",
       "  (127, 0.0),\n",
       "  (129, 0.0),\n",
       "  (131, 0.0),\n",
       "  (133, 0.0),\n",
       "  (137, 0.0),\n",
       "  (147, 0.0),\n",
       "  (149, 0.0),\n",
       "  (152, 0.0),\n",
       "  (153, 0.0),\n",
       "  (154, 0.0),\n",
       "  (156, 0.0),\n",
       "  (165, 0.0),\n",
       "  (167, 0.0),\n",
       "  (172, 0.0),\n",
       "  (174, 0.0),\n",
       "  (175, 0.0),\n",
       "  (177, 0.0),\n",
       "  (179, 0.0),\n",
       "  (185, 0.0),\n",
       "  (191, 0.0),\n",
       "  (194, 0.0),\n",
       "  (200, 0.0),\n",
       "  (210, 0.0),\n",
       "  (220, 0.0),\n",
       "  (225, 0.0),\n",
       "  (233, 0.0),\n",
       "  (239, 0.0),\n",
       "  (240, 0.0),\n",
       "  (244, 0.0),\n",
       "  (251, 0.0),\n",
       "  (255, 0.0),\n",
       "  (258, 0.0),\n",
       "  (262, 0.0),\n",
       "  (268, 0.0),\n",
       "  (277, 0.0),\n",
       "  (278, 0.0),\n",
       "  (283, 0.0),\n",
       "  (287, 0.0),\n",
       "  (299, 0.0),\n",
       "  (325, 0.0),\n",
       "  (333, 0.0),\n",
       "  (338, 0.0),\n",
       "  (339, 0.0),\n",
       "  (346, 0.0),\n",
       "  (349, 0.0),\n",
       "  (351, 0.0),\n",
       "  (356, 0.0),\n",
       "  (367, 0.0),\n",
       "  (368, 0.0),\n",
       "  (371, 0.0),\n",
       "  (373, 0.0),\n",
       "  (374, 0.0),\n",
       "  (404, 0.0),\n",
       "  (405, 0.0),\n",
       "  (416, 0.0),\n",
       "  (418, 0.0),\n",
       "  (421, 0.0),\n",
       "  (426, 0.0),\n",
       "  (427, 0.0),\n",
       "  (435, 0.0),\n",
       "  (437, 0.0),\n",
       "  (446, 0.0),\n",
       "  (449, 0.0),\n",
       "  (460, 0.0),\n",
       "  (462, 0.0),\n",
       "  (465, 0.0),\n",
       "  (466, 0.0),\n",
       "  (467, 0.0),\n",
       "  (469, 0.0),\n",
       "  (473, 0.0),\n",
       "  (475, 0.0),\n",
       "  (478, 0.0),\n",
       "  (479, 0.0),\n",
       "  (481, 0.0),\n",
       "  (485, 0.0),\n",
       "  (493, 0.0),\n",
       "  (494, 0.0),\n",
       "  (499, 0.0),\n",
       "  (500, 0.0),\n",
       "  (504, 0.0),\n",
       "  (510, 0.0),\n",
       "  (515, 0.0),\n",
       "  (519, 0.0),\n",
       "  (521, 0.0),\n",
       "  (522, 0.0),\n",
       "  (525, 0.0),\n",
       "  (536, 0.0),\n",
       "  (543, 0.0),\n",
       "  (548, 0.0),\n",
       "  (549, 0.0),\n",
       "  (551, 0.0),\n",
       "  (553, 0.0),\n",
       "  (557, 0.0),\n",
       "  (559, 0.0),\n",
       "  (569, 0.0),\n",
       "  (573, 0.0),\n",
       "  (583, 0.0),\n",
       "  (594, 0.0),\n",
       "  (596, 0.0),\n",
       "  (598, 0.0),\n",
       "  (600, 0.0),\n",
       "  (610, 0.0),\n",
       "  (617, 0.0),\n",
       "  (622, 0.0),\n",
       "  (623, 0.0),\n",
       "  (627, 0.0),\n",
       "  (630, 0.0),\n",
       "  (631, 0.0),\n",
       "  (648, 0.0),\n",
       "  (653, 0.0),\n",
       "  (659, 0.0),\n",
       "  (660, 0.0),\n",
       "  (666, 0.0),\n",
       "  (673, 0.0),\n",
       "  (675, 0.0),\n",
       "  (677, 0.0),\n",
       "  (681, 0.0),\n",
       "  (682, 0.0),\n",
       "  (686, 0.0),\n",
       "  (693, 0.0),\n",
       "  (705, 0.0),\n",
       "  (708, 0.0),\n",
       "  (713, 0.0),\n",
       "  (714, 0.0),\n",
       "  (717, 0.0),\n",
       "  (718, 0.0),\n",
       "  (722, 0.0),\n",
       "  (723, 0.0),\n",
       "  (726, 0.0),\n",
       "  (728, 0.0),\n",
       "  (730, 0.0),\n",
       "  (731, 0.0),\n",
       "  (733, 0.0),\n",
       "  (737, 0.0),\n",
       "  (739, 0.0),\n",
       "  (742, 0.0),\n",
       "  (744, 0.0),\n",
       "  (747, 0.0),\n",
       "  (756, 0.0),\n",
       "  (760, 0.0),\n",
       "  (767, 0.0),\n",
       "  (769, 0.0),\n",
       "  (780, 0.0),\n",
       "  (785, 0.0),\n",
       "  (789, 0.0),\n",
       "  (792, 0.0),\n",
       "  (798, 0.0),\n",
       "  (807, 0.0),\n",
       "  (810, 0.0),\n",
       "  (812, 0.0),\n",
       "  (813, 0.0),\n",
       "  (835, 0.0),\n",
       "  (836, 0.0),\n",
       "  (840, 0.0),\n",
       "  (841, 0.0),\n",
       "  (845, 0.0),\n",
       "  (846, 0.0),\n",
       "  (851, 0.0),\n",
       "  (856, 0.0),\n",
       "  (860, 0.0),\n",
       "  (861, 0.0),\n",
       "  (876, 0.0),\n",
       "  (880, 0.0),\n",
       "  (895, 0.0),\n",
       "  (899, 0.0),\n",
       "  (908, 0.0),\n",
       "  (909, 0.0),\n",
       "  (912, 0.0),\n",
       "  (913, 0.0),\n",
       "  (914, 0.0),\n",
       "  (916, 0.0),\n",
       "  (917, 0.0),\n",
       "  (922, 0.0),\n",
       "  (924, 0.0),\n",
       "  (926, 0.0),\n",
       "  (928, 0.0),\n",
       "  (929, 0.0),\n",
       "  (930, 0.0),\n",
       "  (931, 0.0),\n",
       "  (935, 0.0),\n",
       "  (936, 0.0),\n",
       "  (940, 0.0),\n",
       "  (941, 0.0),\n",
       "  (942, 0.0),\n",
       "  (945, 0.0),\n",
       "  (947, 0.0),\n",
       "  (948, 0.0),\n",
       "  (950, 0.0),\n",
       "  (951, 0.0),\n",
       "  (954, 0.0),\n",
       "  (958, 0.0),\n",
       "  (960, 0.0),\n",
       "  (961, 0.0),\n",
       "  (964, 0.0),\n",
       "  (965, 0.0),\n",
       "  (966, 0.0),\n",
       "  (967, 0.0),\n",
       "  (969, 0.0),\n",
       "  (970, 0.0),\n",
       "  (974, 0.0),\n",
       "  (975, 0.0),\n",
       "  (980, 0.0),\n",
       "  (986, 0.0),\n",
       "  (991, 0.0),\n",
       "  (993, 0.0),\n",
       "  (995, 0.0),\n",
       "  (998, 0.0),\n",
       "  (999, 0.0)])"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bTtz6Su-uV_M",
    "outputId": "f4cc9f8a-3c7c-4176-bfe4-a455364552f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch_no 0\n",
      "done with the 0th calculation: 717\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 718\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 720\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 718\n",
      "result for n_pert: 10 is 718.25\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 713\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 724\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 715\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 719\n",
      "result for n_pert: 20 is 717.75\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 712\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 722\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 720\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 717\n",
      "result for n_pert: 30 is 717.75\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 716\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 717\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 713\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 714\n",
      "result for n_pert: 40 is 715.0\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 717\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 723\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 722\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 720\n",
      "result for n_pert: 50 is 720.5\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 719\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 712\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 712\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 720\n",
      "result for n_pert: 60 is 715.75\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 723\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 721\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 722\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 711\n",
      "result for n_pert: 70 is 719.25\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 715\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 715\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 717\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 715\n",
      "result for n_pert: 80 is 715.5\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 721\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 716\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 716\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 714\n",
      "result for n_pert: 90 is 716.75\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 718\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 719\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 724\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 723\n",
      "result for n_pert: 100 is 721.0\n"
     ]
    }
   ],
   "source": [
    "results_1 = []\n",
    "for n_pert in range(10, 110, 10):\n",
    "  n = targeted_diversity_average(learn, n_pert, 95, 4)\n",
    "  print(f'result for n_pert: {n_pert} is {n}')\n",
    "  results_1.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8h1LVsfwuV_w"
   },
   "outputs": [],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U6dnR1-3uV_4",
    "outputId": "5f9f3716-2737-444a-c2c5-5e19f6956e3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f59cf2afe48>]"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_hist = sorted(hist, key=lambda x: x[0], reverse = False)\n",
    "values = [elem[1] for elem in sorted_hist]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WIUnUh6PuV_-",
    "outputId": "ae887125-3923-4a18-ae13-44268f2b2d3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch_no 0\n",
      "at batch_no 100\n",
      "at batch_no 200\n",
      "at batch_no 300\n",
      "at batch_no 400\n",
      "at batch_no 500\n",
      "at batch_no 600\n",
      "at batch_no 700\n",
      "at batch_no 800\n",
      "at batch_no 900\n",
      "at batch_no 1000\n",
      "at batch_no 1100\n",
      "at batch_no 1200\n",
      "at batch_no 1300\n",
      "at batch_no 1400\n",
      "at batch_no 1500\n",
      "at batch_no 1600\n",
      "at batch_no 1700\n",
      "at batch_no 1800\n",
      "at batch_no 1900\n",
      "at batch_no 2000\n",
      "at batch_no 2100\n",
      "at batch_no 2200\n",
      "at batch_no 2300\n",
      "at batch_no 2400\n",
      "at batch_no 2500\n",
      "at batch_no 2600\n",
      "at batch_no 2700\n",
      "at batch_no 2800\n",
      "at batch_no 2900\n",
      "at batch_no 3000\n",
      "at batch_no 3100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(885,\n",
       " [(815, 234.00),\n",
       "  (652, 177.00),\n",
       "  (646, 163.00),\n",
       "  (580, 160.00),\n",
       "  (611, 156.00),\n",
       "  (489, 145.00),\n",
       "  (591, 144.00),\n",
       "  (621, 141.00),\n",
       "  (737, 139.00),\n",
       "  (904, 130.00),\n",
       "  (94, 129.00),\n",
       "  (868, 127.00),\n",
       "  (582, 125.00),\n",
       "  (497, 123.00),\n",
       "  (893, 120.00),\n",
       "  (794, 118.00),\n",
       "  (116, 115.00),\n",
       "  (955, 115.00),\n",
       "  (979, 114.00),\n",
       "  (679, 112.00),\n",
       "  (721, 109.00),\n",
       "  (39, 108.00),\n",
       "  (565, 108.00),\n",
       "  (741, 106.00),\n",
       "  (491, 105.00),\n",
       "  (562, 103.00),\n",
       "  (839, 102.00),\n",
       "  (109, 101.00),\n",
       "  (162, 101.00),\n",
       "  (549, 99.00),\n",
       "  (46, 97.00),\n",
       "  (48, 96.00),\n",
       "  (84, 95.00),\n",
       "  (750, 95.00),\n",
       "  (82, 94.00),\n",
       "  (973, 94.00),\n",
       "  (151, 93.00),\n",
       "  (492, 93.00),\n",
       "  (695, 93.00),\n",
       "  (199, 91.00),\n",
       "  (843, 89.00),\n",
       "  (51, 88.00),\n",
       "  (971, 88.00),\n",
       "  (640, 87.00),\n",
       "  (424, 86.00),\n",
       "  (669, 86.00),\n",
       "  (692, 86.00),\n",
       "  (879, 86.00),\n",
       "  (281, 85.00),\n",
       "  (47, 84.00),\n",
       "  (783, 84.00),\n",
       "  (203, 83.00),\n",
       "  (310, 83.00),\n",
       "  (382, 83.00),\n",
       "  (411, 83.00),\n",
       "  (866, 83.00),\n",
       "  (743, 82.00),\n",
       "  (364, 81.00),\n",
       "  (577, 81.00),\n",
       "  (197, 80.00),\n",
       "  (318, 80.00),\n",
       "  (319, 80.00),\n",
       "  (406, 80.00),\n",
       "  (725, 80.00),\n",
       "  (828, 80.00),\n",
       "  (180, 79.00),\n",
       "  (189, 79.00),\n",
       "  (208, 79.00),\n",
       "  (298, 79.00),\n",
       "  (703, 79.00),\n",
       "  (754, 79.00),\n",
       "  (905, 79.00),\n",
       "  (956, 79.00),\n",
       "  (847, 78.00),\n",
       "  (76, 77.00),\n",
       "  (182, 77.00),\n",
       "  (342, 77.00),\n",
       "  (455, 77.00),\n",
       "  (572, 77.00),\n",
       "  (711, 77.00),\n",
       "  (762, 77.00),\n",
       "  (896, 77.00),\n",
       "  (963, 77.00),\n",
       "  (217, 76.00),\n",
       "  (440, 76.00),\n",
       "  (824, 76.00),\n",
       "  (830, 76.00),\n",
       "  (55, 75.00),\n",
       "  (219, 75.00),\n",
       "  (270, 75.00),\n",
       "  (700, 75.00),\n",
       "  (730, 75.00),\n",
       "  (791, 75.00),\n",
       "  (128, 74.00),\n",
       "  (849, 74.00),\n",
       "  (938, 74.00),\n",
       "  (982, 74.00),\n",
       "  (237, 73.00),\n",
       "  (343, 73.00),\n",
       "  (363, 73.00),\n",
       "  (454, 73.00),\n",
       "  (570, 73.00),\n",
       "  (645, 73.00),\n",
       "  (735, 73.00),\n",
       "  (778, 73.00),\n",
       "  (825, 73.00),\n",
       "  (222, 72.00),\n",
       "  (304, 72.00),\n",
       "  (436, 72.00),\n",
       "  (483, 72.00),\n",
       "  (800, 72.00),\n",
       "  (826, 72.00),\n",
       "  (61, 71.00),\n",
       "  (74, 71.00),\n",
       "  (471, 71.00),\n",
       "  (472, 71.00),\n",
       "  (805, 71.00),\n",
       "  (806, 71.00),\n",
       "  (916, 71.00),\n",
       "  (30, 70.00),\n",
       "  (496, 70.00),\n",
       "  (716, 70.00),\n",
       "  (23, 69.00),\n",
       "  (341, 69.00),\n",
       "  (425, 69.00),\n",
       "  (775, 69.00),\n",
       "  (808, 69.00),\n",
       "  (878, 69.00),\n",
       "  (556, 68.00),\n",
       "  (620, 68.00),\n",
       "  (845, 68.00),\n",
       "  (184, 67.00),\n",
       "  (192, 67.00),\n",
       "  (195, 67.00),\n",
       "  (300, 67.00),\n",
       "  (361, 67.00),\n",
       "  (671, 67.00),\n",
       "  (887, 67.00),\n",
       "  (912, 67.00),\n",
       "  (37, 66.00),\n",
       "  (178, 66.00),\n",
       "  (313, 66.00),\n",
       "  (458, 66.00),\n",
       "  (654, 66.00),\n",
       "  (772, 66.00),\n",
       "  (820, 66.00),\n",
       "  (863, 66.00),\n",
       "  (870, 66.00),\n",
       "  (77, 65.00),\n",
       "  (124, 65.00),\n",
       "  (423, 65.00),\n",
       "  (655, 65.00),\n",
       "  (949, 65.00),\n",
       "  (988, 65.00),\n",
       "  (6, 64.00),\n",
       "  (211, 64.00),\n",
       "  (506, 64.00),\n",
       "  (688, 64.00),\n",
       "  (926, 64.00),\n",
       "  (972, 64.00),\n",
       "  (234, 63.00),\n",
       "  (272, 63.00),\n",
       "  (293, 63.00),\n",
       "  (481, 63.00),\n",
       "  (595, 63.00),\n",
       "  (852, 63.00),\n",
       "  (871, 63.00),\n",
       "  (895, 63.00),\n",
       "  (953, 63.00),\n",
       "  (975, 63.00),\n",
       "  (992, 63.00),\n",
       "  (90, 62.00),\n",
       "  (463, 62.00),\n",
       "  (505, 62.00),\n",
       "  (561, 62.00),\n",
       "  (697, 62.00),\n",
       "  (809, 62.00),\n",
       "  (864, 62.00),\n",
       "  (944, 62.00),\n",
       "  (987, 62.00),\n",
       "  (97, 61.00),\n",
       "  (99, 61.00),\n",
       "  (118, 61.00),\n",
       "  (125, 61.00),\n",
       "  (135, 61.00),\n",
       "  (155, 61.00),\n",
       "  (238, 61.00),\n",
       "  (292, 61.00),\n",
       "  (331, 61.00),\n",
       "  (468, 61.00),\n",
       "  (474, 61.00),\n",
       "  (597, 61.00),\n",
       "  (788, 61.00),\n",
       "  (834, 61.00),\n",
       "  (913, 61.00),\n",
       "  (50, 60.00),\n",
       "  (311, 60.00),\n",
       "  (401, 60.00),\n",
       "  (404, 60.00),\n",
       "  (420, 60.00),\n",
       "  (457, 60.00),\n",
       "  (476, 60.00),\n",
       "  (515, 60.00),\n",
       "  (532, 60.00),\n",
       "  (586, 60.00),\n",
       "  (594, 60.00),\n",
       "  (635, 60.00),\n",
       "  (649, 60.00),\n",
       "  (781, 60.00),\n",
       "  (850, 60.00),\n",
       "  (880, 60.00),\n",
       "  (892, 60.00),\n",
       "  (937, 60.00),\n",
       "  (946, 60.00),\n",
       "  (33, 59.00),\n",
       "  (129, 59.00),\n",
       "  (263, 59.00),\n",
       "  (372, 59.00),\n",
       "  (519, 59.00),\n",
       "  (564, 59.00),\n",
       "  (607, 59.00),\n",
       "  (724, 59.00),\n",
       "  (766, 59.00),\n",
       "  (770, 59.00),\n",
       "  (875, 59.00),\n",
       "  (957, 59.00),\n",
       "  (85, 58.00),\n",
       "  (119, 58.00),\n",
       "  (161, 58.00),\n",
       "  (181, 58.00),\n",
       "  (249, 58.00),\n",
       "  (259, 58.00),\n",
       "  (280, 58.00),\n",
       "  (348, 58.00),\n",
       "  (383, 58.00),\n",
       "  (441, 58.00),\n",
       "  (522, 58.00),\n",
       "  (539, 58.00),\n",
       "  (552, 58.00),\n",
       "  (563, 58.00),\n",
       "  (601, 58.00),\n",
       "  (619, 58.00),\n",
       "  (696, 58.00),\n",
       "  (748, 58.00),\n",
       "  (816, 58.00),\n",
       "  (21, 57.00),\n",
       "  (69, 57.00),\n",
       "  (89, 57.00),\n",
       "  (113, 57.00),\n",
       "  (115, 57.00),\n",
       "  (218, 57.00),\n",
       "  (232, 57.00),\n",
       "  (250, 57.00),\n",
       "  (284, 57.00),\n",
       "  (316, 57.00),\n",
       "  (407, 57.00),\n",
       "  (428, 57.00),\n",
       "  (488, 57.00),\n",
       "  (581, 57.00),\n",
       "  (603, 57.00),\n",
       "  (614, 57.00),\n",
       "  (626, 57.00),\n",
       "  (698, 57.00),\n",
       "  (774, 57.00),\n",
       "  (777, 57.00),\n",
       "  (784, 57.00),\n",
       "  (790, 57.00),\n",
       "  (842, 57.00),\n",
       "  (962, 57.00),\n",
       "  (8, 56.00),\n",
       "  (31, 56.00),\n",
       "  (57, 56.00),\n",
       "  (60, 56.00),\n",
       "  (171, 56.00),\n",
       "  (225, 56.00),\n",
       "  (275, 56.00),\n",
       "  (317, 56.00),\n",
       "  (334, 56.00),\n",
       "  (391, 56.00),\n",
       "  (443, 56.00),\n",
       "  (490, 56.00),\n",
       "  (527, 56.00),\n",
       "  (569, 56.00),\n",
       "  (593, 56.00),\n",
       "  (609, 56.00),\n",
       "  (642, 56.00),\n",
       "  (792, 56.00),\n",
       "  (819, 56.00),\n",
       "  (857, 56.00),\n",
       "  (924, 56.00),\n",
       "  (952, 56.00),\n",
       "  (24, 55.00),\n",
       "  (25, 55.00),\n",
       "  (67, 55.00),\n",
       "  (229, 55.00),\n",
       "  (231, 55.00),\n",
       "  (283, 55.00),\n",
       "  (291, 55.00),\n",
       "  (308, 55.00),\n",
       "  (328, 55.00),\n",
       "  (386, 55.00),\n",
       "  (396, 55.00),\n",
       "  (410, 55.00),\n",
       "  (509, 55.00),\n",
       "  (512, 55.00),\n",
       "  (612, 55.00),\n",
       "  (661, 55.00),\n",
       "  (822, 55.00),\n",
       "  (858, 55.00),\n",
       "  (884, 55.00),\n",
       "  (950, 55.00),\n",
       "  (985, 55.00),\n",
       "  (986, 55.00),\n",
       "  (991, 55.00),\n",
       "  (88, 54.00),\n",
       "  (159, 54.00),\n",
       "  (170, 54.00),\n",
       "  (206, 54.00),\n",
       "  (228, 54.00),\n",
       "  (241, 54.00),\n",
       "  (269, 54.00),\n",
       "  (276, 54.00),\n",
       "  (285, 54.00),\n",
       "  (327, 54.00),\n",
       "  (487, 54.00),\n",
       "  (547, 54.00),\n",
       "  (657, 54.00),\n",
       "  (709, 54.00),\n",
       "  (768, 54.00),\n",
       "  (780, 54.00),\n",
       "  (801, 54.00),\n",
       "  (832, 54.00),\n",
       "  (855, 54.00),\n",
       "  (936, 54.00),\n",
       "  (990, 54.00),\n",
       "  (995, 54.00),\n",
       "  (3, 53.00),\n",
       "  (35, 53.00),\n",
       "  (58, 53.00),\n",
       "  (70, 53.00),\n",
       "  (104, 53.00),\n",
       "  (138, 53.00),\n",
       "  (177, 53.00),\n",
       "  (251, 53.00),\n",
       "  (254, 53.00),\n",
       "  (274, 53.00),\n",
       "  (307, 53.00),\n",
       "  (367, 53.00),\n",
       "  (444, 53.00),\n",
       "  (452, 53.00),\n",
       "  (477, 53.00),\n",
       "  (508, 53.00),\n",
       "  (524, 53.00),\n",
       "  (526, 53.00),\n",
       "  (528, 53.00),\n",
       "  (533, 53.00),\n",
       "  (641, 53.00),\n",
       "  (653, 53.00),\n",
       "  (665, 53.00),\n",
       "  (668, 53.00),\n",
       "  (739, 53.00),\n",
       "  (818, 53.00),\n",
       "  (835, 53.00),\n",
       "  (888, 53.00),\n",
       "  (903, 53.00),\n",
       "  (922, 53.00),\n",
       "  (1, 52.00),\n",
       "  (92, 52.00),\n",
       "  (164, 52.00),\n",
       "  (176, 52.00),\n",
       "  (216, 52.00),\n",
       "  (239, 52.00),\n",
       "  (431, 52.00),\n",
       "  (448, 52.00),\n",
       "  (478, 52.00),\n",
       "  (701, 52.00),\n",
       "  (738, 52.00),\n",
       "  (752, 52.00),\n",
       "  (779, 52.00),\n",
       "  (787, 52.00),\n",
       "  (829, 52.00),\n",
       "  (833, 52.00),\n",
       "  (840, 52.00),\n",
       "  (877, 52.00),\n",
       "  (917, 52.00),\n",
       "  (939, 52.00),\n",
       "  (943, 52.00),\n",
       "  (133, 51.00),\n",
       "  (160, 51.00),\n",
       "  (188, 51.00),\n",
       "  (196, 51.00),\n",
       "  (212, 51.00),\n",
       "  (221, 51.00),\n",
       "  (286, 51.00),\n",
       "  (362, 51.00),\n",
       "  (377, 51.00),\n",
       "  (416, 51.00),\n",
       "  (419, 51.00),\n",
       "  (514, 51.00),\n",
       "  (545, 51.00),\n",
       "  (592, 51.00),\n",
       "  (636, 51.00),\n",
       "  (637, 51.00),\n",
       "  (746, 51.00),\n",
       "  (757, 51.00),\n",
       "  (764, 51.00),\n",
       "  (776, 51.00),\n",
       "  (902, 51.00),\n",
       "  (927, 51.00),\n",
       "  (13, 50.00),\n",
       "  (36, 50.00),\n",
       "  (102, 50.00),\n",
       "  (114, 50.00),\n",
       "  (126, 50.00),\n",
       "  (261, 50.00),\n",
       "  (277, 50.00),\n",
       "  (289, 50.00),\n",
       "  (294, 50.00),\n",
       "  (295, 50.00),\n",
       "  (301, 50.00),\n",
       "  (352, 50.00),\n",
       "  (358, 50.00),\n",
       "  (373, 50.00),\n",
       "  (449, 50.00),\n",
       "  (467, 50.00),\n",
       "  (604, 50.00),\n",
       "  (608, 50.00),\n",
       "  (618, 50.00),\n",
       "  (639, 50.00),\n",
       "  (659, 50.00),\n",
       "  (685, 50.00),\n",
       "  (765, 50.00),\n",
       "  (997, 50.00),\n",
       "  (0, 49.00),\n",
       "  (9, 49.00),\n",
       "  (123, 49.00),\n",
       "  (172, 49.00),\n",
       "  (267, 49.00),\n",
       "  (325, 49.00),\n",
       "  (375, 49.00),\n",
       "  (376, 49.00),\n",
       "  (378, 49.00),\n",
       "  (387, 49.00),\n",
       "  (388, 49.00),\n",
       "  (398, 49.00),\n",
       "  (523, 49.00),\n",
       "  (535, 49.00),\n",
       "  (541, 49.00),\n",
       "  (566, 49.00),\n",
       "  (583, 49.00),\n",
       "  (602, 49.00),\n",
       "  (667, 49.00),\n",
       "  (704, 49.00),\n",
       "  (763, 49.00),\n",
       "  (771, 49.00),\n",
       "  (874, 49.00),\n",
       "  (11, 48.00),\n",
       "  (12, 48.00),\n",
       "  (14, 48.00),\n",
       "  (15, 48.00),\n",
       "  (16, 48.00),\n",
       "  (18, 48.00),\n",
       "  (41, 48.00),\n",
       "  (71, 48.00),\n",
       "  (78, 48.00),\n",
       "  (134, 48.00),\n",
       "  (137, 48.00),\n",
       "  (141, 48.00),\n",
       "  (156, 48.00),\n",
       "  (194, 48.00),\n",
       "  (209, 48.00),\n",
       "  (214, 48.00),\n",
       "  (255, 48.00),\n",
       "  (264, 48.00),\n",
       "  (279, 48.00),\n",
       "  (288, 48.00),\n",
       "  (290, 48.00),\n",
       "  (312, 48.00),\n",
       "  (336, 48.00),\n",
       "  (340, 48.00),\n",
       "  (349, 48.00),\n",
       "  (354, 48.00),\n",
       "  (413, 48.00),\n",
       "  (451, 48.00),\n",
       "  (517, 48.00),\n",
       "  (628, 48.00),\n",
       "  (683, 48.00),\n",
       "  (684, 48.00),\n",
       "  (758, 48.00),\n",
       "  (797, 48.00),\n",
       "  (865, 48.00),\n",
       "  (872, 48.00),\n",
       "  (881, 48.00),\n",
       "  (890, 48.00),\n",
       "  (891, 48.00),\n",
       "  (915, 48.00),\n",
       "  (951, 48.00),\n",
       "  (994, 48.00),\n",
       "  (40, 47.00),\n",
       "  (53, 47.00),\n",
       "  (100, 47.00),\n",
       "  (101, 47.00),\n",
       "  (110, 47.00),\n",
       "  (130, 47.00),\n",
       "  (136, 47.00),\n",
       "  (227, 47.00),\n",
       "  (243, 47.00),\n",
       "  (253, 47.00),\n",
       "  (256, 47.00),\n",
       "  (265, 47.00),\n",
       "  (321, 47.00),\n",
       "  (333, 47.00),\n",
       "  (335, 47.00),\n",
       "  (337, 47.00),\n",
       "  (350, 47.00),\n",
       "  (351, 47.00),\n",
       "  (395, 47.00),\n",
       "  (426, 47.00),\n",
       "  (503, 47.00),\n",
       "  (518, 47.00),\n",
       "  (560, 47.00),\n",
       "  (576, 47.00),\n",
       "  (606, 47.00),\n",
       "  (707, 47.00),\n",
       "  (732, 47.00),\n",
       "  (759, 47.00),\n",
       "  (769, 47.00),\n",
       "  (807, 47.00),\n",
       "  (886, 47.00),\n",
       "  (5, 46.00),\n",
       "  (22, 46.00),\n",
       "  (56, 46.00),\n",
       "  (63, 46.00),\n",
       "  (65, 46.00),\n",
       "  (72, 46.00),\n",
       "  (79, 46.00),\n",
       "  (87, 46.00),\n",
       "  (95, 46.00),\n",
       "  (108, 46.00),\n",
       "  (149, 46.00),\n",
       "  (157, 46.00),\n",
       "  (201, 46.00),\n",
       "  (215, 46.00),\n",
       "  (266, 46.00),\n",
       "  (320, 46.00),\n",
       "  (323, 46.00),\n",
       "  (324, 46.00),\n",
       "  (366, 46.00),\n",
       "  (370, 46.00),\n",
       "  (397, 46.00),\n",
       "  (422, 46.00),\n",
       "  (432, 46.00),\n",
       "  (433, 46.00),\n",
       "  (434, 46.00),\n",
       "  (530, 46.00),\n",
       "  (616, 46.00),\n",
       "  (658, 46.00),\n",
       "  (664, 46.00),\n",
       "  (751, 46.00),\n",
       "  (753, 46.00),\n",
       "  (796, 46.00),\n",
       "  (823, 46.00),\n",
       "  (848, 46.00),\n",
       "  (867, 46.00),\n",
       "  (882, 46.00),\n",
       "  (918, 46.00),\n",
       "  (983, 46.00),\n",
       "  (989, 46.00),\n",
       "  (993, 46.00),\n",
       "  (2, 45.00),\n",
       "  (28, 45.00),\n",
       "  (66, 45.00),\n",
       "  (96, 45.00),\n",
       "  (105, 45.00),\n",
       "  (121, 45.00),\n",
       "  (139, 45.00),\n",
       "  (144, 45.00),\n",
       "  (169, 45.00),\n",
       "  (191, 45.00),\n",
       "  (247, 45.00),\n",
       "  (273, 45.00),\n",
       "  (299, 45.00),\n",
       "  (326, 45.00),\n",
       "  (332, 45.00),\n",
       "  (339, 45.00),\n",
       "  (344, 45.00),\n",
       "  (355, 45.00),\n",
       "  (365, 45.00),\n",
       "  (389, 45.00),\n",
       "  (392, 45.00),\n",
       "  (445, 45.00),\n",
       "  (571, 45.00),\n",
       "  (599, 45.00),\n",
       "  (625, 45.00),\n",
       "  (674, 45.00),\n",
       "  (734, 45.00),\n",
       "  (755, 45.00),\n",
       "  (817, 45.00),\n",
       "  (853, 45.00),\n",
       "  (900, 45.00),\n",
       "  (910, 45.00),\n",
       "  (75, 44.00),\n",
       "  (131, 44.00),\n",
       "  (186, 44.00),\n",
       "  (223, 44.00),\n",
       "  (244, 44.00),\n",
       "  (252, 44.00),\n",
       "  (405, 44.00),\n",
       "  (412, 44.00),\n",
       "  (417, 44.00),\n",
       "  (473, 44.00),\n",
       "  (475, 44.00),\n",
       "  (486, 44.00),\n",
       "  (579, 44.00),\n",
       "  (613, 44.00),\n",
       "  (682, 44.00),\n",
       "  (702, 44.00),\n",
       "  (706, 44.00),\n",
       "  (727, 44.00),\n",
       "  (821, 44.00),\n",
       "  (941, 44.00),\n",
       "  (968, 44.00),\n",
       "  (984, 44.00),\n",
       "  (44, 43.00),\n",
       "  (83, 43.00),\n",
       "  (107, 43.00),\n",
       "  (142, 43.00),\n",
       "  (198, 43.00),\n",
       "  (268, 43.00),\n",
       "  (330, 43.00),\n",
       "  (353, 43.00),\n",
       "  (357, 43.00),\n",
       "  (360, 43.00),\n",
       "  (381, 43.00),\n",
       "  (384, 43.00),\n",
       "  (414, 43.00),\n",
       "  (495, 43.00),\n",
       "  (537, 43.00),\n",
       "  (542, 43.00),\n",
       "  (717, 43.00),\n",
       "  (782, 43.00),\n",
       "  (844, 43.00),\n",
       "  (873, 43.00),\n",
       "  (907, 43.00),\n",
       "  (920, 43.00),\n",
       "  (959, 43.00),\n",
       "  (998, 43.00),\n",
       "  (7, 42.00),\n",
       "  (17, 42.00),\n",
       "  (19, 42.00),\n",
       "  (42, 42.00),\n",
       "  (80, 42.00),\n",
       "  (93, 42.00),\n",
       "  (112, 42.00),\n",
       "  (132, 42.00),\n",
       "  (154, 42.00),\n",
       "  (174, 42.00),\n",
       "  (193, 42.00),\n",
       "  (245, 42.00),\n",
       "  (260, 42.00),\n",
       "  (306, 42.00),\n",
       "  (309, 42.00),\n",
       "  (421, 42.00),\n",
       "  (429, 42.00),\n",
       "  (437, 42.00),\n",
       "  (447, 42.00),\n",
       "  (464, 42.00),\n",
       "  (485, 42.00),\n",
       "  (507, 42.00),\n",
       "  (510, 42.00),\n",
       "  (538, 42.00),\n",
       "  (575, 42.00),\n",
       "  (633, 42.00),\n",
       "  (656, 42.00),\n",
       "  (666, 42.00),\n",
       "  (690, 42.00),\n",
       "  (710, 42.00),\n",
       "  (799, 42.00),\n",
       "  (889, 42.00),\n",
       "  (894, 42.00),\n",
       "  (919, 42.00),\n",
       "  (932, 42.00),\n",
       "  (933, 42.00),\n",
       "  (981, 42.00),\n",
       "  (999, 42.00),\n",
       "  (117, 41.00),\n",
       "  (127, 41.00),\n",
       "  (153, 41.00),\n",
       "  (190, 41.00),\n",
       "  (224, 41.00),\n",
       "  (258, 41.00),\n",
       "  (278, 41.00),\n",
       "  (322, 41.00),\n",
       "  (390, 41.00),\n",
       "  (393, 41.00),\n",
       "  (430, 41.00),\n",
       "  (574, 41.00),\n",
       "  (712, 41.00),\n",
       "  (723, 41.00),\n",
       "  (795, 41.00),\n",
       "  (854, 41.00),\n",
       "  (898, 41.00),\n",
       "  (929, 41.00),\n",
       "  (27, 40.00),\n",
       "  (230, 40.00),\n",
       "  (242, 40.00),\n",
       "  (257, 40.00),\n",
       "  (287, 40.00),\n",
       "  (418, 40.00),\n",
       "  (500, 40.00),\n",
       "  (546, 40.00),\n",
       "  (548, 40.00),\n",
       "  (558, 40.00),\n",
       "  (573, 40.00),\n",
       "  (584, 40.00),\n",
       "  (600, 40.00),\n",
       "  (610, 40.00),\n",
       "  (713, 40.00),\n",
       "  (736, 40.00),\n",
       "  (749, 40.00),\n",
       "  (837, 40.00),\n",
       "  (862, 40.00),\n",
       "  (921, 40.00),\n",
       "  (925, 40.00),\n",
       "  (996, 40.00),\n",
       "  (173, 39.00),\n",
       "  (183, 39.00),\n",
       "  (262, 39.00),\n",
       "  (296, 39.00),\n",
       "  (297, 39.00),\n",
       "  (302, 39.00),\n",
       "  (329, 39.00),\n",
       "  (368, 39.00),\n",
       "  (435, 39.00),\n",
       "  (470, 39.00),\n",
       "  (480, 39.00),\n",
       "  (513, 39.00),\n",
       "  (520, 39.00),\n",
       "  (529, 39.00),\n",
       "  (553, 39.00),\n",
       "  (670, 39.00),\n",
       "  (672, 39.00),\n",
       "  (677, 39.00),\n",
       "  (694, 39.00),\n",
       "  (722, 39.00),\n",
       "  (726, 39.00),\n",
       "  (756, 39.00),\n",
       "  (812, 39.00),\n",
       "  (945, 39.00),\n",
       "  (43, 38.00),\n",
       "  (91, 38.00),\n",
       "  (210, 38.00),\n",
       "  (235, 38.00),\n",
       "  (236, 38.00),\n",
       "  (305, 38.00),\n",
       "  (314, 38.00),\n",
       "  (439, 38.00),\n",
       "  (442, 38.00),\n",
       "  (456, 38.00),\n",
       "  (462, 38.00),\n",
       "  (466, 38.00),\n",
       "  (501, 38.00),\n",
       "  (540, 38.00),\n",
       "  (555, 38.00),\n",
       "  (559, 38.00),\n",
       "  (643, 38.00),\n",
       "  (687, 38.00),\n",
       "  (699, 38.00),\n",
       "  (708, 38.00),\n",
       "  (719, 38.00),\n",
       "  (38, 37.00),\n",
       "  (52, 37.00),\n",
       "  (111, 37.00),\n",
       "  (140, 37.00),\n",
       "  (175, 37.00),\n",
       "  (204, 37.00),\n",
       "  (248, 37.00),\n",
       "  (338, 37.00),\n",
       "  (346, 37.00),\n",
       "  (379, 37.00),\n",
       "  (450, 37.00),\n",
       "  (494, 37.00),\n",
       "  (554, 37.00),\n",
       "  (615, 37.00),\n",
       "  (630, 37.00),\n",
       "  (650, 37.00),\n",
       "  (678, 37.00),\n",
       "  (693, 37.00),\n",
       "  (714, 37.00),\n",
       "  (761, 37.00),\n",
       "  (814, 37.00),\n",
       "  (831, 37.00),\n",
       "  (934, 37.00),\n",
       "  (10, 36.00),\n",
       "  (26, 36.00),\n",
       "  (45, 36.00),\n",
       "  (62, 36.00),\n",
       "  (98, 36.00),\n",
       "  (120, 36.00),\n",
       "  (145, 36.00),\n",
       "  (200, 36.00),\n",
       "  (207, 36.00),\n",
       "  (408, 36.00),\n",
       "  (427, 36.00),\n",
       "  (453, 36.00),\n",
       "  (511, 36.00),\n",
       "  (588, 36.00),\n",
       "  (827, 36.00),\n",
       "  (851, 36.00),\n",
       "  (20, 35.00),\n",
       "  (143, 35.00),\n",
       "  (148, 35.00),\n",
       "  (220, 35.00),\n",
       "  (347, 35.00),\n",
       "  (374, 35.00),\n",
       "  (380, 35.00),\n",
       "  (409, 35.00),\n",
       "  (415, 35.00),\n",
       "  (543, 35.00),\n",
       "  (605, 35.00),\n",
       "  (627, 35.00),\n",
       "  (745, 35.00),\n",
       "  (760, 35.00),\n",
       "  (793, 35.00),\n",
       "  (798, 35.00),\n",
       "  (846, 35.00),\n",
       "  (958, 35.00),\n",
       "  (73, 34.00),\n",
       "  (226, 34.00),\n",
       "  (399, 34.00),\n",
       "  (465, 34.00),\n",
       "  (720, 34.00),\n",
       "  (786, 34.00),\n",
       "  (802, 34.00),\n",
       "  (861, 34.00),\n",
       "  (897, 34.00),\n",
       "  (928, 34.00),\n",
       "  (81, 33.00),\n",
       "  (152, 33.00),\n",
       "  (158, 33.00),\n",
       "  (205, 33.00),\n",
       "  (213, 33.00),\n",
       "  (385, 33.00),\n",
       "  (647, 33.00),\n",
       "  (803, 33.00),\n",
       "  (859, 33.00),\n",
       "  (964, 33.00),\n",
       "  (59, 32.00),\n",
       "  (86, 32.00),\n",
       "  (146, 32.00),\n",
       "  (356, 32.00),\n",
       "  (359, 32.00),\n",
       "  (371, 32.00),\n",
       "  (402, 32.00),\n",
       "  (629, 32.00),\n",
       "  (883, 32.00),\n",
       "  (947, 32.00),\n",
       "  (980, 32.00),\n",
       "  (106, 31.00),\n",
       "  (179, 31.00),\n",
       "  (233, 31.00),\n",
       "  (403, 31.00),\n",
       "  (459, 31.00),\n",
       "  (502, 31.00),\n",
       "  (521, 31.00),\n",
       "  (744, 31.00),\n",
       "  (914, 31.00),\n",
       "  (923, 31.00),\n",
       "  (954, 31.00),\n",
       "  (64, 30.00),\n",
       "  (166, 30.00),\n",
       "  (202, 30.00),\n",
       "  (345, 30.00),\n",
       "  (469, 30.00),\n",
       "  (484, 30.00),\n",
       "  (531, 30.00),\n",
       "  (551, 30.00),\n",
       "  (568, 30.00),\n",
       "  (578, 30.00),\n",
       "  (589, 30.00),\n",
       "  (634, 30.00),\n",
       "  (663, 30.00),\n",
       "  (705, 30.00),\n",
       "  (948, 30.00),\n",
       "  (187, 29.00),\n",
       "  (461, 29.00),\n",
       "  (498, 29.00),\n",
       "  (557, 29.00),\n",
       "  (585, 29.00),\n",
       "  (617, 29.00),\n",
       "  (860, 29.00),\n",
       "  (966, 29.00),\n",
       "  (4, 28.00),\n",
       "  (29, 28.00),\n",
       "  (122, 28.00),\n",
       "  (596, 28.00),\n",
       "  (632, 28.00),\n",
       "  (676, 28.00),\n",
       "  (691, 28.00),\n",
       "  (733, 28.00),\n",
       "  (747, 28.00),\n",
       "  (49, 27.00),\n",
       "  (54, 27.00),\n",
       "  (147, 27.00),\n",
       "  (150, 27.00),\n",
       "  (271, 27.00),\n",
       "  (303, 27.00),\n",
       "  (369, 27.00),\n",
       "  (567, 27.00),\n",
       "  (587, 27.00),\n",
       "  (624, 27.00),\n",
       "  (644, 27.00),\n",
       "  (660, 27.00),\n",
       "  (718, 27.00),\n",
       "  (767, 27.00),\n",
       "  (789, 27.00),\n",
       "  (32, 26.00),\n",
       "  (516, 26.00),\n",
       "  (544, 26.00),\n",
       "  (631, 26.00),\n",
       "  (731, 26.00),\n",
       "  (804, 26.00),\n",
       "  (869, 26.00),\n",
       "  (965, 26.00),\n",
       "  (163, 25.00),\n",
       "  (168, 25.00),\n",
       "  (394, 25.00),\n",
       "  (479, 25.00),\n",
       "  (482, 25.00),\n",
       "  (536, 25.00),\n",
       "  (590, 25.00),\n",
       "  (623, 25.00),\n",
       "  (686, 25.00),\n",
       "  (838, 25.00),\n",
       "  (908, 25.00),\n",
       "  (165, 24.00),\n",
       "  (785, 24.00),\n",
       "  (901, 24.00),\n",
       "  (931, 24.00),\n",
       "  (942, 24.00),\n",
       "  (185, 23.00),\n",
       "  (240, 23.00),\n",
       "  (499, 23.00),\n",
       "  (638, 23.00),\n",
       "  (680, 23.00),\n",
       "  (876, 23.00),\n",
       "  (974, 23.00),\n",
       "  (68, 22.00),\n",
       "  (525, 22.00),\n",
       "  (740, 22.00),\n",
       "  (773, 22.00),\n",
       "  (811, 22.00),\n",
       "  (856, 22.00),\n",
       "  (909, 22.00),\n",
       "  (911, 22.00),\n",
       "  (246, 21.00),\n",
       "  (438, 21.00),\n",
       "  (675, 21.00),\n",
       "  (885, 21.00),\n",
       "  (315, 20.00),\n",
       "  (400, 20.00),\n",
       "  (662, 20.00),\n",
       "  (715, 20.00),\n",
       "  (977, 20.00),\n",
       "  (598, 19.00),\n",
       "  (729, 19.00),\n",
       "  (622, 18.00),\n",
       "  (651, 18.00),\n",
       "  (103, 17.00),\n",
       "  (550, 17.00),\n",
       "  (648, 17.00),\n",
       "  (728, 17.00),\n",
       "  (841, 17.00),\n",
       "  (976, 17.00),\n",
       "  (460, 16.00),\n",
       "  (504, 16.00),\n",
       "  (836, 16.00),\n",
       "  (930, 16.00),\n",
       "  (970, 16.00),\n",
       "  (282, 15.00),\n",
       "  (446, 15.00),\n",
       "  (493, 15.00),\n",
       "  (534, 15.00),\n",
       "  (906, 15.00),\n",
       "  (813, 14.00),\n",
       "  (969, 13.00),\n",
       "  (742, 12.00),\n",
       "  (940, 12.00),\n",
       "  (34, 11.00),\n",
       "  (167, 11.00),\n",
       "  (689, 11.00),\n",
       "  (967, 11.00),\n",
       "  (978, 11.00),\n",
       "  (681, 9.00),\n",
       "  (899, 9.00),\n",
       "  (960, 9.00),\n",
       "  (673, 8.00),\n",
       "  (810, 7.00),\n",
       "  (935, 6.00),\n",
       "  (961, 2.00)])"
      ]
     },
     "execution_count": 97,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = targeted_diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zCeAMy0tuWAP",
    "outputId": "798d0db2-8a5d-4d9b-d246-1f200b3a9da6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f59b994b4e0>]"
      ]
     },
     "execution_count": 100,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# on test\n",
    "sorted_hist = sorted(hist, key=lambda x: x[0], reverse = False)\n",
    "values = [elem[1] for elem in sorted_hist]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OFCjzI7UaY3C"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[200][0]\n",
    "x = normalize(x_img.data.cuda())\n",
    "z = torch.tensor([-0.33,  0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33], dtype=torch.float32).cuda()\n",
    "# z = torch.empty(z_dim).uniform_(-1,1).cuda()\n",
    "p = model.forward_single_z(z).detach()\n",
    "\n",
    "p_x = x + p\n",
    "# print(\"img range, noise range\")\n",
    "# print_range(x); print_range(p)\n",
    "adv_label = class_index_to_label(arch(p_x[None]).argmax(1).item())\n",
    "print_big_vector(arch(p_x[None])[0])\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0., 1.])\n",
    "p_img = Image(p)\n",
    "x_img.show()\n",
    "p_img.show()\n",
    "p_x_img.show()\n",
    "\n",
    "\n",
    "# print_range(p)\n",
    "# print_range(denormalize(x))\n",
    "# print_range(p_x)\n",
    "\n",
    "benign_label = class_index_to_label(arch(x[None]).argmax(1).item())\n",
    "\n",
    "print_big_vector(arch(x[None])[0])\n",
    "print(benign_label, adv_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzwsI2P1ZANz"
   },
   "outputs": [],
   "source": [
    "z1 = torch.tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p1 = model.forward_single_z(z1)\n",
    "\n",
    "z2 = torch.tensor([1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p2 = model.forward_single_z(z2)\n",
    "\n",
    "z3 = torch.tensor([1, 1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p3 = model.forward_single_z(z3)\n",
    "\n",
    "l2_distance(p1, p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eroI82OKSnAL"
   },
   "outputs": [],
   "source": [
    "# the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[4][0]\n",
    "x = x_img.data[None].cuda()\n",
    "p = model(x)[0].squeeze().detach() \n",
    "x = x.squeeze()\n",
    "x = normalize(x)\n",
    "\n",
    "p_x = x + p\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0.,1.])\n",
    "p_img = Image(p)\n",
    "# x_img.show()\n",
    "p_img.show()\n",
    "# p_x_img.show()\n",
    "\n",
    "print_range(p)\n",
    "print_range(x)\n",
    "print_range(p_x)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NAG_denoiserTest_withRobustnessMetric.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
