{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ahmad-PH/nag-notebooks/blob/master/NAG_tripletLossExperiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqeZpz16do4y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up-to-date.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os; import subprocess\n",
    "\n",
    "def detect_env():\n",
    "    return 'colab' if 'content' in os.listdir('/') else 'IBM'\n",
    "  \n",
    "def run_shell_command(cmd):\n",
    "  p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "  print(str(p.communicate()[0], 'utf-8'))\n",
    "  \n",
    "if detect_env() == 'colab': root_folder = '/content'\n",
    "elif detect_env() == 'IBM' : root_folder = '/root/Derakhshani/adversarial'\n",
    "python_files_path = root_folder + '/nag-public'\n",
    "\n",
    "if os.path.isdir(python_files_path):\n",
    "  initial_dir = os.getcwd()\n",
    "  os.chdir(python_files_path)\n",
    "  run_shell_command('git pull')\n",
    "  os.chdir(initial_dir)\n",
    "else:\n",
    "  os.chdir('/root/Derakhshani/adversarial')\n",
    "  run_shell_command('git clone https://github.com/ahmad-PH/nag-public.git')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(python_files_path + '/NAG-11May-beforeDenoiser')\n",
    "\n",
    "from nag_util import *\n",
    "import nag_util\n",
    "from environment import *\n",
    "\n",
    "env = create_env()\n",
    "env.setup(cuda_index = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Ev7jcRKoARg"
   },
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.imports import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys; import os; import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.nn import init\n",
    "from typing import Iterable\n",
    "torch.Tensor.ndim = property(lambda x: len(x.shape))\n",
    "\n",
    "def listify(o):\n",
    "    if o is None: return []\n",
    "    if isinstance(o, list): return o\n",
    "    if isinstance(o, Iterable): return list(o)\n",
    "    return [o]\n",
    "\n",
    "class ListContainer():\n",
    "  def __init__(self, items): self.items = listify(items)\n",
    "  def __getitem__(self, idx):\n",
    "    if isinstance(idx, (int, slice)): return self.items[idx]\n",
    "    if isinstance(idx[0], bool):\n",
    "      assert len(idx) == len(self)\n",
    "      return [o for m,o in zip(idx, self.items) if m]\n",
    "    return [self.items[i] for i in idx]\n",
    "  \n",
    "  def __len__(self): return len(self.items)\n",
    "  def __iter__(self): return iter(self.items)\n",
    "  def __setitem__(self, i, o): self.items[i] = o\n",
    "  def __delitem__(self, i): del(self.items[i])\n",
    "  def __repr__(self):\n",
    "    res = f\"{self.__class__.__name__} ({len(self)} items)\\n{self.items[:10]}\"\n",
    "    if len(self)>10: res = res[:-1] + \"...]\"\n",
    "    return res\n",
    "\n",
    "def children(m): return list(m.children())\n",
    "\n",
    "def append_stats_non_normal(hook, mod, inp, outp):\n",
    "  if not hasattr(hook,'stats'): hook.stats = ([],[],[])\n",
    "  means,stds,hists = hook.stats\n",
    "  means.append(outp.data.mean().cpu())\n",
    "  stds .append(outp.data.std().cpu())\n",
    "  hists.append(outp.data.cpu().histc(40,0,10)) #histc isn't implemented on the GPU\n",
    "\n",
    "def append_stats_normal(hook, mod, inp, outp):\n",
    "    if not hasattr(hook,'stats'): hook.stats = ([],[],[])\n",
    "    means,stds,hists = hook.stats\n",
    "    means.append(outp.data.mean().cpu())\n",
    "    stds .append(outp.data.std().cpu())\n",
    "    hists.append(outp.data.cpu().histc(40,-7,7))\n",
    "\n",
    "def get_hist(h):\n",
    "  return torch.stack(h.stats[2]).t().float().log1p()\n",
    "\n",
    "def get_min(h):\n",
    "  h1 = torch.stack(h.stats[2]).t().float()\n",
    "  return h1[:2].sum(0)/h1.sum(0)\n",
    "\n",
    "class Hook():\n",
    "  def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self))\n",
    "  def __del__(self): self.remove()\n",
    "  def remove(self): self.hook.remove()\n",
    "    \n",
    "class Hooks(ListContainer):\n",
    "  def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms.children()])\n",
    "  def __enter__(self, *args): return self\n",
    "  def __exit__ (self, *args): self.remove()\n",
    "  def __del__(self): self.remove()\n",
    "\n",
    "  def __delitem__(self, i):\n",
    "    self[i].remove()\n",
    "    super().__delitem__(i)\n",
    "\n",
    "  def remove(self):\n",
    "    for h in self: h.remove()\n",
    "\n",
    "def init_cnn_(m, f):\n",
    "    if isinstance(m, nn.ConvTranspose2d):\n",
    "      f(m.weight, a=0.1)\n",
    "      if getattr(m, 'bias', None) is not None: m.bias.data.zero_()\n",
    "        \n",
    "    #non-orthogonal\n",
    "    if isinstance(m, nn.Linear):\n",
    "      f(m.weight, a=0.)\n",
    "      if getattr(m, 'bias', None) is not None: m.bias.data.zero_()\n",
    "        \n",
    "    #orthogonal\n",
    "#     if isinstance(m, nn.Linear):\n",
    "#       init.orthogonal_(m.weight)\n",
    "#       m.bias.data.zero_()\n",
    "\n",
    "    for l in m.children(): init_cnn_(l, f)  \n",
    "      \n",
    "def init_cnn(m, uniform=False):\n",
    "    f = init.kaiming_uniform_ if uniform else init.kaiming_normal_\n",
    "    init_cnn_(m, f)\n",
    "\n",
    "class GeneralRelu(nn.Module):\n",
    "  def __init__(self, leak=None, sub=None, maxv=None):\n",
    "    super().__init__()\n",
    "    self.leak,self.sub,self.maxv = leak,sub,maxv\n",
    "\n",
    "  def forward(self, x): \n",
    "    x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n",
    "    if self.sub is not None: x.sub_(self.sub)\n",
    "    if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "    return x\n",
    "  \n",
    "class deconv_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k_size = (4,4), s = (2,2), pad = (1,1), b = True, activation = True):\n",
    "        super(deconv_layer, self).__init__()\n",
    "\n",
    "        self.CT2d = nn.ConvTranspose2d(in_channels = in_ch,\n",
    "                                  out_channels = out_ch,\n",
    "                                  kernel_size = k_size,\n",
    "                                  stride = s, \n",
    "                                  padding = pad,\n",
    "                                  bias = b)\n",
    "        self.BN2d = nn.BatchNorm2d(out_ch)\n",
    "        \n",
    "        self.activation = activation\n",
    "        if self.activation:\n",
    "            self.relu = GeneralRelu(0, 0.2, 5)\n",
    "        \n",
    "        self.weight_init()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.activation:\n",
    "            return self.relu(self.BN2d(self.CT2d(input)))\n",
    "        else:\n",
    "            return self.BN2d(self.CT2d(input))\n",
    "\n",
    "    def weight_init(self):\n",
    "        self.CT2d.weight.data.normal_(mean = 0, std = 0.02)\n",
    "        self.CT2d.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tltucTv2ep9-"
   },
   "outputs": [],
   "source": [
    "# mode = \"sanity_check\"\n",
    "mode = \"normal\"\n",
    "# mode = \"div_metric_calc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50\n",
    "# model = models.resnet152\n",
    "# model = models.vgg16_bn\n",
    "# model = torchvision.models.googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SO1h55obXzOv",
    "outputId": "54414cc5-84d5-4f45-ecab-87374a58dd33"
   },
   "outputs": [],
   "source": [
    "if mode == \"normal\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "elif mode == \"sanity_check\":\n",
    "  env.load_dataset('dataset_sanity_check_small', 'dataset_sanity_check_small')  \n",
    "  env.set_data_path('dataset_sanity_check_small')\n",
    "elif mode == \"div_metric_calc\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "  env.load_test_dataset(str(env.data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "koaQZmjMom7w"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "gpu_flag = True\n",
    "nag_util.batch_size = batch_size; nag_util.gpu_flag = gpu_flag;\n",
    "tfms = get_transforms(do_flip=False, max_rotate=0)\n",
    "data = (ImageList.from_folder(env.data_path)\n",
    "        .split_by_folder(valid=('test' if mode == 'div_metric_calc' else 'valid'))\n",
    "        .label_from_folder()\n",
    "        .transform(tfms, size=224)\n",
    "        .databunch(bs=batch_size, num_workers=1)\n",
    "        .normalize(imagenet_stats))\n",
    "\n",
    "# data.show_batch(rows=2, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDBkRV8yovwV"
   },
   "outputs": [],
   "source": [
    "z_dim = 10\n",
    "arch = SoftmaxWrapper(model(pretrained=True).cuda().eval())\n",
    "nag_util.arch = arch\n",
    "requires_grad(arch, False)\n",
    "\n",
    "# vgg:\n",
    "# layers = []\n",
    "# blocks = [i-1 for i,o in enumerate(children(arch.features)) if isinstance(o, nn.MaxPool2d)]\n",
    "# layers = [arch.features[i] for i in blocks]\n",
    "# layer_weights = [1] * len(layers)\n",
    "\n",
    "layers = [\n",
    "    arch.softmax\n",
    "]\n",
    "\n",
    "layer_weights = [1.] * len(layers)\n",
    "\n",
    "# inception:\n",
    "# layers = [\n",
    "#     arch.Conv2d_1a_3x3,\n",
    "#     arch.Mixed_6e,\n",
    "#     arch.Mixed_7a,\n",
    "#     arch.fc    \n",
    "# ]\n",
    "# layer_weights = [1.0/4.0] * len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Gen(nn.Module):\n",
    "#   def __init__(self, z_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "#     super(Gen, self).__init__()\n",
    "\n",
    "#     self.bs = None\n",
    "#     self.z_dim = z_dim\n",
    "#     self.gf_dim = gf_dim\n",
    "#     self.y_dim = y_dim\n",
    "#     self.df_dim = df_dim\n",
    "#     self.image_shape = image_shape\n",
    "\n",
    "#     self.z_ = nn.Linear(self.z_dim, self.gf_dim * 7 * 4 * 4, bias=True)\n",
    "#     self.z_.bias.data.fill_(0)\n",
    "#     self.BN_ = nn.BatchNorm2d(self.gf_dim * 7)\n",
    "\n",
    "#     self.half = max(self.gf_dim // 2, 1) \n",
    "#     self.quarter = max(self.gf_dim // 4, 1)\n",
    "#     self.eighth = max(self.gf_dim // 8, 1)\n",
    "#     # sixteenth = max(self.gf_dim // 16, 1)\n",
    "    \n",
    "#     self.CT2d_1 = deconv_layer(self.gf_dim * 8, self.gf_dim * 4, k_size = (5,5), pad = (2,2))\n",
    "#     self.CT2d_2 = deconv_layer(self.gf_dim * 5, self.gf_dim * 2)    \n",
    "#     self.CT2d_3 = deconv_layer(self.gf_dim * 2 + self.half, self.gf_dim * 1)\n",
    "#     self.CT2d_4 = deconv_layer(self.gf_dim * 1 + self.quarter, self.gf_dim * 1)\n",
    "#     self.CT2d_5 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "#     self.CT2d_6 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "#     self.CT2d_7 = deconv_layer(self.gf_dim * 1 + self.eighth, 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "#   def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "#     h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "#     h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "#     output = deconv_layer(h_input)\n",
    "#     assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "#             \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "#             \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "#     return output\n",
    "  \n",
    "#   def forward_z(self, z):\n",
    "#     self.bs = z.shape[0]\n",
    "    \n",
    "#     h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "#     assert h0.shape[2:] == (4, 4), \"Unexpected shape, it shoud be (4,4)\"\n",
    "\n",
    "#     h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "#     h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "#     h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "#     h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "#     h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "#     h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "#     h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "#     ksi = 10.0\n",
    "#     output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "#     # this coeff scales the output to be appropriate for images that are \n",
    "#     # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "#     # interval)\n",
    "#     return output_coeff * torch.tanh(h7)\n",
    "  \n",
    "# #   # blind-selection\n",
    "#   def forward(self, inputs):\n",
    "#     self.bs = inputs.shape[0]\n",
    "\n",
    "#     benign_preds_onehot = arch(inputs)\n",
    "#     benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "    \n",
    "#     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "#     for i in range(self.bs):\n",
    "#       random_label = self.randint(0,1000, exclude = benign_preds[i].item())\n",
    "#       z[i][random_label] = 1.\n",
    "    \n",
    "#     z_out = self.forward_z(z)\n",
    "    \n",
    "#     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "\n",
    "# #   #second-best selection: made validation so much worse\n",
    "# #   def forward(self, inputs):\n",
    "# #     self.bs = inputs.shape[0]\n",
    "\n",
    "# #     benign_preds_onehot = arch(inputs)\n",
    "# #     target_preds = torch.topk(benign_preds_onehot, 2, dim = 1).indices[:, 1:]\n",
    "    \n",
    "# #     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "# #     for i in range(self.bs):\n",
    "# #       z[i][target_preds[i]] = 1.\n",
    "    \n",
    "# #     z_out = self.forward_z(z)\n",
    "    \n",
    "# #     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "  \n",
    "# #    def forward(self, inputs):\n",
    "# #     self.bs = inputs.shape[0]\n",
    "\n",
    "# #     benign_preds_onehot = arch(inputs)\n",
    "# #     benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "    \n",
    "# #     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "# #     random_label = self.randint(0,1000, exclude = benign_preds.tolist())\n",
    "# #     for i in range(self.bs):\n",
    "# #       z[i][random_label] = 1.\n",
    "    \n",
    "# #     z_out = self.forward_z(z)\n",
    "    \n",
    "# #     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "  \n",
    "#   @staticmethod\n",
    "#   def randint(low, high, exclude):\n",
    "#     temp = np.random.randint(low, high - 1)\n",
    "#     if temp == exclude:\n",
    "#       temp = temp + 1\n",
    "#     return temp\n",
    "  \n",
    "#   def forward_single_z(self, z):\n",
    "#     return self.forward_z(z[None]).squeeze()\n",
    "           \n",
    "  \n",
    "#   def make_triplet_samples(self, z, margin, r2, r3):\n",
    "#     positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "#     negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "#     return positive_sample, negative_sample\n",
    "\n",
    "#   def random_vector_surface(self, shape, r = 1.):\n",
    "#     mat = torch.randn(size=shape).cuda()\n",
    "#     norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "#     return (mat/norm) * r\n",
    "\n",
    "  \n",
    "#   def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "#     fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "#     fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "#     fraction.unsqueeze_(-1)\n",
    "#     return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "#   def make_z(self, in_shape):\n",
    "#     return torch.empty(in_shape).cuda().uniform_(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-targeted Gen\n",
    "class Gen(nn.Module):\n",
    "  def __init__(self, z_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "    super(Gen, self).__init__()\n",
    "\n",
    "    self.bs = None\n",
    "    self.z_dim = z_dim\n",
    "    self.gf_dim = gf_dim\n",
    "    self.y_dim = y_dim\n",
    "    self.df_dim = df_dim\n",
    "    self.image_shape = image_shape\n",
    "\n",
    "    self.z_ = nn.Linear(self.z_dim, self.gf_dim * 7 * 4 * 4, bias=True)\n",
    "    self.z_.bias.data.fill_(0)\n",
    "    self.BN_ = nn.BatchNorm2d(self.gf_dim * 7)\n",
    "\n",
    "    self.half = max(self.gf_dim // 2, 1) \n",
    "    self.quarter = max(self.gf_dim // 4, 1)\n",
    "    self.eighth = max(self.gf_dim // 8, 1)\n",
    "    # sixteenth = max(self.gf_dim // 16, 1)\n",
    "\n",
    "    self.CT2d_1 = deconv_layer(self.gf_dim * 8, self.gf_dim * 4, k_size = (5,5), pad = (2,2))\n",
    "    self.CT2d_2 = deconv_layer(self.gf_dim * 5, self.gf_dim * 2)    \n",
    "    self.CT2d_3 = deconv_layer(self.gf_dim * 2 + self.half, self.gf_dim * 1)\n",
    "    self.CT2d_4 = deconv_layer(self.gf_dim * 1 + self.quarter, self.gf_dim * 1)\n",
    "    self.CT2d_5 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "    self.CT2d_6 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "    self.CT2d_7 = deconv_layer(self.gf_dim * 1 + self.eighth, 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "  def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "    h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "    h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "    output = deconv_layer(h_input)\n",
    "    assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "            \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "            \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "    return output\n",
    "  \n",
    "  def forward_z(self, z):\n",
    "    self.bs = z.shape[0]\n",
    "    \n",
    "    h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "    assert h0.shape[2:] == (4, 4), \"Non-expected shape, it shoud be (4,4)\"\n",
    "\n",
    "    h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "    h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "    h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "    h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "    h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "    h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "    h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "    ksi = 10.0\n",
    "    output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "    # this coeff scales the output to be appropriate for images that are \n",
    "    # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "    # interval)\n",
    "    return output_coeff * torch.tanh(h7)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    self.bs = inputs.shape[0]\n",
    "    z = inputs.new_empty([self.bs, self.z_dim]).uniform_(-1,1).cuda()\n",
    "    p, n = self.make_triplet_samples(z, 0.1, 0.1, 2.)\n",
    "    \n",
    "    z_out = self.forward_z(z)\n",
    "    p_out = self.forward_z(p)\n",
    "    n_out = self.forward_z(n)\n",
    "    \n",
    "    return z_out, p_out, n_out, inputs\n",
    "#     return z_out, None, None, inputs\n",
    "  \n",
    "  def forward_single_z(self, z):\n",
    "    return self.forward_z(z[None]).squeeze()\n",
    "           \n",
    "  \n",
    "  def make_triplet_samples(self, z, margin, r2, r3):\n",
    "    positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "    negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "    return positive_sample, negative_sample\n",
    "\n",
    "  def random_vector_surface(self, shape, r = 1.):\n",
    "    mat = torch.randn(size=shape).cuda()\n",
    "    norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "    return (mat/norm) * r\n",
    "\n",
    "  \n",
    "  def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "    fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "    fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "    fraction.unsqueeze_(-1)\n",
    "    return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "  def make_z(self, in_shape):\n",
    "    return torch.empty(in_shape).cuda().uniform_(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NkfbLWEQqRA_"
   },
   "outputs": [],
   "source": [
    "def js_distance(x1, x2):\n",
    "  m = 0.5 * (x1 + x2)\n",
    "  return 0.5 * (F.kl_div(x1, m) + F.kl_div(x2, m))\n",
    "\n",
    "def kl_distance(x1, x2):\n",
    "  inp = torch.log(x1)\n",
    "  target = x2\n",
    "  return F.kl_div(inp, target, reduction='batchmean')\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  x1 = tensorify(x1)\n",
    "  x2 = tensorify(x2)\n",
    "  x1 = x1 / torch.sum(x1)\n",
    "  x2 = x2 / torch.sum(x2)\n",
    "  return kl_distance(x1[None], x2[None])\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  if not isinstance(x1, torch.Tensor): x1 = torch.tensor(x1)\n",
    "  if not isinstance(x2, torch.Tensor): x2 = torch.tensor(x2)\n",
    "  x1 = x1 * 100. / torch.sum(x1)\n",
    "  x2 = x2 * 100. / torch.sum(x2)\n",
    "  return torch.norm(x1 - x2, 2)\n",
    "\n",
    "def distance_from_uniform(x):\n",
    "  return distrib_distance(x, [1.] * len(x))\n",
    "\n",
    "def wasserstein_distance(x1, x2):\n",
    "  return torch.mean(x1 - x2)\n",
    "\n",
    "def l1_distance(x1, x2):\n",
    "  return F.l1_loss(x1, x2)\n",
    "\n",
    "def l2_distance(x1, x2):\n",
    "  return F.mse_loss(x1 * 10, x2 * 10)\n",
    "\n",
    "def mse_loss(x1, x2):\n",
    "  return F.mse_loss(x1, x2)\n",
    "\n",
    "def cos_distance(x1, x2, dim = 1):\n",
    "  return -1 * torch.mean(F.cosine_similarity(x1, x2, dim=dim))\n",
    "\n",
    "triplet_call_cnt = 0\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, distance_func, margin):\n",
    "  # max distance when using l1_distance is 2\n",
    "  # max distacne when using l2-distance is sqrt(2)\n",
    "#   print(\"anchor: \", anchor.min(), anchor.max())\n",
    "  ap_dist = distance_func(anchor, positive)\n",
    "  an_dist = distance_func(anchor, negative)\n",
    "\n",
    "  global triplet_call_cnt\n",
    "  triplet_call_cnt += 1\n",
    "  if triplet_call_cnt % 10 in [0,1] : #and anchor.shape[1] == 1000:\n",
    "    print(\"a: \", end=\"\"); print_big_vector(anchor[0])\n",
    "    print(\"p: \", end=\"\"); print_big_vector(positive[0])\n",
    "    print(\"n: \", end=\"\"); print_big_vector(negative[0])\n",
    "    print(\"func:{}, ap_dist: {}, an_dist: {}\".format(distance_func.__name__, ap_dist, an_dist))\n",
    "    \n",
    "  return torch.mean(F.relu(ap_dist - an_dist + margin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsFgfiN8EV7z"
   },
   "outputs": [],
   "source": [
    "def diversity_loss(input, target):\n",
    "#   return -1 * torch.mean(torch.pow(f_x_a-f_x_s,2))\n",
    "  if input.shape[0] != batch_size:\n",
    "    print(\"input shape: \", input.shape)\n",
    "    print(\"target shape: \", target.shape, \"\\n\\n\")\n",
    "  return torch.mean(F.cosine_similarity(\n",
    "    input.view([batch_size, -1]),\n",
    "    target.view([batch_size, -1]), \n",
    "  ))\n",
    "\n",
    "fool_loss_count = 0\n",
    "\n",
    "def fool_loss(input, target):\n",
    "  true_class = torch.argmax(target, dim=1).view(-1,1).long().cuda()\n",
    "  target_probabilities = input.gather(1, true_class)\n",
    "  epsilon = 1e-10\n",
    "  result =  torch.mean(-1 * torch.log(1 - target_probabilities + epsilon))\n",
    "  \n",
    "  global fool_loss_count\n",
    "  fool_loss_count += 1\n",
    "  if fool_loss_count % 40 == 0:\n",
    "    print(\"target probs {}, loss: {}: \".format(target_probabilities, result))\n",
    "    \n",
    "  return result\n",
    "\n",
    "\n",
    "# def fool_loss(model_output, target_labels):\n",
    "#   target_labels = target_labels.view(-1, 1).long().cuda()\n",
    "#   target_probabilities = model_output.gather(1, target_labels)\n",
    "#   epsilon = 1e-10\n",
    "#   # highest possible fool_loss is - log(1e-10) == 23\n",
    "#   result = torch.mean(-1 * torch.log(target_probabilities + epsilon))\n",
    "  \n",
    "#   global fool_loss_count\n",
    "#   fool_loss_count += 1\n",
    "#   if fool_loss_count % 20 == 0:\n",
    "#     print(\"target probs {}, loss: {}: \".format(target_probabilities, result))\n",
    "  \n",
    "#   return result\n",
    "\n",
    "# def validation(gen_output, target):\n",
    "#   perturbations, _, _, clean_images, _, _ = gen_output\n",
    "#   perturbed_images = clean_images + perturbations\n",
    "#   benign_preds = torch.argmax(arch(clean_images), 1)\n",
    "#   adversary_preds = torch.argmax(arch(perturbed_images), 1)\n",
    "#   return (benign_preds != adversary_preds).float().mean()\n",
    "\n",
    "def validation(gen_output, target):\n",
    "  perturbations, _, _, clean_images = gen_output\n",
    "  perturbed_images = clean_images + perturbations\n",
    "  benign_preds = torch.argmax(arch(clean_images), 1)\n",
    "  adversary_preds = torch.argmax(arch(perturbed_images), 1)\n",
    "  return (benign_preds != adversary_preds).float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FeatureLoss(nn.Module):\n",
    "#     def __name__(self):\n",
    "#       return \"feature_loss\"\n",
    "  \n",
    "#     def __init__(self, dis, layers, layer_weights):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # define generator here \n",
    "#         self.dis = dis\n",
    "#         self.diversity_layers = layers\n",
    "#         self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "#         self.weights = layer_weights\n",
    "#         self.metric_names = [\"fool_loss\"] #+ [f\"div_loss_{i}\" for i in range(len(layers))] #maybe Gram\n",
    "# #         self.triplet_hooks = hook_outputs([arch.m.features[4]], detach=False)\n",
    "    \n",
    "#     def make_features(self, x, clone=False):\n",
    "#         y = self.dis(x)\n",
    "#         return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "  \n",
    "#     def forward(self, inp, target):\n",
    "#       sigma_B, _, _, X_B, B_Y, z = inp\n",
    "\n",
    "#       X_A = X_B + sigma_B\n",
    "# #       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "#       A_Y, A_feat = self.make_features(X_A)\n",
    "# #       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "#       chosen_labels = z.argmax(dim=1)\n",
    "#       fooling_loss =  fool_loss(A_Y, chosen_labels)\n",
    "\n",
    "# #       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "# #       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "# #       self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "# #       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "#       self.losses = [fooling_loss]\n",
    "#       self.metrics = dict(zip(self.metric_names, [fooling_loss]))\n",
    "\n",
    "#       return sum(self.losses)\n",
    "  \n",
    "#     def add_perturbation_shuffled(self, inp, perturbation):\n",
    "# #         j = torch.randperm(inp.shape[0])\n",
    "#         j = derangement(inp.shape[0])\n",
    "#         return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-targeted FeatureLoss\n",
    "class FeatureLoss(nn.Module):\n",
    "    def __name__(self):\n",
    "      return \"feature_loss\"\n",
    "  \n",
    "    def __init__(self, dis, layers, layer_weights):\n",
    "        super().__init__()\n",
    "        \n",
    "        # define generator here \n",
    "        self.dis = dis\n",
    "        self.diversity_layers = layers\n",
    "        self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "        self.weights = layer_weights\n",
    "#         self.metric_names = [\"fool_loss\"] + [f\"div_loss_{i}\" for i in range(len(layers))] + ['triplet_loss']# Maybe Gram\n",
    "        self.metric_names = [\"fool_loss\"] + ['div_loss']# Maybe Gram\n",
    "        self.triplet_weight = 4.\n",
    "        self.div_weight = 10.\n",
    "    \n",
    "    def make_features(self, x, clone=False):\n",
    "        y = self.dis(x)\n",
    "        return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "    \n",
    "    # triplet loss\n",
    "#     def forward(self, inp, target):\n",
    "#         sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "#         X_A = self.add_perturbation(X_B, sigma_B) \n",
    "#         X_A_pos = self.add_perturbation(X_B, sigma_pos)\n",
    "#         X_A_neg = self.add_perturbation(X_B, sigma_neg) \n",
    "        \n",
    "#         X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "        \n",
    "#         B_Y, _ = self.make_features(X_B)\n",
    "#         A_Y, A_feat = self.make_features(X_A)\n",
    "# #         _, S_feat = self.make_features(X_S)\n",
    "#         pos_softmax, _ = self.make_features(X_A_pos)\n",
    "#         neg_softmax, _ = self.make_features(X_A_neg)\n",
    "        \n",
    "#         fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "      \n",
    "# #         raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "# #         weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "#         raw_triplet_loss = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "#         weighted_triplet_loss = raw_triplet_loss * self.triplet_weight\n",
    "    \n",
    "# #         self.losses = [fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss]\n",
    "# #         self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses + [weighted_triplet_loss]))\n",
    "#         self.losses = [fooling_loss] + [weighted_triplet_loss]\n",
    "#         self.metrics = dict(zip(self.metric_names, [fooling_loss] + [raw_triplet_loss]))\n",
    "\n",
    "#         return sum(self.losses)\n",
    "\n",
    "\n",
    "#     #use two types of triplet losses\n",
    "#     def forward(self, inp, target):\n",
    "#       sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "#       X_A = self.add_perturbation(X_B, sigma_B) \n",
    "#       X_A_pos = self.add_perturbation(X_B, sigma_pos)\n",
    "#       X_A_neg = self.add_perturbation(X_B, sigma_neg) \n",
    "\n",
    "#       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "#       B_Y, _ = self.make_features(X_B)\n",
    "#       A_Y, A_feat = self.make_features(X_A)\n",
    "#       _, S_feat = self.make_features(X_S)\n",
    "#       pos_softmax, _ = self.make_features(X_A_pos)\n",
    "#       neg_softmax, _ = self.make_features(X_A_neg)\n",
    "\n",
    "#       fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "#       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "#       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "      \n",
    "#       raw_triplet_loss_sm = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "#       weighted_triplet_loss_sm = raw_triplet_loss_sm * self.triplet_weight_sm\n",
    "      \n",
    "#       raw_triplet_loss_noise = triplet_loss(sigma_B, sigma_pos, sigma_neg, l2_distance, 5.)\n",
    "#       weighted_triplet_loss_noise = raw_triplet_loss_noise * self.triplet_weight_noise\n",
    "\n",
    "#       self.losses = [fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss_sm, weighted_triplet_loss_noise] \n",
    "#       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses + [weighted_triplet_loss_sm, weighted_triplet_loss_noise]))\n",
    "\n",
    "#       return sum(self.losses)\n",
    "\n",
    "    # just fooling and diversity\n",
    "    def forward(self, inp, target):\n",
    "      sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "      X_A = self.add_perturbation(X_B, sigma_B) \n",
    "\n",
    "      X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "      B_Y, _ = self.make_features(X_B)\n",
    "      A_Y, A_feat = self.make_features(X_A)\n",
    "      _, S_feat = self.make_features(X_S)\n",
    "\n",
    "      fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "      raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "      weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "      self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "      self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "      return sum(self.losses)\n",
    "  \n",
    "  \n",
    "    def add_perturbation(self, inp, perturbation):\n",
    "        return inp.add(perturbation)\n",
    "  \n",
    "    def add_perturbation_shuffled(self, inp, perturbation):\n",
    "        j = derangement(inp.shape[0])\n",
    "        return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qd9gXUy_ovww"
   },
   "outputs": [],
   "source": [
    "feat_loss = FeatureLoss(arch, layers, layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfZKdYD2MSdi"
   },
   "outputs": [],
   "source": [
    "env.save_filename = 'resnet50_x'\n",
    "# env.save_filename = 'resnet50_17'\n",
    "# env.save_filename = 'vgg16_32'\n",
    "\n",
    "if Path(env.get_csv_path() + '.csv').exists(): raise FileExistsError(\"csv_path already exists\")\n",
    "if Path(env.get_models_path()).exists(): raise FileExistsError(\"models_path already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9J20CBLS8S9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models_directory returned is:  models/52\n"
     ]
    }
   ],
   "source": [
    "learn = None; gen = None; gc.collect()\n",
    "csv_logger = partial(ImmediateCSVLogger, filename= env.temp_csv_path + '/' + env.save_filename)\n",
    "gen = Gen(z_dim=z_dim)\n",
    "init_cnn(gen, True)\n",
    "\n",
    "learn = Learner(data, gen, loss_func = feat_loss, model_dir = env.get_learner_models_dir(), metrics=[validation], callback_fns=[LossMetrics, csv_logger])\n",
    "# learn = Learner(data, Gen(z_dim=10), loss_func = feat_loss, metrics=[validation], callback_fns=LossMetrics, opt_func = optim.SGD)\n",
    "# learn = Learner(data, Gen(z_dim=z_dim), loss_func = feat_loss, metrics=[validation], callback_fns=[LossMetrics, DiversityWeightsScheduler])\n",
    "\n",
    "# load_starting_point(learn, model.__name__, z_dim)\n",
    "# random_seed(42, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wOZYzOHDEdB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GeneralRelu. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type FeatureLoss. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Gen. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type deconv_layer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (9000 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "n02454379,n02454379,n02454379,n02454379,n02454379\n",
       "Path: /root/Derakhshani/adversarial/datasets/dataset;\n",
       "\n",
       "Valid: LabelList (1000 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "n02454379,n02397096,n02090379,n01729977,n02268853\n",
       "Path: /root/Derakhshani/adversarial/datasets/dataset;\n",
       "\n",
       "Test: None, model=Gen(\n",
       "  (z_): Linear(in_features=10, out_features=7168, bias=True)\n",
       "  (BN_): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (CT2d_1): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (BN2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_2): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(320, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_3): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(160, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_4): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(80, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_5): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_6): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_7): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (BN2d): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FeatureLoss(\n",
       "  (dis): SoftmaxWrapper(\n",
       "    (m): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax()\n",
       "  )\n",
       "), metrics=[<function validation at 0x7fe580780b70>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/Derakhshani/adversarial/datasets/dataset'), model_dir='models/52', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), <class 'fastai.callbacks.loss_metrics.LossMetrics'>, functools.partial(<class 'nag_util.ImmediateCSVLogger'>, filename='/root/Derakhshani/adversarial/temp/resnet50_x')], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): Linear(in_features=10, out_features=7168, bias=True)\n",
       "  (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): GeneralRelu()\n",
       "  (5): ConvTranspose2d(320, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): GeneralRelu()\n",
       "  (8): ConvTranspose2d(160, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): GeneralRelu()\n",
       "  (11): ConvTranspose2d(80, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): GeneralRelu()\n",
       "  (14): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (16): GeneralRelu()\n",
       "  (17): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): GeneralRelu()\n",
       "  (20): ConvTranspose2d(72, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (21): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")], add_time=True, silent=None)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !cp \"/content/gdrive/My Drive/DL/models/vgg16_12-last.pth\"  \"/content/\"\n",
    "# learn.load('/content/vgg16_12-last')\n",
    "\n",
    "# load_filename = 'resnet50-11_39'\n",
    "# load_filename = 'resnet50_startpoint_1'\n",
    "load_filename = 'resnet50_43/resnet50_43_39'\n",
    "# load_filename = 'investigate_resnet50_2/3/resnet50_5'\n",
    "# load_filename = 'vgg16_30/vgg16_30_69'\n",
    "# load_filename = 'vgg16_12-last'\n",
    "\n",
    "learn.load('/root/Derakhshani/adversarial/models/' + load_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the selected settings are : \n",
      "\tmode: normal \n",
      "\tnetw-under-attack: resnet50 \n",
      "\tload filename: resnet50_43/resnet50_43_39 \n",
      "\tsave filename: resnet50_x\n",
      "\n",
      "please MAKE SURE that the config is correct.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  load_filename\n",
    "except NameError:\n",
    "  load_filename = None\n",
    "\n",
    "print(\"the selected settings are : \")\n",
    "print(\"\\tmode: {} \\n\\tnetw-under-attack: {} \\n\\tload filename: {} \\n\\tsave filename: {}\\n\".format(\n",
    "  mode, model.__name__, load_filename , env.save_filename\n",
    "))\n",
    "print(\"please MAKE SURE that the config is correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find(1e-6, 100)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def produce_summary(root_folder, n_files):\n",
    "  def writeline(file, values, fmt_string):\n",
    "    file.write(', '.join(fmt_string.format(v) for v in values) + '\\n')\n",
    "  \n",
    "  last_rows = []\n",
    "  for i in range(n_files):\n",
    "    prefix = '/root/Derakhshani/adversarial/textual_notes/CSVs'\n",
    "    df = pd.read_csv(\"{}/{}/{}.csv\".format(prefix, root_folder, i))\n",
    "    last_rows.append(df.iloc[-1][1:-1].values.tolist())\n",
    "  \n",
    "  last_rows = np.array(last_rows)\n",
    "  \n",
    "  labels = list(df.columns[1:-1])\n",
    "  means = np.mean(last_rows, axis=0).tolist()\n",
    "  outfile = open('{}/{}/summary.txt'.format(prefix, root_folder), 'w+')\n",
    "  outfile.write('means: \\n')\n",
    "  writeline(outfile, labels, '{: >11}')\n",
    "  writeline(outfile, means, '{: >11.3}')\n",
    "  outfile.write('\\n')\n",
    "  \n",
    "  operations = ['min', 'min', 'max', 'min', 'min']\n",
    "  results = []\n",
    "  indexes = []\n",
    "  \n",
    "  for i in range(len(operations)):\n",
    "    values = last_rows[:, i]\n",
    "    if operations[i] == 'max': operation = np.max\n",
    "    elif operations[i] == 'min': operation = np.min\n",
    "    result = operation(values)\n",
    "    results.append(result)\n",
    "    indexes.append(values.tolist().index(result))\n",
    "  \n",
    "  outfile.write('bests: \\n')\n",
    "  writeline(outfile, labels, '{: >11}')\n",
    "  writeline(outfile, operations, '{: >11}')\n",
    "  writeline(outfile, results, '{: >11.3}')\n",
    "  writeline(outfile, indexes, '{: >11}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils import dir_util \n",
    "\n",
    "def investigate_initial_settings(n_settings, n_epochs, lr, wd, results_dir):\n",
    "  os.mkdir(env.get_csv_dir() + results_dir)\n",
    "  os.mkdir(env.get_models_dir() + results_dir)\n",
    "  \n",
    "  for setting_ind in range(n_settings):\n",
    "    learn = None; gen = None; gc.collect()\n",
    "    gen = Gen(z_dim = z_dim)\n",
    "    init_cnn(gen, True)\n",
    "    \n",
    "    tmp_csv_filename =  env.temp_csv_path + '/' + results_dir + '/' + str(setting_ind)\n",
    "    csv_logger = partial(ImmediateCSVLogger, filename=tmp_csv_filename)\n",
    "    \n",
    "    learn = Learner(data, gen, loss_func = feat_loss, metrics=[validation], model_dir = env.get_learner_models_dir(), callback_fns=[LossMetrics, csv_logger])\n",
    "    saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=model.__name__ + \"-best\")\n",
    "    saver_every_epoch = SaveModelCallback(learn, every='epoch', name=model.__name__)\n",
    "\n",
    "    learn.fit(n_epochs, lr=lr, wd = wd, callbacks=[saver_best, saver_every_epoch])\n",
    "    \n",
    "    shutil.copyfile(tmp_csv_filename + \".csv\", env.get_csv_dir() + results_dir + '/' + str(setting_ind) + '.csv')\n",
    "    \n",
    "    model_dest = env.get_models_dir() + results_dir + '/' + str(setting_ind)\n",
    "    os.mkdir(model_dest)\n",
    "    dir_util.copy_tree(env.data_path/env.get_learner_models_dir(), model_dest)\n",
    "    shutil.rmtree(env.data_path/env.get_learner_models_dir())  \n",
    "\n",
    "  produce_summary(results_dir, n_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dir = 'investigate_resnet50_x1'\n",
    "# investigate_initial_settings(10, 6, lr = 1e-2, wd = 0.001, results_dir = results_dir)\n",
    "# # shutil.rmtree(env.get_models_dir() + results_dir)\n",
    "# # shutil.rmtree(env.get_csv_dir() + results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='40', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      2.50% [1/40 03:56<2:33:47]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>div_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.592832</td>\n",
       "      <td>6.587547</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.800649</td>\n",
       "      <td>0.578690</td>\n",
       "      <td>03:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='326' class='' max='562', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      58.01% [326/562 02:06<01:31 5.9385]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[3.1672e-02],\n",
      "        [9.2523e-02],\n",
      "        [9.8829e-01],\n",
      "        [4.4484e-02],\n",
      "        [2.3811e-02],\n",
      "        [3.1262e-04],\n",
      "        [8.4996e-01],\n",
      "        [3.1430e-02],\n",
      "        [1.5737e-05],\n",
      "        [4.0525e-02],\n",
      "        [1.1923e-02],\n",
      "        [2.8965e-03],\n",
      "        [7.2877e-01],\n",
      "        [1.4799e-05],\n",
      "        [5.0292e-02],\n",
      "        [4.7140e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.5390766263008118: \n",
      "target probs tensor([[2.0945e-02],\n",
      "        [3.7337e-04],\n",
      "        [9.9099e-01],\n",
      "        [9.9538e-01],\n",
      "        [2.7968e-04],\n",
      "        [6.8110e-02],\n",
      "        [8.7485e-01],\n",
      "        [4.4865e-01],\n",
      "        [5.0273e-03],\n",
      "        [2.1190e-04],\n",
      "        [4.5364e-03],\n",
      "        [1.8277e-04],\n",
      "        [2.1669e-05],\n",
      "        [1.0133e-04],\n",
      "        [2.5348e-06],\n",
      "        [9.8455e-02]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.8104490041732788: \n",
      "target probs tensor([[8.4667e-02],\n",
      "        [4.1571e-03],\n",
      "        [7.6788e-02],\n",
      "        [9.9978e-01],\n",
      "        [5.3107e-01],\n",
      "        [1.5116e-01],\n",
      "        [6.5085e-02],\n",
      "        [1.3081e-01],\n",
      "        [1.7997e-04],\n",
      "        [9.4055e-01],\n",
      "        [9.6080e-01],\n",
      "        [3.1701e-03],\n",
      "        [3.0489e-03],\n",
      "        [5.2855e-04],\n",
      "        [5.8118e-04],\n",
      "        [3.9626e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.0173077583312988: \n",
      "target probs tensor([[8.5708e-01],\n",
      "        [6.0039e-03],\n",
      "        [3.6426e-01],\n",
      "        [9.6317e-01],\n",
      "        [8.9426e-06],\n",
      "        [1.8726e-06],\n",
      "        [3.7793e-04],\n",
      "        [4.2977e-01],\n",
      "        [3.7881e-02],\n",
      "        [3.3833e-02],\n",
      "        [5.0294e-02],\n",
      "        [3.2050e-06],\n",
      "        [1.3672e-02],\n",
      "        [9.7388e-01],\n",
      "        [6.9680e-03],\n",
      "        [2.4218e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.645982027053833: \n",
      "target probs tensor([[2.1799e-06],\n",
      "        [4.9026e-04],\n",
      "        [8.5403e-01],\n",
      "        [8.7953e-01],\n",
      "        [3.6500e-01],\n",
      "        [1.3607e-03],\n",
      "        [4.1327e-04],\n",
      "        [1.3943e-02],\n",
      "        [8.4052e-03],\n",
      "        [3.8635e-01],\n",
      "        [8.7666e-01],\n",
      "        [2.9117e-04],\n",
      "        [8.3654e-04],\n",
      "        [2.6738e-03],\n",
      "        [8.1093e-06],\n",
      "        [1.9711e-05]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.44403576850891113: \n",
      "target probs tensor([[4.1626e-08],\n",
      "        [8.2546e-05],\n",
      "        [9.7681e-04],\n",
      "        [1.7274e-04],\n",
      "        [5.8945e-02],\n",
      "        [1.1750e-03],\n",
      "        [2.9683e-04],\n",
      "        [9.4611e-01],\n",
      "        [9.5257e-01],\n",
      "        [1.6376e-01],\n",
      "        [3.7889e-05],\n",
      "        [5.6478e-06],\n",
      "        [7.9214e-01],\n",
      "        [6.0486e-03],\n",
      "        [4.0495e-06],\n",
      "        [6.7172e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.556414783000946: \n",
      "target probs tensor([[3.0118e-02],\n",
      "        [1.5992e-05],\n",
      "        [8.4554e-01],\n",
      "        [9.9875e-01],\n",
      "        [2.4871e-01],\n",
      "        [9.8161e-01],\n",
      "        [3.8562e-04],\n",
      "        [3.6946e-03],\n",
      "        [3.4460e-01],\n",
      "        [6.2550e-01],\n",
      "        [3.0545e-03],\n",
      "        [1.9155e-03],\n",
      "        [6.9905e-01],\n",
      "        [1.8866e-03],\n",
      "        [4.6564e-01],\n",
      "        [1.1043e-04]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.0068027973175049: \n",
      "target probs tensor([[4.2548e-04],\n",
      "        [2.9988e-03],\n",
      "        [4.5793e-04],\n",
      "        [4.1119e-02],\n",
      "        [7.3394e-03],\n",
      "        [5.7935e-04],\n",
      "        [5.0219e-01],\n",
      "        [2.1774e-04],\n",
      "        [9.5848e-02],\n",
      "        [9.5050e-02],\n",
      "        [9.9999e-01],\n",
      "        [3.0010e-01],\n",
      "        [2.5004e-05],\n",
      "        [4.2752e-05],\n",
      "        [9.6652e-01],\n",
      "        [8.6221e-06]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.9967447519302368: \n",
      "target probs tensor([[4.0817e-02],\n",
      "        [3.4453e-03],\n",
      "        [6.4698e-01],\n",
      "        [6.5481e-11],\n",
      "        [1.1435e-02],\n",
      "        [5.2889e-06],\n",
      "        [4.4090e-05],\n",
      "        [7.9853e-04],\n",
      "        [8.0732e-01],\n",
      "        [4.2595e-02],\n",
      "        [1.9565e-01],\n",
      "        [7.2145e-04],\n",
      "        [5.9231e-03],\n",
      "        [9.9984e-01],\n",
      "        [5.7638e-01],\n",
      "        [1.7189e-04]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.7891503572463989: \n",
      "target probs tensor([[2.1273e-03],\n",
      "        [1.1958e-03],\n",
      "        [2.3672e-02],\n",
      "        [6.2517e-01],\n",
      "        [1.5287e-06],\n",
      "        [9.9988e-01],\n",
      "        [2.3409e-05],\n",
      "        [3.7014e-03],\n",
      "        [9.4331e-01],\n",
      "        [9.7782e-01],\n",
      "        [4.5356e-03],\n",
      "        [3.0302e-02],\n",
      "        [9.4353e-01],\n",
      "        [3.2323e-01],\n",
      "        [2.1433e-03],\n",
      "        [2.1639e-07]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.2535473108291626: \n",
      "target probs tensor([[1.0154e-02],\n",
      "        [8.0981e-03],\n",
      "        [1.2118e-07],\n",
      "        [6.7725e-02],\n",
      "        [4.2686e-03],\n",
      "        [1.1951e-03],\n",
      "        [1.3037e-01],\n",
      "        [5.9280e-01],\n",
      "        [1.9388e-02],\n",
      "        [1.1505e-01],\n",
      "        [9.9976e-01],\n",
      "        [3.6244e-02],\n",
      "        [9.7027e-01],\n",
      "        [2.3280e-01],\n",
      "        [8.1250e-06],\n",
      "        [9.6107e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.042649507522583: \n",
      "target probs tensor([[2.4401e-02],\n",
      "        [1.4241e-01],\n",
      "        [2.5627e-02],\n",
      "        [2.9925e-01],\n",
      "        [4.0727e-07],\n",
      "        [6.2183e-06],\n",
      "        [9.7634e-01],\n",
      "        [4.3178e-06],\n",
      "        [2.7484e-08],\n",
      "        [8.9203e-01],\n",
      "        [6.0573e-03],\n",
      "        [9.9454e-01],\n",
      "        [3.8295e-05],\n",
      "        [2.3112e-04],\n",
      "        [1.0369e-06],\n",
      "        [7.8191e-04]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.7342205047607422: \n",
      "target probs tensor([[2.2134e-01],\n",
      "        [1.3624e-04],\n",
      "        [1.2764e-01],\n",
      "        [1.9834e-02],\n",
      "        [3.8974e-01],\n",
      "        [3.9485e-02],\n",
      "        [7.9426e-03],\n",
      "        [2.6674e-01],\n",
      "        [9.3002e-02],\n",
      "        [2.8523e-04],\n",
      "        [2.2185e-02],\n",
      "        [3.5448e-04],\n",
      "        [1.2524e-06],\n",
      "        [1.2099e-02],\n",
      "        [2.1939e-02],\n",
      "        [2.6714e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.10782106965780258: \n",
      "target probs tensor([[1.3860e-03],\n",
      "        [5.0260e-05],\n",
      "        [6.6612e-05],\n",
      "        [3.3759e-03],\n",
      "        [1.6432e-01],\n",
      "        [9.8757e-01],\n",
      "        [2.9270e-07],\n",
      "        [5.6956e-01],\n",
      "        [4.1739e-02],\n",
      "        [9.9012e-04],\n",
      "        [1.9203e-04],\n",
      "        [2.4888e-04],\n",
      "        [9.9753e-01],\n",
      "        [9.1526e-01],\n",
      "        [5.7930e-04],\n",
      "        [2.0194e-06]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.8705894947052002: \n",
      "target probs tensor([[1.5124e-01],\n",
      "        [8.2001e-06],\n",
      "        [4.1053e-01],\n",
      "        [7.5084e-01],\n",
      "        [7.4044e-04],\n",
      "        [9.9891e-01],\n",
      "        [6.4663e-08],\n",
      "        [1.9473e-04],\n",
      "        [1.5584e-06],\n",
      "        [8.3164e-05],\n",
      "        [1.4831e-06],\n",
      "        [1.5494e-01],\n",
      "        [2.8852e-01],\n",
      "        [6.3358e-03],\n",
      "        [1.5365e-03],\n",
      "        [9.8244e-02]], device='cuda:1'), loss: 0.5951845049858093: \n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "input shape:  torch.Size([8, 1000])\n",
      "target shape:  torch.Size([8, 1000]) \n",
      "\n",
      "\n",
      "Better model found at epoch 0 with validation value: 0.6909999847412109.\n",
      "target probs tensor([[1.1645e-01],\n",
      "        [2.4960e-04],\n",
      "        [5.1975e-05],\n",
      "        [9.9975e-01],\n",
      "        [2.2445e-07],\n",
      "        [9.0797e-08],\n",
      "        [7.8407e-03],\n",
      "        [6.4122e-08],\n",
      "        [9.5612e-07],\n",
      "        [8.1325e-08],\n",
      "        [1.2928e-02],\n",
      "        [1.5868e-02],\n",
      "        [1.9234e-04],\n",
      "        [7.6780e-01],\n",
      "        [1.0138e-02],\n",
      "        [5.0172e-03]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.6216751337051392: \n",
      "target probs tensor([[3.5503e-04],\n",
      "        [1.0516e-05],\n",
      "        [1.6737e-02],\n",
      "        [1.8031e-02],\n",
      "        [9.0651e-07],\n",
      "        [1.2368e-01],\n",
      "        [9.6217e-01],\n",
      "        [9.9857e-01],\n",
      "        [2.0493e-03],\n",
      "        [5.4945e-01],\n",
      "        [1.1388e-03],\n",
      "        [9.4592e-03],\n",
      "        [1.8206e-01],\n",
      "        [6.6592e-02],\n",
      "        [9.4142e-01],\n",
      "        [9.9877e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.2880550622940063: \n",
      "target probs tensor([[3.3812e-03],\n",
      "        [5.6021e-01],\n",
      "        [1.1454e-01],\n",
      "        [3.5002e-03],\n",
      "        [1.3078e-02],\n",
      "        [3.5847e-05],\n",
      "        [1.1509e-04],\n",
      "        [4.7521e-08],\n",
      "        [1.0079e-02],\n",
      "        [1.0792e-06],\n",
      "        [9.9357e-01],\n",
      "        [8.5031e-01],\n",
      "        [9.7944e-04],\n",
      "        [7.1844e-04],\n",
      "        [2.9778e-01],\n",
      "        [1.2910e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.525766909122467: \n",
      "target probs tensor([[5.2471e-05],\n",
      "        [5.3910e-02],\n",
      "        [1.2508e-02],\n",
      "        [9.9756e-01],\n",
      "        [2.7796e-01],\n",
      "        [6.6547e-06],\n",
      "        [2.4497e-05],\n",
      "        [1.2438e-05],\n",
      "        [8.2739e-01],\n",
      "        [3.0641e-01],\n",
      "        [1.0932e-02],\n",
      "        [1.1767e-04],\n",
      "        [1.7225e-01],\n",
      "        [1.1532e-01],\n",
      "        [8.6172e-02],\n",
      "        [5.2801e-06]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.559012770652771: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[4.6807e-07],\n",
      "        [1.3505e-08],\n",
      "        [4.4182e-02],\n",
      "        [5.1505e-03],\n",
      "        [5.2754e-01],\n",
      "        [6.6834e-04],\n",
      "        [1.2807e-01],\n",
      "        [4.8740e-03],\n",
      "        [6.6453e-01],\n",
      "        [6.7562e-06],\n",
      "        [5.0399e-01],\n",
      "        [3.9649e-02],\n",
      "        [7.4958e-01],\n",
      "        [6.9961e-08],\n",
      "        [9.2473e-03],\n",
      "        [3.5817e-04]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.2606799006462097: \n",
      "target probs tensor([[3.4409e-03],\n",
      "        [2.7063e-04],\n",
      "        [1.5606e-01],\n",
      "        [5.2328e-01],\n",
      "        [3.3169e-05],\n",
      "        [1.8378e-03],\n",
      "        [1.0937e-03],\n",
      "        [4.8812e-05],\n",
      "        [5.9867e-01],\n",
      "        [9.9835e-07],\n",
      "        [7.7936e-01],\n",
      "        [2.3792e-01],\n",
      "        [2.3090e-01],\n",
      "        [1.5054e-03],\n",
      "        [9.9985e-01],\n",
      "        [1.7995e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.8054421544075012: \n",
      "target probs tensor([[6.2742e-01],\n",
      "        [1.2410e-03],\n",
      "        [3.2348e-03],\n",
      "        [1.8736e-01],\n",
      "        [9.6224e-02],\n",
      "        [1.3710e-02],\n",
      "        [2.4125e-04],\n",
      "        [2.2535e-01],\n",
      "        [6.6336e-02],\n",
      "        [1.0325e-04],\n",
      "        [5.1125e-06],\n",
      "        [5.0211e-01],\n",
      "        [8.5554e-01],\n",
      "        [1.4805e-01],\n",
      "        [3.5221e-06],\n",
      "        [3.5116e-03]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.27715441584587097: \n",
      "target probs tensor([[3.7036e-01],\n",
      "        [8.1889e-01],\n",
      "        [4.7178e-04],\n",
      "        [1.3029e-07],\n",
      "        [3.7875e-04],\n",
      "        [7.2154e-01],\n",
      "        [3.0162e-06],\n",
      "        [2.8083e-06],\n",
      "        [7.4201e-01],\n",
      "        [9.9210e-01],\n",
      "        [9.9349e-01],\n",
      "        [2.5338e-04],\n",
      "        [4.6102e-07],\n",
      "        [4.6089e-02],\n",
      "        [3.8767e-01],\n",
      "        [7.6191e-01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.0408155918121338: \n"
     ]
    }
   ],
   "source": [
    "if mode == \"sanity_check\":\n",
    "  print(\"\\n\\n\\nWARNING: you are training on a sanity_check dataset.\\n\\n\\n\\n\")\n",
    "\n",
    "saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=env.save_filename + \"-best\")\n",
    "saver_every_epoch = SaveModelCallback(learn, every='epoch', name=env.save_filename)\n",
    "\n",
    "# with Hooks(gen, append_stats_normal) as hooks:\n",
    "#   learn.fit(1, lr=5e-03, wd = 0., callbacks=[saver_best, saver_every_epoch])\n",
    "  \n",
    "# 40 is too much, use just 20\n",
    "learn.fit(40, lr=1e-2, wd = 0., callbacks=[saver_best, saver_every_epoch])\n",
    "\n",
    "# # learn.fit(70, lr=1e-02, wd = 0.001, callbacks=[saver_best, saver_every_epoch])\n",
    "\n",
    "# learn.fit(60, lr=1e-2, wd = 0.001, callbacks=[saver_best, saver_every_epoch])\n",
    "\n",
    "# for i in range(10):\n",
    "#   learn.fit_one_cycle(7, wd = 0.,max_lr=1., div_factor = 1000.) \n",
    "  \n",
    "# learn.fit_one_cycle(5, max_lr=2e-2) #used for vgg-19-bn\n",
    "# learn.fit_one_cycle(5, max_lr=3e-3) # used for resnet50\n",
    "\n",
    "shutil.copyfile(env.temp_csv_path + '/' + env.save_filename + \".csv\", env.get_csv_path() + '.csv')\n",
    "shutil.copytree(env.data_path/env.get_learner_models_dir(), env.get_models_path())\n",
    "shutil.rmtree(env.data_path/env.get_learner_models_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram\n",
    "fig, axes = plt.subplots(len(hooks),1, figsize=(30,12))\n",
    "for ax,h in zip(axes.flatten(), hooks):\n",
    "  ax.imshow(get_hist(h), origin='lower')\n",
    "  ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mean and std\n",
    "fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "for h in hooks:\n",
    "  ms, ss, _ = h.stats\n",
    "  ax0.plot(ms[:100])\n",
    "  ax1.plot(ss[:100])\n",
    "plt.legend(range(len(hooks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "for h in hooks:\n",
    "  ms, ss, _ = h.stats\n",
    "  ax0.plot(ms)\n",
    "  ax1.plot(ss)\n",
    "plt.legend(range(len(hooks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero precentage:\n",
    "fig,axes = plt.subplots(len(hooks),1, figsize=(30,30))\n",
    "for ax,h in zip(axes.flatten(), hooks):\n",
    "    ax.plot(get_min(h))\n",
    "    ax.set_ylim(0,1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MO2fZ-hSSUzJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# z1 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "# z2 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "z1 = torch.tensor([0.8, -0.5] * 5).cuda()\n",
    "z2 = torch.tensor([-1.] * 10).cuda()\n",
    "print(\"z1: \", z1)\n",
    "print(\"z2: \", z2)\n",
    "print(\"distance: \", torch.norm(z1-z2,p=2))\n",
    "model = learn.model.eval()\n",
    "\n",
    "z_s = interpolate(z1, z2, 0.1)\n",
    "print(len(z_s))\n",
    "\n",
    "for i,z in enumerate(z_s):\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n",
    "  #img.save('./pics/' + str(i) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DGuGN7B7V0Xt"
   },
   "outputs": [],
   "source": [
    "def generate_perturbations(learn, n_perturbations):\n",
    "  initial_training_mode = learn.model.training\n",
    "  \n",
    "  model = learn.model.eval()\n",
    "  input_img = (learn.data.valid_ds[0][0].data)[None].cuda()\n",
    "  perturbations = []\n",
    "  for i in range(n_perturbations):\n",
    "    perturbation = model(input_img)[0].squeeze()\n",
    "    perturbations.append(perturbation)\n",
    "    \n",
    "  learn.model.train(initial_training_mode)  \n",
    "  return perturbations\n",
    "\n",
    "def compute_prediction_histogram(learn, perturbation, verbose=False):\n",
    "  pred_hist = [0] * 1000\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 and verbose: print (\"at batch no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbation[None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      pred_hist[pred] += 1\n",
    "  return pred_hist\n",
    "\n",
    "\n",
    "def compute_mean_prediction_histogram(learn, perturbations):\n",
    "  pred_histogram = torch.tensor([0] * 1000).detach_()\n",
    "  for j, perturbation in enumerate(perturbations):\n",
    "    pred_histogram_j = torch.tensor(compute_prediction_histogram(learn, perturbation, True)).detach_()\n",
    "    pred_histogram += pred_histogram_j\n",
    "    print(\"finished creating histogram for the {}th perturbation\".format(j))\n",
    "  \n",
    "  pred_histogram = pred_histogram.float() / len(perturbations)\n",
    "  return pred_histogram.tolist()\n",
    "\n",
    "\n",
    "def diversity(learn, n_perturbations, percentage = 95):\n",
    "  pred_histogram = compute_mean_prediction_histogram(\n",
    "      learn, generate_perturbations(learn, n_perturbations)\n",
    "  )\n",
    "  print(\"finished creating the prediction histogram\")\n",
    "  pred_histogram_sum = np.sum(pred_histogram)\n",
    "\n",
    "  indexed_pred_histogram = [(i, hist_element) for i,hist_element in  \n",
    "                            enumerate(pred_histogram)]\n",
    "\n",
    "  indexed_pred_histogram.sort(key=lambda x: x[1], reverse = True)\n",
    "\n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  top_classes = []\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = indexed_pred_histogram[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / pred_histogram_sum) * 100.\n",
    "    top_classes.append(hist_elem[0])\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, indexed_pred_histogram, top_classes\n",
    "\n",
    "# idea : have 200 noises (1 for each class), then start iterating the dataset, and for each image, randomly apply one noise and record the result\n",
    "def targeted_diversity(learn, n_perturbations = 200, percentage = 95):\n",
    "  model = learn.model.eval()\n",
    "\n",
    "  one_hot_conditions = [torch.empty(z_dim).uniform_(0,1).cuda().detach() for _ in range(n_perturbations)]\n",
    "#   for i in range(z_dim):\n",
    "#     one_hot_conditions[i][i] = 1.\n",
    "\n",
    "  perturbations = [model.forward_single_z(z) for z in one_hot_conditions]\n",
    "\n",
    "  hist = [0.] * z_dim\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 : print(\"at batch_no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbations[np.random.randint(0,len(perturbations))][None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      hist[pred] += 1\n",
    "\n",
    "  pred_histogram_sum = np.sum(hist)\n",
    "  indexed_pred_histogram = [(i, hist_element) for i,hist_element in  \n",
    "                            enumerate(hist)]\n",
    "\n",
    "  indexed_pred_histogram.sort(key=lambda x: x[1], reverse = True)\n",
    "\n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = indexed_pred_histogram[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / pred_histogram_sum) * 100.\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, indexed_pred_histogram\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "finished creating histogram for the 0th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 1th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 2th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 3th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 4th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 5th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 6th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 7th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 8th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 9th perturbation\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(220,\n",
       " [(971, 302.10),\n",
       "  (854, 210.50),\n",
       "  (611, 127.00),\n",
       "  (750, 40.50),\n",
       "  (721, 20.00),\n",
       "  (741, 10.50),\n",
       "  (645, 7.40),\n",
       "  (84, 5.90),\n",
       "  (808, 5.30),\n",
       "  (509, 5.20),\n",
       "  (815, 5.10),\n",
       "  (703, 4.50),\n",
       "  (594, 4.20),\n",
       "  (973, 4.20),\n",
       "  (646, 3.30),\n",
       "  (824, 3.20),\n",
       "  (506, 3.10),\n",
       "  (794, 3.10),\n",
       "  (489, 2.90),\n",
       "  (881, 2.90),\n",
       "  (735, 2.40),\n",
       "  (775, 2.30),\n",
       "  (641, 2.20),\n",
       "  (68, 2.10),\n",
       "  (806, 2.10),\n",
       "  (580, 2.00),\n",
       "  (108, 1.90),\n",
       "  (464, 1.90),\n",
       "  (748, 1.90),\n",
       "  (944, 1.90),\n",
       "  (109, 1.80),\n",
       "  (946, 1.80),\n",
       "  (800, 1.70),\n",
       "  (879, 1.70),\n",
       "  (60, 1.60),\n",
       "  (893, 1.60),\n",
       "  (123, 1.40),\n",
       "  (858, 1.40),\n",
       "  (898, 1.40),\n",
       "  (55, 1.30),\n",
       "  (128, 1.30),\n",
       "  (492, 1.30),\n",
       "  (696, 1.30),\n",
       "  (96, 1.20),\n",
       "  (360, 1.20),\n",
       "  (411, 1.20),\n",
       "  (429, 1.20),\n",
       "  (440, 1.20),\n",
       "  (472, 1.20),\n",
       "  (533, 1.20),\n",
       "  (816, 1.20),\n",
       "  (911, 1.20),\n",
       "  (963, 1.20),\n",
       "  (982, 1.20),\n",
       "  (86, 1.10),\n",
       "  (354, 1.10),\n",
       "  (454, 1.10),\n",
       "  (468, 1.10),\n",
       "  (474, 1.10),\n",
       "  (549, 1.10),\n",
       "  (575, 1.10),\n",
       "  (738, 1.10),\n",
       "  (787, 1.10),\n",
       "  (865, 1.10),\n",
       "  (987, 1.10),\n",
       "  (25, 1.00),\n",
       "  (37, 1.00),\n",
       "  (52, 1.00),\n",
       "  (57, 1.00),\n",
       "  (91, 1.00),\n",
       "  (118, 1.00),\n",
       "  (163, 1.00),\n",
       "  (193, 1.00),\n",
       "  (242, 1.00),\n",
       "  (247, 1.00),\n",
       "  (274, 1.00),\n",
       "  (281, 1.00),\n",
       "  (290, 1.00),\n",
       "  (292, 1.00),\n",
       "  (314, 1.00),\n",
       "  (376, 1.00),\n",
       "  (393, 1.00),\n",
       "  (507, 1.00),\n",
       "  (566, 1.00),\n",
       "  (621, 1.00),\n",
       "  (625, 1.00),\n",
       "  (637, 1.00),\n",
       "  (709, 1.00),\n",
       "  (716, 1.00),\n",
       "  (783, 1.00),\n",
       "  (937, 1.00),\n",
       "  (939, 1.00),\n",
       "  (953, 1.00),\n",
       "  (7, 0.90),\n",
       "  (76, 0.90),\n",
       "  (110, 0.90),\n",
       "  (192, 0.90),\n",
       "  (260, 0.90),\n",
       "  (300, 0.90),\n",
       "  (301, 0.90),\n",
       "  (342, 0.90),\n",
       "  (378, 0.90),\n",
       "  (398, 0.90),\n",
       "  (457, 0.90),\n",
       "  (496, 0.90),\n",
       "  (498, 0.90),\n",
       "  (502, 0.90),\n",
       "  (567, 0.90),\n",
       "  (593, 0.90),\n",
       "  (614, 0.90),\n",
       "  (654, 0.90),\n",
       "  (684, 0.90),\n",
       "  (768, 0.90),\n",
       "  (850, 0.90),\n",
       "  (868, 0.90),\n",
       "  (907, 0.90),\n",
       "  (923, 0.90),\n",
       "  (934, 0.90),\n",
       "  (955, 0.90),\n",
       "  (992, 0.90),\n",
       "  (33, 0.80),\n",
       "  (74, 0.80),\n",
       "  (92, 0.80),\n",
       "  (120, 0.80),\n",
       "  (160, 0.80),\n",
       "  (164, 0.80),\n",
       "  (218, 0.80),\n",
       "  (309, 0.80),\n",
       "  (316, 0.80),\n",
       "  (327, 0.80),\n",
       "  (355, 0.80),\n",
       "  (401, 0.80),\n",
       "  (476, 0.80),\n",
       "  (508, 0.80),\n",
       "  (576, 0.80),\n",
       "  (612, 0.80),\n",
       "  (629, 0.80),\n",
       "  (661, 0.80),\n",
       "  (694, 0.80),\n",
       "  (826, 0.80),\n",
       "  (864, 0.80),\n",
       "  (131, 0.70),\n",
       "  (151, 0.70),\n",
       "  (189, 0.70),\n",
       "  (200, 0.70),\n",
       "  (302, 0.70),\n",
       "  (304, 0.70),\n",
       "  (306, 0.70),\n",
       "  (347, 0.70),\n",
       "  (381, 0.70),\n",
       "  (406, 0.70),\n",
       "  (446, 0.70),\n",
       "  (483, 0.70),\n",
       "  (488, 0.70),\n",
       "  (520, 0.70),\n",
       "  (528, 0.70),\n",
       "  (535, 0.70),\n",
       "  (555, 0.70),\n",
       "  (582, 0.70),\n",
       "  (643, 0.70),\n",
       "  (737, 0.70),\n",
       "  (753, 0.70),\n",
       "  (779, 0.70),\n",
       "  (863, 0.70),\n",
       "  (889, 0.70),\n",
       "  (892, 0.70),\n",
       "  (906, 0.70),\n",
       "  (100, 0.60),\n",
       "  (176, 0.60),\n",
       "  (235, 0.60),\n",
       "  (307, 0.60),\n",
       "  (331, 0.60),\n",
       "  (334, 0.60),\n",
       "  (363, 0.60),\n",
       "  (392, 0.60),\n",
       "  (497, 0.60),\n",
       "  (563, 0.60),\n",
       "  (581, 0.60),\n",
       "  (791, 0.60),\n",
       "  (825, 0.60),\n",
       "  (857, 0.60),\n",
       "  (962, 0.60),\n",
       "  (42, 0.50),\n",
       "  (83, 0.50),\n",
       "  (94, 0.50),\n",
       "  (99, 0.50),\n",
       "  (121, 0.50),\n",
       "  (124, 0.50),\n",
       "  (198, 0.50),\n",
       "  (205, 0.50),\n",
       "  (230, 0.50),\n",
       "  (336, 0.50),\n",
       "  (369, 0.50),\n",
       "  (417, 0.50),\n",
       "  (424, 0.50),\n",
       "  (490, 0.50),\n",
       "  (679, 0.50),\n",
       "  (729, 0.50),\n",
       "  (777, 0.50),\n",
       "  (831, 0.50),\n",
       "  (872, 0.50),\n",
       "  (890, 0.50),\n",
       "  (996, 0.50),\n",
       "  (31, 0.40),\n",
       "  (46, 0.40),\n",
       "  (47, 0.40),\n",
       "  (48, 0.40),\n",
       "  (61, 0.40),\n",
       "  (115, 0.40),\n",
       "  (186, 0.40),\n",
       "  (219, 0.40),\n",
       "  (231, 0.40),\n",
       "  (275, 0.40),\n",
       "  (291, 0.40),\n",
       "  (321, 0.40),\n",
       "  (348, 0.40),\n",
       "  (364, 0.40),\n",
       "  (375, 0.40),\n",
       "  (408, 0.40),\n",
       "  (415, 0.40),\n",
       "  (459, 0.40),\n",
       "  (484, 0.40),\n",
       "  (541, 0.40),\n",
       "  (552, 0.40),\n",
       "  (558, 0.40),\n",
       "  (561, 0.40),\n",
       "  (564, 0.40),\n",
       "  (672, 0.40),\n",
       "  (736, 0.40),\n",
       "  (758, 0.40),\n",
       "  (762, 0.40),\n",
       "  (790, 0.40),\n",
       "  (801, 0.40),\n",
       "  (832, 0.40),\n",
       "  (839, 0.40),\n",
       "  (878, 0.40),\n",
       "  (910, 0.40),\n",
       "  (968, 0.40),\n",
       "  (981, 0.40),\n",
       "  (17, 0.30),\n",
       "  (24, 0.30),\n",
       "  (28, 0.30),\n",
       "  (30, 0.30),\n",
       "  (78, 0.30),\n",
       "  (87, 0.30),\n",
       "  (90, 0.30),\n",
       "  (119, 0.30),\n",
       "  (144, 0.30),\n",
       "  (182, 0.30),\n",
       "  (234, 0.30),\n",
       "  (293, 0.30),\n",
       "  (310, 0.30),\n",
       "  (343, 0.30),\n",
       "  (434, 0.30),\n",
       "  (445, 0.30),\n",
       "  (453, 0.30),\n",
       "  (455, 0.30),\n",
       "  (456, 0.30),\n",
       "  (518, 0.30),\n",
       "  (523, 0.30),\n",
       "  (526, 0.30),\n",
       "  (538, 0.30),\n",
       "  (546, 0.30),\n",
       "  (584, 0.30),\n",
       "  (586, 0.30),\n",
       "  (603, 0.30),\n",
       "  (607, 0.30),\n",
       "  (697, 0.30),\n",
       "  (700, 0.30),\n",
       "  (705, 0.30),\n",
       "  (730, 0.30),\n",
       "  (781, 0.30),\n",
       "  (788, 0.30),\n",
       "  (822, 0.30),\n",
       "  (823, 0.30),\n",
       "  (843, 0.30),\n",
       "  (852, 0.30),\n",
       "  (880, 0.30),\n",
       "  (887, 0.30),\n",
       "  (947, 0.30),\n",
       "  (958, 0.30),\n",
       "  (989, 0.30),\n",
       "  (8, 0.20),\n",
       "  (9, 0.20),\n",
       "  (10, 0.20),\n",
       "  (15, 0.20),\n",
       "  (18, 0.20),\n",
       "  (39, 0.20),\n",
       "  (50, 0.20),\n",
       "  (62, 0.20),\n",
       "  (63, 0.20),\n",
       "  (65, 0.20),\n",
       "  (71, 0.20),\n",
       "  (72, 0.20),\n",
       "  (93, 0.20),\n",
       "  (98, 0.20),\n",
       "  (102, 0.20),\n",
       "  (105, 0.20),\n",
       "  (116, 0.20),\n",
       "  (132, 0.20),\n",
       "  (134, 0.20),\n",
       "  (136, 0.20),\n",
       "  (141, 0.20),\n",
       "  (188, 0.20),\n",
       "  (195, 0.20),\n",
       "  (206, 0.20),\n",
       "  (214, 0.20),\n",
       "  (228, 0.20),\n",
       "  (237, 0.20),\n",
       "  (253, 0.20),\n",
       "  (256, 0.20),\n",
       "  (289, 0.20),\n",
       "  (298, 0.20),\n",
       "  (305, 0.20),\n",
       "  (315, 0.20),\n",
       "  (317, 0.20),\n",
       "  (319, 0.20),\n",
       "  (350, 0.20),\n",
       "  (361, 0.20),\n",
       "  (383, 0.20),\n",
       "  (423, 0.20),\n",
       "  (425, 0.20),\n",
       "  (427, 0.20),\n",
       "  (428, 0.20),\n",
       "  (443, 0.20),\n",
       "  (452, 0.20),\n",
       "  (491, 0.20),\n",
       "  (514, 0.20),\n",
       "  (527, 0.20),\n",
       "  (530, 0.20),\n",
       "  (562, 0.20),\n",
       "  (565, 0.20),\n",
       "  (569, 0.20),\n",
       "  (572, 0.20),\n",
       "  (579, 0.20),\n",
       "  (585, 0.20),\n",
       "  (588, 0.20),\n",
       "  (604, 0.20),\n",
       "  (616, 0.20),\n",
       "  (618, 0.20),\n",
       "  (619, 0.20),\n",
       "  (633, 0.20),\n",
       "  (636, 0.20),\n",
       "  (652, 0.20),\n",
       "  (656, 0.20),\n",
       "  (676, 0.20),\n",
       "  (688, 0.20),\n",
       "  (692, 0.20),\n",
       "  (695, 0.20),\n",
       "  (711, 0.20),\n",
       "  (717, 0.20),\n",
       "  (724, 0.20),\n",
       "  (733, 0.20),\n",
       "  (746, 0.20),\n",
       "  (751, 0.20),\n",
       "  (754, 0.20),\n",
       "  (756, 0.20),\n",
       "  (778, 0.20),\n",
       "  (786, 0.20),\n",
       "  (793, 0.20),\n",
       "  (809, 0.20),\n",
       "  (817, 0.20),\n",
       "  (819, 0.20),\n",
       "  (820, 0.20),\n",
       "  (828, 0.20),\n",
       "  (829, 0.20),\n",
       "  (849, 0.20),\n",
       "  (853, 0.20),\n",
       "  (870, 0.20),\n",
       "  (871, 0.20),\n",
       "  (873, 0.20),\n",
       "  (886, 0.20),\n",
       "  (905, 0.20),\n",
       "  (921, 0.20),\n",
       "  (922, 0.20),\n",
       "  (932, 0.20),\n",
       "  (957, 0.20),\n",
       "  (997, 0.20),\n",
       "  (999, 0.20),\n",
       "  (1, 0.10),\n",
       "  (36, 0.10),\n",
       "  (45, 0.10),\n",
       "  (53, 0.10),\n",
       "  (59, 0.10),\n",
       "  (66, 0.10),\n",
       "  (97, 0.10),\n",
       "  (122, 0.10),\n",
       "  (150, 0.10),\n",
       "  (155, 0.10),\n",
       "  (169, 0.10),\n",
       "  (183, 0.10),\n",
       "  (184, 0.10),\n",
       "  (187, 0.10),\n",
       "  (211, 0.10),\n",
       "  (224, 0.10),\n",
       "  (226, 0.10),\n",
       "  (229, 0.10),\n",
       "  (233, 0.10),\n",
       "  (236, 0.10),\n",
       "  (243, 0.10),\n",
       "  (254, 0.10),\n",
       "  (255, 0.10),\n",
       "  (264, 0.10),\n",
       "  (267, 0.10),\n",
       "  (268, 0.10),\n",
       "  (284, 0.10),\n",
       "  (294, 0.10),\n",
       "  (337, 0.10),\n",
       "  (351, 0.10),\n",
       "  (353, 0.10),\n",
       "  (365, 0.10),\n",
       "  (366, 0.10),\n",
       "  (374, 0.10),\n",
       "  (379, 0.10),\n",
       "  (384, 0.10),\n",
       "  (387, 0.10),\n",
       "  (397, 0.10),\n",
       "  (407, 0.10),\n",
       "  (414, 0.10),\n",
       "  (416, 0.10),\n",
       "  (418, 0.10),\n",
       "  (431, 0.10),\n",
       "  (433, 0.10),\n",
       "  (444, 0.10),\n",
       "  (448, 0.10),\n",
       "  (471, 0.10),\n",
       "  (479, 0.10),\n",
       "  (481, 0.10),\n",
       "  (495, 0.10),\n",
       "  (503, 0.10),\n",
       "  (512, 0.10),\n",
       "  (515, 0.10),\n",
       "  (516, 0.10),\n",
       "  (521, 0.10),\n",
       "  (522, 0.10),\n",
       "  (532, 0.10),\n",
       "  (570, 0.10),\n",
       "  (574, 0.10),\n",
       "  (577, 0.10),\n",
       "  (606, 0.10),\n",
       "  (609, 0.10),\n",
       "  (613, 0.10),\n",
       "  (627, 0.10),\n",
       "  (639, 0.10),\n",
       "  (653, 0.10),\n",
       "  (659, 0.10),\n",
       "  (664, 0.10),\n",
       "  (670, 0.10),\n",
       "  (687, 0.10),\n",
       "  (698, 0.10),\n",
       "  (699, 0.10),\n",
       "  (701, 0.10),\n",
       "  (702, 0.10),\n",
       "  (763, 0.10),\n",
       "  (765, 0.10),\n",
       "  (767, 0.10),\n",
       "  (770, 0.10),\n",
       "  (782, 0.10),\n",
       "  (784, 0.10),\n",
       "  (805, 0.10),\n",
       "  (821, 0.10),\n",
       "  (830, 0.10),\n",
       "  (833, 0.10),\n",
       "  (834, 0.10),\n",
       "  (847, 0.10),\n",
       "  (867, 0.10),\n",
       "  (874, 0.10),\n",
       "  (882, 0.10),\n",
       "  (883, 0.10),\n",
       "  (885, 0.10),\n",
       "  (888, 0.10),\n",
       "  (914, 0.10),\n",
       "  (917, 0.10),\n",
       "  (920, 0.10),\n",
       "  (924, 0.10),\n",
       "  (938, 0.10),\n",
       "  (954, 0.10),\n",
       "  (972, 0.10),\n",
       "  (991, 0.10),\n",
       "  (0, 0.00),\n",
       "  (2, 0.00),\n",
       "  (3, 0.00),\n",
       "  (4, 0.00),\n",
       "  (5, 0.00),\n",
       "  (6, 0.00),\n",
       "  (11, 0.00),\n",
       "  (12, 0.00),\n",
       "  (13, 0.00),\n",
       "  (14, 0.00),\n",
       "  (16, 0.00),\n",
       "  (19, 0.00),\n",
       "  (20, 0.00),\n",
       "  (21, 0.00),\n",
       "  (22, 0.00),\n",
       "  (23, 0.00),\n",
       "  (26, 0.00),\n",
       "  (27, 0.00),\n",
       "  (29, 0.00),\n",
       "  (32, 0.00),\n",
       "  (34, 0.00),\n",
       "  (35, 0.00),\n",
       "  (38, 0.00),\n",
       "  (40, 0.00),\n",
       "  (41, 0.00),\n",
       "  (43, 0.00),\n",
       "  (44, 0.00),\n",
       "  (49, 0.00),\n",
       "  (51, 0.00),\n",
       "  (54, 0.00),\n",
       "  (56, 0.00),\n",
       "  (58, 0.00),\n",
       "  (64, 0.00),\n",
       "  (67, 0.00),\n",
       "  (69, 0.00),\n",
       "  (70, 0.00),\n",
       "  (73, 0.00),\n",
       "  (75, 0.00),\n",
       "  (77, 0.00),\n",
       "  (79, 0.00),\n",
       "  (80, 0.00),\n",
       "  (81, 0.00),\n",
       "  (82, 0.00),\n",
       "  (85, 0.00),\n",
       "  (88, 0.00),\n",
       "  (89, 0.00),\n",
       "  (95, 0.00),\n",
       "  (101, 0.00),\n",
       "  (103, 0.00),\n",
       "  (104, 0.00),\n",
       "  (106, 0.00),\n",
       "  (107, 0.00),\n",
       "  (111, 0.00),\n",
       "  (112, 0.00),\n",
       "  (113, 0.00),\n",
       "  (114, 0.00),\n",
       "  (117, 0.00),\n",
       "  (125, 0.00),\n",
       "  (126, 0.00),\n",
       "  (127, 0.00),\n",
       "  (129, 0.00),\n",
       "  (130, 0.00),\n",
       "  (133, 0.00),\n",
       "  (135, 0.00),\n",
       "  (137, 0.00),\n",
       "  (138, 0.00),\n",
       "  (139, 0.00),\n",
       "  (140, 0.00),\n",
       "  (142, 0.00),\n",
       "  (143, 0.00),\n",
       "  (145, 0.00),\n",
       "  (146, 0.00),\n",
       "  (147, 0.00),\n",
       "  (148, 0.00),\n",
       "  (149, 0.00),\n",
       "  (152, 0.00),\n",
       "  (153, 0.00),\n",
       "  (154, 0.00),\n",
       "  (156, 0.00),\n",
       "  (157, 0.00),\n",
       "  (158, 0.00),\n",
       "  (159, 0.00),\n",
       "  (161, 0.00),\n",
       "  (162, 0.00),\n",
       "  (165, 0.00),\n",
       "  (166, 0.00),\n",
       "  (167, 0.00),\n",
       "  (168, 0.00),\n",
       "  (170, 0.00),\n",
       "  (171, 0.00),\n",
       "  (172, 0.00),\n",
       "  (173, 0.00),\n",
       "  (174, 0.00),\n",
       "  (175, 0.00),\n",
       "  (177, 0.00),\n",
       "  (178, 0.00),\n",
       "  (179, 0.00),\n",
       "  (180, 0.00),\n",
       "  (181, 0.00),\n",
       "  (185, 0.00),\n",
       "  (190, 0.00),\n",
       "  (191, 0.00),\n",
       "  (194, 0.00),\n",
       "  (196, 0.00),\n",
       "  (197, 0.00),\n",
       "  (199, 0.00),\n",
       "  (201, 0.00),\n",
       "  (202, 0.00),\n",
       "  (203, 0.00),\n",
       "  (204, 0.00),\n",
       "  (207, 0.00),\n",
       "  (208, 0.00),\n",
       "  (209, 0.00),\n",
       "  (210, 0.00),\n",
       "  (212, 0.00),\n",
       "  (213, 0.00),\n",
       "  (215, 0.00),\n",
       "  (216, 0.00),\n",
       "  (217, 0.00),\n",
       "  (220, 0.00),\n",
       "  (221, 0.00),\n",
       "  (222, 0.00),\n",
       "  (223, 0.00),\n",
       "  (225, 0.00),\n",
       "  (227, 0.00),\n",
       "  (232, 0.00),\n",
       "  (238, 0.00),\n",
       "  (239, 0.00),\n",
       "  (240, 0.00),\n",
       "  (241, 0.00),\n",
       "  (244, 0.00),\n",
       "  (245, 0.00),\n",
       "  (246, 0.00),\n",
       "  (248, 0.00),\n",
       "  (249, 0.00),\n",
       "  (250, 0.00),\n",
       "  (251, 0.00),\n",
       "  (252, 0.00),\n",
       "  (257, 0.00),\n",
       "  (258, 0.00),\n",
       "  (259, 0.00),\n",
       "  (261, 0.00),\n",
       "  (262, 0.00),\n",
       "  (263, 0.00),\n",
       "  (265, 0.00),\n",
       "  (266, 0.00),\n",
       "  (269, 0.00),\n",
       "  (270, 0.00),\n",
       "  (271, 0.00),\n",
       "  (272, 0.00),\n",
       "  (273, 0.00),\n",
       "  (276, 0.00),\n",
       "  (277, 0.00),\n",
       "  (278, 0.00),\n",
       "  (279, 0.00),\n",
       "  (280, 0.00),\n",
       "  (282, 0.00),\n",
       "  (283, 0.00),\n",
       "  (285, 0.00),\n",
       "  (286, 0.00),\n",
       "  (287, 0.00),\n",
       "  (288, 0.00),\n",
       "  (295, 0.00),\n",
       "  (296, 0.00),\n",
       "  (297, 0.00),\n",
       "  (299, 0.00),\n",
       "  (303, 0.00),\n",
       "  (308, 0.00),\n",
       "  (311, 0.00),\n",
       "  (312, 0.00),\n",
       "  (313, 0.00),\n",
       "  (318, 0.00),\n",
       "  (320, 0.00),\n",
       "  (322, 0.00),\n",
       "  (323, 0.00),\n",
       "  (324, 0.00),\n",
       "  (325, 0.00),\n",
       "  (326, 0.00),\n",
       "  (328, 0.00),\n",
       "  (329, 0.00),\n",
       "  (330, 0.00),\n",
       "  (332, 0.00),\n",
       "  (333, 0.00),\n",
       "  (335, 0.00),\n",
       "  (338, 0.00),\n",
       "  (339, 0.00),\n",
       "  (340, 0.00),\n",
       "  (341, 0.00),\n",
       "  (344, 0.00),\n",
       "  (345, 0.00),\n",
       "  (346, 0.00),\n",
       "  (349, 0.00),\n",
       "  (352, 0.00),\n",
       "  (356, 0.00),\n",
       "  (357, 0.00),\n",
       "  (358, 0.00),\n",
       "  (359, 0.00),\n",
       "  (362, 0.00),\n",
       "  (367, 0.00),\n",
       "  (368, 0.00),\n",
       "  (370, 0.00),\n",
       "  (371, 0.00),\n",
       "  (372, 0.00),\n",
       "  (373, 0.00),\n",
       "  (377, 0.00),\n",
       "  (380, 0.00),\n",
       "  (382, 0.00),\n",
       "  (385, 0.00),\n",
       "  (386, 0.00),\n",
       "  (388, 0.00),\n",
       "  (389, 0.00),\n",
       "  (390, 0.00),\n",
       "  (391, 0.00),\n",
       "  (394, 0.00),\n",
       "  (395, 0.00),\n",
       "  (396, 0.00),\n",
       "  (399, 0.00),\n",
       "  (400, 0.00),\n",
       "  (402, 0.00),\n",
       "  (403, 0.00),\n",
       "  (404, 0.00),\n",
       "  (405, 0.00),\n",
       "  (409, 0.00),\n",
       "  (410, 0.00),\n",
       "  (412, 0.00),\n",
       "  (413, 0.00),\n",
       "  (419, 0.00),\n",
       "  (420, 0.00),\n",
       "  (421, 0.00),\n",
       "  (422, 0.00),\n",
       "  (426, 0.00),\n",
       "  (430, 0.00),\n",
       "  (432, 0.00),\n",
       "  (435, 0.00),\n",
       "  (436, 0.00),\n",
       "  (437, 0.00),\n",
       "  (438, 0.00),\n",
       "  (439, 0.00),\n",
       "  (441, 0.00),\n",
       "  (442, 0.00),\n",
       "  (447, 0.00),\n",
       "  (449, 0.00),\n",
       "  (450, 0.00),\n",
       "  (451, 0.00),\n",
       "  (458, 0.00),\n",
       "  (460, 0.00),\n",
       "  (461, 0.00),\n",
       "  (462, 0.00),\n",
       "  (463, 0.00),\n",
       "  (465, 0.00),\n",
       "  (466, 0.00),\n",
       "  (467, 0.00),\n",
       "  (469, 0.00),\n",
       "  (470, 0.00),\n",
       "  (473, 0.00),\n",
       "  (475, 0.00),\n",
       "  (477, 0.00),\n",
       "  (478, 0.00),\n",
       "  (480, 0.00),\n",
       "  (482, 0.00),\n",
       "  (485, 0.00),\n",
       "  (486, 0.00),\n",
       "  (487, 0.00),\n",
       "  (493, 0.00),\n",
       "  (494, 0.00),\n",
       "  (499, 0.00),\n",
       "  (500, 0.00),\n",
       "  (501, 0.00),\n",
       "  (504, 0.00),\n",
       "  (505, 0.00),\n",
       "  (510, 0.00),\n",
       "  (511, 0.00),\n",
       "  (513, 0.00),\n",
       "  (517, 0.00),\n",
       "  (519, 0.00),\n",
       "  (524, 0.00),\n",
       "  (525, 0.00),\n",
       "  (529, 0.00),\n",
       "  (531, 0.00),\n",
       "  (534, 0.00),\n",
       "  (536, 0.00),\n",
       "  (537, 0.00),\n",
       "  (539, 0.00),\n",
       "  (540, 0.00),\n",
       "  (542, 0.00),\n",
       "  (543, 0.00),\n",
       "  (544, 0.00),\n",
       "  (545, 0.00),\n",
       "  (547, 0.00),\n",
       "  (548, 0.00),\n",
       "  (550, 0.00),\n",
       "  (551, 0.00),\n",
       "  (553, 0.00),\n",
       "  (554, 0.00),\n",
       "  (556, 0.00),\n",
       "  (557, 0.00),\n",
       "  (559, 0.00),\n",
       "  (560, 0.00),\n",
       "  (568, 0.00),\n",
       "  (571, 0.00),\n",
       "  (573, 0.00),\n",
       "  (578, 0.00),\n",
       "  (583, 0.00),\n",
       "  (587, 0.00),\n",
       "  (589, 0.00),\n",
       "  (590, 0.00),\n",
       "  (591, 0.00),\n",
       "  (592, 0.00),\n",
       "  (595, 0.00),\n",
       "  (596, 0.00),\n",
       "  (597, 0.00),\n",
       "  (598, 0.00),\n",
       "  (599, 0.00),\n",
       "  (600, 0.00),\n",
       "  (601, 0.00),\n",
       "  (602, 0.00),\n",
       "  (605, 0.00),\n",
       "  (608, 0.00),\n",
       "  (610, 0.00),\n",
       "  (615, 0.00),\n",
       "  (617, 0.00),\n",
       "  (620, 0.00),\n",
       "  (622, 0.00),\n",
       "  (623, 0.00),\n",
       "  (624, 0.00),\n",
       "  (626, 0.00),\n",
       "  (628, 0.00),\n",
       "  (630, 0.00),\n",
       "  (631, 0.00),\n",
       "  (632, 0.00),\n",
       "  (634, 0.00),\n",
       "  (635, 0.00),\n",
       "  (638, 0.00),\n",
       "  (640, 0.00),\n",
       "  (642, 0.00),\n",
       "  (644, 0.00),\n",
       "  (647, 0.00),\n",
       "  (648, 0.00),\n",
       "  (649, 0.00),\n",
       "  (650, 0.00),\n",
       "  (651, 0.00),\n",
       "  (655, 0.00),\n",
       "  (657, 0.00),\n",
       "  (658, 0.00),\n",
       "  (660, 0.00),\n",
       "  (662, 0.00),\n",
       "  (663, 0.00),\n",
       "  (665, 0.00),\n",
       "  (666, 0.00),\n",
       "  (667, 0.00),\n",
       "  (668, 0.00),\n",
       "  (669, 0.00),\n",
       "  (671, 0.00),\n",
       "  (673, 0.00),\n",
       "  (674, 0.00),\n",
       "  (675, 0.00),\n",
       "  (677, 0.00),\n",
       "  (678, 0.00),\n",
       "  (680, 0.00),\n",
       "  (681, 0.00),\n",
       "  (682, 0.00),\n",
       "  (683, 0.00),\n",
       "  (685, 0.00),\n",
       "  (686, 0.00),\n",
       "  (689, 0.00),\n",
       "  (690, 0.00),\n",
       "  (691, 0.00),\n",
       "  (693, 0.00),\n",
       "  (704, 0.00),\n",
       "  (706, 0.00),\n",
       "  (707, 0.00),\n",
       "  (708, 0.00),\n",
       "  (710, 0.00),\n",
       "  (712, 0.00),\n",
       "  (713, 0.00),\n",
       "  (714, 0.00),\n",
       "  (715, 0.00),\n",
       "  (718, 0.00),\n",
       "  (719, 0.00),\n",
       "  (720, 0.00),\n",
       "  (722, 0.00),\n",
       "  (723, 0.00),\n",
       "  (725, 0.00),\n",
       "  (726, 0.00),\n",
       "  (727, 0.00),\n",
       "  (728, 0.00),\n",
       "  (731, 0.00),\n",
       "  (732, 0.00),\n",
       "  (734, 0.00),\n",
       "  (739, 0.00),\n",
       "  (740, 0.00),\n",
       "  (742, 0.00),\n",
       "  (743, 0.00),\n",
       "  (744, 0.00),\n",
       "  (745, 0.00),\n",
       "  (747, 0.00),\n",
       "  (749, 0.00),\n",
       "  (752, 0.00),\n",
       "  (755, 0.00),\n",
       "  (757, 0.00),\n",
       "  (759, 0.00),\n",
       "  (760, 0.00),\n",
       "  (761, 0.00),\n",
       "  (764, 0.00),\n",
       "  (766, 0.00),\n",
       "  (769, 0.00),\n",
       "  (771, 0.00),\n",
       "  (772, 0.00),\n",
       "  (773, 0.00),\n",
       "  (774, 0.00),\n",
       "  (776, 0.00),\n",
       "  (780, 0.00),\n",
       "  (785, 0.00),\n",
       "  (789, 0.00),\n",
       "  (792, 0.00),\n",
       "  (795, 0.00),\n",
       "  (796, 0.00),\n",
       "  (797, 0.00),\n",
       "  (798, 0.00),\n",
       "  (799, 0.00),\n",
       "  (802, 0.00),\n",
       "  (803, 0.00),\n",
       "  (804, 0.00),\n",
       "  (807, 0.00),\n",
       "  (810, 0.00),\n",
       "  (811, 0.00),\n",
       "  (812, 0.00),\n",
       "  (813, 0.00),\n",
       "  (814, 0.00),\n",
       "  (818, 0.00),\n",
       "  (827, 0.00),\n",
       "  (835, 0.00),\n",
       "  (836, 0.00),\n",
       "  (837, 0.00),\n",
       "  (838, 0.00),\n",
       "  (840, 0.00),\n",
       "  (841, 0.00),\n",
       "  (842, 0.00),\n",
       "  (844, 0.00),\n",
       "  (845, 0.00),\n",
       "  (846, 0.00),\n",
       "  (848, 0.00),\n",
       "  (851, 0.00),\n",
       "  (855, 0.00),\n",
       "  (856, 0.00),\n",
       "  (859, 0.00),\n",
       "  (860, 0.00),\n",
       "  (861, 0.00),\n",
       "  (862, 0.00),\n",
       "  (866, 0.00),\n",
       "  (869, 0.00),\n",
       "  (875, 0.00),\n",
       "  (876, 0.00),\n",
       "  (877, 0.00),\n",
       "  (884, 0.00),\n",
       "  (891, 0.00),\n",
       "  (894, 0.00),\n",
       "  (895, 0.00),\n",
       "  (896, 0.00),\n",
       "  (897, 0.00),\n",
       "  (899, 0.00),\n",
       "  (900, 0.00),\n",
       "  (901, 0.00),\n",
       "  (902, 0.00),\n",
       "  (903, 0.00),\n",
       "  (904, 0.00),\n",
       "  (908, 0.00),\n",
       "  (909, 0.00),\n",
       "  (912, 0.00),\n",
       "  (913, 0.00),\n",
       "  (915, 0.00),\n",
       "  (916, 0.00),\n",
       "  (918, 0.00),\n",
       "  (919, 0.00),\n",
       "  (925, 0.00),\n",
       "  (926, 0.00),\n",
       "  (927, 0.00),\n",
       "  (928, 0.00),\n",
       "  (929, 0.00),\n",
       "  (930, 0.00),\n",
       "  (931, 0.00),\n",
       "  (933, 0.00),\n",
       "  (935, 0.00),\n",
       "  (936, 0.00),\n",
       "  (940, 0.00),\n",
       "  (941, 0.00),\n",
       "  (942, 0.00),\n",
       "  (943, 0.00),\n",
       "  (945, 0.00),\n",
       "  (948, 0.00),\n",
       "  (949, 0.00),\n",
       "  (950, 0.00),\n",
       "  (951, 0.00),\n",
       "  (952, 0.00),\n",
       "  (956, 0.00),\n",
       "  (959, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (964, 0.00),\n",
       "  (965, 0.00),\n",
       "  (966, 0.00),\n",
       "  (967, 0.00),\n",
       "  (969, 0.00),\n",
       "  (970, 0.00),\n",
       "  (974, 0.00),\n",
       "  (975, 0.00),\n",
       "  (976, 0.00),\n",
       "  (977, 0.00),\n",
       "  (978, 0.00),\n",
       "  (979, 0.00),\n",
       "  (980, 0.00),\n",
       "  (983, 0.00),\n",
       "  (984, 0.00),\n",
       "  (985, 0.00),\n",
       "  (986, 0.00),\n",
       "  (988, 0.00),\n",
       "  (990, 0.00),\n",
       "  (993, 0.00),\n",
       "  (994, 0.00),\n",
       "  (995, 0.00),\n",
       "  (998, 0.00)],\n",
       " [971,\n",
       "  854,\n",
       "  611,\n",
       "  750,\n",
       "  721,\n",
       "  741,\n",
       "  645,\n",
       "  84,\n",
       "  808,\n",
       "  509,\n",
       "  815,\n",
       "  703,\n",
       "  594,\n",
       "  973,\n",
       "  646,\n",
       "  824,\n",
       "  506,\n",
       "  794,\n",
       "  489,\n",
       "  881,\n",
       "  735,\n",
       "  775,\n",
       "  641,\n",
       "  68,\n",
       "  806,\n",
       "  580,\n",
       "  108,\n",
       "  464,\n",
       "  748,\n",
       "  944,\n",
       "  109,\n",
       "  946,\n",
       "  800,\n",
       "  879,\n",
       "  60,\n",
       "  893,\n",
       "  123,\n",
       "  858,\n",
       "  898,\n",
       "  55,\n",
       "  128,\n",
       "  492,\n",
       "  696,\n",
       "  96,\n",
       "  360,\n",
       "  411,\n",
       "  429,\n",
       "  440,\n",
       "  472,\n",
       "  533,\n",
       "  816,\n",
       "  911,\n",
       "  963,\n",
       "  982,\n",
       "  86,\n",
       "  354,\n",
       "  454,\n",
       "  468,\n",
       "  474,\n",
       "  549,\n",
       "  575,\n",
       "  738,\n",
       "  787,\n",
       "  865,\n",
       "  987,\n",
       "  25,\n",
       "  37,\n",
       "  52,\n",
       "  57,\n",
       "  91,\n",
       "  118,\n",
       "  163,\n",
       "  193,\n",
       "  242,\n",
       "  247,\n",
       "  274,\n",
       "  281,\n",
       "  290,\n",
       "  292,\n",
       "  314,\n",
       "  376,\n",
       "  393,\n",
       "  507,\n",
       "  566,\n",
       "  621,\n",
       "  625,\n",
       "  637,\n",
       "  709,\n",
       "  716,\n",
       "  783,\n",
       "  937,\n",
       "  939,\n",
       "  953,\n",
       "  7,\n",
       "  76,\n",
       "  110,\n",
       "  192,\n",
       "  260,\n",
       "  300,\n",
       "  301,\n",
       "  342,\n",
       "  378,\n",
       "  398,\n",
       "  457,\n",
       "  496,\n",
       "  498,\n",
       "  502,\n",
       "  567,\n",
       "  593,\n",
       "  614,\n",
       "  654,\n",
       "  684,\n",
       "  768,\n",
       "  850,\n",
       "  868,\n",
       "  907,\n",
       "  923,\n",
       "  934,\n",
       "  955,\n",
       "  992,\n",
       "  33,\n",
       "  74,\n",
       "  92,\n",
       "  120,\n",
       "  160,\n",
       "  164,\n",
       "  218,\n",
       "  309,\n",
       "  316,\n",
       "  327,\n",
       "  355,\n",
       "  401,\n",
       "  476,\n",
       "  508,\n",
       "  576,\n",
       "  612,\n",
       "  629,\n",
       "  661,\n",
       "  694,\n",
       "  826,\n",
       "  864,\n",
       "  131,\n",
       "  151,\n",
       "  189,\n",
       "  200,\n",
       "  302,\n",
       "  304,\n",
       "  306,\n",
       "  347,\n",
       "  381,\n",
       "  406,\n",
       "  446,\n",
       "  483,\n",
       "  488,\n",
       "  520,\n",
       "  528,\n",
       "  535,\n",
       "  555,\n",
       "  582,\n",
       "  643,\n",
       "  737,\n",
       "  753,\n",
       "  779,\n",
       "  863,\n",
       "  889,\n",
       "  892,\n",
       "  906,\n",
       "  100,\n",
       "  176,\n",
       "  235,\n",
       "  307,\n",
       "  331,\n",
       "  334,\n",
       "  363,\n",
       "  392,\n",
       "  497,\n",
       "  563,\n",
       "  581,\n",
       "  791,\n",
       "  825,\n",
       "  857,\n",
       "  962,\n",
       "  42,\n",
       "  83,\n",
       "  94,\n",
       "  99,\n",
       "  121,\n",
       "  124,\n",
       "  198,\n",
       "  205,\n",
       "  230,\n",
       "  336,\n",
       "  369,\n",
       "  417,\n",
       "  424,\n",
       "  490,\n",
       "  679,\n",
       "  729,\n",
       "  777,\n",
       "  831,\n",
       "  872,\n",
       "  890,\n",
       "  996,\n",
       "  31,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  61,\n",
       "  115,\n",
       "  186,\n",
       "  219,\n",
       "  231,\n",
       "  275,\n",
       "  291,\n",
       "  321,\n",
       "  348,\n",
       "  364,\n",
       "  375,\n",
       "  408,\n",
       "  415])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist, tk = diversity(learn, 10, 95)\n",
    "n, hist, tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "finished creating histogram for the 0th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 1th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 2th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 3th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 4th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 5th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 6th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 7th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 8th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 9th perturbation\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(136,\n",
       " [(854, 378.30),\n",
       "  (611, 180.10),\n",
       "  (594, 121.30),\n",
       "  (580, 81.10),\n",
       "  (971, 23.60),\n",
       "  (808, 11.70),\n",
       "  (839, 8.90),\n",
       "  (815, 7.80),\n",
       "  (84, 4.50),\n",
       "  (645, 4.20),\n",
       "  (973, 3.30),\n",
       "  (748, 3.10),\n",
       "  (128, 2.80),\n",
       "  (735, 2.80),\n",
       "  (68, 2.50),\n",
       "  (506, 2.50),\n",
       "  (509, 2.50),\n",
       "  (489, 2.40),\n",
       "  (646, 1.90),\n",
       "  (108, 1.80),\n",
       "  (944, 1.80),\n",
       "  (109, 1.60),\n",
       "  (824, 1.50),\n",
       "  (865, 1.50),\n",
       "  (123, 1.40),\n",
       "  (879, 1.40),\n",
       "  (411, 1.30),\n",
       "  (806, 1.30),\n",
       "  (816, 1.30),\n",
       "  (946, 1.30),\n",
       "  (60, 1.20),\n",
       "  (192, 1.20),\n",
       "  (242, 1.20),\n",
       "  (987, 1.20),\n",
       "  (96, 1.10),\n",
       "  (354, 1.10),\n",
       "  (454, 1.10),\n",
       "  (496, 1.10),\n",
       "  (709, 1.10),\n",
       "  (787, 1.10),\n",
       "  (963, 1.10),\n",
       "  (25, 1.00),\n",
       "  (37, 1.00),\n",
       "  (52, 1.00),\n",
       "  (57, 1.00),\n",
       "  (86, 1.00),\n",
       "  (91, 1.00),\n",
       "  (160, 1.00),\n",
       "  (163, 1.00),\n",
       "  (274, 1.00),\n",
       "  (290, 1.00),\n",
       "  (292, 1.00),\n",
       "  (376, 1.00),\n",
       "  (393, 1.00),\n",
       "  (464, 1.00),\n",
       "  (507, 1.00),\n",
       "  (533, 1.00),\n",
       "  (566, 1.00),\n",
       "  (575, 1.00),\n",
       "  (581, 1.00),\n",
       "  (625, 1.00),\n",
       "  (637, 1.00),\n",
       "  (716, 1.00),\n",
       "  (753, 1.00),\n",
       "  (783, 1.00),\n",
       "  (788, 1.00),\n",
       "  (858, 1.00),\n",
       "  (939, 1.00),\n",
       "  (953, 1.00),\n",
       "  (110, 0.90),\n",
       "  (247, 0.90),\n",
       "  (281, 0.90),\n",
       "  (301, 0.90),\n",
       "  (342, 0.90),\n",
       "  (440, 0.90),\n",
       "  (612, 0.90),\n",
       "  (721, 0.90),\n",
       "  (738, 0.90),\n",
       "  (937, 0.90),\n",
       "  (7, 0.80),\n",
       "  (33, 0.80),\n",
       "  (260, 0.80),\n",
       "  (300, 0.80),\n",
       "  (360, 0.80),\n",
       "  (378, 0.80),\n",
       "  (398, 0.80),\n",
       "  (429, 0.80),\n",
       "  (468, 0.80),\n",
       "  (472, 0.80),\n",
       "  (474, 0.80),\n",
       "  (684, 0.80),\n",
       "  (696, 0.80),\n",
       "  (826, 0.80),\n",
       "  (892, 0.80),\n",
       "  (934, 0.80),\n",
       "  (955, 0.80),\n",
       "  (992, 0.80),\n",
       "  (996, 0.80),\n",
       "  (83, 0.70),\n",
       "  (92, 0.70),\n",
       "  (121, 0.70),\n",
       "  (193, 0.70),\n",
       "  (200, 0.70),\n",
       "  (218, 0.70),\n",
       "  (307, 0.70),\n",
       "  (309, 0.70),\n",
       "  (314, 0.70),\n",
       "  (316, 0.70),\n",
       "  (327, 0.70),\n",
       "  (476, 0.70),\n",
       "  (483, 0.70),\n",
       "  (508, 0.70),\n",
       "  (576, 0.70),\n",
       "  (629, 0.70),\n",
       "  (641, 0.70),\n",
       "  (654, 0.70),\n",
       "  (791, 0.70),\n",
       "  (801, 0.70),\n",
       "  (898, 0.70),\n",
       "  (907, 0.70),\n",
       "  (118, 0.60),\n",
       "  (176, 0.60),\n",
       "  (189, 0.60),\n",
       "  (321, 0.60),\n",
       "  (621, 0.60),\n",
       "  (661, 0.60),\n",
       "  (724, 0.60),\n",
       "  (737, 0.60),\n",
       "  (923, 0.60),\n",
       "  (74, 0.50),\n",
       "  (76, 0.50),\n",
       "  (119, 0.50),\n",
       "  (144, 0.50),\n",
       "  (164, 0.50),\n",
       "  (186, 0.50),\n",
       "  (228, 0.50),\n",
       "  (334, 0.50),\n",
       "  (336, 0.50),\n",
       "  (343, 0.50),\n",
       "  (348, 0.50),\n",
       "  (363, 0.50),\n",
       "  (408, 0.50),\n",
       "  (555, 0.50),\n",
       "  (567, 0.50),\n",
       "  (582, 0.50),\n",
       "  (672, 0.50),\n",
       "  (676, 0.50),\n",
       "  (694, 0.50),\n",
       "  (705, 0.50),\n",
       "  (758, 0.50),\n",
       "  (768, 0.50),\n",
       "  (790, 0.50),\n",
       "  (805, 0.50),\n",
       "  (864, 0.50),\n",
       "  (885, 0.50),\n",
       "  (889, 0.50),\n",
       "  (131, 0.40),\n",
       "  (155, 0.40),\n",
       "  (168, 0.40),\n",
       "  (209, 0.40),\n",
       "  (302, 0.40),\n",
       "  (364, 0.40),\n",
       "  (369, 0.40),\n",
       "  (424, 0.40),\n",
       "  (425, 0.40),\n",
       "  (427, 0.40),\n",
       "  (443, 0.40),\n",
       "  (459, 0.40),\n",
       "  (490, 0.40),\n",
       "  (491, 0.40),\n",
       "  (502, 0.40),\n",
       "  (520, 0.40),\n",
       "  (572, 0.40),\n",
       "  (687, 0.40),\n",
       "  (695, 0.40),\n",
       "  (794, 0.40),\n",
       "  (825, 0.40),\n",
       "  (887, 0.40),\n",
       "  (890, 0.40),\n",
       "  (968, 0.40),\n",
       "  (36, 0.30),\n",
       "  (62, 0.30),\n",
       "  (72, 0.30),\n",
       "  (120, 0.30),\n",
       "  (182, 0.30),\n",
       "  (206, 0.30),\n",
       "  (214, 0.30),\n",
       "  (230, 0.30),\n",
       "  (235, 0.30),\n",
       "  (381, 0.30),\n",
       "  (406, 0.30),\n",
       "  (488, 0.30),\n",
       "  (497, 0.30),\n",
       "  (498, 0.30),\n",
       "  (574, 0.30),\n",
       "  (640, 0.30),\n",
       "  (698, 0.30),\n",
       "  (729, 0.30),\n",
       "  (746, 0.30),\n",
       "  (752, 0.30),\n",
       "  (779, 0.30),\n",
       "  (781, 0.30),\n",
       "  (829, 0.30),\n",
       "  (843, 0.30),\n",
       "  (857, 0.30),\n",
       "  (878, 0.30),\n",
       "  (911, 0.30),\n",
       "  (998, 0.30),\n",
       "  (8, 0.20),\n",
       "  (45, 0.20),\n",
       "  (59, 0.20),\n",
       "  (63, 0.20),\n",
       "  (66, 0.20),\n",
       "  (98, 0.20),\n",
       "  (105, 0.20),\n",
       "  (116, 0.20),\n",
       "  (124, 0.20),\n",
       "  (125, 0.20),\n",
       "  (136, 0.20),\n",
       "  (140, 0.20),\n",
       "  (141, 0.20),\n",
       "  (142, 0.20),\n",
       "  (150, 0.20),\n",
       "  (187, 0.20),\n",
       "  (188, 0.20),\n",
       "  (198, 0.20),\n",
       "  (199, 0.20),\n",
       "  (202, 0.20),\n",
       "  (205, 0.20),\n",
       "  (219, 0.20),\n",
       "  (223, 0.20),\n",
       "  (224, 0.20),\n",
       "  (233, 0.20),\n",
       "  (236, 0.20),\n",
       "  (249, 0.20),\n",
       "  (254, 0.20),\n",
       "  (273, 0.20),\n",
       "  (275, 0.20),\n",
       "  (276, 0.20),\n",
       "  (284, 0.20),\n",
       "  (289, 0.20),\n",
       "  (291, 0.20),\n",
       "  (294, 0.20),\n",
       "  (295, 0.20),\n",
       "  (298, 0.20),\n",
       "  (305, 0.20),\n",
       "  (329, 0.20),\n",
       "  (330, 0.20),\n",
       "  (331, 0.20),\n",
       "  (341, 0.20),\n",
       "  (345, 0.20),\n",
       "  (355, 0.20),\n",
       "  (358, 0.20),\n",
       "  (365, 0.20),\n",
       "  (367, 0.20),\n",
       "  (385, 0.20),\n",
       "  (388, 0.20),\n",
       "  (392, 0.20),\n",
       "  (401, 0.20),\n",
       "  (415, 0.20),\n",
       "  (417, 0.20),\n",
       "  (448, 0.20),\n",
       "  (453, 0.20),\n",
       "  (455, 0.20),\n",
       "  (457, 0.20),\n",
       "  (492, 0.20),\n",
       "  (528, 0.20),\n",
       "  (535, 0.20),\n",
       "  (539, 0.20),\n",
       "  (549, 0.20),\n",
       "  (561, 0.20),\n",
       "  (562, 0.20),\n",
       "  (579, 0.20),\n",
       "  (585, 0.20),\n",
       "  (586, 0.20),\n",
       "  (607, 0.20),\n",
       "  (624, 0.20),\n",
       "  (636, 0.20),\n",
       "  (643, 0.20),\n",
       "  (664, 0.20),\n",
       "  (703, 0.20),\n",
       "  (741, 0.20),\n",
       "  (796, 0.20),\n",
       "  (823, 0.20),\n",
       "  (863, 0.20),\n",
       "  (880, 0.20),\n",
       "  (893, 0.20),\n",
       "  (958, 0.20),\n",
       "  (962, 0.20),\n",
       "  (981, 0.20),\n",
       "  (984, 0.20),\n",
       "  (986, 0.20),\n",
       "  (997, 0.20),\n",
       "  (24, 0.10),\n",
       "  (30, 0.10),\n",
       "  (42, 0.10),\n",
       "  (61, 0.10),\n",
       "  (65, 0.10),\n",
       "  (67, 0.10),\n",
       "  (79, 0.10),\n",
       "  (82, 0.10),\n",
       "  (90, 0.10),\n",
       "  (100, 0.10),\n",
       "  (102, 0.10),\n",
       "  (115, 0.10),\n",
       "  (159, 0.10),\n",
       "  (170, 0.10),\n",
       "  (282, 0.10),\n",
       "  (293, 0.10),\n",
       "  (337, 0.10),\n",
       "  (344, 0.10),\n",
       "  (347, 0.10),\n",
       "  (349, 0.10),\n",
       "  (384, 0.10),\n",
       "  (445, 0.10),\n",
       "  (486, 0.10),\n",
       "  (519, 0.10),\n",
       "  (530, 0.10),\n",
       "  (577, 0.10),\n",
       "  (588, 0.10),\n",
       "  (597, 0.10),\n",
       "  (603, 0.10),\n",
       "  (652, 0.10),\n",
       "  (656, 0.10),\n",
       "  (679, 0.10),\n",
       "  (704, 0.10),\n",
       "  (754, 0.10),\n",
       "  (762, 0.10),\n",
       "  (774, 0.10),\n",
       "  (775, 0.10),\n",
       "  (820, 0.10),\n",
       "  (888, 0.10),\n",
       "  (910, 0.10),\n",
       "  (912, 0.10),\n",
       "  (917, 0.10),\n",
       "  (947, 0.10),\n",
       "  (979, 0.10),\n",
       "  (982, 0.10),\n",
       "  (0, 0.00),\n",
       "  (1, 0.00),\n",
       "  (2, 0.00),\n",
       "  (3, 0.00),\n",
       "  (4, 0.00),\n",
       "  (5, 0.00),\n",
       "  (6, 0.00),\n",
       "  (9, 0.00),\n",
       "  (10, 0.00),\n",
       "  (11, 0.00),\n",
       "  (12, 0.00),\n",
       "  (13, 0.00),\n",
       "  (14, 0.00),\n",
       "  (15, 0.00),\n",
       "  (16, 0.00),\n",
       "  (17, 0.00),\n",
       "  (18, 0.00),\n",
       "  (19, 0.00),\n",
       "  (20, 0.00),\n",
       "  (21, 0.00),\n",
       "  (22, 0.00),\n",
       "  (23, 0.00),\n",
       "  (26, 0.00),\n",
       "  (27, 0.00),\n",
       "  (28, 0.00),\n",
       "  (29, 0.00),\n",
       "  (31, 0.00),\n",
       "  (32, 0.00),\n",
       "  (34, 0.00),\n",
       "  (35, 0.00),\n",
       "  (38, 0.00),\n",
       "  (39, 0.00),\n",
       "  (40, 0.00),\n",
       "  (41, 0.00),\n",
       "  (43, 0.00),\n",
       "  (44, 0.00),\n",
       "  (46, 0.00),\n",
       "  (47, 0.00),\n",
       "  (48, 0.00),\n",
       "  (49, 0.00),\n",
       "  (50, 0.00),\n",
       "  (51, 0.00),\n",
       "  (53, 0.00),\n",
       "  (54, 0.00),\n",
       "  (55, 0.00),\n",
       "  (56, 0.00),\n",
       "  (58, 0.00),\n",
       "  (64, 0.00),\n",
       "  (69, 0.00),\n",
       "  (70, 0.00),\n",
       "  (71, 0.00),\n",
       "  (73, 0.00),\n",
       "  (75, 0.00),\n",
       "  (77, 0.00),\n",
       "  (78, 0.00),\n",
       "  (80, 0.00),\n",
       "  (81, 0.00),\n",
       "  (85, 0.00),\n",
       "  (87, 0.00),\n",
       "  (88, 0.00),\n",
       "  (89, 0.00),\n",
       "  (93, 0.00),\n",
       "  (94, 0.00),\n",
       "  (95, 0.00),\n",
       "  (97, 0.00),\n",
       "  (99, 0.00),\n",
       "  (101, 0.00),\n",
       "  (103, 0.00),\n",
       "  (104, 0.00),\n",
       "  (106, 0.00),\n",
       "  (107, 0.00),\n",
       "  (111, 0.00),\n",
       "  (112, 0.00),\n",
       "  (113, 0.00),\n",
       "  (114, 0.00),\n",
       "  (117, 0.00),\n",
       "  (122, 0.00),\n",
       "  (126, 0.00),\n",
       "  (127, 0.00),\n",
       "  (129, 0.00),\n",
       "  (130, 0.00),\n",
       "  (132, 0.00),\n",
       "  (133, 0.00),\n",
       "  (134, 0.00),\n",
       "  (135, 0.00),\n",
       "  (137, 0.00),\n",
       "  (138, 0.00),\n",
       "  (139, 0.00),\n",
       "  (143, 0.00),\n",
       "  (145, 0.00),\n",
       "  (146, 0.00),\n",
       "  (147, 0.00),\n",
       "  (148, 0.00),\n",
       "  (149, 0.00),\n",
       "  (151, 0.00),\n",
       "  (152, 0.00),\n",
       "  (153, 0.00),\n",
       "  (154, 0.00),\n",
       "  (156, 0.00),\n",
       "  (157, 0.00),\n",
       "  (158, 0.00),\n",
       "  (161, 0.00),\n",
       "  (162, 0.00),\n",
       "  (165, 0.00),\n",
       "  (166, 0.00),\n",
       "  (167, 0.00),\n",
       "  (169, 0.00),\n",
       "  (171, 0.00),\n",
       "  (172, 0.00),\n",
       "  (173, 0.00),\n",
       "  (174, 0.00),\n",
       "  (175, 0.00),\n",
       "  (177, 0.00),\n",
       "  (178, 0.00),\n",
       "  (179, 0.00),\n",
       "  (180, 0.00),\n",
       "  (181, 0.00),\n",
       "  (183, 0.00),\n",
       "  (184, 0.00),\n",
       "  (185, 0.00),\n",
       "  (190, 0.00),\n",
       "  (191, 0.00),\n",
       "  (194, 0.00),\n",
       "  (195, 0.00),\n",
       "  (196, 0.00),\n",
       "  (197, 0.00),\n",
       "  (201, 0.00),\n",
       "  (203, 0.00),\n",
       "  (204, 0.00),\n",
       "  (207, 0.00),\n",
       "  (208, 0.00),\n",
       "  (210, 0.00),\n",
       "  (211, 0.00),\n",
       "  (212, 0.00),\n",
       "  (213, 0.00),\n",
       "  (215, 0.00),\n",
       "  (216, 0.00),\n",
       "  (217, 0.00),\n",
       "  (220, 0.00),\n",
       "  (221, 0.00),\n",
       "  (222, 0.00),\n",
       "  (225, 0.00),\n",
       "  (226, 0.00),\n",
       "  (227, 0.00),\n",
       "  (229, 0.00),\n",
       "  (231, 0.00),\n",
       "  (232, 0.00),\n",
       "  (234, 0.00),\n",
       "  (237, 0.00),\n",
       "  (238, 0.00),\n",
       "  (239, 0.00),\n",
       "  (240, 0.00),\n",
       "  (241, 0.00),\n",
       "  (243, 0.00),\n",
       "  (244, 0.00),\n",
       "  (245, 0.00),\n",
       "  (246, 0.00),\n",
       "  (248, 0.00),\n",
       "  (250, 0.00),\n",
       "  (251, 0.00),\n",
       "  (252, 0.00),\n",
       "  (253, 0.00),\n",
       "  (255, 0.00),\n",
       "  (256, 0.00),\n",
       "  (257, 0.00),\n",
       "  (258, 0.00),\n",
       "  (259, 0.00),\n",
       "  (261, 0.00),\n",
       "  (262, 0.00),\n",
       "  (263, 0.00),\n",
       "  (264, 0.00),\n",
       "  (265, 0.00),\n",
       "  (266, 0.00),\n",
       "  (267, 0.00),\n",
       "  (268, 0.00),\n",
       "  (269, 0.00),\n",
       "  (270, 0.00),\n",
       "  (271, 0.00),\n",
       "  (272, 0.00),\n",
       "  (277, 0.00),\n",
       "  (278, 0.00),\n",
       "  (279, 0.00),\n",
       "  (280, 0.00),\n",
       "  (283, 0.00),\n",
       "  (285, 0.00),\n",
       "  (286, 0.00),\n",
       "  (287, 0.00),\n",
       "  (288, 0.00),\n",
       "  (296, 0.00),\n",
       "  (297, 0.00),\n",
       "  (299, 0.00),\n",
       "  (303, 0.00),\n",
       "  (304, 0.00),\n",
       "  (306, 0.00),\n",
       "  (308, 0.00),\n",
       "  (310, 0.00),\n",
       "  (311, 0.00),\n",
       "  (312, 0.00),\n",
       "  (313, 0.00),\n",
       "  (315, 0.00),\n",
       "  (317, 0.00),\n",
       "  (318, 0.00),\n",
       "  (319, 0.00),\n",
       "  (320, 0.00),\n",
       "  (322, 0.00),\n",
       "  (323, 0.00),\n",
       "  (324, 0.00),\n",
       "  (325, 0.00),\n",
       "  (326, 0.00),\n",
       "  (328, 0.00),\n",
       "  (332, 0.00),\n",
       "  (333, 0.00),\n",
       "  (335, 0.00),\n",
       "  (338, 0.00),\n",
       "  (339, 0.00),\n",
       "  (340, 0.00),\n",
       "  (346, 0.00),\n",
       "  (350, 0.00),\n",
       "  (351, 0.00),\n",
       "  (352, 0.00),\n",
       "  (353, 0.00),\n",
       "  (356, 0.00),\n",
       "  (357, 0.00),\n",
       "  (359, 0.00),\n",
       "  (361, 0.00),\n",
       "  (362, 0.00),\n",
       "  (366, 0.00),\n",
       "  (368, 0.00),\n",
       "  (370, 0.00),\n",
       "  (371, 0.00),\n",
       "  (372, 0.00),\n",
       "  (373, 0.00),\n",
       "  (374, 0.00),\n",
       "  (375, 0.00),\n",
       "  (377, 0.00),\n",
       "  (379, 0.00),\n",
       "  (380, 0.00),\n",
       "  (382, 0.00),\n",
       "  (383, 0.00),\n",
       "  (386, 0.00),\n",
       "  (387, 0.00),\n",
       "  (389, 0.00),\n",
       "  (390, 0.00),\n",
       "  (391, 0.00),\n",
       "  (394, 0.00),\n",
       "  (395, 0.00),\n",
       "  (396, 0.00),\n",
       "  (397, 0.00),\n",
       "  (399, 0.00),\n",
       "  (400, 0.00),\n",
       "  (402, 0.00),\n",
       "  (403, 0.00),\n",
       "  (404, 0.00),\n",
       "  (405, 0.00),\n",
       "  (407, 0.00),\n",
       "  (409, 0.00),\n",
       "  (410, 0.00),\n",
       "  (412, 0.00),\n",
       "  (413, 0.00),\n",
       "  (414, 0.00),\n",
       "  (416, 0.00),\n",
       "  (418, 0.00),\n",
       "  (419, 0.00),\n",
       "  (420, 0.00),\n",
       "  (421, 0.00),\n",
       "  (422, 0.00),\n",
       "  (423, 0.00),\n",
       "  (426, 0.00),\n",
       "  (428, 0.00),\n",
       "  (430, 0.00),\n",
       "  (431, 0.00),\n",
       "  (432, 0.00),\n",
       "  (433, 0.00),\n",
       "  (434, 0.00),\n",
       "  (435, 0.00),\n",
       "  (436, 0.00),\n",
       "  (437, 0.00),\n",
       "  (438, 0.00),\n",
       "  (439, 0.00),\n",
       "  (441, 0.00),\n",
       "  (442, 0.00),\n",
       "  (444, 0.00),\n",
       "  (446, 0.00),\n",
       "  (447, 0.00),\n",
       "  (449, 0.00),\n",
       "  (450, 0.00),\n",
       "  (451, 0.00),\n",
       "  (452, 0.00),\n",
       "  (456, 0.00),\n",
       "  (458, 0.00),\n",
       "  (460, 0.00),\n",
       "  (461, 0.00),\n",
       "  (462, 0.00),\n",
       "  (463, 0.00),\n",
       "  (465, 0.00),\n",
       "  (466, 0.00),\n",
       "  (467, 0.00),\n",
       "  (469, 0.00),\n",
       "  (470, 0.00),\n",
       "  (471, 0.00),\n",
       "  (473, 0.00),\n",
       "  (475, 0.00),\n",
       "  (477, 0.00),\n",
       "  (478, 0.00),\n",
       "  (479, 0.00),\n",
       "  (480, 0.00),\n",
       "  (481, 0.00),\n",
       "  (482, 0.00),\n",
       "  (484, 0.00),\n",
       "  (485, 0.00),\n",
       "  (487, 0.00),\n",
       "  (493, 0.00),\n",
       "  (494, 0.00),\n",
       "  (495, 0.00),\n",
       "  (499, 0.00),\n",
       "  (500, 0.00),\n",
       "  (501, 0.00),\n",
       "  (503, 0.00),\n",
       "  (504, 0.00),\n",
       "  (505, 0.00),\n",
       "  (510, 0.00),\n",
       "  (511, 0.00),\n",
       "  (512, 0.00),\n",
       "  (513, 0.00),\n",
       "  (514, 0.00),\n",
       "  (515, 0.00),\n",
       "  (516, 0.00),\n",
       "  (517, 0.00),\n",
       "  (518, 0.00),\n",
       "  (521, 0.00),\n",
       "  (522, 0.00),\n",
       "  (523, 0.00),\n",
       "  (524, 0.00),\n",
       "  (525, 0.00),\n",
       "  (526, 0.00),\n",
       "  (527, 0.00),\n",
       "  (529, 0.00),\n",
       "  (531, 0.00),\n",
       "  (532, 0.00),\n",
       "  (534, 0.00),\n",
       "  (536, 0.00),\n",
       "  (537, 0.00),\n",
       "  (538, 0.00),\n",
       "  (540, 0.00),\n",
       "  (541, 0.00),\n",
       "  (542, 0.00),\n",
       "  (543, 0.00),\n",
       "  (544, 0.00),\n",
       "  (545, 0.00),\n",
       "  (546, 0.00),\n",
       "  (547, 0.00),\n",
       "  (548, 0.00),\n",
       "  (550, 0.00),\n",
       "  (551, 0.00),\n",
       "  (552, 0.00),\n",
       "  (553, 0.00),\n",
       "  (554, 0.00),\n",
       "  (556, 0.00),\n",
       "  (557, 0.00),\n",
       "  (558, 0.00),\n",
       "  (559, 0.00),\n",
       "  (560, 0.00),\n",
       "  (563, 0.00),\n",
       "  (564, 0.00),\n",
       "  (565, 0.00),\n",
       "  (568, 0.00),\n",
       "  (569, 0.00),\n",
       "  (570, 0.00),\n",
       "  (571, 0.00),\n",
       "  (573, 0.00),\n",
       "  (578, 0.00),\n",
       "  (583, 0.00),\n",
       "  (584, 0.00),\n",
       "  (587, 0.00),\n",
       "  (589, 0.00),\n",
       "  (590, 0.00),\n",
       "  (591, 0.00),\n",
       "  (592, 0.00),\n",
       "  (593, 0.00),\n",
       "  (595, 0.00),\n",
       "  (596, 0.00),\n",
       "  (598, 0.00),\n",
       "  (599, 0.00),\n",
       "  (600, 0.00),\n",
       "  (601, 0.00),\n",
       "  (602, 0.00),\n",
       "  (604, 0.00),\n",
       "  (605, 0.00),\n",
       "  (606, 0.00),\n",
       "  (608, 0.00),\n",
       "  (609, 0.00),\n",
       "  (610, 0.00),\n",
       "  (613, 0.00),\n",
       "  (614, 0.00),\n",
       "  (615, 0.00),\n",
       "  (616, 0.00),\n",
       "  (617, 0.00),\n",
       "  (618, 0.00),\n",
       "  (619, 0.00),\n",
       "  (620, 0.00),\n",
       "  (622, 0.00),\n",
       "  (623, 0.00),\n",
       "  (626, 0.00),\n",
       "  (627, 0.00),\n",
       "  (628, 0.00),\n",
       "  (630, 0.00),\n",
       "  (631, 0.00),\n",
       "  (632, 0.00),\n",
       "  (633, 0.00),\n",
       "  (634, 0.00),\n",
       "  (635, 0.00),\n",
       "  (638, 0.00),\n",
       "  (639, 0.00),\n",
       "  (642, 0.00),\n",
       "  (644, 0.00),\n",
       "  (647, 0.00),\n",
       "  (648, 0.00),\n",
       "  (649, 0.00),\n",
       "  (650, 0.00),\n",
       "  (651, 0.00),\n",
       "  (653, 0.00),\n",
       "  (655, 0.00),\n",
       "  (657, 0.00),\n",
       "  (658, 0.00),\n",
       "  (659, 0.00),\n",
       "  (660, 0.00),\n",
       "  (662, 0.00),\n",
       "  (663, 0.00),\n",
       "  (665, 0.00),\n",
       "  (666, 0.00),\n",
       "  (667, 0.00),\n",
       "  (668, 0.00),\n",
       "  (669, 0.00),\n",
       "  (670, 0.00),\n",
       "  (671, 0.00),\n",
       "  (673, 0.00),\n",
       "  (674, 0.00),\n",
       "  (675, 0.00),\n",
       "  (677, 0.00),\n",
       "  (678, 0.00),\n",
       "  (680, 0.00),\n",
       "  (681, 0.00),\n",
       "  (682, 0.00),\n",
       "  (683, 0.00),\n",
       "  (685, 0.00),\n",
       "  (686, 0.00),\n",
       "  (688, 0.00),\n",
       "  (689, 0.00),\n",
       "  (690, 0.00),\n",
       "  (691, 0.00),\n",
       "  (692, 0.00),\n",
       "  (693, 0.00),\n",
       "  (697, 0.00),\n",
       "  (699, 0.00),\n",
       "  (700, 0.00),\n",
       "  (701, 0.00),\n",
       "  (702, 0.00),\n",
       "  (706, 0.00),\n",
       "  (707, 0.00),\n",
       "  (708, 0.00),\n",
       "  (710, 0.00),\n",
       "  (711, 0.00),\n",
       "  (712, 0.00),\n",
       "  (713, 0.00),\n",
       "  (714, 0.00),\n",
       "  (715, 0.00),\n",
       "  (717, 0.00),\n",
       "  (718, 0.00),\n",
       "  (719, 0.00),\n",
       "  (720, 0.00),\n",
       "  (722, 0.00),\n",
       "  (723, 0.00),\n",
       "  (725, 0.00),\n",
       "  (726, 0.00),\n",
       "  (727, 0.00),\n",
       "  (728, 0.00),\n",
       "  (730, 0.00),\n",
       "  (731, 0.00),\n",
       "  (732, 0.00),\n",
       "  (733, 0.00),\n",
       "  (734, 0.00),\n",
       "  (736, 0.00),\n",
       "  (739, 0.00),\n",
       "  (740, 0.00),\n",
       "  (742, 0.00),\n",
       "  (743, 0.00),\n",
       "  (744, 0.00),\n",
       "  (745, 0.00),\n",
       "  (747, 0.00),\n",
       "  (749, 0.00),\n",
       "  (750, 0.00),\n",
       "  (751, 0.00),\n",
       "  (755, 0.00),\n",
       "  (756, 0.00),\n",
       "  (757, 0.00),\n",
       "  (759, 0.00),\n",
       "  (760, 0.00),\n",
       "  (761, 0.00),\n",
       "  (763, 0.00),\n",
       "  (764, 0.00),\n",
       "  (765, 0.00),\n",
       "  (766, 0.00),\n",
       "  (767, 0.00),\n",
       "  (769, 0.00),\n",
       "  (770, 0.00),\n",
       "  (771, 0.00),\n",
       "  (772, 0.00),\n",
       "  (773, 0.00),\n",
       "  (776, 0.00),\n",
       "  (777, 0.00),\n",
       "  (778, 0.00),\n",
       "  (780, 0.00),\n",
       "  (782, 0.00),\n",
       "  (784, 0.00),\n",
       "  (785, 0.00),\n",
       "  (786, 0.00),\n",
       "  (789, 0.00),\n",
       "  (792, 0.00),\n",
       "  (793, 0.00),\n",
       "  (795, 0.00),\n",
       "  (797, 0.00),\n",
       "  (798, 0.00),\n",
       "  (799, 0.00),\n",
       "  (800, 0.00),\n",
       "  (802, 0.00),\n",
       "  (803, 0.00),\n",
       "  (804, 0.00),\n",
       "  (807, 0.00),\n",
       "  (809, 0.00),\n",
       "  (810, 0.00),\n",
       "  (811, 0.00),\n",
       "  (812, 0.00),\n",
       "  (813, 0.00),\n",
       "  (814, 0.00),\n",
       "  (817, 0.00),\n",
       "  (818, 0.00),\n",
       "  (819, 0.00),\n",
       "  (821, 0.00),\n",
       "  (822, 0.00),\n",
       "  (827, 0.00),\n",
       "  (828, 0.00),\n",
       "  (830, 0.00),\n",
       "  (831, 0.00),\n",
       "  (832, 0.00),\n",
       "  (833, 0.00),\n",
       "  (834, 0.00),\n",
       "  (835, 0.00),\n",
       "  (836, 0.00),\n",
       "  (837, 0.00),\n",
       "  (838, 0.00),\n",
       "  (840, 0.00),\n",
       "  (841, 0.00),\n",
       "  (842, 0.00),\n",
       "  (844, 0.00),\n",
       "  (845, 0.00),\n",
       "  (846, 0.00),\n",
       "  (847, 0.00),\n",
       "  (848, 0.00),\n",
       "  (849, 0.00),\n",
       "  (850, 0.00),\n",
       "  (851, 0.00),\n",
       "  (852, 0.00),\n",
       "  (853, 0.00),\n",
       "  (855, 0.00),\n",
       "  (856, 0.00),\n",
       "  (859, 0.00),\n",
       "  (860, 0.00),\n",
       "  (861, 0.00),\n",
       "  (862, 0.00),\n",
       "  (866, 0.00),\n",
       "  (867, 0.00),\n",
       "  (868, 0.00),\n",
       "  (869, 0.00),\n",
       "  (870, 0.00),\n",
       "  (871, 0.00),\n",
       "  (872, 0.00),\n",
       "  (873, 0.00),\n",
       "  (874, 0.00),\n",
       "  (875, 0.00),\n",
       "  (876, 0.00),\n",
       "  (877, 0.00),\n",
       "  (881, 0.00),\n",
       "  (882, 0.00),\n",
       "  (883, 0.00),\n",
       "  (884, 0.00),\n",
       "  (886, 0.00),\n",
       "  (891, 0.00),\n",
       "  (894, 0.00),\n",
       "  (895, 0.00),\n",
       "  (896, 0.00),\n",
       "  (897, 0.00),\n",
       "  (899, 0.00),\n",
       "  (900, 0.00),\n",
       "  (901, 0.00),\n",
       "  (902, 0.00),\n",
       "  (903, 0.00),\n",
       "  (904, 0.00),\n",
       "  (905, 0.00),\n",
       "  (906, 0.00),\n",
       "  (908, 0.00),\n",
       "  (909, 0.00),\n",
       "  (913, 0.00),\n",
       "  (914, 0.00),\n",
       "  (915, 0.00),\n",
       "  (916, 0.00),\n",
       "  (918, 0.00),\n",
       "  (919, 0.00),\n",
       "  (920, 0.00),\n",
       "  (921, 0.00),\n",
       "  (922, 0.00),\n",
       "  (924, 0.00),\n",
       "  (925, 0.00),\n",
       "  (926, 0.00),\n",
       "  (927, 0.00),\n",
       "  (928, 0.00),\n",
       "  (929, 0.00),\n",
       "  (930, 0.00),\n",
       "  (931, 0.00),\n",
       "  (932, 0.00),\n",
       "  (933, 0.00),\n",
       "  (935, 0.00),\n",
       "  (936, 0.00),\n",
       "  (938, 0.00),\n",
       "  (940, 0.00),\n",
       "  (941, 0.00),\n",
       "  (942, 0.00),\n",
       "  (943, 0.00),\n",
       "  (945, 0.00),\n",
       "  (948, 0.00),\n",
       "  (949, 0.00),\n",
       "  (950, 0.00),\n",
       "  (951, 0.00),\n",
       "  (952, 0.00),\n",
       "  (954, 0.00),\n",
       "  (956, 0.00),\n",
       "  (957, 0.00),\n",
       "  (959, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (964, 0.00),\n",
       "  (965, 0.00),\n",
       "  (966, 0.00),\n",
       "  (967, 0.00),\n",
       "  (969, 0.00),\n",
       "  (970, 0.00),\n",
       "  (972, 0.00),\n",
       "  (974, 0.00),\n",
       "  (975, 0.00),\n",
       "  (976, 0.00),\n",
       "  (977, 0.00),\n",
       "  (978, 0.00),\n",
       "  (980, 0.00),\n",
       "  (983, 0.00),\n",
       "  (985, 0.00),\n",
       "  (988, 0.00),\n",
       "  (989, 0.00),\n",
       "  (990, 0.00),\n",
       "  (991, 0.00),\n",
       "  (993, 0.00),\n",
       "  (994, 0.00),\n",
       "  (995, 0.00),\n",
       "  (999, 0.00)],\n",
       " [854,\n",
       "  611,\n",
       "  594,\n",
       "  580,\n",
       "  971,\n",
       "  808,\n",
       "  839,\n",
       "  815,\n",
       "  84,\n",
       "  645,\n",
       "  973,\n",
       "  748,\n",
       "  128,\n",
       "  735,\n",
       "  68,\n",
       "  506,\n",
       "  509,\n",
       "  489,\n",
       "  646,\n",
       "  108,\n",
       "  944,\n",
       "  109,\n",
       "  824,\n",
       "  865,\n",
       "  123,\n",
       "  879,\n",
       "  411,\n",
       "  806,\n",
       "  816,\n",
       "  946,\n",
       "  60,\n",
       "  192,\n",
       "  242,\n",
       "  987,\n",
       "  96,\n",
       "  354,\n",
       "  454,\n",
       "  496,\n",
       "  709,\n",
       "  787,\n",
       "  963,\n",
       "  25,\n",
       "  37,\n",
       "  52,\n",
       "  57,\n",
       "  86,\n",
       "  91,\n",
       "  160,\n",
       "  163,\n",
       "  274,\n",
       "  290,\n",
       "  292,\n",
       "  376,\n",
       "  393,\n",
       "  464,\n",
       "  507,\n",
       "  533,\n",
       "  566,\n",
       "  575,\n",
       "  581,\n",
       "  625,\n",
       "  637,\n",
       "  716,\n",
       "  753,\n",
       "  783,\n",
       "  788,\n",
       "  858,\n",
       "  939,\n",
       "  953,\n",
       "  110,\n",
       "  247,\n",
       "  281,\n",
       "  301,\n",
       "  342,\n",
       "  440,\n",
       "  612,\n",
       "  721,\n",
       "  738,\n",
       "  937,\n",
       "  7,\n",
       "  33,\n",
       "  260,\n",
       "  300,\n",
       "  360,\n",
       "  378,\n",
       "  398,\n",
       "  429,\n",
       "  468,\n",
       "  472,\n",
       "  474,\n",
       "  684,\n",
       "  696,\n",
       "  826,\n",
       "  892,\n",
       "  934,\n",
       "  955,\n",
       "  992,\n",
       "  996,\n",
       "  83,\n",
       "  92,\n",
       "  121,\n",
       "  193,\n",
       "  200,\n",
       "  218,\n",
       "  307,\n",
       "  309,\n",
       "  314,\n",
       "  316,\n",
       "  327,\n",
       "  476,\n",
       "  483,\n",
       "  508,\n",
       "  576,\n",
       "  629,\n",
       "  641,\n",
       "  654,\n",
       "  791,\n",
       "  801,\n",
       "  898,\n",
       "  907,\n",
       "  118,\n",
       "  176,\n",
       "  189,\n",
       "  321,\n",
       "  621,\n",
       "  661,\n",
       "  724,\n",
       "  737,\n",
       "  923,\n",
       "  74,\n",
       "  76,\n",
       "  119,\n",
       "  144,\n",
       "  164,\n",
       "  186,\n",
       "  228])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist, tk = diversity(learn, 10, 95)\n",
    "n, hist, tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "finished creating histogram for the 0th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 1th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 2th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 3th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 4th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 5th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 6th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 7th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 8th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 9th perturbation\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(170,\n",
       " [(611, 419.90),\n",
       "  (854, 164.80),\n",
       "  (971, 148.10),\n",
       "  (646, 11.90),\n",
       "  (703, 9.30),\n",
       "  (580, 8.70),\n",
       "  (509, 6.80),\n",
       "  (808, 6.20),\n",
       "  (888, 5.20),\n",
       "  (427, 5.10),\n",
       "  (84, 4.90),\n",
       "  (815, 4.40),\n",
       "  (645, 4.20),\n",
       "  (519, 2.80),\n",
       "  (973, 2.80),\n",
       "  (489, 2.50),\n",
       "  (506, 2.40),\n",
       "  (737, 2.20),\n",
       "  (68, 2.10),\n",
       "  (750, 2.10),\n",
       "  (721, 1.90),\n",
       "  (128, 1.80),\n",
       "  (806, 1.80),\n",
       "  (944, 1.80),\n",
       "  (108, 1.70),\n",
       "  (109, 1.70),\n",
       "  (123, 1.70),\n",
       "  (863, 1.70),\n",
       "  (621, 1.60),\n",
       "  (963, 1.60),\n",
       "  (824, 1.50),\n",
       "  (858, 1.50),\n",
       "  (440, 1.40),\n",
       "  (594, 1.40),\n",
       "  (464, 1.30),\n",
       "  (735, 1.30),\n",
       "  (748, 1.30),\n",
       "  (96, 1.20),\n",
       "  (411, 1.20),\n",
       "  (641, 1.20),\n",
       "  (816, 1.20),\n",
       "  (60, 1.10),\n",
       "  (292, 1.10),\n",
       "  (316, 1.10),\n",
       "  (429, 1.10),\n",
       "  (454, 1.10),\n",
       "  (496, 1.10),\n",
       "  (790, 1.10),\n",
       "  (987, 1.10),\n",
       "  (25, 1.00),\n",
       "  (37, 1.00),\n",
       "  (52, 1.00),\n",
       "  (57, 1.00),\n",
       "  (86, 1.00),\n",
       "  (91, 1.00),\n",
       "  (163, 1.00),\n",
       "  (242, 1.00),\n",
       "  (247, 1.00),\n",
       "  (274, 1.00),\n",
       "  (290, 1.00),\n",
       "  (314, 1.00),\n",
       "  (334, 1.00),\n",
       "  (376, 1.00),\n",
       "  (393, 1.00),\n",
       "  (507, 1.00),\n",
       "  (533, 1.00),\n",
       "  (566, 1.00),\n",
       "  (575, 1.00),\n",
       "  (625, 1.00),\n",
       "  (637, 1.00),\n",
       "  (716, 1.00),\n",
       "  (738, 1.00),\n",
       "  (753, 1.00),\n",
       "  (783, 1.00),\n",
       "  (787, 1.00),\n",
       "  (826, 1.00),\n",
       "  (865, 1.00),\n",
       "  (921, 1.00),\n",
       "  (937, 1.00),\n",
       "  (939, 1.00),\n",
       "  (953, 1.00),\n",
       "  (958, 1.00),\n",
       "  (118, 0.90),\n",
       "  (160, 0.90),\n",
       "  (192, 0.90),\n",
       "  (281, 0.90),\n",
       "  (342, 0.90),\n",
       "  (474, 0.90),\n",
       "  (476, 0.90),\n",
       "  (502, 0.90),\n",
       "  (565, 0.90),\n",
       "  (582, 0.90),\n",
       "  (612, 0.90),\n",
       "  (661, 0.90),\n",
       "  (709, 0.90),\n",
       "  (879, 0.90),\n",
       "  (110, 0.80),\n",
       "  (134, 0.80),\n",
       "  (164, 0.80),\n",
       "  (218, 0.80),\n",
       "  (301, 0.80),\n",
       "  (302, 0.80),\n",
       "  (327, 0.80),\n",
       "  (363, 0.80),\n",
       "  (401, 0.80),\n",
       "  (425, 0.80),\n",
       "  (472, 0.80),\n",
       "  (488, 0.80),\n",
       "  (562, 0.80),\n",
       "  (694, 0.80),\n",
       "  (696, 0.80),\n",
       "  (898, 0.80),\n",
       "  (907, 0.80),\n",
       "  (923, 0.80),\n",
       "  (946, 0.80),\n",
       "  (7, 0.70),\n",
       "  (120, 0.70),\n",
       "  (124, 0.70),\n",
       "  (200, 0.70),\n",
       "  (260, 0.70),\n",
       "  (300, 0.70),\n",
       "  (336, 0.70),\n",
       "  (354, 0.70),\n",
       "  (378, 0.70),\n",
       "  (398, 0.70),\n",
       "  (490, 0.70),\n",
       "  (498, 0.70),\n",
       "  (705, 0.70),\n",
       "  (775, 0.70),\n",
       "  (825, 0.70),\n",
       "  (864, 0.70),\n",
       "  (889, 0.70),\n",
       "  (955, 0.70),\n",
       "  (992, 0.70),\n",
       "  (186, 0.60),\n",
       "  (189, 0.60),\n",
       "  (360, 0.60),\n",
       "  (468, 0.60),\n",
       "  (483, 0.60),\n",
       "  (508, 0.60),\n",
       "  (581, 0.60),\n",
       "  (654, 0.60),\n",
       "  (741, 0.60),\n",
       "  (892, 0.60),\n",
       "  (934, 0.60),\n",
       "  (33, 0.50),\n",
       "  (76, 0.50),\n",
       "  (105, 0.50),\n",
       "  (151, 0.50),\n",
       "  (176, 0.50),\n",
       "  (193, 0.50),\n",
       "  (228, 0.50),\n",
       "  (298, 0.50),\n",
       "  (307, 0.50),\n",
       "  (309, 0.50),\n",
       "  (331, 0.50),\n",
       "  (343, 0.50),\n",
       "  (347, 0.50),\n",
       "  (355, 0.50),\n",
       "  (408, 0.50),\n",
       "  (497, 0.50),\n",
       "  (555, 0.50),\n",
       "  (576, 0.50),\n",
       "  (588, 0.50),\n",
       "  (629, 0.50),\n",
       "  (684, 0.50),\n",
       "  (768, 0.50),\n",
       "  (791, 0.50),\n",
       "  (852, 0.50),\n",
       "  (872, 0.50),\n",
       "  (881, 0.50),\n",
       "  (890, 0.50),\n",
       "  (968, 0.50),\n",
       "  (982, 0.50),\n",
       "  (996, 0.50),\n",
       "  (45, 0.40),\n",
       "  (61, 0.40),\n",
       "  (92, 0.40),\n",
       "  (99, 0.40),\n",
       "  (121, 0.40),\n",
       "  (131, 0.40),\n",
       "  (304, 0.40),\n",
       "  (321, 0.40),\n",
       "  (348, 0.40),\n",
       "  (392, 0.40),\n",
       "  (417, 0.40),\n",
       "  (455, 0.40),\n",
       "  (492, 0.40),\n",
       "  (528, 0.40),\n",
       "  (567, 0.40),\n",
       "  (586, 0.40),\n",
       "  (695, 0.40),\n",
       "  (762, 0.40),\n",
       "  (779, 0.40),\n",
       "  (800, 0.40),\n",
       "  (801, 0.40),\n",
       "  (805, 0.40),\n",
       "  (868, 0.40),\n",
       "  (878, 0.40),\n",
       "  (911, 0.40),\n",
       "  (15, 0.30),\n",
       "  (46, 0.30),\n",
       "  (53, 0.30),\n",
       "  (55, 0.30),\n",
       "  (66, 0.30),\n",
       "  (74, 0.30),\n",
       "  (83, 0.30),\n",
       "  (100, 0.30),\n",
       "  (116, 0.30),\n",
       "  (119, 0.30),\n",
       "  (214, 0.30),\n",
       "  (230, 0.30),\n",
       "  (235, 0.30),\n",
       "  (293, 0.30),\n",
       "  (340, 0.30),\n",
       "  (364, 0.30),\n",
       "  (369, 0.30),\n",
       "  (381, 0.30),\n",
       "  (406, 0.30),\n",
       "  (520, 0.30),\n",
       "  (577, 0.30),\n",
       "  (595, 0.30),\n",
       "  (636, 0.30),\n",
       "  (672, 0.30),\n",
       "  (711, 0.30),\n",
       "  (758, 0.30),\n",
       "  (781, 0.30),\n",
       "  (820, 0.30),\n",
       "  (839, 0.30),\n",
       "  (843, 0.30),\n",
       "  (857, 0.30),\n",
       "  (880, 0.30),\n",
       "  (882, 0.30),\n",
       "  (904, 0.30),\n",
       "  (917, 0.30),\n",
       "  (947, 0.30),\n",
       "  (948, 0.30),\n",
       "  (962, 0.30),\n",
       "  (981, 0.30),\n",
       "  (28, 0.20),\n",
       "  (30, 0.20),\n",
       "  (31, 0.20),\n",
       "  (42, 0.20),\n",
       "  (47, 0.20),\n",
       "  (48, 0.20),\n",
       "  (62, 0.20),\n",
       "  (67, 0.20),\n",
       "  (72, 0.20),\n",
       "  (79, 0.20),\n",
       "  (102, 0.20),\n",
       "  (115, 0.20),\n",
       "  (122, 0.20),\n",
       "  (144, 0.20),\n",
       "  (182, 0.20),\n",
       "  (219, 0.20),\n",
       "  (253, 0.20),\n",
       "  (305, 0.20),\n",
       "  (306, 0.20),\n",
       "  (310, 0.20),\n",
       "  (313, 0.20),\n",
       "  (337, 0.20),\n",
       "  (375, 0.20),\n",
       "  (384, 0.20),\n",
       "  (407, 0.20),\n",
       "  (415, 0.20),\n",
       "  (424, 0.20),\n",
       "  (432, 0.20),\n",
       "  (445, 0.20),\n",
       "  (452, 0.20),\n",
       "  (453, 0.20),\n",
       "  (457, 0.20),\n",
       "  (459, 0.20),\n",
       "  (484, 0.20),\n",
       "  (491, 0.20),\n",
       "  (523, 0.20),\n",
       "  (546, 0.20),\n",
       "  (593, 0.20),\n",
       "  (603, 0.20),\n",
       "  (633, 0.20),\n",
       "  (640, 0.20),\n",
       "  (643, 0.20),\n",
       "  (671, 0.20),\n",
       "  (676, 0.20),\n",
       "  (697, 0.20),\n",
       "  (725, 0.20),\n",
       "  (729, 0.20),\n",
       "  (730, 0.20),\n",
       "  (754, 0.20),\n",
       "  (794, 0.20),\n",
       "  (821, 0.20),\n",
       "  (823, 0.20),\n",
       "  (829, 0.20),\n",
       "  (845, 0.20),\n",
       "  (847, 0.20),\n",
       "  (850, 0.20),\n",
       "  (887, 0.20),\n",
       "  (906, 0.20),\n",
       "  (932, 0.20),\n",
       "  (1, 0.10),\n",
       "  (8, 0.10),\n",
       "  (9, 0.10),\n",
       "  (17, 0.10),\n",
       "  (24, 0.10),\n",
       "  (36, 0.10),\n",
       "  (39, 0.10),\n",
       "  (41, 0.10),\n",
       "  (49, 0.10),\n",
       "  (58, 0.10),\n",
       "  (64, 0.10),\n",
       "  (65, 0.10),\n",
       "  (71, 0.10),\n",
       "  (75, 0.10),\n",
       "  (87, 0.10),\n",
       "  (90, 0.10),\n",
       "  (94, 0.10),\n",
       "  (98, 0.10),\n",
       "  (113, 0.10),\n",
       "  (125, 0.10),\n",
       "  (132, 0.10),\n",
       "  (139, 0.10),\n",
       "  (141, 0.10),\n",
       "  (169, 0.10),\n",
       "  (184, 0.10),\n",
       "  (198, 0.10),\n",
       "  (205, 0.10),\n",
       "  (206, 0.10),\n",
       "  (211, 0.10),\n",
       "  (213, 0.10),\n",
       "  (226, 0.10),\n",
       "  (231, 0.10),\n",
       "  (234, 0.10),\n",
       "  (249, 0.10),\n",
       "  (252, 0.10),\n",
       "  (254, 0.10),\n",
       "  (267, 0.10),\n",
       "  (268, 0.10),\n",
       "  (272, 0.10),\n",
       "  (273, 0.10),\n",
       "  (275, 0.10),\n",
       "  (282, 0.10),\n",
       "  (291, 0.10),\n",
       "  (294, 0.10),\n",
       "  (303, 0.10),\n",
       "  (319, 0.10),\n",
       "  (351, 0.10),\n",
       "  (352, 0.10),\n",
       "  (362, 0.10),\n",
       "  (365, 0.10),\n",
       "  (383, 0.10),\n",
       "  (387, 0.10),\n",
       "  (388, 0.10),\n",
       "  (412, 0.10),\n",
       "  (413, 0.10),\n",
       "  (416, 0.10),\n",
       "  (420, 0.10),\n",
       "  (423, 0.10),\n",
       "  (428, 0.10),\n",
       "  (442, 0.10),\n",
       "  (443, 0.10),\n",
       "  (444, 0.10),\n",
       "  (446, 0.10),\n",
       "  (449, 0.10),\n",
       "  (462, 0.10),\n",
       "  (473, 0.10),\n",
       "  (477, 0.10),\n",
       "  (481, 0.10),\n",
       "  (503, 0.10),\n",
       "  (514, 0.10),\n",
       "  (515, 0.10),\n",
       "  (525, 0.10),\n",
       "  (526, 0.10),\n",
       "  (527, 0.10),\n",
       "  (530, 0.10),\n",
       "  (532, 0.10),\n",
       "  (535, 0.10),\n",
       "  (536, 0.10),\n",
       "  (547, 0.10),\n",
       "  (552, 0.10),\n",
       "  (561, 0.10),\n",
       "  (563, 0.10),\n",
       "  (570, 0.10),\n",
       "  (572, 0.10),\n",
       "  (585, 0.10),\n",
       "  (607, 0.10),\n",
       "  (613, 0.10),\n",
       "  (624, 0.10),\n",
       "  (627, 0.10),\n",
       "  (652, 0.10),\n",
       "  (653, 0.10),\n",
       "  (665, 0.10),\n",
       "  (667, 0.10),\n",
       "  (679, 0.10),\n",
       "  (692, 0.10),\n",
       "  (701, 0.10),\n",
       "  (702, 0.10),\n",
       "  (704, 0.10),\n",
       "  (733, 0.10),\n",
       "  (746, 0.10),\n",
       "  (751, 0.10),\n",
       "  (756, 0.10),\n",
       "  (759, 0.10),\n",
       "  (765, 0.10),\n",
       "  (777, 0.10),\n",
       "  (778, 0.10),\n",
       "  (786, 0.10),\n",
       "  (788, 0.10),\n",
       "  (793, 0.10),\n",
       "  (817, 0.10),\n",
       "  (819, 0.10),\n",
       "  (822, 0.10),\n",
       "  (828, 0.10),\n",
       "  (830, 0.10),\n",
       "  (831, 0.10),\n",
       "  (841, 0.10),\n",
       "  (853, 0.10),\n",
       "  (859, 0.10),\n",
       "  (866, 0.10),\n",
       "  (867, 0.10),\n",
       "  (870, 0.10),\n",
       "  (871, 0.10),\n",
       "  (886, 0.10),\n",
       "  (893, 0.10),\n",
       "  (899, 0.10),\n",
       "  (905, 0.10),\n",
       "  (910, 0.10),\n",
       "  (912, 0.10),\n",
       "  (920, 0.10),\n",
       "  (922, 0.10),\n",
       "  (938, 0.10),\n",
       "  (954, 0.10),\n",
       "  (956, 0.10),\n",
       "  (975, 0.10),\n",
       "  (979, 0.10),\n",
       "  (989, 0.10),\n",
       "  (991, 0.10),\n",
       "  (997, 0.10),\n",
       "  (0, 0.00),\n",
       "  (2, 0.00),\n",
       "  (3, 0.00),\n",
       "  (4, 0.00),\n",
       "  (5, 0.00),\n",
       "  (6, 0.00),\n",
       "  (10, 0.00),\n",
       "  (11, 0.00),\n",
       "  (12, 0.00),\n",
       "  (13, 0.00),\n",
       "  (14, 0.00),\n",
       "  (16, 0.00),\n",
       "  (18, 0.00),\n",
       "  (19, 0.00),\n",
       "  (20, 0.00),\n",
       "  (21, 0.00),\n",
       "  (22, 0.00),\n",
       "  (23, 0.00),\n",
       "  (26, 0.00),\n",
       "  (27, 0.00),\n",
       "  (29, 0.00),\n",
       "  (32, 0.00),\n",
       "  (34, 0.00),\n",
       "  (35, 0.00),\n",
       "  (38, 0.00),\n",
       "  (40, 0.00),\n",
       "  (43, 0.00),\n",
       "  (44, 0.00),\n",
       "  (50, 0.00),\n",
       "  (51, 0.00),\n",
       "  (54, 0.00),\n",
       "  (56, 0.00),\n",
       "  (59, 0.00),\n",
       "  (63, 0.00),\n",
       "  (69, 0.00),\n",
       "  (70, 0.00),\n",
       "  (73, 0.00),\n",
       "  (77, 0.00),\n",
       "  (78, 0.00),\n",
       "  (80, 0.00),\n",
       "  (81, 0.00),\n",
       "  (82, 0.00),\n",
       "  (85, 0.00),\n",
       "  (88, 0.00),\n",
       "  (89, 0.00),\n",
       "  (93, 0.00),\n",
       "  (95, 0.00),\n",
       "  (97, 0.00),\n",
       "  (101, 0.00),\n",
       "  (103, 0.00),\n",
       "  (104, 0.00),\n",
       "  (106, 0.00),\n",
       "  (107, 0.00),\n",
       "  (111, 0.00),\n",
       "  (112, 0.00),\n",
       "  (114, 0.00),\n",
       "  (117, 0.00),\n",
       "  (126, 0.00),\n",
       "  (127, 0.00),\n",
       "  (129, 0.00),\n",
       "  (130, 0.00),\n",
       "  (133, 0.00),\n",
       "  (135, 0.00),\n",
       "  (136, 0.00),\n",
       "  (137, 0.00),\n",
       "  (138, 0.00),\n",
       "  (140, 0.00),\n",
       "  (142, 0.00),\n",
       "  (143, 0.00),\n",
       "  (145, 0.00),\n",
       "  (146, 0.00),\n",
       "  (147, 0.00),\n",
       "  (148, 0.00),\n",
       "  (149, 0.00),\n",
       "  (150, 0.00),\n",
       "  (152, 0.00),\n",
       "  (153, 0.00),\n",
       "  (154, 0.00),\n",
       "  (155, 0.00),\n",
       "  (156, 0.00),\n",
       "  (157, 0.00),\n",
       "  (158, 0.00),\n",
       "  (159, 0.00),\n",
       "  (161, 0.00),\n",
       "  (162, 0.00),\n",
       "  (165, 0.00),\n",
       "  (166, 0.00),\n",
       "  (167, 0.00),\n",
       "  (168, 0.00),\n",
       "  (170, 0.00),\n",
       "  (171, 0.00),\n",
       "  (172, 0.00),\n",
       "  (173, 0.00),\n",
       "  (174, 0.00),\n",
       "  (175, 0.00),\n",
       "  (177, 0.00),\n",
       "  (178, 0.00),\n",
       "  (179, 0.00),\n",
       "  (180, 0.00),\n",
       "  (181, 0.00),\n",
       "  (183, 0.00),\n",
       "  (185, 0.00),\n",
       "  (187, 0.00),\n",
       "  (188, 0.00),\n",
       "  (190, 0.00),\n",
       "  (191, 0.00),\n",
       "  (194, 0.00),\n",
       "  (195, 0.00),\n",
       "  (196, 0.00),\n",
       "  (197, 0.00),\n",
       "  (199, 0.00),\n",
       "  (201, 0.00),\n",
       "  (202, 0.00),\n",
       "  (203, 0.00),\n",
       "  (204, 0.00),\n",
       "  (207, 0.00),\n",
       "  (208, 0.00),\n",
       "  (209, 0.00),\n",
       "  (210, 0.00),\n",
       "  (212, 0.00),\n",
       "  (215, 0.00),\n",
       "  (216, 0.00),\n",
       "  (217, 0.00),\n",
       "  (220, 0.00),\n",
       "  (221, 0.00),\n",
       "  (222, 0.00),\n",
       "  (223, 0.00),\n",
       "  (224, 0.00),\n",
       "  (225, 0.00),\n",
       "  (227, 0.00),\n",
       "  (229, 0.00),\n",
       "  (232, 0.00),\n",
       "  (233, 0.00),\n",
       "  (236, 0.00),\n",
       "  (237, 0.00),\n",
       "  (238, 0.00),\n",
       "  (239, 0.00),\n",
       "  (240, 0.00),\n",
       "  (241, 0.00),\n",
       "  (243, 0.00),\n",
       "  (244, 0.00),\n",
       "  (245, 0.00),\n",
       "  (246, 0.00),\n",
       "  (248, 0.00),\n",
       "  (250, 0.00),\n",
       "  (251, 0.00),\n",
       "  (255, 0.00),\n",
       "  (256, 0.00),\n",
       "  (257, 0.00),\n",
       "  (258, 0.00),\n",
       "  (259, 0.00),\n",
       "  (261, 0.00),\n",
       "  (262, 0.00),\n",
       "  (263, 0.00),\n",
       "  (264, 0.00),\n",
       "  (265, 0.00),\n",
       "  (266, 0.00),\n",
       "  (269, 0.00),\n",
       "  (270, 0.00),\n",
       "  (271, 0.00),\n",
       "  (276, 0.00),\n",
       "  (277, 0.00),\n",
       "  (278, 0.00),\n",
       "  (279, 0.00),\n",
       "  (280, 0.00),\n",
       "  (283, 0.00),\n",
       "  (284, 0.00),\n",
       "  (285, 0.00),\n",
       "  (286, 0.00),\n",
       "  (287, 0.00),\n",
       "  (288, 0.00),\n",
       "  (289, 0.00),\n",
       "  (295, 0.00),\n",
       "  (296, 0.00),\n",
       "  (297, 0.00),\n",
       "  (299, 0.00),\n",
       "  (308, 0.00),\n",
       "  (311, 0.00),\n",
       "  (312, 0.00),\n",
       "  (315, 0.00),\n",
       "  (317, 0.00),\n",
       "  (318, 0.00),\n",
       "  (320, 0.00),\n",
       "  (322, 0.00),\n",
       "  (323, 0.00),\n",
       "  (324, 0.00),\n",
       "  (325, 0.00),\n",
       "  (326, 0.00),\n",
       "  (328, 0.00),\n",
       "  (329, 0.00),\n",
       "  (330, 0.00),\n",
       "  (332, 0.00),\n",
       "  (333, 0.00),\n",
       "  (335, 0.00),\n",
       "  (338, 0.00),\n",
       "  (339, 0.00),\n",
       "  (341, 0.00),\n",
       "  (344, 0.00),\n",
       "  (345, 0.00),\n",
       "  (346, 0.00),\n",
       "  (349, 0.00),\n",
       "  (350, 0.00),\n",
       "  (353, 0.00),\n",
       "  (356, 0.00),\n",
       "  (357, 0.00),\n",
       "  (358, 0.00),\n",
       "  (359, 0.00),\n",
       "  (361, 0.00),\n",
       "  (366, 0.00),\n",
       "  (367, 0.00),\n",
       "  (368, 0.00),\n",
       "  (370, 0.00),\n",
       "  (371, 0.00),\n",
       "  (372, 0.00),\n",
       "  (373, 0.00),\n",
       "  (374, 0.00),\n",
       "  (377, 0.00),\n",
       "  (379, 0.00),\n",
       "  (380, 0.00),\n",
       "  (382, 0.00),\n",
       "  (385, 0.00),\n",
       "  (386, 0.00),\n",
       "  (389, 0.00),\n",
       "  (390, 0.00),\n",
       "  (391, 0.00),\n",
       "  (394, 0.00),\n",
       "  (395, 0.00),\n",
       "  (396, 0.00),\n",
       "  (397, 0.00),\n",
       "  (399, 0.00),\n",
       "  (400, 0.00),\n",
       "  (402, 0.00),\n",
       "  (403, 0.00),\n",
       "  (404, 0.00),\n",
       "  (405, 0.00),\n",
       "  (409, 0.00),\n",
       "  (410, 0.00),\n",
       "  (414, 0.00),\n",
       "  (418, 0.00),\n",
       "  (419, 0.00),\n",
       "  (421, 0.00),\n",
       "  (422, 0.00),\n",
       "  (426, 0.00),\n",
       "  (430, 0.00),\n",
       "  (431, 0.00),\n",
       "  (433, 0.00),\n",
       "  (434, 0.00),\n",
       "  (435, 0.00),\n",
       "  (436, 0.00),\n",
       "  (437, 0.00),\n",
       "  (438, 0.00),\n",
       "  (439, 0.00),\n",
       "  (441, 0.00),\n",
       "  (447, 0.00),\n",
       "  (448, 0.00),\n",
       "  (450, 0.00),\n",
       "  (451, 0.00),\n",
       "  (456, 0.00),\n",
       "  (458, 0.00),\n",
       "  (460, 0.00),\n",
       "  (461, 0.00),\n",
       "  (463, 0.00),\n",
       "  (465, 0.00),\n",
       "  (466, 0.00),\n",
       "  (467, 0.00),\n",
       "  (469, 0.00),\n",
       "  (470, 0.00),\n",
       "  (471, 0.00),\n",
       "  (475, 0.00),\n",
       "  (478, 0.00),\n",
       "  (479, 0.00),\n",
       "  (480, 0.00),\n",
       "  (482, 0.00),\n",
       "  (485, 0.00),\n",
       "  (486, 0.00),\n",
       "  (487, 0.00),\n",
       "  (493, 0.00),\n",
       "  (494, 0.00),\n",
       "  (495, 0.00),\n",
       "  (499, 0.00),\n",
       "  (500, 0.00),\n",
       "  (501, 0.00),\n",
       "  (504, 0.00),\n",
       "  (505, 0.00),\n",
       "  (510, 0.00),\n",
       "  (511, 0.00),\n",
       "  (512, 0.00),\n",
       "  (513, 0.00),\n",
       "  (516, 0.00),\n",
       "  (517, 0.00),\n",
       "  (518, 0.00),\n",
       "  (521, 0.00),\n",
       "  (522, 0.00),\n",
       "  (524, 0.00),\n",
       "  (529, 0.00),\n",
       "  (531, 0.00),\n",
       "  (534, 0.00),\n",
       "  (537, 0.00),\n",
       "  (538, 0.00),\n",
       "  (539, 0.00),\n",
       "  (540, 0.00),\n",
       "  (541, 0.00),\n",
       "  (542, 0.00),\n",
       "  (543, 0.00),\n",
       "  (544, 0.00),\n",
       "  (545, 0.00),\n",
       "  (548, 0.00),\n",
       "  (549, 0.00),\n",
       "  (550, 0.00),\n",
       "  (551, 0.00),\n",
       "  (553, 0.00),\n",
       "  (554, 0.00),\n",
       "  (556, 0.00),\n",
       "  (557, 0.00),\n",
       "  (558, 0.00),\n",
       "  (559, 0.00),\n",
       "  (560, 0.00),\n",
       "  (564, 0.00),\n",
       "  (568, 0.00),\n",
       "  (569, 0.00),\n",
       "  (571, 0.00),\n",
       "  (573, 0.00),\n",
       "  (574, 0.00),\n",
       "  (578, 0.00),\n",
       "  (579, 0.00),\n",
       "  (583, 0.00),\n",
       "  (584, 0.00),\n",
       "  (587, 0.00),\n",
       "  (589, 0.00),\n",
       "  (590, 0.00),\n",
       "  (591, 0.00),\n",
       "  (592, 0.00),\n",
       "  (596, 0.00),\n",
       "  (597, 0.00),\n",
       "  (598, 0.00),\n",
       "  (599, 0.00),\n",
       "  (600, 0.00),\n",
       "  (601, 0.00),\n",
       "  (602, 0.00),\n",
       "  (604, 0.00),\n",
       "  (605, 0.00),\n",
       "  (606, 0.00),\n",
       "  (608, 0.00),\n",
       "  (609, 0.00),\n",
       "  (610, 0.00),\n",
       "  (614, 0.00),\n",
       "  (615, 0.00),\n",
       "  (616, 0.00),\n",
       "  (617, 0.00),\n",
       "  (618, 0.00),\n",
       "  (619, 0.00),\n",
       "  (620, 0.00),\n",
       "  (622, 0.00),\n",
       "  (623, 0.00),\n",
       "  (626, 0.00),\n",
       "  (628, 0.00),\n",
       "  (630, 0.00),\n",
       "  (631, 0.00),\n",
       "  (632, 0.00),\n",
       "  (634, 0.00),\n",
       "  (635, 0.00),\n",
       "  (638, 0.00),\n",
       "  (639, 0.00),\n",
       "  (642, 0.00),\n",
       "  (644, 0.00),\n",
       "  (647, 0.00),\n",
       "  (648, 0.00),\n",
       "  (649, 0.00),\n",
       "  (650, 0.00),\n",
       "  (651, 0.00),\n",
       "  (655, 0.00),\n",
       "  (656, 0.00),\n",
       "  (657, 0.00),\n",
       "  (658, 0.00),\n",
       "  (659, 0.00),\n",
       "  (660, 0.00),\n",
       "  (662, 0.00),\n",
       "  (663, 0.00),\n",
       "  (664, 0.00),\n",
       "  (666, 0.00),\n",
       "  (668, 0.00),\n",
       "  (669, 0.00),\n",
       "  (670, 0.00),\n",
       "  (673, 0.00),\n",
       "  (674, 0.00),\n",
       "  (675, 0.00),\n",
       "  (677, 0.00),\n",
       "  (678, 0.00),\n",
       "  (680, 0.00),\n",
       "  (681, 0.00),\n",
       "  (682, 0.00),\n",
       "  (683, 0.00),\n",
       "  (685, 0.00),\n",
       "  (686, 0.00),\n",
       "  (687, 0.00),\n",
       "  (688, 0.00),\n",
       "  (689, 0.00),\n",
       "  (690, 0.00),\n",
       "  (691, 0.00),\n",
       "  (693, 0.00),\n",
       "  (698, 0.00),\n",
       "  (699, 0.00),\n",
       "  (700, 0.00),\n",
       "  (706, 0.00),\n",
       "  (707, 0.00),\n",
       "  (708, 0.00),\n",
       "  (710, 0.00),\n",
       "  (712, 0.00),\n",
       "  (713, 0.00),\n",
       "  (714, 0.00),\n",
       "  (715, 0.00),\n",
       "  (717, 0.00),\n",
       "  (718, 0.00),\n",
       "  (719, 0.00),\n",
       "  (720, 0.00),\n",
       "  (722, 0.00),\n",
       "  (723, 0.00),\n",
       "  (724, 0.00),\n",
       "  (726, 0.00),\n",
       "  (727, 0.00),\n",
       "  (728, 0.00),\n",
       "  (731, 0.00),\n",
       "  (732, 0.00),\n",
       "  (734, 0.00),\n",
       "  (736, 0.00),\n",
       "  (739, 0.00),\n",
       "  (740, 0.00),\n",
       "  (742, 0.00),\n",
       "  (743, 0.00),\n",
       "  (744, 0.00),\n",
       "  (745, 0.00),\n",
       "  (747, 0.00),\n",
       "  (749, 0.00),\n",
       "  (752, 0.00),\n",
       "  (755, 0.00),\n",
       "  (757, 0.00),\n",
       "  (760, 0.00),\n",
       "  (761, 0.00),\n",
       "  (763, 0.00),\n",
       "  (764, 0.00),\n",
       "  (766, 0.00),\n",
       "  (767, 0.00),\n",
       "  (769, 0.00),\n",
       "  (770, 0.00),\n",
       "  (771, 0.00),\n",
       "  (772, 0.00),\n",
       "  (773, 0.00),\n",
       "  (774, 0.00),\n",
       "  (776, 0.00),\n",
       "  (780, 0.00),\n",
       "  (782, 0.00),\n",
       "  (784, 0.00),\n",
       "  (785, 0.00),\n",
       "  (789, 0.00),\n",
       "  (792, 0.00),\n",
       "  (795, 0.00),\n",
       "  (796, 0.00),\n",
       "  (797, 0.00),\n",
       "  (798, 0.00),\n",
       "  (799, 0.00),\n",
       "  (802, 0.00),\n",
       "  (803, 0.00),\n",
       "  (804, 0.00),\n",
       "  (807, 0.00),\n",
       "  (809, 0.00),\n",
       "  (810, 0.00),\n",
       "  (811, 0.00),\n",
       "  (812, 0.00),\n",
       "  (813, 0.00),\n",
       "  (814, 0.00),\n",
       "  (818, 0.00),\n",
       "  (827, 0.00),\n",
       "  (832, 0.00),\n",
       "  (833, 0.00),\n",
       "  (834, 0.00),\n",
       "  (835, 0.00),\n",
       "  (836, 0.00),\n",
       "  (837, 0.00),\n",
       "  (838, 0.00),\n",
       "  (840, 0.00),\n",
       "  (842, 0.00),\n",
       "  (844, 0.00),\n",
       "  (846, 0.00),\n",
       "  (848, 0.00),\n",
       "  (849, 0.00),\n",
       "  (851, 0.00),\n",
       "  (855, 0.00),\n",
       "  (856, 0.00),\n",
       "  (860, 0.00),\n",
       "  (861, 0.00),\n",
       "  (862, 0.00),\n",
       "  (869, 0.00),\n",
       "  (873, 0.00),\n",
       "  (874, 0.00),\n",
       "  (875, 0.00),\n",
       "  (876, 0.00),\n",
       "  (877, 0.00),\n",
       "  (883, 0.00),\n",
       "  (884, 0.00),\n",
       "  (885, 0.00),\n",
       "  (891, 0.00),\n",
       "  (894, 0.00),\n",
       "  (895, 0.00),\n",
       "  (896, 0.00),\n",
       "  (897, 0.00),\n",
       "  (900, 0.00),\n",
       "  (901, 0.00),\n",
       "  (902, 0.00),\n",
       "  (903, 0.00),\n",
       "  (908, 0.00),\n",
       "  (909, 0.00),\n",
       "  (913, 0.00),\n",
       "  (914, 0.00),\n",
       "  (915, 0.00),\n",
       "  (916, 0.00),\n",
       "  (918, 0.00),\n",
       "  (919, 0.00),\n",
       "  (924, 0.00),\n",
       "  (925, 0.00),\n",
       "  (926, 0.00),\n",
       "  (927, 0.00),\n",
       "  (928, 0.00),\n",
       "  (929, 0.00),\n",
       "  (930, 0.00),\n",
       "  (931, 0.00),\n",
       "  (933, 0.00),\n",
       "  (935, 0.00),\n",
       "  (936, 0.00),\n",
       "  (940, 0.00),\n",
       "  (941, 0.00),\n",
       "  (942, 0.00),\n",
       "  (943, 0.00),\n",
       "  (945, 0.00),\n",
       "  (949, 0.00),\n",
       "  (950, 0.00),\n",
       "  (951, 0.00),\n",
       "  (952, 0.00),\n",
       "  (957, 0.00),\n",
       "  (959, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (964, 0.00),\n",
       "  (965, 0.00),\n",
       "  (966, 0.00),\n",
       "  (967, 0.00),\n",
       "  (969, 0.00),\n",
       "  (970, 0.00),\n",
       "  (972, 0.00),\n",
       "  (974, 0.00),\n",
       "  (976, 0.00),\n",
       "  (977, 0.00),\n",
       "  (978, 0.00),\n",
       "  (980, 0.00),\n",
       "  (983, 0.00),\n",
       "  (984, 0.00),\n",
       "  (985, 0.00),\n",
       "  (986, 0.00),\n",
       "  (988, 0.00),\n",
       "  (990, 0.00),\n",
       "  (993, 0.00),\n",
       "  (994, 0.00),\n",
       "  (995, 0.00),\n",
       "  (998, 0.00),\n",
       "  (999, 0.00)],\n",
       " [611,\n",
       "  854,\n",
       "  971,\n",
       "  646,\n",
       "  703,\n",
       "  580,\n",
       "  509,\n",
       "  808,\n",
       "  888,\n",
       "  427,\n",
       "  84,\n",
       "  815,\n",
       "  645,\n",
       "  519,\n",
       "  973,\n",
       "  489,\n",
       "  506,\n",
       "  737,\n",
       "  68,\n",
       "  750,\n",
       "  721,\n",
       "  128,\n",
       "  806,\n",
       "  944,\n",
       "  108,\n",
       "  109,\n",
       "  123,\n",
       "  863,\n",
       "  621,\n",
       "  963,\n",
       "  824,\n",
       "  858,\n",
       "  440,\n",
       "  594,\n",
       "  464,\n",
       "  735,\n",
       "  748,\n",
       "  96,\n",
       "  411,\n",
       "  641,\n",
       "  816,\n",
       "  60,\n",
       "  292,\n",
       "  316,\n",
       "  429,\n",
       "  454,\n",
       "  496,\n",
       "  790,\n",
       "  987,\n",
       "  25,\n",
       "  37,\n",
       "  52,\n",
       "  57,\n",
       "  86,\n",
       "  91,\n",
       "  163,\n",
       "  242,\n",
       "  247,\n",
       "  274,\n",
       "  290,\n",
       "  314,\n",
       "  334,\n",
       "  376,\n",
       "  393,\n",
       "  507,\n",
       "  533,\n",
       "  566,\n",
       "  575,\n",
       "  625,\n",
       "  637,\n",
       "  716,\n",
       "  738,\n",
       "  753,\n",
       "  783,\n",
       "  787,\n",
       "  826,\n",
       "  865,\n",
       "  921,\n",
       "  937,\n",
       "  939,\n",
       "  953,\n",
       "  958,\n",
       "  118,\n",
       "  160,\n",
       "  192,\n",
       "  281,\n",
       "  342,\n",
       "  474,\n",
       "  476,\n",
       "  502,\n",
       "  565,\n",
       "  582,\n",
       "  612,\n",
       "  661,\n",
       "  709,\n",
       "  879,\n",
       "  110,\n",
       "  134,\n",
       "  164,\n",
       "  218,\n",
       "  301,\n",
       "  302,\n",
       "  327,\n",
       "  363,\n",
       "  401,\n",
       "  425,\n",
       "  472,\n",
       "  488,\n",
       "  562,\n",
       "  694,\n",
       "  696,\n",
       "  898,\n",
       "  907,\n",
       "  923,\n",
       "  946,\n",
       "  7,\n",
       "  120,\n",
       "  124,\n",
       "  200,\n",
       "  260,\n",
       "  300,\n",
       "  336,\n",
       "  354,\n",
       "  378,\n",
       "  398,\n",
       "  490,\n",
       "  498,\n",
       "  705,\n",
       "  775,\n",
       "  825,\n",
       "  864,\n",
       "  889,\n",
       "  955,\n",
       "  992,\n",
       "  186,\n",
       "  189,\n",
       "  360,\n",
       "  468,\n",
       "  483,\n",
       "  508,\n",
       "  581,\n",
       "  654,\n",
       "  741,\n",
       "  892,\n",
       "  934,\n",
       "  33,\n",
       "  76,\n",
       "  105,\n",
       "  151,\n",
       "  176,\n",
       "  193,\n",
       "  228,\n",
       "  298,\n",
       "  307,\n",
       "  309,\n",
       "  331,\n",
       "  343,\n",
       "  347,\n",
       "  355,\n",
       "  408,\n",
       "  497,\n",
       "  555,\n",
       "  576,\n",
       "  588,\n",
       "  629,\n",
       "  684,\n",
       "  768,\n",
       "  791,\n",
       "  852,\n",
       "  872])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist, tk = diversity(learn, 10, 95)\n",
    "n, hist, tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe6574b7ac8>]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_hist = sorted(hist, key=lambda x: x[0], reverse = False)\n",
    "values = [elem[1] for elem in sorted_hist]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.9347)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "finished creating histogram for the 0th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "finished creating histogram for the 1th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "finished creating histogram for the 2th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "finished creating histogram for the 3th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "finished creating histogram for the 4th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "finished creating histogram for the 5th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "finished creating histogram for the 6th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "finished creating histogram for the 7th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "finished creating histogram for the 8th perturbation\n",
      "at batch no 0\n",
      "at batch no 100\n",
      "at batch no 200\n",
      "at batch no 300\n",
      "at batch no 400\n",
      "at batch no 500\n",
      "at batch no 600\n",
      "at batch no 700\n",
      "at batch no 800\n",
      "at batch no 900\n",
      "at batch no 1000\n",
      "at batch no 1100\n",
      "at batch no 1200\n",
      "at batch no 1300\n",
      "at batch no 1400\n",
      "at batch no 1500\n",
      "at batch no 1600\n",
      "at batch no 1700\n",
      "at batch no 1800\n",
      "at batch no 1900\n",
      "at batch no 2000\n",
      "at batch no 2100\n",
      "at batch no 2200\n",
      "at batch no 2300\n",
      "at batch no 2400\n",
      "at batch no 2500\n",
      "at batch no 2600\n",
      "at batch no 2700\n",
      "at batch no 2800\n",
      "at batch no 2900\n",
      "at batch no 3000\n",
      "at batch no 3100\n",
      "finished creating histogram for the 9th perturbation\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(449,\n",
       " [(854, 9919.40),\n",
       "  (971, 6144.30),\n",
       "  (594, 5587.50),\n",
       "  (611, 4617.10),\n",
       "  (580, 4150.50),\n",
       "  (703, 935.40),\n",
       "  (721, 810.20),\n",
       "  (509, 708.00),\n",
       "  (750, 697.60),\n",
       "  (646, 485.20),\n",
       "  (839, 426.20),\n",
       "  (815, 351.20),\n",
       "  (808, 338.00),\n",
       "  (582, 258.40),\n",
       "  (84, 252.70),\n",
       "  (427, 209.10),\n",
       "  (737, 199.00),\n",
       "  (824, 195.60),\n",
       "  (645, 188.00),\n",
       "  (748, 168.40),\n",
       "  (519, 149.00),\n",
       "  (888, 141.40),\n",
       "  (790, 129.50),\n",
       "  (800, 125.00),\n",
       "  (921, 120.60),\n",
       "  (881, 118.20),\n",
       "  (741, 109.50),\n",
       "  (401, 109.00),\n",
       "  (973, 103.70),\n",
       "  (489, 85.40),\n",
       "  (863, 82.10),\n",
       "  (506, 82.00),\n",
       "  (735, 79.20),\n",
       "  (109, 78.20),\n",
       "  (944, 76.30),\n",
       "  (440, 76.00),\n",
       "  (806, 73.40),\n",
       "  (621, 64.30),\n",
       "  (730, 62.00),\n",
       "  (679, 60.90),\n",
       "  (562, 60.60),\n",
       "  (39, 59.70),\n",
       "  (497, 59.30),\n",
       "  (911, 58.70),\n",
       "  (794, 57.30),\n",
       "  (788, 56.30),\n",
       "  (791, 56.00),\n",
       "  (581, 54.40),\n",
       "  (565, 54.30),\n",
       "  (916, 54.10),\n",
       "  (825, 52.80),\n",
       "  (879, 52.30),\n",
       "  (155, 51.60),\n",
       "  (868, 51.20),\n",
       "  (917, 50.80),\n",
       "  (946, 49.80),\n",
       "  (411, 49.50),\n",
       "  (907, 49.30),\n",
       "  (354, 49.10),\n",
       "  (134, 48.80),\n",
       "  (692, 48.80),\n",
       "  (424, 48.50),\n",
       "  (128, 48.40),\n",
       "  (340, 47.30),\n",
       "  (878, 47.10),\n",
       "  (533, 46.70),\n",
       "  (992, 46.40),\n",
       "  (99, 45.20),\n",
       "  (116, 45.10),\n",
       "  (893, 44.80),\n",
       "  (695, 44.40),\n",
       "  (522, 44.20),\n",
       "  (455, 43.60),\n",
       "  (936, 43.50),\n",
       "  (904, 42.20),\n",
       "  (987, 42.10),\n",
       "  (425, 41.70),\n",
       "  (955, 41.70),\n",
       "  (300, 41.50),\n",
       "  (304, 39.60),\n",
       "  (454, 39.50),\n",
       "  (60, 39.40),\n",
       "  (468, 39.30),\n",
       "  (446, 38.50),\n",
       "  (136, 37.40),\n",
       "  (892, 36.20),\n",
       "  (880, 35.70),\n",
       "  (866, 35.40),\n",
       "  (898, 35.40),\n",
       "  (709, 34.90),\n",
       "  (687, 34.70),\n",
       "  (385, 34.30),\n",
       "  (627, 34.00),\n",
       "  (476, 33.80),\n",
       "  (135, 32.90),\n",
       "  (641, 32.90),\n",
       "  (457, 32.60),\n",
       "  (407, 32.50),\n",
       "  (981, 32.50),\n",
       "  (323, 32.40),\n",
       "  (636, 32.20),\n",
       "  (716, 32.10),\n",
       "  (217, 32.00),\n",
       "  (241, 32.00),\n",
       "  (886, 31.70),\n",
       "  (404, 31.20),\n",
       "  (459, 31.00),\n",
       "  (61, 30.50),\n",
       "  (25, 30.40),\n",
       "  (348, 30.40),\n",
       "  (151, 30.10),\n",
       "  (595, 30.10),\n",
       "  (781, 30.10),\n",
       "  (417, 30.00),\n",
       "  (472, 30.00),\n",
       "  (958, 29.60),\n",
       "  (593, 29.40),\n",
       "  (948, 29.20),\n",
       "  (37, 29.00),\n",
       "  (444, 29.00),\n",
       "  (671, 29.00),\n",
       "  (8, 28.70),\n",
       "  (292, 28.70),\n",
       "  (436, 28.70),\n",
       "  (406, 28.40),\n",
       "  (829, 28.40),\n",
       "  (57, 28.30),\n",
       "  (290, 28.00),\n",
       "  (643, 28.00),\n",
       "  (193, 27.90),\n",
       "  (738, 27.90),\n",
       "  (478, 27.60),\n",
       "  (110, 27.40),\n",
       "  (242, 27.40),\n",
       "  (46, 27.30),\n",
       "  (858, 27.30),\n",
       "  (133, 27.20),\n",
       "  (779, 27.10),\n",
       "  (23, 27.00),\n",
       "  (954, 27.00),\n",
       "  (199, 26.80),\n",
       "  (654, 26.80),\n",
       "  (218, 26.70),\n",
       "  (327, 26.70),\n",
       "  (90, 26.50),\n",
       "  (614, 26.40),\n",
       "  (108, 26.20),\n",
       "  (700, 26.10),\n",
       "  (319, 26.00),\n",
       "  (963, 26.00),\n",
       "  (705, 25.90),\n",
       "  (661, 25.70),\n",
       "  (805, 25.70),\n",
       "  (775, 25.60),\n",
       "  (852, 25.60),\n",
       "  (184, 25.50),\n",
       "  (364, 25.50),\n",
       "  (203, 25.40),\n",
       "  (762, 25.40),\n",
       "  (434, 25.30),\n",
       "  (94, 25.20),\n",
       "  (96, 25.10),\n",
       "  (119, 25.10),\n",
       "  (723, 25.10),\n",
       "  (986, 24.90),\n",
       "  (864, 24.80),\n",
       "  (7, 24.50),\n",
       "  (591, 24.50),\n",
       "  (231, 24.40),\n",
       "  (949, 24.40),\n",
       "  (55, 24.30),\n",
       "  (170, 24.10),\n",
       "  (885, 24.00),\n",
       "  (474, 23.60),\n",
       "  (754, 23.60),\n",
       "  (76, 23.50),\n",
       "  (254, 23.40),\n",
       "  (182, 23.20),\n",
       "  (293, 23.10),\n",
       "  (603, 23.10),\n",
       "  (496, 23.00),\n",
       "  (238, 22.80),\n",
       "  (642, 22.80),\n",
       "  (50, 22.40),\n",
       "  (88, 22.40),\n",
       "  (65, 22.30),\n",
       "  (79, 22.30),\n",
       "  (751, 22.10),\n",
       "  (953, 22.00),\n",
       "  (585, 21.90),\n",
       "  (912, 21.90),\n",
       "  (352, 21.70),\n",
       "  (724, 21.70),\n",
       "  (624, 21.60),\n",
       "  (192, 21.50),\n",
       "  (599, 21.40),\n",
       "  (67, 21.30),\n",
       "  (576, 21.20),\n",
       "  (572, 21.10),\n",
       "  (652, 21.10),\n",
       "  (783, 21.00),\n",
       "  (566, 20.80),\n",
       "  (772, 20.70),\n",
       "  (947, 20.70),\n",
       "  (982, 20.70),\n",
       "  (58, 20.60),\n",
       "  (195, 20.60),\n",
       "  (688, 20.60),\n",
       "  (0, 20.50),\n",
       "  (129, 20.20),\n",
       "  (922, 20.20),\n",
       "  (97, 20.10),\n",
       "  (328, 20.00),\n",
       "  (555, 20.00),\n",
       "  (865, 20.00),\n",
       "  (82, 19.90),\n",
       "  (717, 19.90),\n",
       "  (275, 19.80),\n",
       "  (363, 19.80),\n",
       "  (492, 19.80),\n",
       "  (563, 19.80),\n",
       "  (588, 19.70),\n",
       "  (612, 19.70),\n",
       "  (998, 19.70),\n",
       "  (219, 19.60),\n",
       "  (309, 19.50),\n",
       "  (396, 19.50),\n",
       "  (237, 19.30),\n",
       "  (331, 19.20),\n",
       "  (822, 19.20),\n",
       "  (281, 19.10),\n",
       "  (549, 19.10),\n",
       "  (752, 19.10),\n",
       "  (311, 19.00),\n",
       "  (668, 18.90),\n",
       "  (453, 18.80),\n",
       "  (220, 18.70),\n",
       "  (255, 18.70),\n",
       "  (251, 18.60),\n",
       "  (701, 18.60),\n",
       "  (758, 18.60),\n",
       "  (698, 18.50),\n",
       "  (579, 18.30),\n",
       "  (216, 18.10),\n",
       "  (464, 18.10),\n",
       "  (625, 18.10),\n",
       "  (479, 18.00),\n",
       "  (342, 17.90),\n",
       "  (640, 17.80),\n",
       "  (123, 17.70),\n",
       "  (172, 17.70),\n",
       "  (575, 17.70),\n",
       "  (870, 17.60),\n",
       "  (230, 17.50),\n",
       "  (56, 17.40),\n",
       "  (74, 17.30),\n",
       "  (159, 17.30),\n",
       "  (48, 17.20),\n",
       "  (561, 17.10),\n",
       "  (831, 17.00),\n",
       "  (249, 16.90),\n",
       "  (345, 16.90),\n",
       "  (398, 16.90),\n",
       "  (467, 16.90),\n",
       "  (635, 16.90),\n",
       "  (383, 16.80),\n",
       "  (100, 16.70),\n",
       "  (985, 16.70),\n",
       "  (201, 16.60),\n",
       "  (214, 16.60),\n",
       "  (274, 16.60),\n",
       "  (843, 16.60),\n",
       "  (999, 16.60),\n",
       "  (194, 16.50),\n",
       "  (278, 16.50),\n",
       "  (381, 16.50),\n",
       "  (816, 16.50),\n",
       "  (637, 16.40),\n",
       "  (768, 16.30),\n",
       "  (850, 16.30),\n",
       "  (243, 16.20),\n",
       "  (826, 16.20),\n",
       "  (316, 16.10),\n",
       "  (890, 16.10),\n",
       "  (288, 16.00),\n",
       "  (124, 15.90),\n",
       "  (141, 15.90),\n",
       "  (211, 15.90),\n",
       "  (234, 15.80),\n",
       "  (239, 15.80),\n",
       "  (264, 15.80),\n",
       "  (191, 15.70),\n",
       "  (343, 15.70),\n",
       "  (950, 15.70),\n",
       "  (161, 15.60),\n",
       "  (664, 15.60),\n",
       "  (694, 15.60),\n",
       "  (665, 15.50),\n",
       "  (749, 15.50),\n",
       "  (832, 15.50),\n",
       "  (30, 15.20),\n",
       "  (125, 15.20),\n",
       "  (819, 15.20),\n",
       "  (28, 15.10),\n",
       "  (130, 15.00),\n",
       "  (162, 15.00),\n",
       "  (938, 15.00),\n",
       "  (197, 14.90),\n",
       "  (225, 14.90),\n",
       "  (520, 14.90),\n",
       "  (837, 14.90),\n",
       "  (321, 14.80),\n",
       "  (765, 14.80),\n",
       "  (91, 14.70),\n",
       "  (224, 14.70),\n",
       "  (263, 14.60),\n",
       "  (131, 14.50),\n",
       "  (198, 14.50),\n",
       "  (490, 14.50),\n",
       "  (77, 14.40),\n",
       "  (253, 14.20),\n",
       "  (362, 14.20),\n",
       "  (186, 14.10),\n",
       "  (221, 14.10),\n",
       "  (358, 14.10),\n",
       "  (462, 14.10),\n",
       "  (577, 14.10),\n",
       "  (699, 14.10),\n",
       "  (443, 14.00),\n",
       "  (952, 14.00),\n",
       "  (294, 13.90),\n",
       "  (560, 13.90),\n",
       "  (113, 13.80),\n",
       "  (138, 13.80),\n",
       "  (387, 13.80),\n",
       "  (247, 13.70),\n",
       "  (547, 13.70),\n",
       "  (853, 13.70),\n",
       "  (86, 13.60),\n",
       "  (491, 13.60),\n",
       "  (173, 13.50),\n",
       "  (235, 13.50),\n",
       "  (984, 13.50),\n",
       "  (102, 13.40),\n",
       "  (365, 13.40),\n",
       "  (415, 13.40),\n",
       "  (448, 13.40),\n",
       "  (541, 13.40),\n",
       "  (185, 13.30),\n",
       "  (209, 13.30),\n",
       "  (187, 13.20),\n",
       "  (189, 13.20),\n",
       "  (227, 13.10),\n",
       "  (711, 13.10),\n",
       "  (874, 13.10),\n",
       "  (92, 13.00),\n",
       "  (176, 13.00),\n",
       "  (353, 13.00),\n",
       "  (658, 13.00),\n",
       "  (696, 13.00),\n",
       "  (918, 13.00),\n",
       "  (330, 12.90),\n",
       "  (346, 12.90),\n",
       "  (414, 12.90),\n",
       "  (118, 12.80),\n",
       "  (359, 12.70),\n",
       "  (51, 12.60),\n",
       "  (514, 12.60),\n",
       "  (538, 12.60),\n",
       "  (817, 12.60),\n",
       "  (276, 12.50),\n",
       "  (27, 12.40),\n",
       "  (156, 12.40),\n",
       "  (213, 12.40),\n",
       "  (339, 12.40),\n",
       "  (857, 12.40),\n",
       "  (9, 12.30),\n",
       "  (15, 12.30),\n",
       "  (301, 12.30),\n",
       "  (439, 12.30),\n",
       "  (121, 12.20),\n",
       "  (433, 12.20),\n",
       "  (47, 12.10),\n",
       "  (63, 12.10),\n",
       "  (282, 12.10),\n",
       "  (376, 12.10),\n",
       "  (616, 12.10),\n",
       "  (659, 12.10),\n",
       "  (670, 12.10),\n",
       "  (905, 12.10),\n",
       "  (144, 12.00),\n",
       "  (337, 12.00),\n",
       "  (569, 12.00),\n",
       "  (860, 12.00),\n",
       "  (226, 11.80),\n",
       "  (656, 11.80),\n",
       "  (734, 11.80),\n",
       "  (756, 11.80),\n",
       "  (770, 11.70),\n",
       "  (393, 11.60),\n",
       "  (990, 11.60),\n",
       "  (44, 11.50),\n",
       "  (72, 11.50),\n",
       "  (559, 11.50),\n",
       "  (584, 11.50),\n",
       "  (849, 11.50),\n",
       "  (157, 11.40),\n",
       "  (206, 11.40),\n",
       "  (458, 11.40),\n",
       "  (488, 11.40),\n",
       "  (142, 11.30),\n",
       "  (175, 11.30),\n",
       "  (248, 11.30),\n",
       "  (418, 11.30),\n",
       "  (746, 11.30),\n",
       "  (820, 11.30),\n",
       "  (887, 11.20),\n",
       "  (939, 11.20),\n",
       "  (991, 11.20),\n",
       "  (449, 11.10),\n",
       "  (502, 11.10),\n",
       "  (956, 11.10),\n",
       "  (112, 10.90),\n",
       "  (188, 10.90),\n",
       "  (160, 10.80),\n",
       "  (727, 10.80),\n",
       "  (943, 10.80),\n",
       "  (317, 10.70),\n",
       "  (483, 10.70),\n",
       "  (552, 10.70),\n",
       "  (98, 10.60),\n",
       "  (215, 10.60),\n",
       "  (409, 10.60),\n",
       "  (429, 10.60),\n",
       "  (71, 10.50),\n",
       "  (177, 10.50),\n",
       "  (42, 10.40),\n",
       "  (351, 10.40),\n",
       "  (530, 10.40),\n",
       "  (586, 10.40),\n",
       "  (693, 10.40),\n",
       "  (171, 10.30),\n",
       "  (178, 10.30),\n",
       "  (228, 10.30),\n",
       "  (445, 10.30),\n",
       "  (867, 10.30),\n",
       "  (910, 10.30),\n",
       "  (181, 10.20),\n",
       "  (420, 10.20),\n",
       "  (932, 10.20),\n",
       "  (937, 10.20),\n",
       "  (573, 10.10),\n",
       "  (70, 10.00),\n",
       "  (223, 10.00),\n",
       "  (232, 10.00),\n",
       "  (361, 10.00),\n",
       "  (766, 10.00),\n",
       "  (36, 9.90),\n",
       "  (41, 9.90),\n",
       "  (308, 9.90),\n",
       "  (545, 9.90),\n",
       "  (355, 9.80),\n",
       "  (388, 9.80),\n",
       "  (609, 9.80),\n",
       "  (334, 9.70),\n",
       "  (360, 9.70),\n",
       "  (481, 9.70),\n",
       "  (24, 9.60),\n",
       "  (267, 9.60),\n",
       "  (975, 9.60),\n",
       "  (298, 9.50),\n",
       "  (318, 9.50),\n",
       "  (408, 9.50),\n",
       "  (601, 9.50),\n",
       "  (684, 9.50),\n",
       "  (196, 9.40),\n",
       "  (539, 9.40),\n",
       "  (821, 9.40),\n",
       "  (18, 9.30),\n",
       "  (259, 9.30),\n",
       "  (512, 9.30),\n",
       "  (537, 9.30),\n",
       "  (208, 9.20),\n",
       "  (322, 9.20),\n",
       "  (336, 9.20),\n",
       "  (382, 9.20),\n",
       "  (725, 9.20),\n",
       "  (83, 9.10),\n",
       "  (127, 9.10),\n",
       "  (236, 9.10),\n",
       "  (271, 9.10),\n",
       "  (325, 9.10),\n",
       "  (410, 9.10),\n",
       "  (470, 9.10),\n",
       "  (256, 9.00),\n",
       "  (277, 9.00),\n",
       "  (284, 9.00),\n",
       "  (390, 9.00),\n",
       "  (620, 9.00),\n",
       "  (45, 8.90),\n",
       "  (210, 8.90),\n",
       "  (260, 8.90),\n",
       "  (280, 8.90),\n",
       "  (314, 8.90),\n",
       "  (655, 8.90),\n",
       "  (782, 8.90),\n",
       "  (786, 8.90),\n",
       "  (261, 8.80),\n",
       "  (838, 8.80),\n",
       "  (52, 8.70),\n",
       "  (62, 8.70),\n",
       "  (164, 8.70),\n",
       "  (200, 8.70),\n",
       "  (518, 8.70),\n",
       "  (320, 8.60),\n",
       "  (372, 8.60),\n",
       "  (456, 8.60),\n",
       "  (531, 8.60),\n",
       "  (915, 8.60),\n",
       "  (994, 8.60),\n",
       "  (602, 8.50),\n",
       "  (828, 8.50),\n",
       "  (207, 8.40),\n",
       "  (528, 8.40),\n",
       "  (101, 8.30),\n",
       "  (338, 8.30),\n",
       "  (801, 8.30),\n",
       "  (840, 8.30),\n",
       "  (957, 8.30),\n",
       "  (432, 8.20),\n",
       "  (676, 8.20),\n",
       "  (753, 8.20),\n",
       "  (757, 8.20),\n",
       "  (31, 8.10),\n",
       "  (306, 8.10),\n",
       "  (759, 8.10),\n",
       "  (386, 8.00),\n",
       "  (543, 8.00),\n",
       "  (104, 7.90),\n",
       "  (163, 7.90),\n",
       "  (329, 7.90),\n",
       "  (515, 7.90),\n",
       "  (690, 7.90),\n",
       "  (769, 7.90),\n",
       "  (889, 7.90),\n",
       "  (608, 7.80),\n",
       "  (629, 7.80),\n",
       "  (667, 7.80),\n",
       "  (704, 7.80),\n",
       "  (763, 7.80),\n",
       "  (764, 7.80),\n",
       "  (861, 7.80),\n",
       "  (906, 7.80),\n",
       "  (286, 7.70),\n",
       "  (375, 7.70),\n",
       "  (523, 7.70),\n",
       "  (546, 7.70),\n",
       "  (295, 7.60),\n",
       "  (302, 7.60),\n",
       "  (313, 7.60),\n",
       "  (367, 7.60),\n",
       "  (400, 7.60),\n",
       "  (498, 7.60),\n",
       "  (14, 7.50),\n",
       "  (17, 7.40),\n",
       "  (180, 7.40),\n",
       "  (272, 7.40),\n",
       "  (389, 7.40),\n",
       "  (571, 7.40),\n",
       "  (697, 7.40),\n",
       "  (988, 7.40),\n",
       "  (85, 7.30),\n",
       "  (669, 7.30),\n",
       "  (707, 7.30),\n",
       "  (115, 7.20),\n",
       "  (205, 7.20),\n",
       "  (229, 7.20),\n",
       "  (451, 7.20),\n",
       "  (558, 7.20),\n",
       "  (872, 7.20),\n",
       "  (997, 7.20),\n",
       "  (148, 7.10),\n",
       "  (307, 7.10),\n",
       "  (471, 7.10),\n",
       "  (604, 7.10),\n",
       "  (875, 7.10),\n",
       "  (487, 7.00),\n",
       "  (639, 6.90),\n",
       "  (653, 6.90),\n",
       "  (54, 6.80),\n",
       "  (166, 6.80),\n",
       "  (179, 6.80),\n",
       "  (392, 6.80),\n",
       "  (510, 6.80),\n",
       "  (341, 6.70),\n",
       "  (373, 6.70),\n",
       "  (395, 6.70),\n",
       "  (834, 6.70),\n",
       "  (16, 6.60),\n",
       "  (283, 6.60),\n",
       "  (356, 6.60),\n",
       "  (672, 6.60),\n",
       "  (927, 6.60),\n",
       "  (273, 6.50),\n",
       "  (556, 6.50),\n",
       "  (244, 6.40),\n",
       "  (412, 6.40),\n",
       "  (503, 6.40),\n",
       "  (733, 6.40),\n",
       "  (830, 6.40),\n",
       "  (882, 6.40),\n",
       "  (269, 6.30),\n",
       "  (441, 6.30),\n",
       "  (463, 6.30),\n",
       "  (968, 6.30),\n",
       "  (107, 6.20),\n",
       "  (291, 6.20),\n",
       "  (312, 6.20),\n",
       "  (350, 6.20),\n",
       "  (357, 6.20),\n",
       "  (423, 6.20),\n",
       "  (507, 6.20),\n",
       "  (508, 6.20),\n",
       "  (835, 6.20),\n",
       "  (855, 6.20),\n",
       "  (303, 6.10),\n",
       "  (482, 6.10),\n",
       "  (305, 6.00),\n",
       "  (486, 6.00),\n",
       "  (564, 6.00),\n",
       "  (792, 6.00),\n",
       "  (924, 6.00),\n",
       "  (68, 5.90),\n",
       "  (212, 5.90),\n",
       "  (262, 5.90),\n",
       "  (714, 5.90),\n",
       "  (40, 5.80),\n",
       "  (169, 5.80),\n",
       "  (202, 5.80),\n",
       "  (484, 5.80),\n",
       "  (722, 5.80),\n",
       "  (795, 5.80),\n",
       "  (807, 5.80),\n",
       "  (368, 5.70),\n",
       "  (374, 5.70),\n",
       "  (823, 5.70),\n",
       "  (845, 5.70),\n",
       "  (989, 5.70),\n",
       "  (347, 5.60),\n",
       "  (371, 5.60),\n",
       "  (677, 5.60),\n",
       "  (732, 5.60),\n",
       "  (114, 5.50),\n",
       "  (117, 5.50),\n",
       "  (165, 5.50),\n",
       "  (535, 5.50),\n",
       "  (554, 5.50),\n",
       "  (618, 5.50),\n",
       "  (894, 5.50),\n",
       "  (570, 5.40),\n",
       "  (979, 5.40),\n",
       "  (11, 5.30),\n",
       "  (399, 5.30),\n",
       "  (421, 5.30),\n",
       "  (430, 5.30),\n",
       "  (477, 5.30),\n",
       "  (532, 5.30),\n",
       "  (776, 5.30),\n",
       "  (842, 5.30),\n",
       "  (919, 5.30),\n",
       "  (75, 5.20),\n",
       "  (240, 5.20),\n",
       "  (428, 5.20),\n",
       "  (517, 5.20),\n",
       "  (891, 5.20),\n",
       "  (53, 5.10),\n",
       "  (683, 5.10),\n",
       "  (685, 5.10),\n",
       "  (761, 5.10),\n",
       "  (814, 5.10),\n",
       "  (995, 5.10),\n",
       "  (21, 5.00),\n",
       "  (132, 5.00),\n",
       "  (154, 5.00),\n",
       "  (168, 5.00),\n",
       "  (258, 5.00),\n",
       "  (397, 5.00),\n",
       "  (706, 5.00),\n",
       "  (774, 5.00),\n",
       "  (64, 4.90),\n",
       "  (93, 4.90),\n",
       "  (183, 4.90),\n",
       "  (592, 4.90),\n",
       "  (597, 4.90),\n",
       "  (606, 4.90),\n",
       "  (959, 4.90),\n",
       "  (80, 4.80),\n",
       "  (787, 4.80),\n",
       "  (43, 4.70),\n",
       "  (619, 4.70),\n",
       "  (873, 4.70),\n",
       "  (920, 4.70),\n",
       "  (972, 4.70),\n",
       "  (379, 4.60),\n",
       "  (426, 4.60),\n",
       "  (447, 4.60),\n",
       "  (607, 4.60),\n",
       "  (931, 4.60),\n",
       "  (150, 4.50),\n",
       "  (252, 4.50),\n",
       "  (626, 4.50),\n",
       "  (268, 4.40),\n",
       "  (370, 4.40),\n",
       "  (574, 4.40),\n",
       "  (796, 4.40),\n",
       "  (139, 4.30),\n",
       "  (326, 4.30),\n",
       "  (513, 4.30),\n",
       "  (524, 4.30),\n",
       "  (784, 4.30),\n",
       "  (847, 4.30),\n",
       "  (996, 4.30),\n",
       "  (6, 4.20),\n",
       "  (32, 4.20),\n",
       "  (527, 4.20),\n",
       "  (628, 4.20),\n",
       "  (859, 4.20),\n",
       "  (900, 4.20),\n",
       "  (966, 4.20),\n",
       "  (12, 4.10),\n",
       "  (22, 4.10),\n",
       "  (289, 4.10),\n",
       "  (767, 4.10),\n",
       "  (871, 4.10),\n",
       "  (174, 4.00),\n",
       "  (310, 4.00),\n",
       "  (344, 4.00),\n",
       "  (380, 4.00),\n",
       "  (391, 4.00),\n",
       "  (402, 4.00),\n",
       "  (495, 4.00),\n",
       "  (802, 4.00),\n",
       "  (877, 4.00),\n",
       "  (951, 4.00),\n",
       "  (81, 3.90),\n",
       "  (222, 3.90),\n",
       "  (265, 3.90),\n",
       "  (349, 3.90),\n",
       "  (945, 3.90),\n",
       "  (35, 3.80),\n",
       "  (377, 3.80),\n",
       "  (413, 3.80),\n",
       "  (578, 3.80),\n",
       "  (902, 3.80),\n",
       "  (1, 3.70),\n",
       "  (126, 3.70),\n",
       "  (153, 3.70),\n",
       "  (287, 3.70),\n",
       "  (297, 3.70),\n",
       "  (416, 3.70),\n",
       "  (511, 3.70),\n",
       "  (605, 3.70),\n",
       "  (718, 3.70),\n",
       "  (836, 3.70),\n",
       "  (962, 3.70),\n",
       "  (69, 3.60),\n",
       "  (137, 3.60),\n",
       "  (250, 3.60),\n",
       "  (419, 3.60),\n",
       "  (644, 3.60),\n",
       "  (797, 3.60),\n",
       "  (983, 3.60),\n",
       "  (73, 3.50),\n",
       "  (95, 3.50),\n",
       "  (158, 3.50),\n",
       "  (257, 3.50),\n",
       "  (405, 3.50),\n",
       "  (494, 3.50),\n",
       "  (553, 3.50),\n",
       "  (638, 3.50),\n",
       "  (862, 3.50),\n",
       "  (120, 3.40),\n",
       "  (140, 3.40),\n",
       "  (567, 3.40),\n",
       "  (736, 3.40),\n",
       "  (903, 3.40),\n",
       "  (49, 3.30),\n",
       "  (145, 3.30),\n",
       "  (270, 3.30),\n",
       "  (542, 3.30),\n",
       "  (633, 3.30),\n",
       "  (674, 3.30),\n",
       "  (793, 3.30),\n",
       "  (856, 3.30),\n",
       "  (930, 3.30),\n",
       "  (105, 3.20),\n",
       "  (245, 3.20),\n",
       "  (452, 3.20),\n",
       "  (466, 3.20),\n",
       "  (505, 3.20),\n",
       "  (780, 3.20),\n",
       "  (993, 3.20),\n",
       "  (59, 3.10),\n",
       "  (596, 3.10),\n",
       "  (809, 3.10),\n",
       "  (33, 3.00),\n",
       "  (324, 3.00),\n",
       "  (663, 3.00),\n",
       "  (846, 3.00),\n",
       "  (913, 3.00),\n",
       "  (941, 3.00),\n",
       "  (10, 2.90),\n",
       "  (369, 2.90),\n",
       "  (485, 2.90),\n",
       "  (632, 2.90),\n",
       "  (778, 2.90),\n",
       "  (378, 2.80),\n",
       "  (526, 2.80),\n",
       "  (789, 2.80),\n",
       "  (884, 2.80),\n",
       "  (146, 2.70),\n",
       "  (525, 2.70),\n",
       "  (536, 2.70),\n",
       "  (755, 2.70),\n",
       "  (2, 2.60),\n",
       "  (450, 2.60),\n",
       "  (630, 2.60),\n",
       "  (731, 2.60),\n",
       "  (233, 2.50),\n",
       "  (266, 2.50),\n",
       "  (299, 2.50),\n",
       "  (469, 2.40),\n",
       "  (650, 2.40),\n",
       "  (710, 2.40),\n",
       "  (720, 2.40),\n",
       "  (798, 2.40),\n",
       "  (812, 2.40),\n",
       "  (833, 2.40),\n",
       "  (883, 2.40),\n",
       "  (5, 2.30),\n",
       "  (78, 2.30),\n",
       "  (143, 2.30),\n",
       "  (167, 2.30),\n",
       "  (551, 2.30),\n",
       "  (631, 2.30),\n",
       "  (673, 2.30),\n",
       "  (908, 2.30),\n",
       "  (394, 2.20),\n",
       "  (613, 2.20),\n",
       "  (934, 2.20),\n",
       "  (20, 2.10),\n",
       "  (38, 2.10),\n",
       "  (87, 2.10),\n",
       "  (147, 2.10),\n",
       "  (204, 2.10),\n",
       "  (279, 2.10),\n",
       "  (366, 2.10),\n",
       "  (777, 2.10),\n",
       "  (501, 2.00),\n",
       "  (666, 2.00),\n",
       "  (897, 2.00),\n",
       "  (19, 1.90),\n",
       "  (475, 1.90),\n",
       "  (811, 1.90),\n",
       "  (964, 1.90),\n",
       "  (66, 1.80),\n",
       "  (315, 1.80),\n",
       "  (544, 1.80),\n",
       "  (557, 1.80),\n",
       "  (583, 1.80),\n",
       "  (660, 1.80),\n",
       "  (708, 1.80),\n",
       "  (773, 1.80),\n",
       "  (841, 1.80),\n",
       "  (869, 1.80),\n",
       "  (923, 1.80),\n",
       "  (122, 1.70),\n",
       "  (461, 1.70),\n",
       "  (587, 1.70),\n",
       "  (729, 1.70),\n",
       "  (13, 1.60),\n",
       "  (333, 1.60),\n",
       "  (437, 1.60),\n",
       "  (529, 1.60),\n",
       "  (681, 1.60),\n",
       "  (804, 1.60),\n",
       "  (848, 1.60),\n",
       "  (190, 1.50),\n",
       "  (246, 1.50),\n",
       "  (460, 1.50),\n",
       "  (480, 1.50),\n",
       "  (682, 1.50),\n",
       "  (702, 1.50),\n",
       "  (728, 1.50),\n",
       "  (827, 1.50),\n",
       "  (34, 1.40),\n",
       "  (149, 1.40),\n",
       "  (285, 1.40),\n",
       "  (540, 1.40),\n",
       "  (548, 1.40),\n",
       "  (600, 1.40),\n",
       "  (675, 1.40),\n",
       "  (813, 1.40),\n",
       "  (89, 1.30),\n",
       "  (516, 1.30),\n",
       "  (623, 1.30),\n",
       "  (649, 1.30),\n",
       "  (744, 1.30),\n",
       "  (818, 1.30),\n",
       "  (933, 1.30),\n",
       "  (26, 1.20),\n",
       "  (106, 1.20),\n",
       "  (473, 1.20),\n",
       "  (589, 1.20),\n",
       "  (760, 1.20),\n",
       "  (851, 1.20),\n",
       "  (504, 1.10),\n",
       "  (657, 1.10),\n",
       "  (719, 1.10),\n",
       "  (899, 1.10),\n",
       "  (152, 1.00),\n",
       "  (431, 1.00),\n",
       "  (500, 1.00),\n",
       "  (568, 1.00),\n",
       "  (678, 1.00),\n",
       "  (742, 1.00),\n",
       "  (465, 0.90),\n",
       "  (598, 0.90),\n",
       "  (610, 0.90),\n",
       "  (615, 0.90),\n",
       "  (726, 0.90),\n",
       "  (803, 0.90),\n",
       "  (926, 0.90),\n",
       "  (929, 0.90),\n",
       "  (335, 0.80),\n",
       "  (743, 0.80),\n",
       "  (909, 0.80),\n",
       "  (914, 0.80),\n",
       "  (969, 0.80),\n",
       "  (435, 0.70),\n",
       "  (442, 0.70),\n",
       "  (499, 0.70),\n",
       "  (521, 0.70),\n",
       "  (942, 0.70),\n",
       "  (296, 0.60),\n",
       "  (622, 0.60),\n",
       "  (634, 0.60),\n",
       "  (647, 0.60),\n",
       "  (799, 0.60),\n",
       "  (895, 0.60),\n",
       "  (103, 0.50),\n",
       "  (111, 0.50),\n",
       "  (332, 0.50),\n",
       "  (384, 0.50),\n",
       "  (493, 0.50),\n",
       "  (648, 0.50),\n",
       "  (715, 0.50),\n",
       "  (740, 0.50),\n",
       "  (745, 0.50),\n",
       "  (844, 0.50),\n",
       "  (590, 0.40),\n",
       "  (651, 0.40),\n",
       "  (785, 0.40),\n",
       "  (876, 0.40),\n",
       "  (896, 0.40),\n",
       "  (4, 0.30),\n",
       "  (686, 0.30),\n",
       "  (901, 0.30),\n",
       "  (967, 0.30),\n",
       "  (3, 0.20),\n",
       "  (422, 0.20),\n",
       "  (438, 0.20),\n",
       "  (550, 0.20),\n",
       "  (617, 0.20),\n",
       "  (680, 0.20),\n",
       "  (689, 0.20),\n",
       "  (739, 0.20),\n",
       "  (747, 0.20),\n",
       "  (940, 0.20),\n",
       "  (977, 0.20),\n",
       "  (978, 0.20),\n",
       "  (691, 0.10),\n",
       "  (925, 0.10),\n",
       "  (29, 0.00),\n",
       "  (403, 0.00),\n",
       "  (534, 0.00),\n",
       "  (662, 0.00),\n",
       "  (712, 0.00),\n",
       "  (713, 0.00),\n",
       "  (771, 0.00),\n",
       "  (810, 0.00),\n",
       "  (928, 0.00),\n",
       "  (935, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (965, 0.00),\n",
       "  (970, 0.00),\n",
       "  (974, 0.00),\n",
       "  (976, 0.00),\n",
       "  (980, 0.00)],\n",
       " [854,\n",
       "  971,\n",
       "  594,\n",
       "  611,\n",
       "  580,\n",
       "  703,\n",
       "  721,\n",
       "  509,\n",
       "  750,\n",
       "  646,\n",
       "  839,\n",
       "  815,\n",
       "  808,\n",
       "  582,\n",
       "  84,\n",
       "  427,\n",
       "  737,\n",
       "  824,\n",
       "  645,\n",
       "  748,\n",
       "  519,\n",
       "  888,\n",
       "  790,\n",
       "  800,\n",
       "  921,\n",
       "  881,\n",
       "  741,\n",
       "  401,\n",
       "  973,\n",
       "  489,\n",
       "  863,\n",
       "  506,\n",
       "  735,\n",
       "  109,\n",
       "  944,\n",
       "  440,\n",
       "  806,\n",
       "  621,\n",
       "  730,\n",
       "  679,\n",
       "  562,\n",
       "  39,\n",
       "  497,\n",
       "  911,\n",
       "  794,\n",
       "  788,\n",
       "  791,\n",
       "  581,\n",
       "  565,\n",
       "  916,\n",
       "  825,\n",
       "  879,\n",
       "  155,\n",
       "  868,\n",
       "  917,\n",
       "  946,\n",
       "  411,\n",
       "  907,\n",
       "  354,\n",
       "  134,\n",
       "  692,\n",
       "  424,\n",
       "  128,\n",
       "  340,\n",
       "  878,\n",
       "  533,\n",
       "  992,\n",
       "  99,\n",
       "  116,\n",
       "  893,\n",
       "  695,\n",
       "  522,\n",
       "  455,\n",
       "  936,\n",
       "  904,\n",
       "  987,\n",
       "  425,\n",
       "  955,\n",
       "  300,\n",
       "  304,\n",
       "  454,\n",
       "  60,\n",
       "  468,\n",
       "  446,\n",
       "  136,\n",
       "  892,\n",
       "  880,\n",
       "  866,\n",
       "  898,\n",
       "  709,\n",
       "  687,\n",
       "  385,\n",
       "  627,\n",
       "  476,\n",
       "  135,\n",
       "  641,\n",
       "  457,\n",
       "  407,\n",
       "  981,\n",
       "  323,\n",
       "  636,\n",
       "  716,\n",
       "  217,\n",
       "  241,\n",
       "  886,\n",
       "  404,\n",
       "  459,\n",
       "  61,\n",
       "  25,\n",
       "  348,\n",
       "  151,\n",
       "  595,\n",
       "  781,\n",
       "  417,\n",
       "  472,\n",
       "  958,\n",
       "  593,\n",
       "  948,\n",
       "  37,\n",
       "  444,\n",
       "  671,\n",
       "  8,\n",
       "  292,\n",
       "  436,\n",
       "  406,\n",
       "  829,\n",
       "  57,\n",
       "  290,\n",
       "  643,\n",
       "  193,\n",
       "  738,\n",
       "  478,\n",
       "  110,\n",
       "  242,\n",
       "  46,\n",
       "  858,\n",
       "  133,\n",
       "  779,\n",
       "  23,\n",
       "  954,\n",
       "  199,\n",
       "  654,\n",
       "  218,\n",
       "  327,\n",
       "  90,\n",
       "  614,\n",
       "  108,\n",
       "  700,\n",
       "  319,\n",
       "  963,\n",
       "  705,\n",
       "  661,\n",
       "  805,\n",
       "  775,\n",
       "  852,\n",
       "  184,\n",
       "  364,\n",
       "  203,\n",
       "  762,\n",
       "  434,\n",
       "  94,\n",
       "  96,\n",
       "  119,\n",
       "  723,\n",
       "  986,\n",
       "  864,\n",
       "  7,\n",
       "  591,\n",
       "  231,\n",
       "  949,\n",
       "  55,\n",
       "  170,\n",
       "  885,\n",
       "  474,\n",
       "  754,\n",
       "  76,\n",
       "  254,\n",
       "  182,\n",
       "  293,\n",
       "  603,\n",
       "  496,\n",
       "  238,\n",
       "  642,\n",
       "  50,\n",
       "  88,\n",
       "  65,\n",
       "  79,\n",
       "  751,\n",
       "  953,\n",
       "  585,\n",
       "  912,\n",
       "  352,\n",
       "  724,\n",
       "  624,\n",
       "  192,\n",
       "  599,\n",
       "  67,\n",
       "  576,\n",
       "  572,\n",
       "  652,\n",
       "  783,\n",
       "  566,\n",
       "  772,\n",
       "  947,\n",
       "  982,\n",
       "  58,\n",
       "  195,\n",
       "  688,\n",
       "  0,\n",
       "  129,\n",
       "  922,\n",
       "  97,\n",
       "  328,\n",
       "  555,\n",
       "  865,\n",
       "  82,\n",
       "  717,\n",
       "  275,\n",
       "  363,\n",
       "  492,\n",
       "  563,\n",
       "  588,\n",
       "  612,\n",
       "  998,\n",
       "  219,\n",
       "  309,\n",
       "  396,\n",
       "  237,\n",
       "  331,\n",
       "  822,\n",
       "  281,\n",
       "  549,\n",
       "  752,\n",
       "  311,\n",
       "  668,\n",
       "  453,\n",
       "  220,\n",
       "  255,\n",
       "  251,\n",
       "  701,\n",
       "  758,\n",
       "  698,\n",
       "  579,\n",
       "  216,\n",
       "  464,\n",
       "  625,\n",
       "  479,\n",
       "  342,\n",
       "  640,\n",
       "  123,\n",
       "  172,\n",
       "  575,\n",
       "  870,\n",
       "  230,\n",
       "  56,\n",
       "  74,\n",
       "  159,\n",
       "  48,\n",
       "  561,\n",
       "  831,\n",
       "  249,\n",
       "  345,\n",
       "  398,\n",
       "  467,\n",
       "  635,\n",
       "  383,\n",
       "  100,\n",
       "  985,\n",
       "  201,\n",
       "  214,\n",
       "  274,\n",
       "  843,\n",
       "  999,\n",
       "  194,\n",
       "  278,\n",
       "  381,\n",
       "  816,\n",
       "  637,\n",
       "  768,\n",
       "  850,\n",
       "  243,\n",
       "  826,\n",
       "  316,\n",
       "  890,\n",
       "  288,\n",
       "  124,\n",
       "  141,\n",
       "  211,\n",
       "  234,\n",
       "  239,\n",
       "  264,\n",
       "  191,\n",
       "  343,\n",
       "  950,\n",
       "  161,\n",
       "  664,\n",
       "  694,\n",
       "  665,\n",
       "  749,\n",
       "  832,\n",
       "  30,\n",
       "  125,\n",
       "  819,\n",
       "  28,\n",
       "  130,\n",
       "  162,\n",
       "  938,\n",
       "  197,\n",
       "  225,\n",
       "  520,\n",
       "  837,\n",
       "  321,\n",
       "  765,\n",
       "  91,\n",
       "  224,\n",
       "  263,\n",
       "  131,\n",
       "  198,\n",
       "  490,\n",
       "  77,\n",
       "  253,\n",
       "  362,\n",
       "  186,\n",
       "  221,\n",
       "  358,\n",
       "  462,\n",
       "  577,\n",
       "  699,\n",
       "  443,\n",
       "  952,\n",
       "  294,\n",
       "  560,\n",
       "  113,\n",
       "  138,\n",
       "  387,\n",
       "  247,\n",
       "  547,\n",
       "  853,\n",
       "  86,\n",
       "  491,\n",
       "  173,\n",
       "  235,\n",
       "  984,\n",
       "  102,\n",
       "  365,\n",
       "  415,\n",
       "  448,\n",
       "  541,\n",
       "  185,\n",
       "  209,\n",
       "  187,\n",
       "  189,\n",
       "  227,\n",
       "  711,\n",
       "  874,\n",
       "  92,\n",
       "  176,\n",
       "  353,\n",
       "  658,\n",
       "  696,\n",
       "  918,\n",
       "  330,\n",
       "  346,\n",
       "  414,\n",
       "  118,\n",
       "  359,\n",
       "  51,\n",
       "  514,\n",
       "  538,\n",
       "  817,\n",
       "  276,\n",
       "  27,\n",
       "  156,\n",
       "  213,\n",
       "  339,\n",
       "  857,\n",
       "  9,\n",
       "  15,\n",
       "  301,\n",
       "  439,\n",
       "  121,\n",
       "  433,\n",
       "  47,\n",
       "  63,\n",
       "  282,\n",
       "  376,\n",
       "  616,\n",
       "  659,\n",
       "  670,\n",
       "  905,\n",
       "  144,\n",
       "  337,\n",
       "  569,\n",
       "  860,\n",
       "  226,\n",
       "  656,\n",
       "  734,\n",
       "  756,\n",
       "  770,\n",
       "  393,\n",
       "  990,\n",
       "  44,\n",
       "  72,\n",
       "  559,\n",
       "  584,\n",
       "  849,\n",
       "  157,\n",
       "  206,\n",
       "  458,\n",
       "  488,\n",
       "  142,\n",
       "  175,\n",
       "  248,\n",
       "  418,\n",
       "  746,\n",
       "  820,\n",
       "  887,\n",
       "  939,\n",
       "  991,\n",
       "  449,\n",
       "  502,\n",
       "  956,\n",
       "  112,\n",
       "  188,\n",
       "  160,\n",
       "  727,\n",
       "  943,\n",
       "  317,\n",
       "  483,\n",
       "  552,\n",
       "  98,\n",
       "  215,\n",
       "  409,\n",
       "  429,\n",
       "  71,\n",
       "  177,\n",
       "  42,\n",
       "  351,\n",
       "  530,\n",
       "  586,\n",
       "  693,\n",
       "  171,\n",
       "  178,\n",
       "  228,\n",
       "  445,\n",
       "  867,\n",
       "  910,\n",
       "  181,\n",
       "  420])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist, tk = diversity(learn, 10, 95)\n",
    "n, hist, tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe5e3e9dcc0>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_hist = sorted(hist, key=lambda x: x[0], reverse = False)\n",
    "values = [elem[1] for elem in sorted_hist]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.5969)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fCvFG0VMKts"
   },
   "outputs": [],
   "source": [
    "def make_triplet_samples(z, margin, r2, r3):\n",
    "  positive_sample = z + random_vector_volume(z.shape, 0, margin).cuda() \n",
    "  negative_sample = z + random_vector_volume(z.shape, r2, r3).cuda()\n",
    "  return positive_sample, negative_sample\n",
    "\n",
    "def random_vector_surface(shape, r = 1.):\n",
    "  mat = torch.randn(size=shape).cuda()\n",
    "  norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "  return (mat/norm) * r\n",
    "\n",
    "def random_vector_volume(shape, inner_r, outer_r):\n",
    "  fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "  fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "  fraction.unsqueeze_(-1)\n",
    "  return random_vector_surface(shape, 1) * fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PizmBkGqMKtu"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_frequent(x):\n",
    "  return Counter(x).most_common(1)[0]\n",
    "\n",
    "def preds_around(center, radius, n_preds, model, dummy_img):\n",
    "  z_s = random_vector_volume([n_preds, 10], radius, radius + 0.01) + center[None]\n",
    "  noises = model.forward_z(z_s)\n",
    "  perturbed_imgs = noises + dummy_img \n",
    "  return torch.argmax(arch(perturbed_imgs), 1)\n",
    "  \n",
    "def most_freq_pred_around(center, radius, n_preds, model, dummy_img):\n",
    "  preds = preds_around(center, radius, n_preds, model, dummy_img)\n",
    "  most_freq = most_frequent(preds.tolist())\n",
    "  return (class_index_to_label(most_freq[0]), most_freq[1]/n_preds)\n",
    "\n",
    "def investigate_neighborhood(z, step, model, dummy_img):\n",
    "  with torch.no_grad():\n",
    "    result = []\n",
    "    for radius in np.arange(0.1, 6., step):\n",
    "#       print(\"creating {} more preds\".format(int(10 + 5 * (radius ** 2))))\n",
    "      most_freq_pred = most_freq_pred_around(z, radius, int(10 + 5 * (radius ** 2)), model, dummy_img)\n",
    "      result.append((radius, most_freq_pred))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-btRW4qMKtw",
    "outputId": "90e81f6a-1b9e-45a4-ae82-bda370319bd9"
   },
   "outputs": [],
   "source": [
    "#experiment 1\n",
    "\n",
    "z = torch.tensor([0.5] * 10).cuda()\n",
    "# z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "# z_s = z[None]\n",
    "\n",
    "model = learn.model.eval()\n",
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "  \n",
    "for i in range(6):\n",
    "  z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "  print(\"investigation for: \", z)\n",
    "  for elem in investigate_neighborhood(z, 0.5, model, x_img):\n",
    "    print(elem)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 1-1: modified investigate_z\n",
    "z_investigate_path = '/root/Derakhshani/adversarial/textual_notes/investigate_z_{}.txt'.format(env.save_filename)\n",
    "if Path(z_investigate_path).exists(): raise FileExistsError(\"file already exists\")\n",
    "file = open(str(z_investigate_path), 'w')\n",
    "        \n",
    "for i, (z, noise) in enumerate(zip(pruned_z_s, pruned_noises)):\n",
    "  hist = compute_prediction_histogram(learn, noise)\n",
    "  indexed_hist = [(i, val) for i, val in enumerate(hist)]\n",
    "  sorted_hist = sorted(indexed_hist, key=lambda x: x[1], reverse=True)\n",
    "  labeled_hist = [(class_index_to_label(i), count) for i, count in sorted_hist]\n",
    "  print(\"result {}:\".format(i))\n",
    "  print(big_vector_to_str(z))\n",
    "  print(labeled_hist[:6])\n",
    "  print(\"\\n\\n\")\n",
    "  \n",
    "  file.write(\"result {}:\\n\".format(i))\n",
    "  file.write(big_vector_to_str(z) + \"\\n\")\n",
    "  file.write(str(labeled_hist[:6]))\n",
    "  file.write(\"\\n\\n\\n\")\n",
    "  file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bp6YOnipMKtz"
   },
   "outputs": [],
   "source": [
    "#experiment 2\n",
    "import itertools\n",
    "z_s = [torch.tensor(t).cuda() for t in itertools.product( *([[-0.33, 0.33]] * 10) )]\n",
    "model = learn.model.eval()\n",
    "noises = []\n",
    "with torch.no_grad():\n",
    "  for z in z_s:\n",
    "    noises.append(model.forward_single_z(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "55lErWDyMKt1",
    "outputId": "93d8fb71-3fd5-44a5-d3ec-8013e13f17ba"
   },
   "outputs": [],
   "source": [
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "\n",
    "preds = []\n",
    "for noise in noises:\n",
    "  perturbed_img = x_img + noise\n",
    "  preds.append(torch.argmax(arch(perturbed_img[None]), 1)[0].item())\n",
    "\n",
    "from collections import Counter\n",
    "result = [(class_index_to_label(index), count) for index, count in Counter(preds).most_common(5)]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4WSg-wBFMKt5"
   },
   "outputs": [],
   "source": [
    "#experiment 3\n",
    "import itertools\n",
    "dimension_values = [[-0.9, 0.9]] * z_dim\n",
    "for i in range(z_dim):\n",
    "  if i % 100 != 0:\n",
    "    dimension_values[i] = [0.]\n",
    "# dimension_values[0] = [0.]\n",
    "# dimension_values[3] = [0.]\n",
    "# dimension_values[6] = [0.]\n",
    "# dimension_values[9] = [0.]\n",
    "pruned_z_s = [torch.tensor(t).cuda() for t in itertools.product(*dimension_values)]\n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 3: for the targeted-attack case\n",
    "pruned_z_s = []\n",
    "for i in range(z_dim):\n",
    "  new_z = torch.zeros(z_dim).cuda()\n",
    "  new_z[i] = 1.\n",
    "  pruned_z_s.append(new_z)\n",
    "  \n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 3-1: noises for \n",
    "pruned_z_s = []\n",
    "# for i in range(z_dim):\n",
    "#   new_z = torch.empty(z_dim).uniform_(0,1).cuda().detach()\n",
    "#   pruned_z_s.append(new_z)\n",
    "\n",
    "for i in range(z_dim):\n",
    "  new_z = torch.zeros(z_dim).cuda().detach()\n",
    "  new_z[i] = 1.\n",
    "  pruned_z_s.append(new_z)\n",
    "  \n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise in pruned_noises[0:200]:\n",
    "  img = noise_to_image(noise)\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider web\n",
    "z_values = [\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33],\n",
    "  [-0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33,  0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33,  0.33,  0.33],\n",
    "  [-0.33,  0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_values = [\n",
    "  # window screen\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuAVZzmKMKt9",
    "outputId": "7e6643e0-ce55-438b-e0ae-79bde3ee4cef"
   },
   "outputs": [],
   "source": [
    "#vgg-16_12 most repeated labels:\n",
    "l = [(611, 215.0),\n",
    "  (474, 194.1),\n",
    "  (398, 120.3),\n",
    "  (721, 79.6),\n",
    "  (741, 73.5),\n",
    "  (510, 62.5)]\n",
    "\n",
    "[(class_index_to_label(index), count) for index, count in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAgk-YyWc3rG"
   },
   "outputs": [],
   "source": [
    "# learn.recorder.plot_losses()\n",
    "# learn.recorder.plot_lr()\n",
    "# learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTHG4Bt7VDYp"
   },
   "outputs": [],
   "source": [
    "fooling_rates = []\n",
    "model = learn.model.eval()\n",
    "learn.metrics = [validation_single_perturbation]\n",
    "for i in range(10):\n",
    "  global_perturbations = model(torch.rand(1, 3, 224, 244).cuda())[0]\n",
    "  nag_util.global_perturbations = global_perturbations\n",
    "  fooling_rates.append(learn.validate()[1].cpu().item())\n",
    "  print(\"%d : %f\"%(i, fooling_rates[-1]))\n",
    "\n",
    "mean = np.mean(fooling_rates)\n",
    "stddev = np.std(fooling_rates)\n",
    "print(mean, stddev); print(fooling_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "OFCjzI7UaY3C",
    "outputId": "740185b4-dd54-46f4-b0af-79ee452568e1"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[200][0]\n",
    "x = normalize(x_img.data.cuda())\n",
    "z = torch.tensor([-0.33,  0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33], dtype=torch.float32).cuda()\n",
    "# z = torch.empty(z_dim).uniform_(-1,1).cuda()\n",
    "p = model.forward_single_z(z).detach()\n",
    "\n",
    "p_x = x + p\n",
    "# print(\"img range, noise range\")\n",
    "# print_range(x); print_range(p)\n",
    "adv_label = class_index_to_label(arch(p_x[None]).argmax(1).item())\n",
    "print_big_vector(arch(p_x[None])[0])\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0., 1.])\n",
    "p_img = Image(p)\n",
    "x_img.show()\n",
    "p_img.show()\n",
    "p_x_img.show()\n",
    "\n",
    "\n",
    "# print_range(p)\n",
    "# print_range(denormalize(x))\n",
    "# print_range(p_x)\n",
    "\n",
    "benign_label = class_index_to_label(arch(x[None]).argmax(1).item())\n",
    "\n",
    "print_big_vector(arch(x[None])[0])\n",
    "print(benign_label, adv_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzwsI2P1ZANz"
   },
   "outputs": [],
   "source": [
    "z1 = torch.tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p1 = model.forward_single_z(z1)\n",
    "\n",
    "z2 = torch.tensor([1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p2 = model.forward_single_z(z2)\n",
    "\n",
    "z3 = torch.tensor([1, 1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p3 = model.forward_single_z(z3)\n",
    "\n",
    "l2_distance(p1, p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eroI82OKSnAL"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[4][0]\n",
    "x = x_img.data[None].cuda()\n",
    "p = model(x)[0].squeeze().detach() \n",
    "x = x.squeeze()\n",
    "x = normalize(x)\n",
    "\n",
    "p_x = x + p\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0.,1.])\n",
    "p_img = Image(p)\n",
    "# x_img.show()\n",
    "p_img.show()\n",
    "# p_x_img.show()\n",
    "\n",
    "print_range(p)\n",
    "print_range(x)\n",
    "print_range(p_x)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NAG-tripletLossExperiment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
