{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ahmad-PH/nag-notebooks/blob/master/NAG_tripletLossExperiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqeZpz16do4y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up-to-date.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os; import subprocess\n",
    "\n",
    "def detect_env():\n",
    "    return 'colab' if 'content' in os.listdir('/') else 'IBM'\n",
    "  \n",
    "def run_shell_command(cmd):\n",
    "  p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "  print(str(p.communicate()[0], 'utf-8'))\n",
    "  \n",
    "if detect_env() == 'colab': root_folder = '/content'\n",
    "elif detect_env() == 'IBM' : root_folder = '/root/Derakhshani/adversarial'\n",
    "python_files_path = root_folder + '/nag-public'\n",
    "\n",
    "if os.path.isdir(python_files_path):\n",
    "  initial_dir = os.getcwd()\n",
    "  os.chdir(python_files_path)\n",
    "  run_shell_command('git pull')\n",
    "  os.chdir(initial_dir)\n",
    "else:\n",
    "  os.chdir('/root/Derakhshani/adversarial')\n",
    "  run_shell_command('git clone https://github.com/ahmad-PH/nag-public.git')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(python_files_path + '/NAG-11May-beforeDenoiser')\n",
    "\n",
    "from nag_util import *\n",
    "import nag_util\n",
    "from environment import *\n",
    "\n",
    "env = create_env()\n",
    "env.setup(cuda_index = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Ev7jcRKoARg"
   },
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.imports import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys; import os; import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.nn import init\n",
    "from typing import Iterable\n",
    "torch.Tensor.ndim = property(lambda x: len(x.shape))\n",
    "\n",
    "def listify(o):\n",
    "    if o is None: return []\n",
    "    if isinstance(o, list): return o\n",
    "    if isinstance(o, Iterable): return list(o)\n",
    "    return [o]\n",
    "\n",
    "class ListContainer():\n",
    "  def __init__(self, items): self.items = listify(items)\n",
    "  def __getitem__(self, idx):\n",
    "    if isinstance(idx, (int, slice)): return self.items[idx]\n",
    "    if isinstance(idx[0], bool):\n",
    "      assert len(idx) == len(self)\n",
    "      return [o for m,o in zip(idx, self.items) if m]\n",
    "    return [self.items[i] for i in idx]\n",
    "  \n",
    "  def __len__(self): return len(self.items)\n",
    "  def __iter__(self): return iter(self.items)\n",
    "  def __setitem__(self, i, o): self.items[i] = o\n",
    "  def __delitem__(self, i): del(self.items[i])\n",
    "  def __repr__(self):\n",
    "    res = f\"{self.__class__.__name__} ({len(self)} items)\\n{self.items[:10]}\"\n",
    "    if len(self)>10: res = res[:-1] + \"...]\"\n",
    "    return res\n",
    "\n",
    "def children(m): return list(m.children())\n",
    "\n",
    "def append_stats_non_normal(hook, mod, inp, outp):\n",
    "  if not hasattr(hook,'stats'): hook.stats = ([],[],[])\n",
    "  means,stds,hists = hook.stats\n",
    "  means.append(outp.data.mean().cpu())\n",
    "  stds .append(outp.data.std().cpu())\n",
    "  hists.append(outp.data.cpu().histc(40,0,10)) #histc isn't implemented on the GPU\n",
    "\n",
    "def append_stats_normal(hook, mod, inp, outp):\n",
    "    if not hasattr(hook,'stats'): hook.stats = ([],[],[])\n",
    "    means,stds,hists = hook.stats\n",
    "    means.append(outp.data.mean().cpu())\n",
    "    stds .append(outp.data.std().cpu())\n",
    "    hists.append(outp.data.cpu().histc(40,-7,7))\n",
    "\n",
    "def get_hist(h):\n",
    "  return torch.stack(h.stats[2]).t().float().log1p()\n",
    "\n",
    "def get_min(h):\n",
    "  h1 = torch.stack(h.stats[2]).t().float()\n",
    "  return h1[:2].sum(0)/h1.sum(0)\n",
    "\n",
    "class Hook():\n",
    "  def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self))\n",
    "  def __del__(self): self.remove()\n",
    "  def remove(self): self.hook.remove()\n",
    "    \n",
    "class Hooks(ListContainer):\n",
    "  def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms.children()])\n",
    "  def __enter__(self, *args): return self\n",
    "  def __exit__ (self, *args): self.remove()\n",
    "  def __del__(self): self.remove()\n",
    "\n",
    "  def __delitem__(self, i):\n",
    "    self[i].remove()\n",
    "    super().__delitem__(i)\n",
    "\n",
    "  def remove(self):\n",
    "    for h in self: h.remove()\n",
    "\n",
    "def init_cnn_(m, f):\n",
    "    if isinstance(m, nn.ConvTranspose2d):\n",
    "      f(m.weight, a=0.1)\n",
    "      if getattr(m, 'bias', None) is not None: m.bias.data.zero_()\n",
    "        \n",
    "    #non-orthogonal\n",
    "    if isinstance(m, nn.Linear):\n",
    "      f(m.weight, a=0.)\n",
    "      if getattr(m, 'bias', None) is not None: m.bias.data.zero_()\n",
    "        \n",
    "    #orthogonal\n",
    "#     if isinstance(m, nn.Linear):\n",
    "#       init.orthogonal_(m.weight)\n",
    "#       m.bias.data.zero_()\n",
    "\n",
    "    for l in m.children(): init_cnn_(l, f)  \n",
    "      \n",
    "def init_cnn(m, uniform=False):\n",
    "    f = init.kaiming_uniform_ if uniform else init.kaiming_normal_\n",
    "    init_cnn_(m, f)\n",
    "\n",
    "class GeneralRelu(nn.Module):\n",
    "  def __init__(self, leak=None, sub=None, maxv=None):\n",
    "    super().__init__()\n",
    "    self.leak,self.sub,self.maxv = leak,sub,maxv\n",
    "\n",
    "  def forward(self, x): \n",
    "    x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n",
    "    if self.sub is not None: x.sub_(self.sub)\n",
    "    if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "    return x\n",
    "  \n",
    "class deconv_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k_size = (4,4), s = (2,2), pad = (1,1), b = True, activation = True):\n",
    "        super(deconv_layer, self).__init__()\n",
    "\n",
    "        self.CT2d = nn.ConvTranspose2d(in_channels = in_ch,\n",
    "                                  out_channels = out_ch,\n",
    "                                  kernel_size = k_size,\n",
    "                                  stride = s, \n",
    "                                  padding = pad,\n",
    "                                  bias = b)\n",
    "        self.BN2d = nn.BatchNorm2d(out_ch)\n",
    "        \n",
    "        self.activation = activation\n",
    "        if self.activation:\n",
    "            self.relu = GeneralRelu(0, 0.2, 5)\n",
    "        \n",
    "        self.weight_init()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.activation:\n",
    "            return self.relu(self.BN2d(self.CT2d(input)))\n",
    "        else:\n",
    "            return self.BN2d(self.CT2d(input))\n",
    "\n",
    "    def weight_init(self):\n",
    "        self.CT2d.weight.data.normal_(mean = 0, std = 0.02)\n",
    "        self.CT2d.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tltucTv2ep9-"
   },
   "outputs": [],
   "source": [
    "# mode = \"sanity_check\"\n",
    "mode = \"normal\"\n",
    "# mode = \"div_metric_calc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50\n",
    "# model = models.resnet152\n",
    "# model = models.vgg16_bn\n",
    "# model = torchvision.models.googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SO1h55obXzOv",
    "outputId": "54414cc5-84d5-4f45-ecab-87374a58dd33"
   },
   "outputs": [],
   "source": [
    "if mode == \"normal\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "elif mode == \"sanity_check\":\n",
    "  env.load_dataset('dataset_sanity_check_small', 'dataset_sanity_check_small')  \n",
    "  env.set_data_path('dataset_sanity_check_small')\n",
    "elif mode == \"div_metric_calc\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "  env.load_test_dataset(str(env.data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "koaQZmjMom7w"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "gpu_flag = True\n",
    "nag_util.batch_size = batch_size; nag_util.gpu_flag = gpu_flag;\n",
    "tfms = get_transforms(do_flip=False, max_rotate=0)\n",
    "data = (ImageList.from_folder(env.data_path)\n",
    "        .split_by_folder(valid=('test' if mode == 'div_metric_calc' else 'valid'))\n",
    "        .label_from_folder()\n",
    "        .transform(tfms, size=224)\n",
    "        .databunch(bs=batch_size, num_workers=1)\n",
    "        .normalize(imagenet_stats))\n",
    "\n",
    "# data.show_batch(rows=2, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDBkRV8yovwV"
   },
   "outputs": [],
   "source": [
    "z_dim = 10\n",
    "arch = SoftmaxWrapper(model(pretrained=True).cuda().eval())\n",
    "nag_util.arch = arch\n",
    "requires_grad(arch, False)\n",
    "\n",
    "# vgg:\n",
    "# layers = []\n",
    "# blocks = [i-1 for i,o in enumerate(children(arch.features)) if isinstance(o, nn.MaxPool2d)]\n",
    "# layers = [arch.features[i] for i in blocks]\n",
    "# layer_weights = [1] * len(layers)\n",
    "\n",
    "layers = [\n",
    "    arch.softmax\n",
    "]\n",
    "\n",
    "layer_weights = [1.] * len(layers)\n",
    "\n",
    "# inception:\n",
    "# layers = [\n",
    "#     arch.Conv2d_1a_3x3,\n",
    "#     arch.Mixed_6e,\n",
    "#     arch.Mixed_7a,\n",
    "#     arch.fc    \n",
    "# ]\n",
    "# layer_weights = [1.0/4.0] * len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Gen(nn.Module):\n",
    "#   def __init__(self, z_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "#     super(Gen, self).__init__()\n",
    "\n",
    "#     self.bs = None\n",
    "#     self.z_dim = z_dim\n",
    "#     self.gf_dim = gf_dim\n",
    "#     self.y_dim = y_dim\n",
    "#     self.df_dim = df_dim\n",
    "#     self.image_shape = image_shape\n",
    "\n",
    "#     self.z_ = nn.Linear(self.z_dim, self.gf_dim * 7 * 4 * 4, bias=True)\n",
    "#     self.z_.bias.data.fill_(0)\n",
    "#     self.BN_ = nn.BatchNorm2d(self.gf_dim * 7)\n",
    "\n",
    "#     self.half = max(self.gf_dim // 2, 1) \n",
    "#     self.quarter = max(self.gf_dim // 4, 1)\n",
    "#     self.eighth = max(self.gf_dim // 8, 1)\n",
    "#     # sixteenth = max(self.gf_dim // 16, 1)\n",
    "    \n",
    "#     self.CT2d_1 = deconv_layer(self.gf_dim * 8, self.gf_dim * 4, k_size = (5,5), pad = (2,2))\n",
    "#     self.CT2d_2 = deconv_layer(self.gf_dim * 5, self.gf_dim * 2)    \n",
    "#     self.CT2d_3 = deconv_layer(self.gf_dim * 2 + self.half, self.gf_dim * 1)\n",
    "#     self.CT2d_4 = deconv_layer(self.gf_dim * 1 + self.quarter, self.gf_dim * 1)\n",
    "#     self.CT2d_5 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "#     self.CT2d_6 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "#     self.CT2d_7 = deconv_layer(self.gf_dim * 1 + self.eighth, 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "#   def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "#     h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "#     h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "#     output = deconv_layer(h_input)\n",
    "#     assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "#             \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "#             \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "#     return output\n",
    "  \n",
    "#   def forward_z(self, z):\n",
    "#     self.bs = z.shape[0]\n",
    "    \n",
    "#     h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "#     assert h0.shape[2:] == (4, 4), \"Unexpected shape, it shoud be (4,4)\"\n",
    "\n",
    "#     h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "#     h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "#     h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "#     h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "#     h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "#     h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "#     h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "#     ksi = 10.0\n",
    "#     output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "#     # this coeff scales the output to be appropriate for images that are \n",
    "#     # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "#     # interval)\n",
    "#     return output_coeff * torch.tanh(h7)\n",
    "  \n",
    "# #   # blind-selection\n",
    "#   def forward(self, inputs):\n",
    "#     self.bs = inputs.shape[0]\n",
    "\n",
    "#     benign_preds_onehot = arch(inputs)\n",
    "#     benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "    \n",
    "#     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "#     for i in range(self.bs):\n",
    "#       random_label = self.randint(0,1000, exclude = benign_preds[i].item())\n",
    "#       z[i][random_label] = 1.\n",
    "    \n",
    "#     z_out = self.forward_z(z)\n",
    "    \n",
    "#     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "\n",
    "# #   #second-best selection: made validation so much worse\n",
    "# #   def forward(self, inputs):\n",
    "# #     self.bs = inputs.shape[0]\n",
    "\n",
    "# #     benign_preds_onehot = arch(inputs)\n",
    "# #     target_preds = torch.topk(benign_preds_onehot, 2, dim = 1).indices[:, 1:]\n",
    "    \n",
    "# #     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "# #     for i in range(self.bs):\n",
    "# #       z[i][target_preds[i]] = 1.\n",
    "    \n",
    "# #     z_out = self.forward_z(z)\n",
    "    \n",
    "# #     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "  \n",
    "# #    def forward(self, inputs):\n",
    "# #     self.bs = inputs.shape[0]\n",
    "\n",
    "# #     benign_preds_onehot = arch(inputs)\n",
    "# #     benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "    \n",
    "# #     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "# #     random_label = self.randint(0,1000, exclude = benign_preds.tolist())\n",
    "# #     for i in range(self.bs):\n",
    "# #       z[i][random_label] = 1.\n",
    "    \n",
    "# #     z_out = self.forward_z(z)\n",
    "    \n",
    "# #     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "  \n",
    "#   @staticmethod\n",
    "#   def randint(low, high, exclude):\n",
    "#     temp = np.random.randint(low, high - 1)\n",
    "#     if temp == exclude:\n",
    "#       temp = temp + 1\n",
    "#     return temp\n",
    "  \n",
    "#   def forward_single_z(self, z):\n",
    "#     return self.forward_z(z[None]).squeeze()\n",
    "           \n",
    "  \n",
    "#   def make_triplet_samples(self, z, margin, r2, r3):\n",
    "#     positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "#     negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "#     return positive_sample, negative_sample\n",
    "\n",
    "#   def random_vector_surface(self, shape, r = 1.):\n",
    "#     mat = torch.randn(size=shape).cuda()\n",
    "#     norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "#     return (mat/norm) * r\n",
    "\n",
    "  \n",
    "#   def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "#     fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "#     fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "#     fraction.unsqueeze_(-1)\n",
    "#     return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "#   def make_z(self, in_shape):\n",
    "#     return torch.empty(in_shape).cuda().uniform_(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-targeted Gen\n",
    "class Gen(nn.Module):\n",
    "  def __init__(self, z_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "    super(Gen, self).__init__()\n",
    "\n",
    "    self.bs = None\n",
    "    self.z_dim = z_dim\n",
    "    self.gf_dim = gf_dim\n",
    "    self.y_dim = y_dim\n",
    "    self.df_dim = df_dim\n",
    "    self.image_shape = image_shape\n",
    "\n",
    "    self.z_ = nn.Linear(self.z_dim, self.gf_dim * 7 * 4 * 4, bias=True)\n",
    "    self.z_.bias.data.fill_(0)\n",
    "    self.BN_ = nn.BatchNorm2d(self.gf_dim * 7)\n",
    "\n",
    "    self.half = max(self.gf_dim // 2, 1) \n",
    "    self.quarter = max(self.gf_dim // 4, 1)\n",
    "    self.eighth = max(self.gf_dim // 8, 1)\n",
    "    # sixteenth = max(self.gf_dim // 16, 1)\n",
    "\n",
    "    self.CT2d_1 = deconv_layer(self.gf_dim * 8, self.gf_dim * 4, k_size = (5,5), pad = (2,2))\n",
    "    self.CT2d_2 = deconv_layer(self.gf_dim * 5, self.gf_dim * 2)    \n",
    "    self.CT2d_3 = deconv_layer(self.gf_dim * 2 + self.half, self.gf_dim * 1)\n",
    "    self.CT2d_4 = deconv_layer(self.gf_dim * 1 + self.quarter, self.gf_dim * 1)\n",
    "    self.CT2d_5 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "    self.CT2d_6 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "    self.CT2d_7 = deconv_layer(self.gf_dim * 1 + self.eighth, 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "  def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "    h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "    h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "    output = deconv_layer(h_input)\n",
    "    assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "            \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "            \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "    return output\n",
    "  \n",
    "  def forward_z(self, z):\n",
    "    self.bs = z.shape[0]\n",
    "    \n",
    "    h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "    assert h0.shape[2:] == (4, 4), \"Non-expected shape, it shoud be (4,4)\"\n",
    "\n",
    "    h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "    h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "    h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "    h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "    h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "    h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "    h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "    ksi = 10.0\n",
    "    output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "    # this coeff scales the output to be appropriate for images that are \n",
    "    # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "    # interval)\n",
    "    return output_coeff * torch.tanh(h7)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    self.bs = inputs.shape[0]\n",
    "    z = inputs.new_empty([self.bs, self.z_dim]).uniform_(-1,1).cuda()\n",
    "    p, n = self.make_triplet_samples(z, 0.2, 1., 3.)\n",
    "    \n",
    "    z_out = self.forward_z(z)\n",
    "    p_out = self.forward_z(p)\n",
    "    n_out = self.forward_z(n)\n",
    "    \n",
    "    return z_out, p_out, n_out, inputs\n",
    "#     return z_out, None, None, inputs\n",
    "  \n",
    "  def forward_single_z(self, z):\n",
    "    return self.forward_z(z[None]).squeeze()\n",
    "           \n",
    "  \n",
    "  def make_triplet_samples(self, z, margin, r2, r3):\n",
    "    positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "    negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "    return positive_sample, negative_sample\n",
    "\n",
    "  def random_vector_surface(self, shape, r = 1.):\n",
    "    mat = torch.randn(size=shape).cuda()\n",
    "    norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "    return (mat/norm) * r\n",
    "\n",
    "  \n",
    "  def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "    fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "    fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "    fraction.unsqueeze_(-1)\n",
    "    return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "  def make_z(self, in_shape):\n",
    "    return torch.empty(in_shape).cuda().uniform_(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NkfbLWEQqRA_"
   },
   "outputs": [],
   "source": [
    "def js_distance(x1, x2):\n",
    "  m = 0.5 * (x1 + x2)\n",
    "  return 0.5 * (F.kl_div(x1, m) + F.kl_div(x2, m))\n",
    "\n",
    "def kl_distance(x1, x2):\n",
    "  inp = torch.log(x1)\n",
    "  target = x2\n",
    "  return F.kl_div(inp, target, reduction='batchmean')\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  x1 = tensorify(x1)\n",
    "  x2 = tensorify(x2)\n",
    "  x1 = x1 / torch.sum(x1)\n",
    "  x2 = x2 / torch.sum(x2)\n",
    "  return kl_distance(x1[None], x2[None])\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  if not isinstance(x1, torch.Tensor): x1 = torch.tensor(x1)\n",
    "  if not isinstance(x2, torch.Tensor): x2 = torch.tensor(x2)\n",
    "  x1 = x1 * 100. / torch.sum(x1)\n",
    "  x2 = x2 * 100. / torch.sum(x2)\n",
    "  return torch.norm(x1 - x2, 2)\n",
    "\n",
    "def distance_from_uniform(x):\n",
    "  return distrib_distance(x, [1.] * len(x))\n",
    "\n",
    "def wasserstein_distance(x1, x2):\n",
    "  return torch.mean(x1 - x2)\n",
    "\n",
    "def l1_distance(x1, x2):\n",
    "  return F.l1_loss(x1, x2)\n",
    "\n",
    "def l2_distance(x1, x2):\n",
    "  return F.mse_loss(x1 * 10, x2 * 10)\n",
    "\n",
    "def mse_loss(x1, x2):\n",
    "  return F.mse_loss(x1, x2)\n",
    "\n",
    "def cos_distance(x1, x2, dim = 1):\n",
    "  return -1 * torch.mean(F.cosine_similarity(x1, x2, dim=dim))\n",
    "\n",
    "triplet_call_cnt = 0\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, distance_func, margin):\n",
    "  # max distance when using l1_distance is 2\n",
    "  # max distacne when using l2-distance is sqrt(2)\n",
    "#   print(\"anchor: \", anchor.min(), anchor.max())\n",
    "  ap_dist = distance_func(anchor, positive)\n",
    "  an_dist = distance_func(anchor, negative)\n",
    "\n",
    "  global triplet_call_cnt\n",
    "  triplet_call_cnt += 1\n",
    "  if triplet_call_cnt % 10 in [0,1] : #and anchor.shape[1] == 1000:\n",
    "    print(\"a: \", end=\"\"); print_big_vector(anchor[0])\n",
    "    print(\"p: \", end=\"\"); print_big_vector(positive[0])\n",
    "    print(\"n: \", end=\"\"); print_big_vector(negative[0])\n",
    "    print(\"func:{}, ap_dist: {}, an_dist: {}\".format(distance_func.__name__, ap_dist, an_dist))\n",
    "    \n",
    "  return torch.mean(F.relu(ap_dist - an_dist + margin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsFgfiN8EV7z"
   },
   "outputs": [],
   "source": [
    "def diversity_loss(input, target):\n",
    "#   return -1 * torch.mean(torch.pow(f_x_a-f_x_s,2))\n",
    "  if input.shape[0] != batch_size:\n",
    "    print(\"input shape: \", input.shape)\n",
    "    print(\"target shape: \", target.shape, \"\\n\\n\")\n",
    "  return torch.mean(F.cosine_similarity(\n",
    "    input.view([batch_size, -1]),\n",
    "    target.view([batch_size, -1]), \n",
    "  ))\n",
    "\n",
    "fool_loss_count = 0\n",
    "\n",
    "def fool_loss(input, target):\n",
    "  true_class = torch.argmax(target, dim=1).view(-1,1).long().cuda()\n",
    "  target_probabilities = input.gather(1, true_class)\n",
    "  epsilon = 1e-10\n",
    "  result =  torch.mean(-1 * torch.log(1 - target_probabilities + epsilon))\n",
    "  \n",
    "  global fool_loss_count\n",
    "  fool_loss_count += 1\n",
    "  if fool_loss_count % 40 == 0:\n",
    "    print(\"target probs {}, loss: {}: \".format(target_probabilities, result))\n",
    "    \n",
    "  return result\n",
    "\n",
    "\n",
    "# def fool_loss(model_output, target_labels):\n",
    "#   target_labels = target_labels.view(-1, 1).long().cuda()\n",
    "#   target_probabilities = model_output.gather(1, target_labels)\n",
    "#   epsilon = 1e-10\n",
    "#   # highest possible fool_loss is - log(1e-10) == 23\n",
    "#   result = torch.mean(-1 * torch.log(target_probabilities + epsilon))\n",
    "  \n",
    "#   global fool_loss_count\n",
    "#   fool_loss_count += 1\n",
    "#   if fool_loss_count % 20 == 0:\n",
    "#     print(\"target probs {}, loss: {}: \".format(target_probabilities, result))\n",
    "  \n",
    "#   return result\n",
    "\n",
    "# def validation(gen_output, target):\n",
    "#   perturbations, _, _, clean_images, _, _ = gen_output\n",
    "#   perturbed_images = clean_images + perturbations\n",
    "#   benign_preds = torch.argmax(arch(clean_images), 1)\n",
    "#   adversary_preds = torch.argmax(arch(perturbed_images), 1)\n",
    "#   return (benign_preds != adversary_preds).float().mean()\n",
    "\n",
    "def validation(gen_output, target):\n",
    "  perturbations, _, _, clean_images = gen_output\n",
    "  perturbed_images = clean_images + perturbations\n",
    "  benign_preds = torch.argmax(arch(clean_images), 1)\n",
    "  adversary_preds = torch.argmax(arch(perturbed_images), 1)\n",
    "  return (benign_preds != adversary_preds).float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FeatureLoss(nn.Module):\n",
    "#     def __name__(self):\n",
    "#       return \"feature_loss\"\n",
    "  \n",
    "#     def __init__(self, dis, layers, layer_weights):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # define generator here \n",
    "#         self.dis = dis\n",
    "#         self.diversity_layers = layers\n",
    "#         self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "#         self.weights = layer_weights\n",
    "#         self.metric_names = [\"fool_loss\"] #+ [f\"div_loss_{i}\" for i in range(len(layers))] #maybe Gram\n",
    "# #         self.triplet_hooks = hook_outputs([arch.m.features[4]], detach=False)\n",
    "    \n",
    "#     def make_features(self, x, clone=False):\n",
    "#         y = self.dis(x)\n",
    "#         return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "  \n",
    "#     def forward(self, inp, target):\n",
    "#       sigma_B, _, _, X_B, B_Y, z = inp\n",
    "\n",
    "#       X_A = X_B + sigma_B\n",
    "# #       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "#       A_Y, A_feat = self.make_features(X_A)\n",
    "# #       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "#       chosen_labels = z.argmax(dim=1)\n",
    "#       fooling_loss =  fool_loss(A_Y, chosen_labels)\n",
    "\n",
    "# #       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "# #       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "# #       self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "# #       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "#       self.losses = [fooling_loss]\n",
    "#       self.metrics = dict(zip(self.metric_names, [fooling_loss]))\n",
    "\n",
    "#       return sum(self.losses)\n",
    "  \n",
    "#     def add_perturbation_shuffled(self, inp, perturbation):\n",
    "# #         j = torch.randperm(inp.shape[0])\n",
    "#         j = derangement(inp.shape[0])\n",
    "#         return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-targeted FeatureLoss\n",
    "class FeatureLoss(nn.Module):\n",
    "    def __name__(self):\n",
    "      return \"feature_loss\"\n",
    "  \n",
    "    def __init__(self, dis, layers, layer_weights):\n",
    "        super().__init__()\n",
    "        \n",
    "        # define generator here \n",
    "        self.dis = dis\n",
    "        self.diversity_layers = layers\n",
    "        self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "        self.weights = layer_weights\n",
    "#         self.metric_names = [\"fool_loss\"] + [f\"div_loss_{i}\" for i in range(len(layers))] + ['triplet_loss']# Maybe Gram\n",
    "        self.metric_names = [\"fool_loss\"] + ['triplet_loss']# Maybe Gram\n",
    "        self.triplet_weight = 4.\n",
    "        self.triplet_weight_noise = 5.\n",
    "        self.triplet_weight_sm = 5.\n",
    "    \n",
    "    def make_features(self, x, clone=False):\n",
    "        y = self.dis(x)\n",
    "        return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "    \n",
    "    def forward(self, inp, target):\n",
    "        sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "        X_A = self.add_perturbation(X_B, sigma_B) \n",
    "        X_A_pos = self.add_perturbation(X_B, sigma_pos)\n",
    "        X_A_neg = self.add_perturbation(X_B, sigma_neg) \n",
    "        \n",
    "        X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "        \n",
    "        B_Y, _ = self.make_features(X_B)\n",
    "        A_Y, A_feat = self.make_features(X_A)\n",
    "#         _, S_feat = self.make_features(X_S)\n",
    "        pos_softmax, _ = self.make_features(X_A_pos)\n",
    "        neg_softmax, _ = self.make_features(X_A_neg)\n",
    "        \n",
    "        fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "      \n",
    "#         raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "#         weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "        raw_triplet_loss = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "        weighted_triplet_loss = raw_triplet_loss * self.triplet_weight\n",
    "    \n",
    "#         self.losses = [fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss]\n",
    "#         self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses + [weighted_triplet_loss]))\n",
    "        self.losses = [fooling_loss] + [weighted_triplet_loss]\n",
    "        self.metrics = dict(zip(self.metric_names, [fooling_loss] + [raw_triplet_loss]))\n",
    "\n",
    "        return sum(self.losses)\n",
    "\n",
    "#     #use two types of triplet losses\n",
    "#     def forward(self, inp, target):\n",
    "#       sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "#       X_A = self.add_perturbation(X_B, sigma_B) \n",
    "#       X_A_pos = self.add_perturbation(X_B, sigma_pos)\n",
    "#       X_A_neg = self.add_perturbation(X_B, sigma_neg) \n",
    "\n",
    "#       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "#       B_Y, _ = self.make_features(X_B)\n",
    "#       A_Y, A_feat = self.make_features(X_A)\n",
    "#       _, S_feat = self.make_features(X_S)\n",
    "#       pos_softmax, _ = self.make_features(X_A_pos)\n",
    "#       neg_softmax, _ = self.make_features(X_A_neg)\n",
    "\n",
    "#       fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "#       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "#       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "      \n",
    "#       raw_triplet_loss_sm = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "#       weighted_triplet_loss_sm = raw_triplet_loss_sm * self.triplet_weight_sm\n",
    "      \n",
    "#       raw_triplet_loss_noise = triplet_loss(sigma_B, sigma_pos, sigma_neg, l2_distance, 5.)\n",
    "#       weighted_triplet_loss_noise = raw_triplet_loss_noise * self.triplet_weight_noise\n",
    "\n",
    "#       self.losses = [fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss_sm, weighted_triplet_loss_noise] \n",
    "#       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses + [weighted_triplet_loss_sm, weighted_triplet_loss_noise]))\n",
    "\n",
    "#       return sum(self.losses)\n",
    "\n",
    "#     # just fooling and diversity\n",
    "#     def forward(self, inp, target):\n",
    "#       sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "#       X_A = self.add_perturbation(X_B, sigma_B) \n",
    "\n",
    "#       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "#       B_Y, _ = self.make_features(X_B)\n",
    "#       A_Y, A_feat = self.make_features(X_A)\n",
    "#       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "#       fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "#       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "#       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "#       self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "#       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "#       return sum(self.losses)\n",
    "  \n",
    "  \n",
    "    def add_perturbation(self, inp, perturbation):\n",
    "        return inp.add(perturbation)\n",
    "  \n",
    "    def add_perturbation_shuffled(self, inp, perturbation):\n",
    "        j = derangement(inp.shape[0])\n",
    "        return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qd9gXUy_ovww"
   },
   "outputs": [],
   "source": [
    "feat_loss = FeatureLoss(arch, layers, layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfZKdYD2MSdi"
   },
   "outputs": [],
   "source": [
    "env.save_filename = 'resnet50_39'\n",
    "# env.save_filename = 'resnet50_17'\n",
    "# env.save_filename = 'vgg16_32'\n",
    "\n",
    "if Path(env.get_csv_path() + '.csv').exists(): raise FileExistsError(\"csv_path already exists\")\n",
    "if Path(env.get_models_path()).exists(): raise FileExistsError(\"models_path already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9J20CBLS8S9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models_directory returned is:  models/27\n"
     ]
    }
   ],
   "source": [
    "learn = None; gen = None; gc.collect()\n",
    "csv_logger = partial(ImmediateCSVLogger, filename= env.temp_csv_path + '/' + env.save_filename)\n",
    "gen = Gen(z_dim=z_dim)\n",
    "init_cnn(gen, True)\n",
    "\n",
    "learn = Learner(data, gen, loss_func = feat_loss, model_dir = env.get_learner_models_dir(), metrics=[validation], callback_fns=[LossMetrics, csv_logger])\n",
    "# learn = Learner(data, Gen(z_dim=10), loss_func = feat_loss, metrics=[validation], callback_fns=LossMetrics, opt_func = optim.SGD)\n",
    "# learn = Learner(data, Gen(z_dim=z_dim), loss_func = feat_loss, metrics=[validation], callback_fns=[LossMetrics, DiversityWeightsScheduler])\n",
    "\n",
    "# load_starting_point(learn, model.__name__, z_dim)\n",
    "# random_seed(42, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wOZYzOHDEdB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GeneralRelu. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type FeatureLoss. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Gen. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type deconv_layer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (9000 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "n02454379,n02454379,n02454379,n02454379,n02454379\n",
       "Path: /root/Derakhshani/adversarial/datasets/dataset;\n",
       "\n",
       "Valid: LabelList (1000 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "n02454379,n02397096,n02090379,n01729977,n02268853\n",
       "Path: /root/Derakhshani/adversarial/datasets/dataset;\n",
       "\n",
       "Test: None, model=Gen(\n",
       "  (z_): Linear(in_features=10, out_features=7168, bias=True)\n",
       "  (BN_): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (CT2d_1): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (BN2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_2): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(320, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_3): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(160, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_4): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(80, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_5): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_6): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_7): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (BN2d): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FeatureLoss(\n",
       "  (dis): SoftmaxWrapper(\n",
       "    (m): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax()\n",
       "  )\n",
       "), metrics=[<function validation at 0x7f34a4128950>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/Derakhshani/adversarial/datasets/dataset'), model_dir='models/27', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), <class 'fastai.callbacks.loss_metrics.LossMetrics'>, functools.partial(<class 'nag_util.ImmediateCSVLogger'>, filename='/root/Derakhshani/adversarial/temp/resnet50_39')], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): Linear(in_features=10, out_features=7168, bias=True)\n",
       "  (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): GeneralRelu()\n",
       "  (5): ConvTranspose2d(320, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): GeneralRelu()\n",
       "  (8): ConvTranspose2d(160, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): GeneralRelu()\n",
       "  (11): ConvTranspose2d(80, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): GeneralRelu()\n",
       "  (14): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (16): GeneralRelu()\n",
       "  (17): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): GeneralRelu()\n",
       "  (20): ConvTranspose2d(72, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (21): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")], add_time=True, silent=None)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !cp \"/content/gdrive/My Drive/DL/models/vgg16_12-last.pth\"  \"/content/\"\n",
    "# learn.load('/content/vgg16_12-last')\n",
    "\n",
    "# load_filename = 'resnet50_startpoint_0'\n",
    "load_filename = 'resnet50_39/resnet50_39_39'\n",
    "# load_filename = 'investigate_resnet50_2/3/resnet50_5'\n",
    "# load_filename = 'vgg16_30/vgg16_30_69'\n",
    "# load_filename = 'vgg16_12-last'\n",
    "\n",
    "learn.load('/root/Derakhshani/adversarial/models/' + load_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the selected settings are : \n",
      "\tmode: normal \n",
      "\tnetw-under-attack: resnet50 \n",
      "\tload filename: resnet50_startpoint_0 \n",
      "\tsave filename: resnet50_39\n",
      "\n",
      "please MAKE SURE that the config is correct.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  load_filename\n",
    "except NameError:\n",
    "  load_filename = None\n",
    "\n",
    "print(\"the selected settings are : \")\n",
    "print(\"\\tmode: {} \\n\\tnetw-under-attack: {} \\n\\tload filename: {} \\n\\tsave filename: {}\\n\".format(\n",
    "  mode, model.__name__, load_filename , env.save_filename\n",
    "))\n",
    "print(\"please MAKE SURE that the config is correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find(1e-6, 100)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from distutils import dir_util \n",
    "\n",
    "# def investigate_initial_settings(n_settings, n_epochs, lr, wd, results_dir):\n",
    "#   os.mkdir(env.get_csv_dir() + results_dir)\n",
    "#   os.mkdir(env.get_models_dir() + results_dir)\n",
    "  \n",
    "#   for setting_ind in range(n_settings):\n",
    "#     learn = None; gen = None; gc.collect()\n",
    "#     gen = Gen(z_dim = z_dim)\n",
    "#     init_cnn(gen, True)\n",
    "    \n",
    "#     tmp_csv_filename =  env.temp_csv_path + '/' + results_dir + '/' + str(setting_ind)\n",
    "#     csv_logger = partial(ImmediateCSVLogger, filename=tmp_csv_filename)\n",
    "    \n",
    "#     learn = Learner(data, gen, loss_func = feat_loss, metrics=[validation], model_dir = env.get_learner_models_dir(), callback_fns=[LossMetrics, csv_logger])\n",
    "#     saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=model.__name__ + \"-best\")\n",
    "#     saver_every_epoch = SaveModelCallback(learn, every='epoch', name=model.__name__)\n",
    "\n",
    "#     learn.fit(n_epochs, lr=lr, wd = wd, callbacks=[saver_best, saver_every_epoch])\n",
    "    \n",
    "#     shutil.copyfile(tmp_csv_filename + \".csv\", env.get_csv_dir() + results_dir + '/' + str(setting_ind) + '.csv')\n",
    "    \n",
    "#     model_dest = env.get_models_dir() + results_dir + '/' + str(setting_ind)\n",
    "#     os.mkdir(model_dest)\n",
    "#     dir_util.copy_tree(env.data_path/env.get_learner_models_dir(), model_dest)\n",
    "#     shutil.rmtree(env.data_path/env.get_learner_models_dir())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dir = 'investigate_resnet50_x2'\n",
    "# investigate_initial_settings(10, 6, lr = 1e-2, wd = 0.001, results_dir = results_dir)\n",
    "# # shutil.rmtree(env.get_models_dir() + results_dir)\n",
    "# # shutil.rmtree(env.get_csv_dir() + results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='40', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      2.50% [1/40 05:58<3:53:20]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>triplet_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.725708</td>\n",
       "      <td>6.692989</td>\n",
       "      <td>0.461000</td>\n",
       "      <td>1.447623</td>\n",
       "      <td>1.311342</td>\n",
       "      <td>05:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='46' class='' max='562', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      8.19% [46/562 00:28<05:14 6.6238]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [26: 0.88, 115: 0.01, 327: 0.04, ]\n",
      "p: [26: 0.84, 115: 0.02, 327: 0.06, ]\n",
      "n: [26: 0.87, 115: 0.01, 327: 0.04, ]\n",
      "func:cos_distance, ap_dist: -0.999826967716217, an_dist: -0.9998937845230103\n",
      "a: [467: 0.02, 499: 0.64, 579: 0.03, 623: 0.01, 777: 0.03, 811: 0.01, 813: 0.02, 882: 0.01, 891: 0.02, ]\n",
      "p: [467: 0.02, 499: 0.65, 579: 0.04, 623: 0.01, 777: 0.02, 813: 0.02, 891: 0.02, ]\n",
      "n: [467: 0.02, 499: 0.59, 579: 0.05, 623: 0.01, 766: 0.01, 777: 0.03, 811: 0.01, 813: 0.03, 882: 0.01, 891: 0.02, ]\n",
      "func:cos_distance, ap_dist: -0.9998683929443359, an_dist: -0.9994585514068604\n",
      "a: [636: 0.79, 748: 0.20, ]\n",
      "p: [636: 0.80, 748: 0.20, ]\n",
      "n: [636: 0.79, 748: 0.21, ]\n",
      "func:cos_distance, ap_dist: -0.9992380738258362, an_dist: -0.9995244145393372\n",
      "a: [49: 0.02, 127: 0.04, 128: 0.08, 131: 0.25, 132: 0.21, 134: 0.33, 135: 0.02, 141: 0.01, ]\n",
      "p: [49: 0.02, 127: 0.04, 128: 0.08, 131: 0.26, 132: 0.21, 134: 0.32, 135: 0.02, 141: 0.01, ]\n",
      "n: [49: 0.02, 127: 0.04, 128: 0.09, 131: 0.25, 132: 0.21, 134: 0.32, 135: 0.02, 141: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9996960759162903, an_dist: -0.9997285008430481\n",
      "a: [34: 1.00, ]\n",
      "p: [34: 1.00, ]\n",
      "n: [34: 1.00, ]\n",
      "func:cos_distance, ap_dist: -0.9996093511581421, an_dist: -0.9996875524520874\n",
      "a: [157: 0.02, 160: 0.04, 169: 0.02, 170: 0.01, 173: 0.02, 176: 0.05, 181: 0.05, 183: 0.03, 188: 0.02, 190: 0.02, 194: 0.05, 199: 0.02, 203: 0.04, 216: 0.02, 218: 0.02, 228: 0.01, 229: 0.01, 230: 0.01, 253: 0.01, 258: 0.02, 265: 0.03, 266: 0.02, 267: 0.04, 574: 0.03, 614: 0.01, ]\n",
      "p: [157: 0.02, 160: 0.04, 169: 0.02, 170: 0.01, 173: 0.02, 176: 0.05, 181: 0.05, 183: 0.03, 188: 0.02, 190: 0.02, 194: 0.05, 199: 0.02, 203: 0.04, 216: 0.02, 218: 0.02, 228: 0.01, 229: 0.01, 230: 0.01, 253: 0.01, 258: 0.02, 265: 0.03, 266: 0.01, 267: 0.04, 574: 0.03, 614: 0.01, 805: 0.01, ]\n",
      "n: [157: 0.02, 160: 0.04, 169: 0.02, 170: 0.01, 173: 0.02, 176: 0.05, 181: 0.05, 183: 0.03, 188: 0.02, 190: 0.02, 194: 0.05, 199: 0.02, 203: 0.04, 216: 0.02, 218: 0.02, 228: 0.01, 229: 0.01, 230: 0.01, 253: 0.01, 258: 0.02, 265: 0.03, 266: 0.02, 267: 0.04, 574: 0.03, 614: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9997607469558716, an_dist: -0.9996864795684814\n",
      "a: [401: 0.01, 545: 0.03, 556: 0.02, 563: 0.04, 579: 0.06, 581: 0.01, 592: 0.04, 622: 0.05, 709: 0.04, 748: 0.02, 749: 0.02, 752: 0.03, 811: 0.03, 815: 0.01, 836: 0.01, 837: 0.02, 878: 0.18, 882: 0.03, 885: 0.04, ]\n",
      "p: [401: 0.01, 545: 0.03, 556: 0.01, 563: 0.04, 579: 0.07, 581: 0.01, 592: 0.03, 622: 0.04, 709: 0.04, 748: 0.02, 749: 0.02, 752: 0.03, 811: 0.02, 815: 0.01, 836: 0.01, 837: 0.02, 878: 0.20, 882: 0.03, 885: 0.05, ]\n",
      "n: [401: 0.01, 545: 0.03, 556: 0.01, 563: 0.04, 579: 0.07, 581: 0.01, 592: 0.04, 622: 0.05, 709: 0.03, 748: 0.02, 749: 0.02, 752: 0.03, 811: 0.03, 815: 0.01, 836: 0.01, 837: 0.02, 878: 0.18, 882: 0.03, 885: 0.04, ]\n",
      "func:cos_distance, ap_dist: -0.9996927976608276, an_dist: -0.9998605251312256\n",
      "target probs tensor([[0.97],\n",
      "        [0.63],\n",
      "        [0.12],\n",
      "        [0.70],\n",
      "        [0.74],\n",
      "        [0.87],\n",
      "        [1.00],\n",
      "        [1.00],\n",
      "        [1.00],\n",
      "        [0.26],\n",
      "        [0.84],\n",
      "        [1.00],\n",
      "        [0.98],\n",
      "        [0.99],\n",
      "        [0.30],\n",
      "        [0.89]], device='cuda:1', grad_fn=<GatherBackward>), loss: 2.9923019409179688: \n",
      "a: [158: 0.01, 237: 0.97, ]\n",
      "p: [158: 0.02, 237: 0.97, ]\n",
      "n: [158: 0.02, 237: 0.97, ]\n",
      "func:cos_distance, ap_dist: -0.9998972415924072, an_dist: -0.9997968673706055\n",
      "a: [61: 0.16, 62: 0.03, 63: 0.07, 401: 0.01, 542: 0.01, 556: 0.02, 577: 0.01, 641: 0.02, 669: 0.05, 700: 0.03, 828: 0.04, 850: 0.03, 904: 0.01, 955: 0.01, 987: 0.01, ]\n",
      "p: [61: 0.17, 62: 0.03, 63: 0.06, 401: 0.01, 542: 0.01, 556: 0.02, 577: 0.01, 641: 0.02, 669: 0.04, 700: 0.02, 828: 0.03, 850: 0.04, 904: 0.01, 955: 0.02, ]\n",
      "n: [61: 0.20, 62: 0.04, 63: 0.06, 401: 0.01, 542: 0.01, 556: 0.02, 641: 0.01, 669: 0.05, 700: 0.02, 828: 0.04, 850: 0.03, 904: 0.01, 955: 0.02, ]\n",
      "func:cos_distance, ap_dist: -0.999472439289093, an_dist: -0.9988924264907837\n",
      "a: [959: 1.00, ]\n",
      "p: [959: 1.00, ]\n",
      "n: [959: 1.00, ]\n",
      "func:cos_distance, ap_dist: -0.9998102188110352, an_dist: -0.9996321201324463\n",
      "a: [260: 1.00, ]\n",
      "p: [260: 1.00, ]\n",
      "n: [260: 1.00, ]\n",
      "func:cos_distance, ap_dist: -0.9997740983963013, an_dist: -0.9980311393737793\n",
      "a: [0: 0.02, 48: 0.01, 61: 0.05, 62: 0.01, 67: 0.12, 161: 0.03, 168: 0.04, 171: 0.01, 180: 0.02, 225: 0.02, 227: 0.01, 237: 0.02, 263: 0.05, 264: 0.01, 363: 0.03, 522: 0.03, 792: 0.01, 805: 0.03, 955: 0.03, 997: 0.06, ]\n",
      "p: [0: 0.02, 48: 0.01, 61: 0.06, 62: 0.01, 67: 0.13, 161: 0.02, 168: 0.03, 171: 0.01, 180: 0.02, 225: 0.02, 227: 0.01, 237: 0.01, 263: 0.05, 363: 0.03, 522: 0.03, 792: 0.01, 805: 0.03, 955: 0.03, 997: 0.06, ]\n",
      "n: [48: 0.01, 61: 0.02, 67: 0.08, 161: 0.02, 168: 0.03, 171: 0.01, 179: 0.01, 180: 0.02, 209: 0.01, 225: 0.01, 227: 0.01, 237: 0.01, 242: 0.01, 263: 0.04, 330: 0.01, 522: 0.02, 792: 0.01, 805: 0.03, 955: 0.02, 997: 0.20, ]\n",
      "func:cos_distance, ap_dist: -0.9995114803314209, an_dist: -0.9833261966705322\n",
      "a: [301: 0.99, ]\n",
      "p: [301: 0.99, ]\n",
      "n: [301: 0.99, ]\n",
      "func:cos_distance, ap_dist: -0.9993800520896912, an_dist: -0.9969308972358704\n",
      "a: [836: 0.57, 837: 0.42, ]\n",
      "p: [836: 0.57, 837: 0.41, ]\n",
      "n: [836: 0.57, 837: 0.41, ]\n",
      "func:cos_distance, ap_dist: -0.999636173248291, an_dist: -0.998322606086731\n",
      "a: [652: 0.99, ]\n",
      "p: [652: 0.99, ]\n",
      "n: [652: 0.99, ]\n",
      "func:cos_distance, ap_dist: -0.9997414350509644, an_dist: -0.9982386827468872\n",
      "target probs tensor([[0.86],\n",
      "        [0.65],\n",
      "        [0.26],\n",
      "        [0.69],\n",
      "        [0.95],\n",
      "        [1.00],\n",
      "        [0.90],\n",
      "        [0.99],\n",
      "        [0.78],\n",
      "        [0.98],\n",
      "        [0.33],\n",
      "        [0.61],\n",
      "        [0.94],\n",
      "        [0.89],\n",
      "        [0.77],\n",
      "        [0.99]], device='cuda:1', grad_fn=<GatherBackward>), loss: 2.76273250579834: \n",
      "a: [425: 0.02, 497: 0.86, 716: 0.02, 904: 0.01, ]\n",
      "p: [425: 0.02, 497: 0.87, 716: 0.02, 904: 0.01, ]\n",
      "n: [425: 0.02, 497: 0.87, 716: 0.02, 904: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9999688863754272, an_dist: -0.9998881816864014\n",
      "a: [478: 0.01, 492: 0.84, 519: 0.03, 611: 0.05, 741: 0.01, 921: 0.01, ]\n",
      "p: [478: 0.01, 492: 0.83, 519: 0.03, 611: 0.05, 741: 0.01, 921: 0.02, ]\n",
      "n: [478: 0.01, 492: 0.84, 519: 0.03, 611: 0.04, 741: 0.01, 921: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9998266100883484, an_dist: -0.9992145299911499\n",
      "a: [73: 0.85, 74: 0.05, 77: 0.07, 78: 0.01, ]\n",
      "p: [73: 0.85, 74: 0.05, 77: 0.07, 78: 0.01, ]\n",
      "n: [73: 0.85, 74: 0.05, 77: 0.07, 78: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9996341466903687, an_dist: -0.9975547194480896\n",
      "a: [411: 0.09, 431: 0.04, 434: 0.01, 443: 0.21, 516: 0.01, 520: 0.07, 529: 0.04, 549: 0.04, 591: 0.34, 709: 0.01, 721: 0.05, 748: 0.01, ]\n",
      "p: [411: 0.09, 431: 0.04, 434: 0.01, 443: 0.21, 516: 0.01, 520: 0.08, 529: 0.04, 549: 0.04, 591: 0.33, 709: 0.01, 721: 0.05, 748: 0.01, ]\n",
      "n: [411: 0.09, 431: 0.04, 443: 0.21, 516: 0.01, 520: 0.07, 529: 0.04, 549: 0.05, 591: 0.33, 709: 0.01, 721: 0.05, 748: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9998270273208618, an_dist: -0.9987143278121948\n",
      "a: [415: 0.03, 509: 0.08, 532: 0.03, 582: 0.31, 762: 0.02, 765: 0.01, 790: 0.01, 930: 0.01, 953: 0.05, 954: 0.01, 955: 0.03, 956: 0.01, 987: 0.04, 998: 0.05, ]\n",
      "p: [118: 0.01, 415: 0.03, 509: 0.08, 532: 0.04, 582: 0.29, 762: 0.02, 765: 0.02, 790: 0.01, 857: 0.01, 930: 0.01, 953: 0.05, 954: 0.01, 955: 0.03, 956: 0.01, 987: 0.03, 998: 0.05, ]\n",
      "n: [118: 0.01, 415: 0.03, 509: 0.08, 532: 0.04, 582: 0.32, 762: 0.03, 765: 0.01, 790: 0.01, 930: 0.01, 953: 0.05, 954: 0.01, 955: 0.02, 956: 0.01, 987: 0.03, 998: 0.04, ]\n",
      "func:cos_distance, ap_dist: -0.9996241927146912, an_dist: -0.9986612200737\n",
      "a: [489: 0.48, 556: 0.01, 839: 0.05, 904: 0.38, ]\n",
      "p: [489: 0.51, 556: 0.01, 839: 0.05, 904: 0.35, ]\n",
      "n: [489: 0.49, 556: 0.01, 752: 0.01, 839: 0.04, 904: 0.38, ]\n",
      "func:cos_distance, ap_dist: -0.9995891451835632, an_dist: -0.9991701245307922\n",
      "a: [398: 0.95, 987: 0.01, ]\n",
      "p: [398: 0.95, 987: 0.01, ]\n",
      "n: [398: 0.95, 987: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9995437860488892, an_dist: -0.9943479895591736\n",
      "a: [640: 1.00, ]\n",
      "p: [640: 1.00, ]\n",
      "n: [640: 1.00, ]\n",
      "func:cos_distance, ap_dist: -0.9990824460983276, an_dist: -0.9903370141983032\n",
      "target probs tensor([[0.07],\n",
      "        [0.00],\n",
      "        [0.18],\n",
      "        [0.03],\n",
      "        [0.92],\n",
      "        [0.22],\n",
      "        [0.25],\n",
      "        [0.16],\n",
      "        [0.83],\n",
      "        [0.12],\n",
      "        [0.23],\n",
      "        [0.98],\n",
      "        [0.46],\n",
      "        [0.01],\n",
      "        [0.78],\n",
      "        [1.00]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.1601672172546387: \n",
      "a: [418: 0.12, 473: 0.01, 499: 0.07, 549: 0.04, 563: 0.04, 623: 0.06, 629: 0.01, 637: 0.01, 695: 0.02, 709: 0.01, 767: 0.03, 777: 0.06, 798: 0.03, 813: 0.03, 902: 0.04, ]\n",
      "p: [418: 0.12, 473: 0.01, 499: 0.08, 549: 0.05, 563: 0.04, 623: 0.06, 637: 0.01, 695: 0.02, 709: 0.01, 767: 0.03, 777: 0.06, 798: 0.03, 813: 0.03, 902: 0.04, ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: [418: 0.16, 499: 0.07, 549: 0.04, 563: 0.05, 623: 0.07, 629: 0.01, 695: 0.02, 709: 0.01, 767: 0.04, 777: 0.05, 798: 0.03, 813: 0.03, 902: 0.03, ]\n",
      "func:cos_distance, ap_dist: -0.9995986223220825, an_dist: -0.9962009191513062\n",
      "a: [258: 1.00, ]\n",
      "p: [258: 1.00, ]\n",
      "n: [258: 1.00, ]\n",
      "func:cos_distance, ap_dist: -0.9994862079620361, an_dist: -0.9911233186721802\n",
      "a: [773: 1.00, ]\n",
      "p: [773: 1.00, ]\n",
      "n: [773: 1.00, ]\n",
      "func:cos_distance, ap_dist: -0.9997302293777466, an_dist: -0.998325526714325\n",
      "a: [271: 0.04, 272: 0.73, 274: 0.02, 280: 0.16, ]\n",
      "p: [271: 0.04, 272: 0.73, 274: 0.02, 280: 0.15, ]\n",
      "n: [271: 0.05, 272: 0.63, 274: 0.03, 278: 0.01, 280: 0.23, ]\n",
      "func:cos_distance, ap_dist: -0.999823808670044, an_dist: -0.9952276349067688\n",
      "a: [401: 0.11, 402: 0.08, 420: 0.05, 432: 0.10, 486: 0.07, 489: 0.01, 546: 0.11, 593: 0.01, 650: 0.01, 776: 0.05, 819: 0.16, 854: 0.02, 862: 0.04, 889: 0.03, ]\n",
      "p: [401: 0.11, 402: 0.08, 420: 0.05, 432: 0.10, 486: 0.07, 489: 0.01, 546: 0.10, 593: 0.01, 650: 0.01, 776: 0.05, 819: 0.15, 854: 0.02, 862: 0.04, 889: 0.03, ]\n",
      "n: [401: 0.11, 402: 0.08, 420: 0.05, 432: 0.10, 486: 0.07, 546: 0.11, 593: 0.01, 776: 0.05, 819: 0.16, 854: 0.02, 862: 0.04, 889: 0.03, ]\n",
      "func:cos_distance, ap_dist: -0.999819278717041, an_dist: -0.9977684020996094\n",
      "a: [0: 0.01, 1: 0.01, 3: 0.03, 4: 0.33, 5: 0.01, 29: 0.02, 40: 0.04, 46: 0.05, 149: 0.03, 389: 0.07, 391: 0.12, 394: 0.03, 749: 0.02, 904: 0.04, ]\n",
      "p: [0: 0.01, 1: 0.01, 3: 0.03, 4: 0.32, 5: 0.01, 29: 0.02, 40: 0.04, 46: 0.05, 149: 0.03, 389: 0.07, 391: 0.12, 394: 0.03, 749: 0.03, 904: 0.04, ]\n",
      "n: [0: 0.01, 1: 0.01, 3: 0.03, 4: 0.32, 5: 0.01, 29: 0.02, 40: 0.04, 46: 0.05, 149: 0.03, 389: 0.07, 391: 0.11, 394: 0.03, 749: 0.02, 904: 0.04, ]\n",
      "func:cos_distance, ap_dist: -0.9997671842575073, an_dist: -0.9988359212875366\n",
      "a: [783: 1.00, ]\n",
      "p: [783: 1.00, ]\n",
      "n: [783: 1.00, ]\n",
      "func:cos_distance, ap_dist: -0.9998464584350586, an_dist: -0.9983237981796265\n",
      "a: [409: 0.69, 426: 0.01, 635: 0.02, 826: 0.04, 835: 0.02, 892: 0.10, ]\n",
      "p: [409: 0.69, 426: 0.01, 635: 0.02, 826: 0.04, 835: 0.01, 892: 0.10, ]\n",
      "n: [409: 0.69, 426: 0.01, 635: 0.01, 826: 0.04, 835: 0.01, 892: 0.10, ]\n",
      "func:cos_distance, ap_dist: -0.9996342658996582, an_dist: -0.9977587461471558\n",
      "target probs tensor([[0.25],\n",
      "        [0.76],\n",
      "        [0.21],\n",
      "        [0.00],\n",
      "        [1.00],\n",
      "        [0.40],\n",
      "        [0.90],\n",
      "        [0.66],\n",
      "        [0.31],\n",
      "        [0.49],\n",
      "        [0.68],\n",
      "        [1.00],\n",
      "        [0.30],\n",
      "        [0.00],\n",
      "        [0.47],\n",
      "        [0.95]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.6751389503479004: \n",
      "a: [326: 0.25, 904: 0.65, ]\n",
      "p: [326: 0.27, 904: 0.64, ]\n",
      "n: [326: 0.22, 904: 0.69, ]\n",
      "func:cos_distance, ap_dist: -0.9991326332092285, an_dist: -0.9571009874343872\n",
      "a: [584: 1.00, ]\n",
      "p: [584: 1.00, ]\n",
      "n: [584: 1.00, ]\n",
      "func:cos_distance, ap_dist: -0.9988774657249451, an_dist: -0.9681376814842224\n",
      "a: [5: 0.02, 65: 0.04, 109: 0.05, 149: 0.06, 392: 0.03, 489: 0.01, 591: 0.05, 741: 0.02, 842: 0.02, 885: 0.12, 904: 0.03, 911: 0.03, 973: 0.06, ]\n",
      "p: [5: 0.02, 65: 0.03, 109: 0.05, 149: 0.05, 392: 0.03, 443: 0.01, 489: 0.01, 591: 0.05, 741: 0.02, 842: 0.02, 885: 0.12, 904: 0.03, 911: 0.03, 973: 0.06, ]\n",
      "n: [109: 0.01, 149: 0.01, 392: 0.01, 443: 0.02, 591: 0.09, 636: 0.01, 741: 0.04, 842: 0.01, 885: 0.28, 904: 0.01, 911: 0.07, 973: 0.02, ]\n",
      "func:cos_distance, ap_dist: -0.9996802806854248, an_dist: -0.9777688980102539\n",
      "a: [222: 0.54, 257: 0.44, ]\n",
      "p: [222: 0.55, 257: 0.44, ]\n",
      "n: [222: 0.62, 257: 0.37, ]\n",
      "func:cos_distance, ap_dist: -0.9995278120040894, an_dist: -0.9921368360519409\n",
      "a: [319: 0.09, 320: 0.21, 489: 0.01, 904: 0.63, ]\n",
      "p: [319: 0.08, 320: 0.20, 489: 0.01, 904: 0.65, ]\n",
      "n: [319: 0.11, 320: 0.21, 489: 0.02, 904: 0.60, ]\n",
      "func:cos_distance, ap_dist: -0.9993093013763428, an_dist: -0.9658844470977783\n",
      "a: [372: 1.00, ]\n",
      "p: [372: 1.00, ]\n",
      "n: [372: 1.00, ]\n",
      "func:cos_distance, ap_dist: -0.9995105266571045, an_dist: -0.9926155805587769\n",
      "a: [482: 0.04, 518: 0.02, 545: 0.02, 556: 0.01, 570: 0.03, 581: 0.02, 584: 0.03, 616: 0.04, 632: 0.07, 722: 0.01, 746: 0.01, 752: 0.02, 811: 0.01, 817: 0.02, 836: 0.01, 902: 0.02, ]\n",
      "p: [447: 0.01, 464: 0.01, 482: 0.05, 518: 0.02, 545: 0.02, 556: 0.01, 570: 0.03, 581: 0.02, 584: 0.03, 616: 0.04, 632: 0.07, 746: 0.01, 752: 0.01, 811: 0.01, 817: 0.01, 836: 0.02, 848: 0.01, 902: 0.02, ]\n",
      "n: [439: 0.01, 482: 0.04, 518: 0.03, 545: 0.05, 556: 0.02, 570: 0.02, 581: 0.03, 584: 0.03, 612: 0.01, 616: 0.04, 632: 0.08, 722: 0.02, 746: 0.02, 752: 0.02, 811: 0.02, 817: 0.02, 836: 0.01, 902: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9998677968978882, an_dist: -0.9904962778091431\n",
      "a: [409: 0.55, 426: 0.02, 635: 0.16, 892: 0.22, ]\n",
      "p: [409: 0.59, 426: 0.02, 635: 0.16, 892: 0.20, ]\n",
      "n: [409: 0.48, 426: 0.02, 635: 0.24, 746: 0.01, 892: 0.20, ]\n",
      "func:cos_distance, ap_dist: -0.99973064661026, an_dist: -0.9956851601600647\n",
      "target probs tensor([[0.55],\n",
      "        [0.97],\n",
      "        [0.60],\n",
      "        [0.49],\n",
      "        [1.00],\n",
      "        [0.13],\n",
      "        [0.55],\n",
      "        [0.41],\n",
      "        [0.59],\n",
      "        [0.64],\n",
      "        [0.61],\n",
      "        [0.12],\n",
      "        [0.93],\n",
      "        [0.44],\n",
      "        [1.00],\n",
      "        [0.17]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.7264626026153564: \n",
      "a: [120: 0.39, 124: 0.55, 125: 0.01, 363: 0.04, ]\n",
      "p: [120: 0.39, 124: 0.54, 125: 0.01, 363: 0.04, ]\n",
      "n: [120: 0.42, 124: 0.52, 125: 0.01, 363: 0.04, ]\n",
      "func:cos_distance, ap_dist: -0.9996262788772583, an_dist: -0.998013973236084\n",
      "a: [368: 0.07, 369: 0.73, 381: 0.02, 557: 0.03, 733: 0.01, 904: 0.01, 920: 0.01, ]\n",
      "p: [368: 0.06, 369: 0.73, 381: 0.02, 557: 0.03, 733: 0.01, 904: 0.02, 920: 0.01, ]\n",
      "n: [368: 0.07, 369: 0.79, 381: 0.02, 557: 0.02, ]\n",
      "func:cos_distance, ap_dist: -0.999963104724884, an_dist: -0.9996753334999084\n",
      "a: [464: 0.04, 514: 0.02, 518: 0.03, 584: 0.04, 684: 0.03, 709: 0.09, 714: 0.05, 746: 0.12, 748: 0.08, 752: 0.02, 768: 0.04, 770: 0.05, 787: 0.01, 805: 0.11, 836: 0.01, ]\n",
      "p: [464: 0.04, 514: 0.02, 518: 0.03, 584: 0.04, 684: 0.03, 709: 0.08, 714: 0.06, 746: 0.12, 748: 0.08, 752: 0.02, 768: 0.04, 770: 0.05, 787: 0.01, 805: 0.11, 836: 0.02, ]\n",
      "n: [464: 0.04, 514: 0.02, 518: 0.03, 584: 0.04, 684: 0.04, 709: 0.08, 714: 0.06, 746: 0.12, 748: 0.07, 752: 0.02, 768: 0.04, 770: 0.05, 787: 0.01, 805: 0.12, 836: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9999438524246216, an_dist: -0.9938369393348694\n",
      "a: [825: 0.95, ]\n",
      "p: [825: 0.95, ]\n",
      "n: [825: 0.95, ]\n",
      "func:cos_distance, ap_dist: -0.9998874068260193, an_dist: -0.999078631401062\n",
      "a: [987: 0.79, 998: 0.15, ]\n",
      "p: [987: 0.79, 998: 0.15, ]\n",
      "n: [987: 0.79, 998: 0.15, ]\n",
      "func:cos_distance, ap_dist: -0.9996311664581299, an_dist: -0.9985513687133789\n",
      "a: [450: 0.01, 802: 0.97, ]\n",
      "p: [450: 0.01, 802: 0.97, ]\n",
      "n: [450: 0.01, 802: 0.97, ]\n",
      "func:cos_distance, ap_dist: -0.9999300837516785, an_dist: -0.9986715316772461\n",
      "a: [411: 0.01, 423: 0.01, 424: 0.03, 532: 0.02, 617: 0.11, 706: 0.01, 743: 0.13, 762: 0.04, 807: 0.10, 822: 0.11, 834: 0.03, 887: 0.02, 896: 0.01, 906: 0.06, ]\n",
      "p: [411: 0.01, 424: 0.03, 532: 0.02, 617: 0.10, 706: 0.01, 743: 0.13, 762: 0.04, 807: 0.10, 822: 0.11, 834: 0.03, 887: 0.02, 896: 0.01, 906: 0.06, ]\n",
      "n: [411: 0.01, 423: 0.01, 424: 0.03, 532: 0.02, 617: 0.11, 706: 0.01, 743: 0.13, 762: 0.04, 807: 0.10, 822: 0.11, 834: 0.03, 887: 0.03, 896: 0.01, 906: 0.06, ]\n",
      "func:cos_distance, ap_dist: -0.9999538660049438, an_dist: -0.9976915121078491\n",
      "a: [275: 0.01, 276: 0.96, ]\n",
      "p: [275: 0.01, 276: 0.96, ]\n",
      "n: [275: 0.01, 276: 0.96, ]\n",
      "func:cos_distance, ap_dist: -0.9999285936355591, an_dist: -0.9973026514053345\n",
      "target probs tensor([[    0.99],\n",
      "        [    0.97],\n",
      "        [    0.50],\n",
      "        [    0.75],\n",
      "        [    0.96],\n",
      "        [    0.84],\n",
      "        [    0.99],\n",
      "        [    0.75],\n",
      "        [    0.00],\n",
      "        [    0.52],\n",
      "        [    0.17],\n",
      "        [    0.48],\n",
      "        [    1.00],\n",
      "        [    0.92],\n",
      "        [    0.00],\n",
      "        [    0.69]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.9867568016052246: \n",
      "a: [606: 0.99, ]\n",
      "p: [606: 0.99, ]\n",
      "n: [606: 0.99, ]\n",
      "func:cos_distance, ap_dist: -0.9999058246612549, an_dist: -0.9984068870544434\n",
      "a: [473: 0.01, 482: 0.04, 527: 0.01, 549: 0.01, 559: 0.02, 586: 0.01, 742: 0.01, 754: 0.03, 811: 0.49, 904: 0.01, 905: 0.04, ]\n",
      "p: [409: 0.02, 473: 0.01, 482: 0.04, 526: 0.01, 527: 0.01, 549: 0.02, 559: 0.02, 561: 0.01, 586: 0.02, 742: 0.01, 754: 0.03, 811: 0.31, 892: 0.01, 904: 0.02, 905: 0.05, ]\n",
      "n: [482: 0.02, 742: 0.01, 754: 0.01, 811: 0.76, 905: 0.02, ]\n",
      "func:cos_distance, ap_dist: -0.9989414215087891, an_dist: -0.9914565086364746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [850: 0.95, 865: 0.03, ]\n",
      "p: [850: 0.95, 865: 0.03, ]\n",
      "n: [850: 0.95, 865: 0.03, ]\n",
      "func:cos_distance, ap_dist: -0.9996010065078735, an_dist: -0.9926738739013672\n",
      "a: [409: 0.01, 419: 0.10, 481: 0.09, 593: 0.01, 626: 0.02, 674: 0.01, 686: 0.01, 692: 0.14, 709: 0.04, 710: 0.22, 711: 0.01, 767: 0.06, 893: 0.05, ]\n",
      "p: [409: 0.01, 419: 0.10, 481: 0.09, 593: 0.01, 626: 0.02, 674: 0.01, 692: 0.14, 709: 0.04, 710: 0.23, 767: 0.05, 893: 0.06, ]\n",
      "n: [409: 0.01, 419: 0.11, 481: 0.08, 593: 0.02, 626: 0.02, 674: 0.01, 686: 0.01, 692: 0.15, 709: 0.04, 710: 0.19, 767: 0.05, 893: 0.05, 902: 0.01, 916: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9998493194580078, an_dist: -0.9977509379386902\n",
      "a: [489: 0.02, 522: 0.02, 582: 0.01, 723: 0.01, 904: 0.09, 948: 0.06, 950: 0.31, 951: 0.09, 954: 0.04, 955: 0.01, 957: 0.11, 990: 0.01, 998: 0.01, ]\n",
      "p: [489: 0.01, 522: 0.02, 582: 0.01, 723: 0.01, 904: 0.09, 948: 0.05, 950: 0.30, 951: 0.09, 954: 0.04, 955: 0.01, 957: 0.12, 990: 0.02, 998: 0.01, ]\n",
      "n: [489: 0.02, 522: 0.02, 582: 0.01, 723: 0.01, 904: 0.09, 948: 0.06, 950: 0.31, 951: 0.09, 954: 0.04, 955: 0.01, 957: 0.11, 990: 0.01, 998: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9996877908706665, an_dist: -0.9901633262634277\n",
      "a: [762: 0.02, 891: 0.04, 923: 0.45, 959: 0.19, 987: 0.17, 998: 0.06, ]\n",
      "p: [762: 0.02, 891: 0.04, 923: 0.47, 959: 0.19, 987: 0.15, 998: 0.05, ]\n",
      "n: [762: 0.02, 891: 0.06, 923: 0.53, 925: 0.01, 959: 0.20, 987: 0.08, 998: 0.03, ]\n",
      "func:cos_distance, ap_dist: -0.9996921420097351, an_dist: -0.9896723628044128\n",
      "a: [525: 0.99, ]\n",
      "p: [525: 0.99, ]\n",
      "n: [525: 0.99, ]\n",
      "func:cos_distance, ap_dist: -0.999607503414154, an_dist: -0.9937485456466675\n",
      "a: [88: 0.99, ]\n",
      "p: [88: 0.99, ]\n",
      "n: [88: 0.95, 93: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9990102052688599, an_dist: -0.9943618774414062\n",
      "target probs tensor([[    0.00],\n",
      "        [    0.00],\n",
      "        [    0.16],\n",
      "        [    0.72],\n",
      "        [    0.70],\n",
      "        [    0.39],\n",
      "        [    0.73],\n",
      "        [    0.01],\n",
      "        [    0.72],\n",
      "        [    0.02],\n",
      "        [    0.58],\n",
      "        [    0.81],\n",
      "        [    0.00],\n",
      "        [    0.97],\n",
      "        [    0.65],\n",
      "        [    0.99]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.1020166873931885: \n",
      "a: [412: 0.01, 503: 0.11, 556: 0.09, 588: 0.64, 631: 0.01, 747: 0.03, 811: 0.02, ]\n",
      "p: [412: 0.01, 503: 0.11, 556: 0.09, 588: 0.64, 631: 0.01, 747: 0.03, 811: 0.02, ]\n",
      "n: [412: 0.02, 503: 0.10, 556: 0.07, 588: 0.69, 631: 0.01, 747: 0.02, 811: 0.02, ]\n",
      "func:cos_distance, ap_dist: -0.999801516532898, an_dist: -0.9905029535293579\n",
      "a: [156: 0.01, 212: 0.01, 215: 0.01, 216: 0.01, 217: 0.59, 218: 0.02, 219: 0.02, 220: 0.14, 377: 0.02, ]\n",
      "p: [156: 0.01, 212: 0.01, 215: 0.01, 216: 0.01, 217: 0.59, 218: 0.02, 219: 0.02, 220: 0.14, 377: 0.02, ]\n",
      "n: [156: 0.01, 215: 0.01, 216: 0.01, 217: 0.54, 218: 0.02, 219: 0.02, 220: 0.17, 377: 0.03, 904: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9997684955596924, an_dist: -0.9677548408508301\n",
      "a: [81: 0.98, ]\n",
      "p: [81: 0.98, ]\n",
      "n: [81: 0.98, ]\n",
      "func:cos_distance, ap_dist: -0.9999387264251709, an_dist: -0.9919980764389038\n",
      "a: [151: 0.02, 163: 0.03, 180: 0.02, 186: 0.01, 190: 0.01, 192: 0.01, 194: 0.04, 204: 0.02, 229: 0.01, 243: 0.03, 245: 0.03, 246: 0.06, 254: 0.02, 283: 0.02, 284: 0.05, 332: 0.03, 341: 0.07, 358: 0.01, 359: 0.09, 424: 0.01, 743: 0.01, 842: 0.01, 906: 0.02, ]\n",
      "p: [151: 0.02, 163: 0.03, 180: 0.03, 186: 0.01, 190: 0.02, 192: 0.01, 194: 0.04, 204: 0.02, 229: 0.01, 243: 0.03, 245: 0.03, 246: 0.05, 254: 0.02, 283: 0.02, 284: 0.05, 332: 0.03, 341: 0.07, 358: 0.01, 359: 0.07, 424: 0.02, 743: 0.02, 842: 0.01, 906: 0.02, ]\n",
      "n: [151: 0.03, 155: 0.01, 163: 0.03, 178: 0.01, 180: 0.02, 186: 0.01, 192: 0.01, 194: 0.03, 204: 0.03, 229: 0.01, 243: 0.04, 245: 0.05, 246: 0.07, 254: 0.03, 268: 0.01, 283: 0.02, 284: 0.08, 332: 0.03, 341: 0.06, 359: 0.04, 424: 0.01, 743: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9990750551223755, an_dist: -0.991411566734314\n",
      "a: [400: 0.01, 456: 0.01, 470: 0.08, 499: 0.01, 556: 0.01, 614: 0.02, 626: 0.03, 644: 0.04, 862: 0.63, 879: 0.06, 982: 0.02, ]\n",
      "p: [456: 0.01, 470: 0.08, 499: 0.01, 556: 0.01, 614: 0.02, 626: 0.03, 644: 0.04, 862: 0.63, 879: 0.06, 982: 0.01, ]\n",
      "n: [400: 0.01, 470: 0.11, 499: 0.01, 556: 0.01, 614: 0.02, 623: 0.01, 626: 0.04, 644: 0.05, 862: 0.61, 879: 0.05, 982: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9999586343765259, an_dist: -0.9768133163452148\n",
      "a: [411: 0.05, 414: 0.11, 424: 0.01, 430: 0.02, 478: 0.01, 556: 0.05, 588: 0.02, 591: 0.01, 636: 0.03, 655: 0.05, 728: 0.01, 790: 0.08, 834: 0.03, 854: 0.02, 869: 0.01, 904: 0.02, 906: 0.03, ]\n",
      "p: [411: 0.06, 414: 0.12, 424: 0.01, 430: 0.02, 478: 0.01, 556: 0.05, 588: 0.02, 591: 0.01, 636: 0.03, 655: 0.05, 728: 0.01, 790: 0.07, 834: 0.03, 854: 0.02, 869: 0.01, 904: 0.02, 906: 0.04, ]\n",
      "n: [411: 0.03, 414: 0.15, 424: 0.01, 478: 0.01, 556: 0.08, 588: 0.03, 591: 0.01, 636: 0.02, 655: 0.05, 728: 0.02, 790: 0.06, 834: 0.02, 904: 0.03, 906: 0.02, ]\n",
      "func:cos_distance, ap_dist: -0.9998834133148193, an_dist: -0.9915965795516968\n",
      "a: [155: 0.01, 158: 0.02, 174: 0.01, 182: 0.02, 189: 0.04, 196: 0.54, 198: 0.14, 201: 0.14, ]\n",
      "p: [155: 0.01, 158: 0.01, 174: 0.01, 182: 0.02, 189: 0.04, 196: 0.54, 198: 0.16, 201: 0.13, ]\n",
      "n: [155: 0.02, 158: 0.02, 164: 0.01, 174: 0.02, 182: 0.03, 188: 0.01, 189: 0.05, 196: 0.45, 198: 0.12, 201: 0.17, ]\n",
      "func:cos_distance, ap_dist: -0.9998648166656494, an_dist: -0.9985286593437195\n",
      "a: [964: 0.97, ]\n",
      "p: [964: 0.96, ]\n",
      "n: [964: 0.97, ]\n",
      "func:cos_distance, ap_dist: -0.9997535943984985, an_dist: -0.9937415719032288\n",
      "target probs tensor([[0.88],\n",
      "        [0.01],\n",
      "        [0.01],\n",
      "        [0.97],\n",
      "        [0.58],\n",
      "        [0.93],\n",
      "        [0.69],\n",
      "        [0.06],\n",
      "        [0.20],\n",
      "        [1.00],\n",
      "        [0.95],\n",
      "        [0.00],\n",
      "        [0.48],\n",
      "        [0.61],\n",
      "        [0.15],\n",
      "        [0.84]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.4161481857299805: \n",
      "a: [472: 0.88, 693: 0.12, ]\n",
      "p: [472: 0.88, 693: 0.12, ]\n",
      "n: [472: 0.86, 693: 0.14, ]\n",
      "func:cos_distance, ap_dist: -0.9994336366653442, an_dist: -0.9959744215011597\n",
      "a: [326: 1.00, ]\n",
      "p: [326: 1.00, ]\n",
      "n: [326: 0.99, ]\n",
      "func:cos_distance, ap_dist: -0.9998835325241089, an_dist: -0.9971404671669006\n",
      "a: [408: 0.04, 561: 0.03, 586: 0.62, 652: 0.01, 744: 0.03, 847: 0.03, 864: 0.18, ]\n",
      "p: [408: 0.04, 561: 0.03, 586: 0.62, 652: 0.01, 744: 0.03, 847: 0.03, 864: 0.18, ]\n",
      "n: [408: 0.03, 561: 0.03, 586: 0.66, 652: 0.01, 744: 0.02, 847: 0.01, 864: 0.17, ]\n",
      "func:cos_distance, ap_dist: -0.9997896552085876, an_dist: -0.9956812858581543\n",
      "a: [151: 0.03, 298: 0.07, 299: 0.75, 359: 0.03, 360: 0.01, 362: 0.06, ]\n",
      "p: [151: 0.03, 298: 0.07, 299: 0.74, 359: 0.03, 360: 0.01, 362: 0.06, ]\n",
      "n: [151: 0.03, 298: 0.07, 299: 0.75, 359: 0.03, 360: 0.01, 362: 0.06, ]\n",
      "func:cos_distance, ap_dist: -0.9997134804725647, an_dist: -0.9882987141609192\n",
      "a: [370: 0.03, 371: 0.10, 373: 0.02, 374: 0.03, 377: 0.03, 378: 0.29, 379: 0.01, 380: 0.30, 381: 0.07, 382: 0.04, 743: 0.02, ]\n",
      "p: [370: 0.03, 371: 0.10, 373: 0.02, 374: 0.03, 377: 0.03, 378: 0.29, 379: 0.01, 380: 0.31, 381: 0.07, 382: 0.04, 743: 0.02, ]\n",
      "n: [370: 0.03, 371: 0.08, 373: 0.02, 374: 0.02, 377: 0.04, 378: 0.22, 379: 0.02, 380: 0.35, 381: 0.07, 382: 0.05, 728: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9995862245559692, an_dist: -0.9898418188095093\n",
      "a: [7: 0.06, 8: 0.93, ]\n",
      "p: [7: 0.06, 8: 0.93, ]\n",
      "n: [7: 0.06, 8: 0.93, ]\n",
      "func:cos_distance, ap_dist: -0.9998661279678345, an_dist: -0.9983866214752197\n",
      "a: [517: 0.02, 555: 0.01, 561: 0.01, 569: 0.45, 864: 0.38, 867: 0.06, ]\n",
      "p: [517: 0.02, 555: 0.01, 561: 0.01, 569: 0.46, 864: 0.37, 867: 0.06, ]\n",
      "n: [517: 0.03, 561: 0.02, 569: 0.55, 803: 0.02, 864: 0.31, 867: 0.02, ]\n",
      "func:cos_distance, ap_dist: -0.9985729455947876, an_dist: -0.9595365524291992\n",
      "a: [102: 0.87, 294: 0.01, 334: 0.07, ]\n",
      "p: [102: 0.86, 294: 0.01, 334: 0.07, ]\n",
      "n: [102: 0.86, 334: 0.09, ]\n",
      "func:cos_distance, ap_dist: -0.998456597328186, an_dist: -0.9612642526626587\n",
      "target probs tensor([[    0.00],\n",
      "        [    0.74],\n",
      "        [    0.82],\n",
      "        [    0.01],\n",
      "        [    0.09],\n",
      "        [    0.94],\n",
      "        [    0.98],\n",
      "        [    0.99],\n",
      "        [    0.85],\n",
      "        [    0.47],\n",
      "        [    0.85],\n",
      "        [    0.00],\n",
      "        [    0.00],\n",
      "        [    0.32],\n",
      "        [    0.79],\n",
      "        [    0.00]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.2849470376968384: \n",
      "a: [489: 0.29, 529: 0.07, 611: 0.19, 750: 0.01, 981: 0.01, ]\n",
      "p: [489: 0.30, 529: 0.08, 611: 0.18, 981: 0.01, ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: [489: 0.31, 529: 0.07, 611: 0.18, 981: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.999885618686676, an_dist: -0.9976547956466675\n",
      "a: [238: 0.18, 240: 0.01, 241: 0.81, ]\n",
      "p: [238: 0.19, 240: 0.01, 241: 0.80, ]\n",
      "n: [238: 0.23, 240: 0.01, 241: 0.76, ]\n",
      "func:cos_distance, ap_dist: -0.9987110495567322, an_dist: -0.9874143600463867\n",
      "a: [499: 0.01, 532: 0.05, 582: 0.09, 606: 0.01, 651: 0.02, 762: 0.21, 765: 0.01, 778: 0.02, 813: 0.07, 831: 0.03, 861: 0.01, 868: 0.03, 905: 0.01, 910: 0.03, 923: 0.02, 982: 0.02, ]\n",
      "p: [532: 0.04, 582: 0.08, 606: 0.01, 651: 0.02, 728: 0.01, 762: 0.21, 765: 0.01, 778: 0.02, 813: 0.06, 831: 0.03, 861: 0.01, 868: 0.03, 905: 0.02, 910: 0.03, 923: 0.02, 982: 0.02, ]\n",
      "n: [532: 0.04, 582: 0.06, 588: 0.01, 651: 0.02, 728: 0.01, 762: 0.20, 765: 0.01, 778: 0.01, 813: 0.06, 831: 0.04, 860: 0.01, 861: 0.01, 868: 0.02, 905: 0.02, 910: 0.03, 923: 0.02, 982: 0.03, ]\n",
      "func:cos_distance, ap_dist: -0.9996804594993591, an_dist: -0.994047999382019\n",
      "a: [398: 0.02, 419: 0.05, 522: 0.36, 574: 0.01, 641: 0.11, 680: 0.02, 805: 0.01, 990: 0.08, ]\n",
      "p: [398: 0.02, 419: 0.04, 522: 0.37, 574: 0.01, 641: 0.11, 680: 0.02, 805: 0.01, 990: 0.08, ]\n",
      "n: [52: 0.03, 78: 0.03, 301: 0.04, 398: 0.02, 419: 0.09, 522: 0.03, 560: 0.01, 600: 0.02, 641: 0.02, 680: 0.02, 719: 0.03, 772: 0.02, 783: 0.01, 784: 0.03, 805: 0.02, 826: 0.01, 957: 0.04, 990: 0.15, ]\n",
      "func:cos_distance, ap_dist: -0.9985247254371643, an_dist: -0.9470200538635254\n",
      "a: [10: 0.98, 904: 0.01, ]\n",
      "p: [10: 0.98, 904: 0.01, ]\n",
      "n: [10: 0.98, 904: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9994716644287109, an_dist: -0.9446138143539429\n",
      "a: [423: 0.72, 424: 0.19, 695: 0.01, ]\n",
      "p: [423: 0.72, 424: 0.19, 695: 0.01, ]\n",
      "n: [423: 0.72, 424: 0.19, 695: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.998972475528717, an_dist: -0.9599934816360474\n",
      "a: [429: 0.01, 573: 0.19, 575: 0.15, 603: 0.01, 621: 0.41, 817: 0.03, 864: 0.01, 870: 0.02, 912: 0.02, 981: 0.01, ]\n",
      "p: [429: 0.01, 573: 0.19, 575: 0.15, 603: 0.01, 621: 0.40, 817: 0.03, 864: 0.01, 870: 0.02, 912: 0.02, 981: 0.01, ]\n",
      "n: [429: 0.01, 573: 0.24, 575: 0.09, 603: 0.01, 621: 0.40, 817: 0.01, 870: 0.05, 912: 0.01, 981: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.999170184135437, an_dist: -0.9457960724830627\n",
      "a: [419: 0.02, 457: 0.02, 502: 0.02, 514: 0.02, 643: 0.85, 774: 0.03, ]\n",
      "p: [419: 0.02, 457: 0.02, 502: 0.02, 514: 0.02, 643: 0.85, 774: 0.03, 796: 0.01, ]\n",
      "n: [419: 0.05, 457: 0.04, 502: 0.01, 514: 0.05, 584: 0.01, 643: 0.72, 774: 0.03, 796: 0.02, ]\n",
      "func:cos_distance, ap_dist: -0.999840497970581, an_dist: -0.9798548817634583\n",
      "target probs tensor([[0.00],\n",
      "        [0.00],\n",
      "        [0.13],\n",
      "        [0.43],\n",
      "        [0.95],\n",
      "        [0.50],\n",
      "        [0.98],\n",
      "        [0.13],\n",
      "        [0.58],\n",
      "        [0.29],\n",
      "        [0.03],\n",
      "        [0.78],\n",
      "        [0.04],\n",
      "        [0.91],\n",
      "        [0.00],\n",
      "        [0.03]], device='cuda:1', grad_fn=<GatherBackward>), loss: 0.8720501661300659: \n",
      "a: [419: 0.03, 516: 0.02, 522: 0.01, 551: 0.01, 556: 0.02, 574: 0.12, 588: 0.15, 591: 0.05, 631: 0.02, 680: 0.03, 720: 0.02, 722: 0.01, 790: 0.01, 904: 0.14, 906: 0.01, 930: 0.02, 969: 0.05, ]\n",
      "p: [419: 0.03, 516: 0.02, 522: 0.01, 551: 0.01, 556: 0.02, 574: 0.12, 588: 0.15, 591: 0.05, 631: 0.02, 680: 0.03, 720: 0.02, 722: 0.01, 790: 0.01, 904: 0.14, 906: 0.01, 930: 0.02, 969: 0.05, ]\n",
      "n: [419: 0.03, 516: 0.02, 522: 0.01, 551: 0.01, 556: 0.02, 574: 0.12, 588: 0.15, 591: 0.05, 631: 0.02, 680: 0.03, 720: 0.02, 722: 0.01, 790: 0.01, 904: 0.14, 906: 0.01, 930: 0.02, 969: 0.05, ]\n",
      "func:cos_distance, ap_dist: -0.9980159997940063, an_dist: -0.9660475254058838\n",
      "a: [523: 0.03, 559: 0.05, 602: 0.02, 615: 0.02, 621: 0.11, 702: 0.01, 746: 0.01, 790: 0.03, 799: 0.02, 840: 0.02, 872: 0.07, 880: 0.01, 882: 0.31, ]\n",
      "p: [522: 0.01, 523: 0.03, 559: 0.06, 602: 0.02, 615: 0.02, 621: 0.11, 702: 0.01, 790: 0.03, 799: 0.02, 840: 0.02, 872: 0.06, 880: 0.01, 882: 0.30, ]\n",
      "n: [522: 0.01, 523: 0.03, 559: 0.06, 602: 0.02, 615: 0.02, 621: 0.11, 693: 0.01, 702: 0.01, 790: 0.03, 799: 0.02, 840: 0.02, 872: 0.07, 880: 0.02, 882: 0.30, ]\n",
      "func:cos_distance, ap_dist: -0.9992486238479614, an_dist: -0.941103458404541\n",
      "a: [61: 0.03, 62: 0.01, 391: 0.02, 414: 0.02, 429: 0.01, 443: 0.01, 489: 0.10, 529: 0.01, 556: 0.06, 591: 0.02, 636: 0.04, 669: 0.02, 721: 0.01, 728: 0.03, 748: 0.03, 752: 0.02, 768: 0.02, 770: 0.01, 805: 0.03, 828: 0.02, 904: 0.03, 912: 0.01, ]\n",
      "p: [61: 0.03, 62: 0.01, 391: 0.02, 414: 0.03, 429: 0.01, 443: 0.01, 489: 0.10, 529: 0.01, 556: 0.06, 591: 0.02, 636: 0.04, 669: 0.02, 721: 0.01, 728: 0.03, 748: 0.03, 752: 0.02, 768: 0.02, 770: 0.01, 805: 0.03, 828: 0.02, 904: 0.03, 912: 0.01, ]\n",
      "n: [61: 0.03, 62: 0.01, 391: 0.02, 414: 0.02, 429: 0.01, 489: 0.10, 556: 0.06, 591: 0.03, 636: 0.04, 669: 0.02, 721: 0.01, 728: 0.02, 748: 0.03, 752: 0.03, 768: 0.02, 770: 0.01, 805: 0.03, 828: 0.02, 904: 0.03, 912: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9997403621673584, an_dist: -0.9964691400527954\n",
      "a: [412: 0.98, ]\n",
      "p: [412: 0.98, ]\n",
      "n: [412: 0.98, ]\n",
      "func:cos_distance, ap_dist: -0.9993457794189453, an_dist: -0.9775524139404297\n",
      "a: [486: 0.21, 889: 0.79, ]\n",
      "p: [486: 0.21, 889: 0.79, ]\n",
      "n: [486: 0.38, 889: 0.62, ]\n",
      "func:cos_distance, ap_dist: -0.9995812773704529, an_dist: -0.9820398092269897\n",
      "a: [900: 0.96, ]\n",
      "p: [900: 0.96, ]\n",
      "n: [900: 0.96, ]\n",
      "func:cos_distance, ap_dist: -0.9969178438186646, an_dist: -0.9425778388977051\n",
      "a: [414: 0.06, 457: 0.01, 489: 0.01, 556: 0.01, 606: 0.02, 608: 0.11, 630: 0.04, 636: 0.05, 728: 0.02, 753: 0.02, 783: 0.01, 788: 0.02, 797: 0.01, 834: 0.03, 869: 0.03, 904: 0.03, ]\n",
      "p: [414: 0.06, 457: 0.01, 489: 0.01, 556: 0.01, 606: 0.02, 608: 0.11, 630: 0.05, 636: 0.05, 728: 0.02, 753: 0.02, 783: 0.01, 788: 0.02, 797: 0.01, 834: 0.03, 869: 0.03, 904: 0.03, ]\n",
      "n: [414: 0.03, 457: 0.01, 514: 0.02, 608: 0.10, 630: 0.12, 636: 0.03, 728: 0.02, 753: 0.02, 788: 0.02, 797: 0.02, 834: 0.08, 869: 0.07, 904: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9985482692718506, an_dist: -0.9710711240768433\n",
      "a: [429: 0.04, 489: 0.08, 490: 0.01, 522: 0.02, 560: 0.02, 577: 0.03, 591: 0.01, 722: 0.01, 746: 0.01, 787: 0.01, 805: 0.04, 815: 0.01, 851: 0.02, 890: 0.01, 904: 0.21, 981: 0.03, ]\n",
      "p: [429: 0.04, 489: 0.08, 490: 0.01, 522: 0.02, 560: 0.02, 577: 0.03, 591: 0.01, 722: 0.01, 746: 0.01, 787: 0.01, 805: 0.05, 815: 0.01, 851: 0.02, 890: 0.01, 904: 0.21, 981: 0.03, ]\n",
      "n: [429: 0.04, 489: 0.08, 490: 0.01, 522: 0.02, 560: 0.02, 577: 0.03, 591: 0.01, 722: 0.01, 746: 0.01, 787: 0.01, 805: 0.04, 815: 0.01, 851: 0.02, 890: 0.01, 904: 0.21, 981: 0.03, ]\n",
      "func:cos_distance, ap_dist: -0.9952718615531921, an_dist: -0.9445056915283203\n",
      "target probs tensor([[0.15],\n",
      "        [0.38],\n",
      "        [0.82],\n",
      "        [0.41],\n",
      "        [0.03],\n",
      "        [1.00],\n",
      "        [0.35],\n",
      "        [0.03],\n",
      "        [0.98],\n",
      "        [0.01],\n",
      "        [0.19],\n",
      "        [0.63],\n",
      "        [0.39],\n",
      "        [0.01],\n",
      "        [0.36],\n",
      "        [1.00]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.5081664323806763: \n",
      "a: [7: 0.02, 8: 0.16, 80: 0.19, 81: 0.20, 82: 0.15, 83: 0.06, 86: 0.06, 138: 0.06, 912: 0.02, ]\n",
      "p: [7: 0.02, 8: 0.16, 80: 0.19, 81: 0.20, 82: 0.15, 83: 0.06, 86: 0.06, 138: 0.06, 912: 0.02, ]\n",
      "n: [7: 0.02, 8: 0.16, 80: 0.19, 81: 0.18, 82: 0.18, 83: 0.06, 86: 0.06, 138: 0.05, 912: 0.02, ]\n",
      "func:cos_distance, ap_dist: -0.9971216917037964, an_dist: -0.9699440002441406\n",
      "a: [482: 0.11, 527: 0.02, 598: 0.03, 606: 0.08, 614: 0.01, 632: 0.02, 707: 0.11, 710: 0.01, 732: 0.06, 782: 0.02, 848: 0.02, 851: 0.23, 877: 0.02, 906: 0.01, ]\n",
      "p: [482: 0.11, 527: 0.02, 598: 0.03, 606: 0.08, 614: 0.01, 632: 0.02, 707: 0.12, 710: 0.02, 732: 0.06, 782: 0.02, 848: 0.02, 851: 0.22, 877: 0.02, 906: 0.01, ]\n",
      "n: [480: 0.01, 481: 0.01, 482: 0.15, 527: 0.02, 598: 0.04, 606: 0.09, 614: 0.02, 632: 0.03, 707: 0.11, 710: 0.02, 732: 0.05, 782: 0.02, 834: 0.01, 848: 0.03, 851: 0.12, 877: 0.02, 906: 0.02, ]\n",
      "func:cos_distance, ap_dist: -0.9996930956840515, an_dist: -0.8753990530967712\n",
      "a: [654: 0.87, 675: 0.02, 705: 0.01, 757: 0.03, 779: 0.05, ]\n",
      "p: [654: 0.85, 675: 0.02, 705: 0.01, 757: 0.03, 779: 0.06, 874: 0.01, ]\n",
      "n: [654: 0.84, 675: 0.02, 705: 0.01, 757: 0.03, 779: 0.06, 874: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9989248514175415, an_dist: -0.9598837494850159\n",
      "a: [468: 0.02, 479: 0.03, 511: 0.07, 581: 0.27, 627: 0.03, 661: 0.25, 670: 0.01, 717: 0.04, 817: 0.17, 864: 0.02, 867: 0.02, ]\n",
      "p: [468: 0.02, 479: 0.03, 511: 0.07, 581: 0.31, 627: 0.03, 661: 0.26, 670: 0.01, 717: 0.04, 817: 0.14, 864: 0.02, 867: 0.01, ]\n",
      "n: [468: 0.02, 479: 0.03, 511: 0.07, 581: 0.28, 627: 0.03, 661: 0.24, 670: 0.01, 717: 0.03, 817: 0.20, 864: 0.01, 867: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9986892938613892, an_dist: -0.9909793138504028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [419: 0.03, 515: 0.03, 520: 0.01, 534: 0.02, 588: 0.01, 615: 0.02, 622: 0.02, 641: 0.03, 680: 0.03, 697: 0.01, 719: 0.01, 747: 0.04, 813: 0.13, 823: 0.04, 834: 0.02, 869: 0.12, 892: 0.03, 906: 0.02, 910: 0.02, ]\n",
      "p: [419: 0.03, 515: 0.03, 520: 0.01, 534: 0.02, 588: 0.01, 615: 0.02, 622: 0.02, 641: 0.03, 680: 0.03, 697: 0.01, 719: 0.01, 747: 0.04, 813: 0.13, 823: 0.04, 834: 0.03, 869: 0.12, 892: 0.03, 906: 0.02, 910: 0.02, ]\n",
      "n: [419: 0.03, 515: 0.03, 520: 0.01, 534: 0.02, 588: 0.01, 615: 0.02, 622: 0.02, 641: 0.03, 680: 0.03, 697: 0.01, 719: 0.01, 747: 0.04, 813: 0.13, 823: 0.04, 834: 0.03, 869: 0.12, 892: 0.03, 906: 0.02, 910: 0.02, ]\n",
      "func:cos_distance, ap_dist: -0.997067391872406, an_dist: -0.9652602672576904\n",
      "a: [431: 0.04, 516: 0.73, 520: 0.14, 559: 0.04, ]\n",
      "p: [431: 0.04, 516: 0.72, 520: 0.13, 559: 0.04, ]\n",
      "n: [431: 0.04, 516: 0.75, 520: 0.11, 559: 0.03, ]\n",
      "func:cos_distance, ap_dist: -0.9997603893280029, an_dist: -0.9341687560081482\n",
      "a: [414: 0.06, 447: 0.02, 480: 0.08, 539: 0.02, 571: 0.01, 614: 0.01, 615: 0.01, 670: 0.01, 678: 0.02, 763: 0.01, 770: 0.01, 778: 0.01, 788: 0.01, 789: 0.02, 860: 0.01, 872: 0.01, 904: 0.02, 905: 0.02, 906: 0.05, ]\n",
      "p: [414: 0.06, 447: 0.02, 480: 0.08, 539: 0.02, 571: 0.02, 614: 0.01, 615: 0.01, 670: 0.01, 678: 0.02, 763: 0.01, 770: 0.01, 778: 0.01, 788: 0.01, 789: 0.02, 860: 0.01, 872: 0.01, 904: 0.02, 905: 0.02, 906: 0.05, ]\n",
      "n: [414: 0.06, 447: 0.02, 480: 0.08, 539: 0.02, 571: 0.01, 614: 0.01, 615: 0.01, 670: 0.01, 678: 0.02, 763: 0.01, 770: 0.02, 788: 0.01, 789: 0.02, 860: 0.01, 872: 0.01, 904: 0.02, 905: 0.02, 906: 0.05, ]\n",
      "func:cos_distance, ap_dist: -0.9987853169441223, an_dist: -0.8690590262413025\n",
      "a: [46: 0.02, 55: 0.22, 59: 0.27, 64: 0.44, ]\n",
      "p: [46: 0.02, 55: 0.26, 59: 0.18, 64: 0.49, ]\n",
      "n: [40: 0.02, 46: 0.04, 47: 0.01, 55: 0.18, 59: 0.25, 64: 0.45, ]\n",
      "func:cos_distance, ap_dist: -0.9986337423324585, an_dist: -0.9722691178321838\n",
      "target probs tensor([[0.17],\n",
      "        [0.90],\n",
      "        [0.38],\n",
      "        [1.00],\n",
      "        [0.96],\n",
      "        [0.90],\n",
      "        [0.84],\n",
      "        [0.19],\n",
      "        [0.00],\n",
      "        [0.71],\n",
      "        [0.42],\n",
      "        [0.01],\n",
      "        [0.82],\n",
      "        [0.97],\n",
      "        [0.97],\n",
      "        [0.01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.6846107244491577: \n",
      "a: [468: 0.01, 498: 0.17, 727: 0.13, 762: 0.01, 779: 0.09, 781: 0.22, 819: 0.05, 854: 0.04, 860: 0.07, ]\n",
      "p: [468: 0.01, 498: 0.17, 727: 0.13, 762: 0.01, 779: 0.09, 781: 0.23, 819: 0.05, 854: 0.04, 860: 0.06, ]\n",
      "n: [468: 0.01, 498: 0.17, 727: 0.12, 762: 0.01, 779: 0.09, 781: 0.22, 819: 0.05, 854: 0.04, 860: 0.06, ]\n",
      "func:cos_distance, ap_dist: -0.9995971918106079, an_dist: -0.9838464260101318\n",
      "a: [7: 0.01, 8: 0.03, 12: 0.01, 21: 0.07, 23: 0.11, 81: 0.05, 82: 0.06, 84: 0.12, 85: 0.01, 86: 0.03, 88: 0.01, 89: 0.05, 90: 0.02, 489: 0.10, 904: 0.20, 912: 0.02, ]\n",
      "p: [7: 0.01, 8: 0.04, 21: 0.08, 23: 0.13, 81: 0.05, 82: 0.07, 84: 0.15, 85: 0.01, 86: 0.04, 88: 0.01, 89: 0.05, 90: 0.01, 91: 0.01, 489: 0.10, 904: 0.11, 912: 0.03, ]\n",
      "n: [7: 0.01, 8: 0.03, 12: 0.01, 21: 0.07, 23: 0.11, 81: 0.06, 82: 0.05, 84: 0.12, 85: 0.01, 86: 0.03, 88: 0.02, 89: 0.06, 90: 0.02, 489: 0.09, 904: 0.21, 912: 0.02, ]\n",
      "func:cos_distance, ap_dist: -0.99459308385849, an_dist: -0.9874483346939087\n",
      "a: [352: 0.01, 353: 0.93, 355: 0.01, ]\n",
      "p: [352: 0.01, 353: 0.93, 355: 0.01, ]\n",
      "n: [352: 0.01, 353: 0.93, 355: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9973572492599487, an_dist: -0.9572917819023132\n",
      "a: [418: 0.01, 481: 0.02, 549: 0.01, 556: 0.02, 563: 0.03, 584: 0.01, 591: 0.06, 606: 0.06, 623: 0.19, 633: 0.02, 749: 0.01, 789: 0.02, 830: 0.01, 831: 0.02, 904: 0.08, ]\n",
      "p: [418: 0.01, 481: 0.02, 549: 0.01, 556: 0.02, 563: 0.03, 584: 0.01, 591: 0.06, 606: 0.06, 623: 0.20, 633: 0.02, 749: 0.01, 789: 0.02, 790: 0.01, 830: 0.01, 831: 0.02, 904: 0.07, ]\n",
      "n: [473: 0.01, 481: 0.01, 556: 0.03, 563: 0.02, 591: 0.06, 606: 0.06, 623: 0.23, 633: 0.01, 749: 0.01, 750: 0.01, 789: 0.03, 831: 0.07, 904: 0.06, 905: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9997424483299255, an_dist: -0.9810366034507751\n",
      "a: [430: 0.02, 611: 0.01, 615: 0.03, 722: 0.03, 743: 0.03, 746: 0.02, 768: 0.04, 805: 0.07, 834: 0.01, 842: 0.02, 865: 0.02, 879: 0.01, 890: 0.10, 904: 0.23, 971: 0.02, ]\n",
      "p: [430: 0.02, 615: 0.04, 722: 0.03, 743: 0.02, 746: 0.02, 768: 0.05, 805: 0.09, 834: 0.01, 842: 0.02, 865: 0.02, 879: 0.01, 890: 0.14, 904: 0.18, 971: 0.01, ]\n",
      "n: [430: 0.03, 615: 0.03, 722: 0.03, 743: 0.05, 746: 0.02, 747: 0.01, 752: 0.01, 768: 0.05, 805: 0.12, 830: 0.01, 834: 0.02, 842: 0.02, 865: 0.01, 890: 0.20, 904: 0.02, 906: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9965941905975342, an_dist: -0.9255803823471069\n",
      "a: [412: 0.01, 414: 0.02, 570: 0.14, 591: 0.03, 615: 0.03, 635: 0.01, 646: 0.01, 691: 0.03, 728: 0.02, 731: 0.01, 746: 0.02, 758: 0.02, 770: 0.03, 774: 0.01, 865: 0.01, 870: 0.01, 917: 0.02, 982: 0.03, ]\n",
      "p: [414: 0.01, 561: 0.01, 570: 0.17, 591: 0.02, 615: 0.02, 691: 0.01, 728: 0.01, 731: 0.01, 746: 0.02, 758: 0.02, 770: 0.10, 774: 0.01, 783: 0.01, 830: 0.01, 870: 0.02, 904: 0.02, 982: 0.02, 983: 0.01, 999: 0.03]\n",
      "n: [412: 0.01, 414: 0.02, 489: 0.01, 556: 0.02, 570: 0.05, 588: 0.01, 591: 0.08, 611: 0.03, 615: 0.03, 635: 0.01, 646: 0.02, 691: 0.05, 728: 0.02, 750: 0.02, 770: 0.02, 774: 0.01, 865: 0.02, 917: 0.06, 918: 0.02, 982: 0.02, ]\n",
      "func:cos_distance, ap_dist: -0.9913760423660278, an_dist: -0.9214833974838257\n",
      "a: [446: 0.02, 458: 0.15, 480: 0.04, 526: 0.03, 527: 0.03, 549: 0.15, 553: 0.04, 571: 0.01, 620: 0.01, 637: 0.02, 664: 0.02, 673: 0.01, 681: 0.02, 742: 0.05, 754: 0.01, 782: 0.04, 798: 0.02, 916: 0.05, 918: 0.01, ]\n",
      "p: [414: 0.01, 446: 0.02, 458: 0.08, 480: 0.04, 526: 0.02, 527: 0.02, 549: 0.19, 553: 0.03, 571: 0.01, 620: 0.01, 636: 0.01, 637: 0.02, 662: 0.01, 664: 0.01, 681: 0.01, 742: 0.08, 782: 0.03, 798: 0.02, 916: 0.04, 918: 0.01, ]\n",
      "n: [446: 0.01, 458: 0.26, 480: 0.02, 508: 0.01, 526: 0.02, 527: 0.03, 549: 0.11, 553: 0.06, 620: 0.01, 664: 0.02, 681: 0.02, 742: 0.03, 754: 0.01, 782: 0.05, 904: 0.01, 907: 0.01, 916: 0.04, 918: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9951955080032349, an_dist: -0.896390438079834\n",
      "a: [78: 0.02, 302: 0.07, 304: 0.51, 306: 0.01, 307: 0.32, 904: 0.02, ]\n",
      "p: [78: 0.02, 302: 0.07, 304: 0.49, 306: 0.01, 307: 0.33, 904: 0.03, ]\n",
      "n: [78: 0.06, 302: 0.08, 304: 0.49, 306: 0.02, 307: 0.30, ]\n",
      "func:cos_distance, ap_dist: -0.9967972636222839, an_dist: -0.9579412341117859\n",
      "target probs tensor([[    0.00],\n",
      "        [    0.03],\n",
      "        [    0.40],\n",
      "        [    0.00],\n",
      "        [    0.82],\n",
      "        [    0.38],\n",
      "        [    0.12],\n",
      "        [    0.00],\n",
      "        [    0.99],\n",
      "        [    0.77],\n",
      "        [    1.00],\n",
      "        [    0.04],\n",
      "        [    0.90],\n",
      "        [    0.42],\n",
      "        [    0.01],\n",
      "        [    0.16]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.2074869871139526: \n",
      "a: [556: 0.84, 904: 0.14, ]\n",
      "p: [556: 0.84, 904: 0.15, ]\n",
      "n: [556: 0.72, 904: 0.27, ]\n",
      "func:cos_distance, ap_dist: -0.9965888261795044, an_dist: -0.8578885793685913\n",
      "a: [539: 0.01, 904: 0.98, ]\n",
      "p: [539: 0.01, 904: 0.98, ]\n",
      "n: [904: 0.99, ]\n",
      "func:cos_distance, ap_dist: -0.991538941860199, an_dist: -0.8988367915153503\n",
      "a: [700: 0.23, 728: 0.02, 999: 0.74]\n",
      "p: [700: 0.23, 728: 0.02, 999: 0.74]\n",
      "n: [700: 0.23, 728: 0.02, 999: 0.75]\n",
      "func:cos_distance, ap_dist: -0.9986922740936279, an_dist: -0.962583065032959\n",
      "a: [443: 0.02, 452: 0.90, 474: 0.01, 850: 0.01, ]\n",
      "p: [443: 0.03, 452: 0.87, 474: 0.02, 850: 0.01, ]\n",
      "n: [452: 0.89, 474: 0.01, 658: 0.01, 911: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.993416428565979, an_dist: -0.8755382299423218\n",
      "a: [497: 0.01, 498: 0.09, 762: 0.11, 819: 0.08, 854: 0.68, ]\n",
      "p: [497: 0.01, 498: 0.09, 762: 0.10, 819: 0.08, 854: 0.69, ]\n",
      "n: [497: 0.01, 498: 0.08, 762: 0.10, 819: 0.09, 854: 0.70, ]\n",
      "func:cos_distance, ap_dist: -0.9890134334564209, an_dist: -0.9661803245544434\n",
      "a: [76: 0.02, 341: 0.02, 342: 0.15, 343: 0.63, 348: 0.01, 349: 0.04, 350: 0.04, ]\n",
      "p: [76: 0.02, 341: 0.02, 342: 0.16, 343: 0.63, 348: 0.01, 349: 0.04, 350: 0.05, ]\n",
      "n: [76: 0.02, 341: 0.02, 342: 0.13, 343: 0.72, 349: 0.01, 350: 0.03, ]\n",
      "func:cos_distance, ap_dist: -0.9993017911911011, an_dist: -0.9180562496185303\n",
      "a: [440: 0.01, 447: 0.08, 477: 0.05, 543: 0.04, 585: 0.02, 587: 0.03, 613: 0.04, 622: 0.12, 720: 0.06, 740: 0.04, 759: 0.03, 763: 0.07, 783: 0.01, 784: 0.13, 872: 0.04, 907: 0.03, ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: [440: 0.01, 447: 0.09, 477: 0.05, 543: 0.04, 585: 0.02, 587: 0.03, 613: 0.03, 622: 0.12, 720: 0.06, 740: 0.04, 759: 0.03, 763: 0.08, 783: 0.01, 784: 0.13, 872: 0.04, 907: 0.03, ]\n",
      "n: [440: 0.02, 447: 0.04, 455: 0.02, 477: 0.02, 543: 0.02, 585: 0.01, 587: 0.02, 613: 0.01, 622: 0.38, 720: 0.07, 740: 0.02, 746: 0.01, 759: 0.04, 763: 0.02, 783: 0.01, 784: 0.12, 872: 0.02, 898: 0.01, 907: 0.02, ]\n",
      "func:cos_distance, ap_dist: -0.9874125719070435, an_dist: -0.8750860691070557\n",
      "a: [199: 0.02, 203: 0.05, 332: 0.80, 904: 0.02, 982: 0.02, ]\n",
      "p: [199: 0.03, 203: 0.05, 332: 0.80, 904: 0.02, 982: 0.02, ]\n",
      "n: [190: 0.02, 199: 0.03, 203: 0.02, 258: 0.01, 331: 0.01, 332: 0.51, 489: 0.03, 728: 0.03, 904: 0.06, 912: 0.01, 982: 0.06, ]\n",
      "func:cos_distance, ap_dist: -0.9890364408493042, an_dist: -0.9390141367912292\n",
      "target probs tensor([[    0.74],\n",
      "        [    0.97],\n",
      "        [    0.11],\n",
      "        [    0.04],\n",
      "        [    0.00],\n",
      "        [    0.10],\n",
      "        [    0.02],\n",
      "        [    0.09],\n",
      "        [    1.00],\n",
      "        [    0.02],\n",
      "        [    0.01],\n",
      "        [    0.81],\n",
      "        [    0.99],\n",
      "        [    0.88],\n",
      "        [    0.53],\n",
      "        [    0.00]], device='cuda:1', grad_fn=<GatherBackward>), loss: 1.235936164855957: \n",
      "a: [168: 0.01, 245: 0.04, 722: 0.01, 752: 0.02, 852: 0.74, ]\n",
      "p: [168: 0.03, 242: 0.01, 245: 0.03, 253: 0.01, 722: 0.01, 752: 0.03, 852: 0.67, ]\n",
      "n: [168: 0.02, 245: 0.02, 253: 0.01, 752: 0.04, 852: 0.75, ]\n",
      "func:cos_distance, ap_dist: -0.9965693354606628, an_dist: -0.9804761409759521\n",
      "a: [431: 0.09, 516: 0.02, 520: 0.14, 564: 0.01, 591: 0.01, 606: 0.03, 706: 0.04, 721: 0.02, 750: 0.17, 786: 0.03, 831: 0.16, 905: 0.03, ]\n",
      "p: [431: 0.10, 516: 0.02, 520: 0.14, 564: 0.01, 591: 0.01, 606: 0.03, 706: 0.04, 721: 0.02, 750: 0.17, 786: 0.03, 831: 0.16, 905: 0.03, ]\n",
      "n: [431: 0.10, 516: 0.03, 520: 0.14, 564: 0.01, 591: 0.01, 606: 0.03, 706: 0.04, 721: 0.02, 750: 0.17, 786: 0.03, 831: 0.16, 905: 0.03, ]\n",
      "func:cos_distance, ap_dist: -0.9957185387611389, an_dist: -0.9053066968917847\n",
      "a: [326: 1.00, ]\n",
      "p: [326: 1.00, ]\n",
      "n: [326: 0.98, ]\n",
      "func:cos_distance, ap_dist: -0.9910204410552979, an_dist: -0.9375081062316895\n",
      "a: [423: 0.03, 424: 0.03, 665: 0.15, 670: 0.66, ]\n",
      "p: [423: 0.03, 424: 0.04, 665: 0.15, 670: 0.65, ]\n",
      "n: [423: 0.02, 424: 0.03, 665: 0.18, 670: 0.66, ]\n",
      "func:cos_distance, ap_dist: -0.9961528778076172, an_dist: -0.8938119411468506\n",
      "a: [149: 0.02, 419: 0.02, 556: 0.01, 577: 0.01, 584: 0.01, 700: 0.44, 721: 0.01, 748: 0.03, 904: 0.01, 948: 0.01, 999: 0.11]\n",
      "p: [112: 0.01, 149: 0.02, 419: 0.02, 556: 0.01, 577: 0.02, 584: 0.01, 700: 0.40, 721: 0.01, 748: 0.02, 849: 0.01, 904: 0.01, 948: 0.02, 999: 0.08]\n",
      "n: [109: 0.01, 149: 0.04, 419: 0.01, 700: 0.59, 721: 0.01, 904: 0.01, 999: 0.15]\n",
      "func:cos_distance, ap_dist: -0.9922075867652893, an_dist: -0.9088454246520996\n",
      "a: [409: 0.01, 421: 0.02, 489: 0.01, 538: 0.02, 576: 0.07, 698: 0.01, 718: 0.38, 743: 0.08, 781: 0.02, 791: 0.02, 819: 0.02, 821: 0.03, 839: 0.04, 860: 0.01, 888: 0.02, ]\n",
      "p: [409: 0.01, 421: 0.02, 489: 0.01, 538: 0.02, 576: 0.07, 698: 0.01, 718: 0.39, 743: 0.08, 781: 0.02, 791: 0.02, 819: 0.02, 821: 0.03, 839: 0.04, 860: 0.01, 888: 0.02, ]\n",
      "n: [409: 0.01, 421: 0.02, 489: 0.01, 538: 0.01, 576: 0.07, 698: 0.01, 718: 0.39, 743: 0.09, 781: 0.02, 791: 0.02, 819: 0.02, 821: 0.02, 839: 0.04, 860: 0.01, 888: 0.02, ]\n",
      "func:cos_distance, ap_dist: -0.9986467361450195, an_dist: -0.8905629515647888\n",
      "a: [423: 0.22, 424: 0.72, 743: 0.01, ]\n",
      "p: [423: 0.22, 424: 0.71, 743: 0.02, ]\n",
      "n: [423: 0.20, 424: 0.79, ]\n",
      "func:cos_distance, ap_dist: -0.9983457922935486, an_dist: -0.9853302240371704\n",
      "a: [419: 0.05, 443: 0.07, 545: 0.02, 549: 0.01, 556: 0.01, 584: 0.01, 588: 0.02, 589: 0.02, 591: 0.03, 680: 0.02, 709: 0.01, 721: 0.07, 741: 0.01, 772: 0.02, 887: 0.01, 904: 0.04, 906: 0.03, 987: 0.10, 998: 0.13, ]\n",
      "p: [419: 0.06, 443: 0.07, 545: 0.02, 549: 0.01, 556: 0.01, 584: 0.01, 588: 0.02, 589: 0.03, 591: 0.03, 680: 0.03, 709: 0.01, 721: 0.07, 741: 0.01, 772: 0.03, 887: 0.01, 904: 0.04, 906: 0.02, 987: 0.11, 998: 0.14, ]\n",
      "n: [828: 0.01, 904: 0.96, ]\n",
      "func:cos_distance, ap_dist: -0.9958816766738892, an_dist: -0.835026204586029\n",
      "target probs tensor([[    0.00],\n",
      "        [    0.03],\n",
      "        [    0.97],\n",
      "        [    0.95],\n",
      "        [    0.02],\n",
      "        [    0.99],\n",
      "        [    0.00],\n",
      "        [    0.09],\n",
      "        [    0.00],\n",
      "        [    0.95],\n",
      "        [    0.02],\n",
      "        [    0.04],\n",
      "        [    0.95],\n",
      "        [    0.08],\n",
      "        [    0.02],\n",
      "        [    1.00]], device='cuda:1'), loss: 1.5250718593597412: \n",
      "a: [446: 0.02, 549: 0.10, 588: 0.02, 591: 0.20, 636: 0.03, 721: 0.13, 748: 0.11, 750: 0.08, 831: 0.03, 893: 0.03, 905: 0.02, ]\n",
      "p: [446: 0.02, 549: 0.10, 588: 0.02, 591: 0.20, 636: 0.03, 721: 0.12, 748: 0.11, 750: 0.08, 831: 0.03, 893: 0.03, 905: 0.02, ]\n",
      "n: [446: 0.02, 532: 0.02, 549: 0.14, 588: 0.05, 591: 0.15, 636: 0.01, 721: 0.12, 723: 0.02, 748: 0.06, 750: 0.08, 762: 0.01, 831: 0.02, 893: 0.02, 905: 0.02, ]\n",
      "func:cos_distance, ap_dist: -0.9991055727005005, an_dist: -0.8714702129364014\n",
      "a: [330: 0.02, 331: 0.98, ]\n",
      "p: [330: 0.02, 331: 0.98, ]\n",
      "n: [331: 0.99, ]\n",
      "func:cos_distance, ap_dist: -0.9997948408126831, an_dist: -0.9881535172462463\n",
      "a: [412: 0.03, 435: 0.02, 478: 0.17, 492: 0.01, 606: 0.08, 662: 0.04, 707: 0.02, 722: 0.01, 742: 0.04, 754: 0.03, 778: 0.03, 782: 0.02, 859: 0.01, 882: 0.01, 896: 0.07, 897: 0.01, 904: 0.04, 999: 0.03]\n",
      "p: [412: 0.03, 435: 0.02, 478: 0.18, 492: 0.01, 606: 0.08, 662: 0.04, 664: 0.01, 707: 0.02, 722: 0.01, 742: 0.04, 754: 0.03, 778: 0.03, 782: 0.02, 859: 0.02, 882: 0.01, 896: 0.06, 897: 0.01, 904: 0.04, 999: 0.03]\n",
      "n: [412: 0.03, 435: 0.02, 478: 0.21, 492: 0.01, 606: 0.08, 662: 0.04, 664: 0.01, 707: 0.02, 722: 0.01, 742: 0.04, 754: 0.03, 778: 0.03, 782: 0.02, 859: 0.01, 882: 0.01, 896: 0.06, 897: 0.01, 904: 0.04, 999: 0.03]\n",
      "func:cos_distance, ap_dist: -0.9980325698852539, an_dist: -0.9024547338485718\n",
      "a: [180: 0.03, 195: 0.18, 215: 0.02, 217: 0.16, 238: 0.02, 811: 0.01, 904: 0.46, ]\n",
      "p: [180: 0.04, 195: 0.21, 215: 0.02, 217: 0.13, 238: 0.02, 556: 0.01, 811: 0.01, 904: 0.46, ]\n",
      "n: [180: 0.04, 195: 0.15, 215: 0.01, 217: 0.17, 238: 0.02, 811: 0.01, 904: 0.48, ]\n",
      "func:cos_distance, ap_dist: -0.9911655187606812, an_dist: -0.9702574014663696\n",
      "a: [62: 0.01, 67: 0.01, 348: 0.85, 349: 0.06, 904: 0.01, ]\n",
      "p: [62: 0.01, 348: 0.86, 349: 0.07, ]\n",
      "n: [62: 0.02, 67: 0.02, 348: 0.82, 349: 0.04, 904: 0.03, ]\n",
      "func:cos_distance, ap_dist: -0.9991987943649292, an_dist: -0.911595344543457\n",
      "a: [100: 0.99, ]\n",
      "p: [100: 0.99, ]\n",
      "n: [100: 1.00, ]\n",
      "func:cos_distance, ap_dist: -0.9990063309669495, an_dist: -0.869008481502533\n",
      "Better model found at epoch 0 with validation value: 0.460999995470047.\n",
      "a: [585: 0.04, 828: 0.02, 845: 0.01, 904: 0.75, 982: 0.02, ]\n",
      "p: [585: 0.03, 828: 0.01, 904: 0.79, 982: 0.01, ]\n",
      "n: [585: 0.03, 828: 0.01, 904: 0.80, 982: 0.01, ]\n",
      "func:cos_distance, ap_dist: -0.9981967210769653, an_dist: -0.8658037185668945\n",
      "a: [529: 0.77, 842: 0.18, ]\n",
      "p: [529: 0.78, 842: 0.18, ]\n",
      "n: [529: 0.81, 842: 0.16, ]\n",
      "func:cos_distance, ap_dist: -0.9987766146659851, an_dist: -0.9556013345718384\n",
      "target probs tensor([[0.97],\n",
      "        [0.08],\n",
      "        [0.36],\n",
      "        [0.99],\n",
      "        [0.03],\n",
      "        [0.19],\n",
      "        [0.00],\n",
      "        [0.75],\n",
      "        [0.02],\n",
      "        [0.82],\n",
      "        [1.00],\n",
      "        [0.91],\n",
      "        [1.00],\n",
      "        [0.79],\n",
      "        [0.57],\n",
      "        [0.01]], device='cuda:1', grad_fn=<GatherBackward>), loss: 2.349853515625: \n",
      "a: [919: 0.97, 920: 0.03, ]\n",
      "p: [919: 0.97, 920: 0.03, ]\n",
      "n: [919: 0.95, 920: 0.04, ]\n",
      "func:cos_distance, ap_dist: -0.9996811747550964, an_dist: -0.9199226498603821\n",
      "a: [518: 0.03, 530: 0.01, 556: 0.03, 607: 0.36, 722: 0.01, 782: 0.01, 805: 0.13, 811: 0.03, 817: 0.02, 854: 0.04, 902: 0.01, 917: 0.02, 920: 0.02, ]\n",
      "p: [518: 0.03, 530: 0.01, 556: 0.03, 607: 0.36, 722: 0.01, 782: 0.01, 805: 0.13, 811: 0.03, 817: 0.02, 854: 0.04, 902: 0.01, 917: 0.02, 920: 0.02, ]\n",
      "n: [518: 0.03, 530: 0.01, 556: 0.03, 607: 0.36, 722: 0.01, 782: 0.01, 805: 0.13, 811: 0.03, 817: 0.02, 854: 0.04, 902: 0.01, 917: 0.02, 920: 0.02, ]\n",
      "func:cos_distance, ap_dist: -0.9978961944580078, an_dist: -0.9229906797409058\n",
      "a: [591: 0.02, 712: 0.01, 809: 0.17, 828: 0.46, 904: 0.15, 923: 0.03, 925: 0.02, ]\n",
      "p: [591: 0.01, 700: 0.01, 712: 0.01, 809: 0.23, 828: 0.45, 904: 0.11, 923: 0.04, 925: 0.03, ]\n",
      "n: [75: 0.01, 591: 0.02, 700: 0.02, 712: 0.01, 809: 0.22, 828: 0.41, 904: 0.14, 923: 0.03, 925: 0.02, ]\n",
      "func:cos_distance, ap_dist: -0.9865810871124268, an_dist: -0.8195617198944092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [179: 0.14, 180: 0.69, 238: 0.05, 242: 0.05, 253: 0.02, ]\n",
      "p: [179: 0.15, 180: 0.70, 238: 0.05, 242: 0.04, 253: 0.02, ]\n",
      "n: [179: 0.15, 180: 0.69, 238: 0.05, 242: 0.04, 253: 0.02, ]\n",
      "func:cos_distance, ap_dist: -0.9998310804367065, an_dist: -0.9315780997276306\n",
      "a: [407: 0.02, 654: 0.05, 734: 0.91, ]\n",
      "p: [407: 0.02, 654: 0.05, 734: 0.91, ]\n",
      "n: [407: 0.02, 654: 0.06, 734: 0.90, ]\n",
      "func:cos_distance, ap_dist: -0.9997478723526001, an_dist: -0.9310113787651062\n",
      "a: [55: 0.93, 64: 0.05, ]\n",
      "p: [55: 0.93, 64: 0.05, ]\n",
      "n: [55: 0.93, 64: 0.06, ]\n",
      "func:cos_distance, ap_dist: -0.9990735054016113, an_dist: -0.9208412170410156\n",
      "a: [269: 0.04, 271: 0.34, 272: 0.01, 273: 0.02, 274: 0.02, 348: 0.03, 349: 0.03, 368: 0.08, 383: 0.01, 387: 0.02, 904: 0.26, ]\n",
      "p: [269: 0.06, 271: 0.44, 272: 0.02, 273: 0.02, 274: 0.03, 348: 0.03, 349: 0.02, 368: 0.06, 387: 0.02, 904: 0.18, ]\n",
      "n: [269: 0.05, 271: 0.34, 272: 0.02, 273: 0.02, 274: 0.02, 348: 0.03, 349: 0.02, 368: 0.06, 383: 0.01, 387: 0.01, 904: 0.28, ]\n",
      "func:cos_distance, ap_dist: -0.9968596696853638, an_dist: -0.8694596290588379\n",
      "a: [159: 0.03, 170: 0.04, 172: 0.09, 173: 0.04, 176: 0.17, 177: 0.08, 179: 0.06, 180: 0.04, 195: 0.01, 205: 0.02, 206: 0.01, 208: 0.05, 213: 0.01, 227: 0.08, 232: 0.03, 262: 0.02, 676: 0.01, 805: 0.01, ]\n",
      "p: [159: 0.03, 170: 0.04, 172: 0.09, 173: 0.03, 176: 0.16, 177: 0.08, 179: 0.08, 180: 0.05, 195: 0.01, 205: 0.03, 206: 0.01, 208: 0.06, 213: 0.01, 227: 0.08, 232: 0.03, 262: 0.01, 676: 0.01, 805: 0.01, ]\n",
      "n: [159: 0.04, 170: 0.06, 172: 0.16, 173: 0.03, 176: 0.15, 177: 0.07, 179: 0.12, 180: 0.13, 195: 0.01, 206: 0.01, 208: 0.02, 227: 0.02, 232: 0.01, 246: 0.02, 676: 0.02, ]\n",
      "func:cos_distance, ap_dist: -0.9977999925613403, an_dist: -0.9187108278274536\n"
     ]
    }
   ],
   "source": [
    "if mode == \"sanity_check\":\n",
    "  print(\"\\n\\n\\nWARNING: you are training on a sanity_check dataset.\\n\\n\\n\\n\")\n",
    "\n",
    "saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=env.save_filename + \"-best\")\n",
    "saver_every_epoch = SaveModelCallback(learn, every='epoch', name=env.save_filename)\n",
    "\n",
    "# with Hooks(gen, append_stats_normal) as hooks:\n",
    "#   learn.fit(1, lr=5e-03, wd = 0., callbacks=[saver_best, saver_every_epoch])\n",
    "  \n",
    "# 40 is too much, use just 20\n",
    "learn.fit(40, lr=1e-2, wd = 0.001, callbacks=[saver_best, saver_every_epoch])\n",
    "\n",
    "# # learn.fit(70, lr=1e-02, wd = 0.001, callbacks=[saver_best, saver_every_epoch])\n",
    "\n",
    "# learn.fit(60, lr=1e-2, wd = 0.001, callbacks=[saver_best, saver_every_epoch])\n",
    "\n",
    "# for i in range(10):\n",
    "#   learn.fit_one_cycle(7, wd = 0.,max_lr=1., div_factor = 1000.) \n",
    "  \n",
    "# learn.fit_one_cycle(5, max_lr=2e-2) #used for vgg-19-bn\n",
    "# learn.fit_one_cycle(5, max_lr=3e-3) # used for resnet50\n",
    "\n",
    "shutil.copyfile(env.temp_csv_path + '/' + env.save_filename + \".csv\", env.get_csv_path() + '.csv')\n",
    "shutil.copytree(env.data_path/env.get_learner_models_dir(), env.get_models_path())\n",
    "shutil.rmtree(env.data_path/env.get_learner_models_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram\n",
    "fig, axes = plt.subplots(len(hooks),1, figsize=(30,12))\n",
    "for ax,h in zip(axes.flatten(), hooks):\n",
    "  ax.imshow(get_hist(h), origin='lower')\n",
    "  ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mean and std\n",
    "fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "for h in hooks:\n",
    "  ms, ss, _ = h.stats\n",
    "  ax0.plot(ms[:100])\n",
    "  ax1.plot(ss[:100])\n",
    "plt.legend(range(len(hooks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "for h in hooks:\n",
    "  ms, ss, _ = h.stats\n",
    "  ax0.plot(ms)\n",
    "  ax1.plot(ss)\n",
    "plt.legend(range(len(hooks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero precentage:\n",
    "fig,axes = plt.subplots(len(hooks),1, figsize=(30,30))\n",
    "for ax,h in zip(axes.flatten(), hooks):\n",
    "    ax.plot(get_min(h))\n",
    "    ax.set_ylim(0,1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MO2fZ-hSSUzJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# z1 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "# z2 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "z1 = torch.tensor([0.8, -0.5] * 5).cuda()\n",
    "z2 = torch.tensor([-1.] * 10).cuda()\n",
    "print(\"z1: \", z1)\n",
    "print(\"z2: \", z2)\n",
    "print(\"distance: \", torch.norm(z1-z2,p=2))\n",
    "model = learn.model.eval()\n",
    "\n",
    "z_s = interpolate(z1, z2, 0.1)\n",
    "print(len(z_s))\n",
    "\n",
    "for i,z in enumerate(z_s):\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n",
    "  #img.save('./pics/' + str(i) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DGuGN7B7V0Xt"
   },
   "outputs": [],
   "source": [
    "def generate_perturbations(learn, n_perturbations):\n",
    "  initial_training_mode = learn.model.training\n",
    "  \n",
    "  model = learn.model.eval()\n",
    "  input_img = (learn.data.valid_ds[0][0].data)[None].cuda()\n",
    "  perturbations = []\n",
    "  for i in range(n_perturbations):\n",
    "    perturbation = model(input_img)[0].squeeze()\n",
    "    perturbations.append(perturbation)\n",
    "    \n",
    "  learn.model.train(initial_training_mode)  \n",
    "  return perturbations\n",
    "\n",
    "def compute_prediction_histogram(learn, perturbation, verbose=False):\n",
    "  pred_hist = [0] * 1000\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 and verbose: print (\"at batch no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbation[None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      pred_hist[pred] += 1\n",
    "  return pred_hist\n",
    "\n",
    "\n",
    "def compute_mean_prediction_histogram(learn, perturbations):\n",
    "  pred_histogram = torch.tensor([0] * 1000).detach_()\n",
    "  for j, perturbation in enumerate(perturbations):\n",
    "    pred_histogram_j = torch.tensor(compute_prediction_histogram(learn, perturbation, True)).detach_()\n",
    "    pred_histogram += pred_histogram_j\n",
    "    print(\"finished creating histogram for the {}th perturbation\".format(j))\n",
    "  \n",
    "  pred_histogram = pred_histogram.float() / len(perturbations)\n",
    "  return pred_histogram.tolist()\n",
    "\n",
    "\n",
    "def diversity(learn, n_perturbations, percentage = 95):\n",
    "  pred_histogram = compute_mean_prediction_histogram(\n",
    "      learn, generate_perturbations(learn, n_perturbations)\n",
    "  )\n",
    "  print(\"finished creating the prediction histogram\")\n",
    "  pred_histogram_sum = np.sum(pred_histogram)\n",
    "\n",
    "  indexed_pred_histogram = [(i, hist_element) for i,hist_element in  \n",
    "                            enumerate(pred_histogram)]\n",
    "\n",
    "  indexed_pred_histogram.sort(key=lambda x: x[1], reverse = True)\n",
    "\n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  top_classes = []\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = indexed_pred_histogram[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / pred_histogram_sum) * 100.\n",
    "    top_classes.append(hist_elem[0])\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, indexed_pred_histogram, top_classes\n",
    "\n",
    "# idea : have 200 noises (1 for each class), then start iterating the dataset, and for each image, randomly apply one noise and record the result\n",
    "def targeted_diversity(learn, n_perturbations = 200, percentage = 95):\n",
    "  model = learn.model.eval()\n",
    "\n",
    "  one_hot_conditions = [torch.empty(z_dim).uniform_(0,1).cuda().detach() for _ in range(n_perturbations)]\n",
    "#   for i in range(z_dim):\n",
    "#     one_hot_conditions[i][i] = 1.\n",
    "\n",
    "  perturbations = [model.forward_single_z(z) for z in one_hot_conditions]\n",
    "\n",
    "  hist = [0.] * z_dim\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 : print(\"at batch_no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbations[np.random.randint(0,len(perturbations))][None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      hist[pred] += 1\n",
    "\n",
    "  pred_histogram_sum = np.sum(hist)\n",
    "  indexed_pred_histogram = [(i, hist_element) for i,hist_element in  \n",
    "                            enumerate(hist)]\n",
    "\n",
    "  indexed_pred_histogram.sort(key=lambda x: x[1], reverse = True)\n",
    "\n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = indexed_pred_histogram[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / pred_histogram_sum) * 100.\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, indexed_pred_histogram\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "finished creating histogram for the 0th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 7th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 8th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 9th perturbation\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(409,\n",
       " [(971, 62.10),\n",
       "  (55, 57.60),\n",
       "  (669, 45.30),\n",
       "  (828, 34.80),\n",
       "  (39, 30.00),\n",
       "  (61, 27.20),\n",
       "  (556, 20.30),\n",
       "  (84, 19.70),\n",
       "  (721, 18.40),\n",
       "  (401, 16.50),\n",
       "  (770, 14.30),\n",
       "  (490, 12.50),\n",
       "  (489, 11.30),\n",
       "  (62, 10.40),\n",
       "  (599, 9.90),\n",
       "  (815, 9.40),\n",
       "  (824, 9.40),\n",
       "  (588, 9.00),\n",
       "  (955, 7.70),\n",
       "  (63, 7.30),\n",
       "  (794, 7.30),\n",
       "  (419, 6.80),\n",
       "  (750, 6.50),\n",
       "  (904, 6.50),\n",
       "  (711, 5.60),\n",
       "  (116, 5.40),\n",
       "  (584, 4.90),\n",
       "  (464, 4.40),\n",
       "  (48, 4.20),\n",
       "  (319, 4.20),\n",
       "  (709, 4.10),\n",
       "  (581, 4.00),\n",
       "  (762, 3.90),\n",
       "  (552, 3.60),\n",
       "  (641, 3.50),\n",
       "  (843, 3.40),\n",
       "  (108, 3.30),\n",
       "  (155, 3.30),\n",
       "  (646, 3.30),\n",
       "  (572, 3.20),\n",
       "  (806, 3.20),\n",
       "  (893, 3.20),\n",
       "  (46, 3.10),\n",
       "  (151, 3.10),\n",
       "  (973, 3.10),\n",
       "  (94, 3.00),\n",
       "  (651, 3.00),\n",
       "  (414, 2.90),\n",
       "  (431, 2.80),\n",
       "  (40, 2.60),\n",
       "  (60, 2.60),\n",
       "  (193, 2.60),\n",
       "  (562, 2.60),\n",
       "  (621, 2.60),\n",
       "  (868, 2.60),\n",
       "  (124, 2.50),\n",
       "  (300, 2.50),\n",
       "  (509, 2.50),\n",
       "  (735, 2.50),\n",
       "  (109, 2.40),\n",
       "  (879, 2.40),\n",
       "  (1, 2.30),\n",
       "  (692, 2.30),\n",
       "  (539, 2.20),\n",
       "  (579, 2.20),\n",
       "  (741, 2.20),\n",
       "  (334, 2.10),\n",
       "  (406, 2.10),\n",
       "  (457, 2.10),\n",
       "  (520, 2.10),\n",
       "  (620, 2.10),\n",
       "  (703, 2.10),\n",
       "  (865, 2.10),\n",
       "  (42, 2.00),\n",
       "  (68, 2.00),\n",
       "  (97, 2.00),\n",
       "  (292, 2.00),\n",
       "  (850, 2.00),\n",
       "  (872, 2.00),\n",
       "  (118, 1.90),\n",
       "  (545, 1.90),\n",
       "  (580, 1.90),\n",
       "  (790, 1.80),\n",
       "  (826, 1.80),\n",
       "  (982, 1.80),\n",
       "  (107, 1.70),\n",
       "  (128, 1.70),\n",
       "  (230, 1.70),\n",
       "  (247, 1.70),\n",
       "  (360, 1.70),\n",
       "  (440, 1.70),\n",
       "  (604, 1.70),\n",
       "  (906, 1.70),\n",
       "  (907, 1.70),\n",
       "  (963, 1.70),\n",
       "  (7, 1.60),\n",
       "  (90, 1.60),\n",
       "  (353, 1.60),\n",
       "  (538, 1.60),\n",
       "  (575, 1.60),\n",
       "  (582, 1.60),\n",
       "  (611, 1.60),\n",
       "  (786, 1.60),\n",
       "  (808, 1.60),\n",
       "  (819, 1.60),\n",
       "  (892, 1.60),\n",
       "  (38, 1.50),\n",
       "  (205, 1.50),\n",
       "  (304, 1.50),\n",
       "  (482, 1.50),\n",
       "  (638, 1.50),\n",
       "  (674, 1.50),\n",
       "  (777, 1.50),\n",
       "  (783, 1.50),\n",
       "  (864, 1.50),\n",
       "  (871, 1.50),\n",
       "  (987, 1.50),\n",
       "  (76, 1.40),\n",
       "  (123, 1.40),\n",
       "  (176, 1.40),\n",
       "  (348, 1.40),\n",
       "  (411, 1.40),\n",
       "  (474, 1.40),\n",
       "  (496, 1.40),\n",
       "  (506, 1.40),\n",
       "  (518, 1.40),\n",
       "  (593, 1.40),\n",
       "  (854, 1.40),\n",
       "  (64, 1.30),\n",
       "  (472, 1.30),\n",
       "  (508, 1.30),\n",
       "  (527, 1.30),\n",
       "  (614, 1.30),\n",
       "  (624, 1.30),\n",
       "  (839, 1.30),\n",
       "  (855, 1.30),\n",
       "  (880, 1.30),\n",
       "  (881, 1.30),\n",
       "  (944, 1.30),\n",
       "  (981, 1.30),\n",
       "  (996, 1.30),\n",
       "  (0, 1.20),\n",
       "  (96, 1.20),\n",
       "  (218, 1.20),\n",
       "  (275, 1.20),\n",
       "  (294, 1.20),\n",
       "  (318, 1.20),\n",
       "  (336, 1.20),\n",
       "  (365, 1.20),\n",
       "  (393, 1.20),\n",
       "  (396, 1.20),\n",
       "  (398, 1.20),\n",
       "  (424, 1.20),\n",
       "  (425, 1.20),\n",
       "  (441, 1.20),\n",
       "  (492, 1.20),\n",
       "  (515, 1.20),\n",
       "  (595, 1.20),\n",
       "  (698, 1.20),\n",
       "  (717, 1.20),\n",
       "  (791, 1.20),\n",
       "  (801, 1.20),\n",
       "  (830, 1.20),\n",
       "  (934, 1.20),\n",
       "  (45, 1.10),\n",
       "  (115, 1.10),\n",
       "  (136, 1.10),\n",
       "  (242, 1.10),\n",
       "  (274, 1.10),\n",
       "  (281, 1.10),\n",
       "  (340, 1.10),\n",
       "  (355, 1.10),\n",
       "  (363, 1.10),\n",
       "  (430, 1.10),\n",
       "  (468, 1.10),\n",
       "  (480, 1.10),\n",
       "  (484, 1.10),\n",
       "  (498, 1.10),\n",
       "  (633, 1.10),\n",
       "  (800, 1.10),\n",
       "  (858, 1.10),\n",
       "  (8, 1.00),\n",
       "  (17, 1.00),\n",
       "  (25, 1.00),\n",
       "  (28, 1.00),\n",
       "  (33, 1.00),\n",
       "  (37, 1.00),\n",
       "  (52, 1.00),\n",
       "  (57, 1.00),\n",
       "  (83, 1.00),\n",
       "  (86, 1.00),\n",
       "  (91, 1.00),\n",
       "  (92, 1.00),\n",
       "  (120, 1.00),\n",
       "  (140, 1.00),\n",
       "  (141, 1.00),\n",
       "  (144, 1.00),\n",
       "  (163, 1.00),\n",
       "  (164, 1.00),\n",
       "  (189, 1.00),\n",
       "  (198, 1.00),\n",
       "  (206, 1.00),\n",
       "  (226, 1.00),\n",
       "  (231, 1.00),\n",
       "  (235, 1.00),\n",
       "  (260, 1.00),\n",
       "  (284, 1.00),\n",
       "  (289, 1.00),\n",
       "  (290, 1.00),\n",
       "  (291, 1.00),\n",
       "  (293, 1.00),\n",
       "  (301, 1.00),\n",
       "  (306, 1.00),\n",
       "  (307, 1.00),\n",
       "  (314, 1.00),\n",
       "  (316, 1.00),\n",
       "  (321, 1.00),\n",
       "  (327, 1.00),\n",
       "  (330, 1.00),\n",
       "  (342, 1.00),\n",
       "  (347, 1.00),\n",
       "  (376, 1.00),\n",
       "  (381, 1.00),\n",
       "  (391, 1.00),\n",
       "  (392, 1.00),\n",
       "  (397, 1.00),\n",
       "  (417, 1.00),\n",
       "  (429, 1.00),\n",
       "  (433, 1.00),\n",
       "  (443, 1.00),\n",
       "  (477, 1.00),\n",
       "  (488, 1.00),\n",
       "  (491, 1.00),\n",
       "  (507, 1.00),\n",
       "  (528, 1.00),\n",
       "  (530, 1.00),\n",
       "  (533, 1.00),\n",
       "  (555, 1.00),\n",
       "  (566, 1.00),\n",
       "  (567, 1.00),\n",
       "  (570, 1.00),\n",
       "  (576, 1.00),\n",
       "  (602, 1.00),\n",
       "  (609, 1.00),\n",
       "  (612, 1.00),\n",
       "  (625, 1.00),\n",
       "  (629, 1.00),\n",
       "  (637, 1.00),\n",
       "  (645, 1.00),\n",
       "  (661, 1.00),\n",
       "  (684, 1.00),\n",
       "  (694, 1.00),\n",
       "  (705, 1.00),\n",
       "  (710, 1.00),\n",
       "  (716, 1.00),\n",
       "  (722, 1.00),\n",
       "  (727, 1.00),\n",
       "  (734, 1.00),\n",
       "  (738, 1.00),\n",
       "  (746, 1.00),\n",
       "  (753, 1.00),\n",
       "  (759, 1.00),\n",
       "  (775, 1.00),\n",
       "  (781, 1.00),\n",
       "  (787, 1.00),\n",
       "  (803, 1.00),\n",
       "  (816, 1.00),\n",
       "  (829, 1.00),\n",
       "  (847, 1.00),\n",
       "  (857, 1.00),\n",
       "  (873, 1.00),\n",
       "  (882, 1.00),\n",
       "  (890, 1.00),\n",
       "  (923, 1.00),\n",
       "  (937, 1.00),\n",
       "  (953, 1.00),\n",
       "  (957, 1.00),\n",
       "  (992, 1.00),\n",
       "  (997, 1.00),\n",
       "  (102, 0.90),\n",
       "  (105, 0.90),\n",
       "  (132, 0.90),\n",
       "  (158, 0.90),\n",
       "  (168, 0.90),\n",
       "  (197, 0.90),\n",
       "  (237, 0.90),\n",
       "  (313, 0.90),\n",
       "  (389, 0.90),\n",
       "  (402, 0.90),\n",
       "  (428, 0.90),\n",
       "  (483, 0.90),\n",
       "  (594, 0.90),\n",
       "  (654, 0.90),\n",
       "  (655, 0.90),\n",
       "  (679, 0.90),\n",
       "  (732, 0.90),\n",
       "  (751, 0.90),\n",
       "  (784, 0.90),\n",
       "  (788, 0.90),\n",
       "  (884, 0.90),\n",
       "  (939, 0.90),\n",
       "  (53, 0.80),\n",
       "  (75, 0.80),\n",
       "  (171, 0.80),\n",
       "  (188, 0.80),\n",
       "  (219, 0.80),\n",
       "  (271, 0.80),\n",
       "  (375, 0.80),\n",
       "  (407, 0.80),\n",
       "  (432, 0.80),\n",
       "  (444, 0.80),\n",
       "  (453, 0.80),\n",
       "  (517, 0.80),\n",
       "  (535, 0.80),\n",
       "  (549, 0.80),\n",
       "  (565, 0.80),\n",
       "  (586, 0.80),\n",
       "  (607, 0.80),\n",
       "  (664, 0.80),\n",
       "  (728, 0.80),\n",
       "  (729, 0.80),\n",
       "  (754, 0.80),\n",
       "  (772, 0.80),\n",
       "  (814, 0.80),\n",
       "  (831, 0.80),\n",
       "  (863, 0.80),\n",
       "  (889, 0.80),\n",
       "  (984, 0.80),\n",
       "  (47, 0.70),\n",
       "  (65, 0.70),\n",
       "  (74, 0.70),\n",
       "  (113, 0.70),\n",
       "  (134, 0.70),\n",
       "  (204, 0.70),\n",
       "  (236, 0.70),\n",
       "  (249, 0.70),\n",
       "  (256, 0.70),\n",
       "  (308, 0.70),\n",
       "  (331, 0.70),\n",
       "  (337, 0.70),\n",
       "  (387, 0.70),\n",
       "  (442, 0.70),\n",
       "  (447, 0.70),\n",
       "  (454, 0.70),\n",
       "  (668, 0.70),\n",
       "  (672, 0.70),\n",
       "  (707, 0.70),\n",
       "  (725, 0.70),\n",
       "  (768, 0.70),\n",
       "  (817, 0.70),\n",
       "  (820, 0.70),\n",
       "  (837, 0.70),\n",
       "  (874, 0.70),\n",
       "  (878, 0.70),\n",
       "  (900, 0.70),\n",
       "  (918, 0.70),\n",
       "  (920, 0.70),\n",
       "  (19, 0.60),\n",
       "  (24, 0.60),\n",
       "  (50, 0.60),\n",
       "  (138, 0.60),\n",
       "  (160, 0.60),\n",
       "  (178, 0.60),\n",
       "  (186, 0.60),\n",
       "  (213, 0.60),\n",
       "  (305, 0.60),\n",
       "  (395, 0.60),\n",
       "  (445, 0.60),\n",
       "  (485, 0.60),\n",
       "  (505, 0.60),\n",
       "  (546, 0.60),\n",
       "  (616, 0.60),\n",
       "  (619, 0.60),\n",
       "  (642, 0.60),\n",
       "  (644, 0.60),\n",
       "  (650, 0.60),\n",
       "  (670, 0.60),\n",
       "  (700, 0.60),\n",
       "  (701, 0.60),\n",
       "  (706, 0.60),\n",
       "  (737, 0.60),\n",
       "  (743, 0.60),\n",
       "  (745, 0.60),\n",
       "  (766, 0.60),\n",
       "  (821, 0.60),\n",
       "  (832, 0.60),\n",
       "  (852, 0.60),\n",
       "  (867, 0.60),\n",
       "  (870, 0.60),\n",
       "  (915, 0.60),\n",
       "  (976, 0.60),\n",
       "  (985, 0.60),\n",
       "  (9, 0.50),\n",
       "  (15, 0.50),\n",
       "  (31, 0.50),\n",
       "  (72, 0.50),\n",
       "  (87, 0.50),\n",
       "  (195, 0.50),\n",
       "  (228, 0.50),\n",
       "  (323, 0.50),\n",
       "  (343, 0.50),\n",
       "  (394, 0.50),\n",
       "  (408, 0.50),\n",
       "  (481, 0.50),\n",
       "  (495, 0.50),\n",
       "  (514, 0.50),\n",
       "  (526, 0.50),\n",
       "  (544, 0.50),\n",
       "  (564, 0.50),\n",
       "  (591, 0.50),\n",
       "  (627, 0.50),\n",
       "  (639, 0.50),\n",
       "  (687, 0.50),\n",
       "  (758, 0.50),\n",
       "  (822, 0.50),\n",
       "  (894, 0.50),\n",
       "  (898, 0.50),\n",
       "  (917, 0.50),\n",
       "  (921, 0.50),\n",
       "  (946, 0.50),\n",
       "  (21, 0.40),\n",
       "  (129, 0.40),\n",
       "  (191, 0.40),\n",
       "  (211, 0.40),\n",
       "  (214, 0.40),\n",
       "  (238, 0.40),\n",
       "  (273, 0.40),\n",
       "  (309, 0.40),\n",
       "  (350, 0.40),\n",
       "  (354, 0.40),\n",
       "  (358, 0.40),\n",
       "  (388, 0.40),\n",
       "  (399, 0.40),\n",
       "  (410, 0.40),\n",
       "  (423, 0.40),\n",
       "  (632, 0.40),\n",
       "  (643, 0.40),\n",
       "  (656, 0.40),\n",
       "  (691, 0.40),\n",
       "  (699, 0.40),\n",
       "  (764, 0.40),\n",
       "  (811, 0.40),\n",
       "  (833, 0.40),\n",
       "  (886, 0.40),\n",
       "  (888, 0.40),\n",
       "  (99, 0.30),\n",
       "  (100, 0.30),\n",
       "  (110, 0.30),\n",
       "  (182, 0.30),\n",
       "  (229, 0.30),\n",
       "  (234, 0.30),\n",
       "  (252, 0.30),\n",
       "  (265, 0.30),\n",
       "  (276, 0.30),\n",
       "  (298, 0.30),\n",
       "  (310, 0.30),\n",
       "  (344, 0.30),\n",
       "  (361, 0.30),\n",
       "  (367, 0.30),\n",
       "  (369, 0.30),\n",
       "  (374, 0.30),\n",
       "  (384, 0.30),\n",
       "  (409, 0.30),\n",
       "  (412, 0.30),\n",
       "  (420, 0.30),\n",
       "  (436, 0.30),\n",
       "  (459, 0.30),\n",
       "  (476, 0.30),\n",
       "  (523, 0.30),\n",
       "  (529, 0.30),\n",
       "  (560, 0.30),\n",
       "  (573, 0.30),\n",
       "  (585, 0.30),\n",
       "  (606, 0.30),\n",
       "  (640, 0.30),\n",
       "  (676, 0.30),\n",
       "  (733, 0.30),\n",
       "  (748, 0.30),\n",
       "  (752, 0.30),\n",
       "  (774, 0.30),\n",
       "  (796, 0.30),\n",
       "  (805, 0.30),\n",
       "  (809, 0.30),\n",
       "  (823, 0.30),\n",
       "  (842, 0.30),\n",
       "  (853, 0.30),\n",
       "  (949, 0.30),\n",
       "  (962, 0.30),\n",
       "  (970, 0.30),\n",
       "  (51, 0.20),\n",
       "  (67, 0.20),\n",
       "  (71, 0.20),\n",
       "  (82, 0.20),\n",
       "  (85, 0.20),\n",
       "  (98, 0.20),\n",
       "  (125, 0.20),\n",
       "  (146, 0.20),\n",
       "  (162, 0.20),\n",
       "  (166, 0.20),\n",
       "  (173, 0.20),\n",
       "  (181, 0.20),\n",
       "  (192, 0.20),\n",
       "  (199, 0.20),\n",
       "  (222, 0.20),\n",
       "  (254, 0.20),\n",
       "  (315, 0.20),\n",
       "  (341, 0.20),\n",
       "  (345, 0.20),\n",
       "  (349, 0.20),\n",
       "  (386, 0.20),\n",
       "  (415, 0.20),\n",
       "  (467, 0.20),\n",
       "  (486, 0.20),\n",
       "  (497, 0.20),\n",
       "  (502, 0.20),\n",
       "  (531, 0.20),\n",
       "  (537, 0.20),\n",
       "  (541, 0.20),\n",
       "  (558, 0.20),\n",
       "  (613, 0.20),\n",
       "  (671, 0.20),\n",
       "  (680, 0.20),\n",
       "  (696, 0.20),\n",
       "  (697, 0.20),\n",
       "  (730, 0.20),\n",
       "  (742, 0.20),\n",
       "  (756, 0.20),\n",
       "  (813, 0.20),\n",
       "  (825, 0.20),\n",
       "  (846, 0.20),\n",
       "  (876, 0.20),\n",
       "  (885, 0.20),\n",
       "  (899, 0.20),\n",
       "  (901, 0.20),\n",
       "  (910, 0.20),\n",
       "  (911, 0.20),\n",
       "  (950, 0.20),\n",
       "  (952, 0.20),\n",
       "  (959, 0.20),\n",
       "  (11, 0.10),\n",
       "  (12, 0.10),\n",
       "  (14, 0.10),\n",
       "  (20, 0.10),\n",
       "  (23, 0.10),\n",
       "  (35, 0.10),\n",
       "  (44, 0.10),\n",
       "  (56, 0.10),\n",
       "  (58, 0.10),\n",
       "  (93, 0.10),\n",
       "  (95, 0.10),\n",
       "  (112, 0.10),\n",
       "  (117, 0.10),\n",
       "  (130, 0.10),\n",
       "  (131, 0.10),\n",
       "  (135, 0.10),\n",
       "  (139, 0.10),\n",
       "  (145, 0.10),\n",
       "  (148, 0.10),\n",
       "  (172, 0.10),\n",
       "  (177, 0.10),\n",
       "  (183, 0.10),\n",
       "  (184, 0.10),\n",
       "  (187, 0.10),\n",
       "  (196, 0.10),\n",
       "  (207, 0.10),\n",
       "  (208, 0.10),\n",
       "  (224, 0.10),\n",
       "  (232, 0.10),\n",
       "  (241, 0.10),\n",
       "  (250, 0.10),\n",
       "  (251, 0.10),\n",
       "  (263, 0.10),\n",
       "  (264, 0.10),\n",
       "  (266, 0.10),\n",
       "  (267, 0.10),\n",
       "  (269, 0.10),\n",
       "  (270, 0.10),\n",
       "  (280, 0.10),\n",
       "  (285, 0.10),\n",
       "  (295, 0.10),\n",
       "  (296, 0.10),\n",
       "  (311, 0.10),\n",
       "  (317, 0.10),\n",
       "  (320, 0.10),\n",
       "  (322, 0.10),\n",
       "  (326, 0.10),\n",
       "  (335, 0.10),\n",
       "  (351, 0.10),\n",
       "  (364, 0.10),\n",
       "  (366, 0.10),\n",
       "  (377, 0.10),\n",
       "  (378, 0.10),\n",
       "  (379, 0.10),\n",
       "  (383, 0.10),\n",
       "  (413, 0.10),\n",
       "  (427, 0.10),\n",
       "  (438, 0.10),\n",
       "  (450, 0.10),\n",
       "  (458, 0.10),\n",
       "  (463, 0.10),\n",
       "  (470, 0.10),\n",
       "  (479, 0.10),\n",
       "  (487, 0.10),\n",
       "  (503, 0.10),\n",
       "  (511, 0.10),\n",
       "  (519, 0.10),\n",
       "  (522, 0.10),\n",
       "  (524, 0.10),\n",
       "  (532, 0.10),\n",
       "  (547, 0.10),\n",
       "  (561, 0.10),\n",
       "  (563, 0.10),\n",
       "  (578, 0.10),\n",
       "  (603, 0.10),\n",
       "  (605, 0.10),\n",
       "  (608, 0.10),\n",
       "  (618, 0.10),\n",
       "  (622, 0.10),\n",
       "  (636, 0.10),\n",
       "  (658, 0.10),\n",
       "  (662, 0.10),\n",
       "  (667, 0.10),\n",
       "  (681, 0.10),\n",
       "  (695, 0.10),\n",
       "  (719, 0.10),\n",
       "  (723, 0.10),\n",
       "  (724, 0.10),\n",
       "  (739, 0.10),\n",
       "  (755, 0.10),\n",
       "  (757, 0.10),\n",
       "  (761, 0.10),\n",
       "  (765, 0.10),\n",
       "  (779, 0.10),\n",
       "  (782, 0.10),\n",
       "  (793, 0.10),\n",
       "  (807, 0.10),\n",
       "  (845, 0.10),\n",
       "  (848, 0.10),\n",
       "  (859, 0.10),\n",
       "  (861, 0.10),\n",
       "  (866, 0.10),\n",
       "  (877, 0.10),\n",
       "  (919, 0.10),\n",
       "  (932, 0.10),\n",
       "  (936, 0.10),\n",
       "  (958, 0.10),\n",
       "  (968, 0.10),\n",
       "  (972, 0.10),\n",
       "  (978, 0.10),\n",
       "  (989, 0.10),\n",
       "  (2, 0.00),\n",
       "  (3, 0.00),\n",
       "  (4, 0.00),\n",
       "  (5, 0.00),\n",
       "  (6, 0.00),\n",
       "  (10, 0.00),\n",
       "  (13, 0.00),\n",
       "  (16, 0.00),\n",
       "  (18, 0.00),\n",
       "  (22, 0.00),\n",
       "  (26, 0.00),\n",
       "  (27, 0.00),\n",
       "  (29, 0.00),\n",
       "  (30, 0.00),\n",
       "  (32, 0.00),\n",
       "  (34, 0.00),\n",
       "  (36, 0.00),\n",
       "  (41, 0.00),\n",
       "  (43, 0.00),\n",
       "  (49, 0.00),\n",
       "  (54, 0.00),\n",
       "  (59, 0.00),\n",
       "  (66, 0.00),\n",
       "  (69, 0.00),\n",
       "  (70, 0.00),\n",
       "  (73, 0.00),\n",
       "  (77, 0.00),\n",
       "  (78, 0.00),\n",
       "  (79, 0.00),\n",
       "  (80, 0.00),\n",
       "  (81, 0.00),\n",
       "  (88, 0.00),\n",
       "  (89, 0.00),\n",
       "  (101, 0.00),\n",
       "  (103, 0.00),\n",
       "  (104, 0.00),\n",
       "  (106, 0.00),\n",
       "  (111, 0.00),\n",
       "  (114, 0.00),\n",
       "  (119, 0.00),\n",
       "  (121, 0.00),\n",
       "  (122, 0.00),\n",
       "  (126, 0.00),\n",
       "  (127, 0.00),\n",
       "  (133, 0.00),\n",
       "  (137, 0.00),\n",
       "  (142, 0.00),\n",
       "  (143, 0.00),\n",
       "  (147, 0.00),\n",
       "  (149, 0.00),\n",
       "  (150, 0.00),\n",
       "  (152, 0.00),\n",
       "  (153, 0.00),\n",
       "  (154, 0.00),\n",
       "  (156, 0.00),\n",
       "  (157, 0.00),\n",
       "  (159, 0.00),\n",
       "  (161, 0.00),\n",
       "  (165, 0.00),\n",
       "  (167, 0.00),\n",
       "  (169, 0.00),\n",
       "  (170, 0.00),\n",
       "  (174, 0.00),\n",
       "  (175, 0.00),\n",
       "  (179, 0.00),\n",
       "  (180, 0.00),\n",
       "  (185, 0.00),\n",
       "  (190, 0.00),\n",
       "  (194, 0.00),\n",
       "  (200, 0.00),\n",
       "  (201, 0.00),\n",
       "  (202, 0.00),\n",
       "  (203, 0.00),\n",
       "  (209, 0.00),\n",
       "  (210, 0.00),\n",
       "  (212, 0.00),\n",
       "  (215, 0.00),\n",
       "  (216, 0.00),\n",
       "  (217, 0.00),\n",
       "  (220, 0.00),\n",
       "  (221, 0.00),\n",
       "  (223, 0.00),\n",
       "  (225, 0.00),\n",
       "  (227, 0.00),\n",
       "  (233, 0.00),\n",
       "  (239, 0.00),\n",
       "  (240, 0.00),\n",
       "  (243, 0.00),\n",
       "  (244, 0.00),\n",
       "  (245, 0.00),\n",
       "  (246, 0.00),\n",
       "  (248, 0.00),\n",
       "  (253, 0.00),\n",
       "  (255, 0.00),\n",
       "  (257, 0.00),\n",
       "  (258, 0.00),\n",
       "  (259, 0.00),\n",
       "  (261, 0.00),\n",
       "  (262, 0.00),\n",
       "  (268, 0.00),\n",
       "  (272, 0.00),\n",
       "  (277, 0.00),\n",
       "  (278, 0.00),\n",
       "  (279, 0.00),\n",
       "  (282, 0.00),\n",
       "  (283, 0.00),\n",
       "  (286, 0.00),\n",
       "  (287, 0.00),\n",
       "  (288, 0.00),\n",
       "  (297, 0.00),\n",
       "  (299, 0.00),\n",
       "  (302, 0.00),\n",
       "  (303, 0.00),\n",
       "  (312, 0.00),\n",
       "  (324, 0.00),\n",
       "  (325, 0.00),\n",
       "  (328, 0.00),\n",
       "  (329, 0.00),\n",
       "  (332, 0.00),\n",
       "  (333, 0.00),\n",
       "  (338, 0.00),\n",
       "  (339, 0.00),\n",
       "  (346, 0.00),\n",
       "  (352, 0.00),\n",
       "  (356, 0.00),\n",
       "  (357, 0.00),\n",
       "  (359, 0.00),\n",
       "  (362, 0.00),\n",
       "  (368, 0.00),\n",
       "  (370, 0.00),\n",
       "  (371, 0.00),\n",
       "  (372, 0.00),\n",
       "  (373, 0.00),\n",
       "  (380, 0.00),\n",
       "  (382, 0.00),\n",
       "  (385, 0.00),\n",
       "  (390, 0.00),\n",
       "  (400, 0.00),\n",
       "  (403, 0.00),\n",
       "  (404, 0.00),\n",
       "  (405, 0.00),\n",
       "  (416, 0.00),\n",
       "  (418, 0.00),\n",
       "  (421, 0.00),\n",
       "  (422, 0.00),\n",
       "  (426, 0.00),\n",
       "  (434, 0.00),\n",
       "  (435, 0.00),\n",
       "  (437, 0.00),\n",
       "  (439, 0.00),\n",
       "  (446, 0.00),\n",
       "  (448, 0.00),\n",
       "  (449, 0.00),\n",
       "  (451, 0.00),\n",
       "  (452, 0.00),\n",
       "  (455, 0.00),\n",
       "  (456, 0.00),\n",
       "  (460, 0.00),\n",
       "  (461, 0.00),\n",
       "  (462, 0.00),\n",
       "  (465, 0.00),\n",
       "  (466, 0.00),\n",
       "  (469, 0.00),\n",
       "  (471, 0.00),\n",
       "  (473, 0.00),\n",
       "  (475, 0.00),\n",
       "  (478, 0.00),\n",
       "  (493, 0.00),\n",
       "  (494, 0.00),\n",
       "  (499, 0.00),\n",
       "  (500, 0.00),\n",
       "  (501, 0.00),\n",
       "  (504, 0.00),\n",
       "  (510, 0.00),\n",
       "  (512, 0.00),\n",
       "  (513, 0.00),\n",
       "  (516, 0.00),\n",
       "  (521, 0.00),\n",
       "  (525, 0.00),\n",
       "  (534, 0.00),\n",
       "  (536, 0.00),\n",
       "  (540, 0.00),\n",
       "  (542, 0.00),\n",
       "  (543, 0.00),\n",
       "  (548, 0.00),\n",
       "  (550, 0.00),\n",
       "  (551, 0.00),\n",
       "  (553, 0.00),\n",
       "  (554, 0.00),\n",
       "  (557, 0.00),\n",
       "  (559, 0.00),\n",
       "  (568, 0.00),\n",
       "  (569, 0.00),\n",
       "  (571, 0.00),\n",
       "  (574, 0.00),\n",
       "  (577, 0.00),\n",
       "  (583, 0.00),\n",
       "  (587, 0.00),\n",
       "  (589, 0.00),\n",
       "  (590, 0.00),\n",
       "  (592, 0.00),\n",
       "  (596, 0.00),\n",
       "  (597, 0.00),\n",
       "  (598, 0.00),\n",
       "  (600, 0.00),\n",
       "  (601, 0.00),\n",
       "  (610, 0.00),\n",
       "  (615, 0.00),\n",
       "  (617, 0.00),\n",
       "  (623, 0.00),\n",
       "  (626, 0.00),\n",
       "  (628, 0.00),\n",
       "  (630, 0.00),\n",
       "  (631, 0.00),\n",
       "  (634, 0.00),\n",
       "  (635, 0.00),\n",
       "  (647, 0.00),\n",
       "  (648, 0.00),\n",
       "  (649, 0.00),\n",
       "  (652, 0.00),\n",
       "  (653, 0.00),\n",
       "  (657, 0.00),\n",
       "  (659, 0.00),\n",
       "  (660, 0.00),\n",
       "  (663, 0.00),\n",
       "  (665, 0.00),\n",
       "  (666, 0.00),\n",
       "  (673, 0.00),\n",
       "  (675, 0.00),\n",
       "  (677, 0.00),\n",
       "  (678, 0.00),\n",
       "  (682, 0.00),\n",
       "  (683, 0.00),\n",
       "  (685, 0.00),\n",
       "  (686, 0.00),\n",
       "  (688, 0.00),\n",
       "  (689, 0.00),\n",
       "  (690, 0.00),\n",
       "  (693, 0.00),\n",
       "  (702, 0.00),\n",
       "  (704, 0.00),\n",
       "  (708, 0.00),\n",
       "  (712, 0.00),\n",
       "  (713, 0.00),\n",
       "  (714, 0.00),\n",
       "  (715, 0.00),\n",
       "  (718, 0.00),\n",
       "  (720, 0.00),\n",
       "  (726, 0.00),\n",
       "  (731, 0.00),\n",
       "  (736, 0.00),\n",
       "  (740, 0.00),\n",
       "  (744, 0.00),\n",
       "  (747, 0.00),\n",
       "  (749, 0.00),\n",
       "  (760, 0.00),\n",
       "  (763, 0.00),\n",
       "  (767, 0.00),\n",
       "  (769, 0.00),\n",
       "  (771, 0.00),\n",
       "  (773, 0.00),\n",
       "  (776, 0.00),\n",
       "  (778, 0.00),\n",
       "  (780, 0.00),\n",
       "  (785, 0.00),\n",
       "  (789, 0.00),\n",
       "  (792, 0.00),\n",
       "  (795, 0.00),\n",
       "  (797, 0.00),\n",
       "  (798, 0.00),\n",
       "  (799, 0.00),\n",
       "  (802, 0.00),\n",
       "  (804, 0.00),\n",
       "  (810, 0.00),\n",
       "  (812, 0.00),\n",
       "  (818, 0.00),\n",
       "  (827, 0.00),\n",
       "  (834, 0.00),\n",
       "  (835, 0.00),\n",
       "  (836, 0.00),\n",
       "  (838, 0.00),\n",
       "  (840, 0.00),\n",
       "  (841, 0.00),\n",
       "  (844, 0.00),\n",
       "  (849, 0.00),\n",
       "  (851, 0.00),\n",
       "  (856, 0.00),\n",
       "  (860, 0.00),\n",
       "  (862, 0.00),\n",
       "  (869, 0.00),\n",
       "  (875, 0.00),\n",
       "  (883, 0.00),\n",
       "  (887, 0.00),\n",
       "  (891, 0.00),\n",
       "  (895, 0.00),\n",
       "  (896, 0.00),\n",
       "  (897, 0.00),\n",
       "  (902, 0.00),\n",
       "  (903, 0.00),\n",
       "  (905, 0.00),\n",
       "  (908, 0.00),\n",
       "  (909, 0.00),\n",
       "  (912, 0.00),\n",
       "  (913, 0.00),\n",
       "  (914, 0.00),\n",
       "  (916, 0.00),\n",
       "  (922, 0.00),\n",
       "  (924, 0.00),\n",
       "  (925, 0.00),\n",
       "  (926, 0.00),\n",
       "  (927, 0.00),\n",
       "  (928, 0.00),\n",
       "  (929, 0.00),\n",
       "  (930, 0.00),\n",
       "  (931, 0.00),\n",
       "  (933, 0.00),\n",
       "  (935, 0.00),\n",
       "  (938, 0.00),\n",
       "  (940, 0.00),\n",
       "  (941, 0.00),\n",
       "  (942, 0.00),\n",
       "  (943, 0.00),\n",
       "  (945, 0.00),\n",
       "  (947, 0.00),\n",
       "  (948, 0.00),\n",
       "  (951, 0.00),\n",
       "  (954, 0.00),\n",
       "  (956, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (964, 0.00),\n",
       "  (965, 0.00),\n",
       "  (966, 0.00),\n",
       "  (967, 0.00),\n",
       "  (969, 0.00),\n",
       "  (974, 0.00),\n",
       "  (975, 0.00),\n",
       "  (977, 0.00),\n",
       "  (979, 0.00),\n",
       "  (980, 0.00),\n",
       "  (983, 0.00),\n",
       "  (986, 0.00),\n",
       "  (988, 0.00),\n",
       "  (990, 0.00),\n",
       "  (991, 0.00),\n",
       "  (993, 0.00),\n",
       "  (994, 0.00),\n",
       "  (995, 0.00),\n",
       "  (998, 0.00),\n",
       "  (999, 0.00)],\n",
       " [971,\n",
       "  55,\n",
       "  669,\n",
       "  828,\n",
       "  39,\n",
       "  61,\n",
       "  556,\n",
       "  84,\n",
       "  721,\n",
       "  401,\n",
       "  770,\n",
       "  490,\n",
       "  489,\n",
       "  62,\n",
       "  599,\n",
       "  815,\n",
       "  824,\n",
       "  588,\n",
       "  955,\n",
       "  63,\n",
       "  794,\n",
       "  419,\n",
       "  750,\n",
       "  904,\n",
       "  711,\n",
       "  116,\n",
       "  584,\n",
       "  464,\n",
       "  48,\n",
       "  319,\n",
       "  709,\n",
       "  581,\n",
       "  762,\n",
       "  552,\n",
       "  641,\n",
       "  843,\n",
       "  108,\n",
       "  155,\n",
       "  646,\n",
       "  572,\n",
       "  806,\n",
       "  893,\n",
       "  46,\n",
       "  151,\n",
       "  973,\n",
       "  94,\n",
       "  651,\n",
       "  414,\n",
       "  431,\n",
       "  40,\n",
       "  60,\n",
       "  193,\n",
       "  562,\n",
       "  621,\n",
       "  868,\n",
       "  124,\n",
       "  300,\n",
       "  509,\n",
       "  735,\n",
       "  109,\n",
       "  879,\n",
       "  1,\n",
       "  692,\n",
       "  539,\n",
       "  579,\n",
       "  741,\n",
       "  334,\n",
       "  406,\n",
       "  457,\n",
       "  520,\n",
       "  620,\n",
       "  703,\n",
       "  865,\n",
       "  42,\n",
       "  68,\n",
       "  97,\n",
       "  292,\n",
       "  850,\n",
       "  872,\n",
       "  118,\n",
       "  545,\n",
       "  580,\n",
       "  790,\n",
       "  826,\n",
       "  982,\n",
       "  107,\n",
       "  128,\n",
       "  230,\n",
       "  247,\n",
       "  360,\n",
       "  440,\n",
       "  604,\n",
       "  906,\n",
       "  907,\n",
       "  963,\n",
       "  7,\n",
       "  90,\n",
       "  353,\n",
       "  538,\n",
       "  575,\n",
       "  582,\n",
       "  611,\n",
       "  786,\n",
       "  808,\n",
       "  819,\n",
       "  892,\n",
       "  38,\n",
       "  205,\n",
       "  304,\n",
       "  482,\n",
       "  638,\n",
       "  674,\n",
       "  777,\n",
       "  783,\n",
       "  864,\n",
       "  871,\n",
       "  987,\n",
       "  76,\n",
       "  123,\n",
       "  176,\n",
       "  348,\n",
       "  411,\n",
       "  474,\n",
       "  496,\n",
       "  506,\n",
       "  518,\n",
       "  593,\n",
       "  854,\n",
       "  64,\n",
       "  472,\n",
       "  508,\n",
       "  527,\n",
       "  614,\n",
       "  624,\n",
       "  839,\n",
       "  855,\n",
       "  880,\n",
       "  881,\n",
       "  944,\n",
       "  981,\n",
       "  996,\n",
       "  0,\n",
       "  96,\n",
       "  218,\n",
       "  275,\n",
       "  294,\n",
       "  318,\n",
       "  336,\n",
       "  365,\n",
       "  393,\n",
       "  396,\n",
       "  398,\n",
       "  424,\n",
       "  425,\n",
       "  441,\n",
       "  492,\n",
       "  515,\n",
       "  595,\n",
       "  698,\n",
       "  717,\n",
       "  791,\n",
       "  801,\n",
       "  830,\n",
       "  934,\n",
       "  45,\n",
       "  115,\n",
       "  136,\n",
       "  242,\n",
       "  274,\n",
       "  281,\n",
       "  340,\n",
       "  355,\n",
       "  363,\n",
       "  430,\n",
       "  468,\n",
       "  480,\n",
       "  484,\n",
       "  498,\n",
       "  633,\n",
       "  800,\n",
       "  858,\n",
       "  8,\n",
       "  17,\n",
       "  25,\n",
       "  28,\n",
       "  33,\n",
       "  37,\n",
       "  52,\n",
       "  57,\n",
       "  83,\n",
       "  86,\n",
       "  91,\n",
       "  92,\n",
       "  120,\n",
       "  140,\n",
       "  141,\n",
       "  144,\n",
       "  163,\n",
       "  164,\n",
       "  189,\n",
       "  198,\n",
       "  206,\n",
       "  226,\n",
       "  231,\n",
       "  235,\n",
       "  260,\n",
       "  284,\n",
       "  289,\n",
       "  290,\n",
       "  291,\n",
       "  293,\n",
       "  301,\n",
       "  306,\n",
       "  307,\n",
       "  314,\n",
       "  316,\n",
       "  321,\n",
       "  327,\n",
       "  330,\n",
       "  342,\n",
       "  347,\n",
       "  376,\n",
       "  381,\n",
       "  391,\n",
       "  392,\n",
       "  397,\n",
       "  417,\n",
       "  429,\n",
       "  433,\n",
       "  443,\n",
       "  477,\n",
       "  488,\n",
       "  491,\n",
       "  507,\n",
       "  528,\n",
       "  530,\n",
       "  533,\n",
       "  555,\n",
       "  566,\n",
       "  567,\n",
       "  570,\n",
       "  576,\n",
       "  602,\n",
       "  609,\n",
       "  612,\n",
       "  625,\n",
       "  629,\n",
       "  637,\n",
       "  645,\n",
       "  661,\n",
       "  684,\n",
       "  694,\n",
       "  705,\n",
       "  710,\n",
       "  716,\n",
       "  722,\n",
       "  727,\n",
       "  734,\n",
       "  738,\n",
       "  746,\n",
       "  753,\n",
       "  759,\n",
       "  775,\n",
       "  781,\n",
       "  787,\n",
       "  803,\n",
       "  816,\n",
       "  829,\n",
       "  847,\n",
       "  857,\n",
       "  873,\n",
       "  882,\n",
       "  890,\n",
       "  923,\n",
       "  937,\n",
       "  953,\n",
       "  957,\n",
       "  992,\n",
       "  997,\n",
       "  102,\n",
       "  105,\n",
       "  132,\n",
       "  158,\n",
       "  168,\n",
       "  197,\n",
       "  237,\n",
       "  313,\n",
       "  389,\n",
       "  402,\n",
       "  428,\n",
       "  483,\n",
       "  594,\n",
       "  654,\n",
       "  655,\n",
       "  679,\n",
       "  732,\n",
       "  751,\n",
       "  784,\n",
       "  788,\n",
       "  884,\n",
       "  939,\n",
       "  53,\n",
       "  75,\n",
       "  171,\n",
       "  188,\n",
       "  219,\n",
       "  271,\n",
       "  375,\n",
       "  407,\n",
       "  432,\n",
       "  444,\n",
       "  453,\n",
       "  517,\n",
       "  535,\n",
       "  549,\n",
       "  565,\n",
       "  586,\n",
       "  607,\n",
       "  664,\n",
       "  728,\n",
       "  729,\n",
       "  754,\n",
       "  772,\n",
       "  814,\n",
       "  831,\n",
       "  863,\n",
       "  889,\n",
       "  984,\n",
       "  47,\n",
       "  65,\n",
       "  74,\n",
       "  113,\n",
       "  134,\n",
       "  204,\n",
       "  236,\n",
       "  249,\n",
       "  256,\n",
       "  308,\n",
       "  331,\n",
       "  337,\n",
       "  387,\n",
       "  442,\n",
       "  447,\n",
       "  454,\n",
       "  668,\n",
       "  672,\n",
       "  707,\n",
       "  725,\n",
       "  768,\n",
       "  817,\n",
       "  820,\n",
       "  837,\n",
       "  874,\n",
       "  878,\n",
       "  900,\n",
       "  918,\n",
       "  920,\n",
       "  19,\n",
       "  24,\n",
       "  50,\n",
       "  138,\n",
       "  160,\n",
       "  178,\n",
       "  186,\n",
       "  213,\n",
       "  305,\n",
       "  395,\n",
       "  445,\n",
       "  485,\n",
       "  505,\n",
       "  546,\n",
       "  616,\n",
       "  619,\n",
       "  642,\n",
       "  644,\n",
       "  650,\n",
       "  670,\n",
       "  700,\n",
       "  701,\n",
       "  706,\n",
       "  737,\n",
       "  743,\n",
       "  745,\n",
       "  766,\n",
       "  821,\n",
       "  832,\n",
       "  852,\n",
       "  867,\n",
       "  870,\n",
       "  915,\n",
       "  976,\n",
       "  985,\n",
       "  9,\n",
       "  15,\n",
       "  31,\n",
       "  72,\n",
       "  87,\n",
       "  195,\n",
       "  228,\n",
       "  323,\n",
       "  343,\n",
       "  394,\n",
       "  408,\n",
       "  481,\n",
       "  495,\n",
       "  514,\n",
       "  526,\n",
       "  544,\n",
       "  564])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist, tk = diversity(learn, 10, 95)\n",
    "n, hist, tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "finished creating histogram for the 0th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 1th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 2th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 3th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 4th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 5th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 6th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 7th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 8th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 9th perturbation\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(372,\n",
       " [(971, 118.50),\n",
       "  (669, 48.20),\n",
       "  (61, 38.00),\n",
       "  (55, 36.30),\n",
       "  (828, 31.70),\n",
       "  (39, 29.80),\n",
       "  (721, 22.90),\n",
       "  (401, 19.00),\n",
       "  (62, 13.40),\n",
       "  (770, 13.10),\n",
       "  (588, 12.80),\n",
       "  (904, 11.70),\n",
       "  (489, 11.20),\n",
       "  (490, 11.20),\n",
       "  (750, 10.50),\n",
       "  (63, 9.80),\n",
       "  (556, 9.00),\n",
       "  (711, 8.50),\n",
       "  (84, 8.40),\n",
       "  (599, 8.40),\n",
       "  (955, 8.30),\n",
       "  (794, 8.20),\n",
       "  (824, 7.60),\n",
       "  (419, 7.20),\n",
       "  (431, 6.00),\n",
       "  (584, 5.20),\n",
       "  (709, 4.90),\n",
       "  (464, 4.20),\n",
       "  (319, 3.90),\n",
       "  (581, 3.80),\n",
       "  (641, 3.80),\n",
       "  (116, 3.70),\n",
       "  (414, 3.60),\n",
       "  (108, 3.40),\n",
       "  (1, 3.30),\n",
       "  (48, 3.30),\n",
       "  (60, 3.20),\n",
       "  (843, 3.20),\n",
       "  (155, 3.10),\n",
       "  (604, 3.10),\n",
       "  (741, 3.10),\n",
       "  (973, 3.10),\n",
       "  (94, 3.00),\n",
       "  (539, 2.90),\n",
       "  (552, 2.90),\n",
       "  (762, 2.90),\n",
       "  (806, 2.90),\n",
       "  (46, 2.80),\n",
       "  (124, 2.60),\n",
       "  (151, 2.60),\n",
       "  (572, 2.60),\n",
       "  (893, 2.60),\n",
       "  (520, 2.50),\n",
       "  (865, 2.50),\n",
       "  (40, 2.40),\n",
       "  (406, 2.40),\n",
       "  (193, 2.30),\n",
       "  (621, 2.30),\n",
       "  (735, 2.30),\n",
       "  (772, 2.20),\n",
       "  (826, 2.20),\n",
       "  (68, 2.00),\n",
       "  (292, 2.00),\n",
       "  (300, 2.00),\n",
       "  (457, 2.00),\n",
       "  (651, 2.00),\n",
       "  (872, 2.00),\n",
       "  (7, 1.90),\n",
       "  (230, 1.90),\n",
       "  (496, 1.90),\n",
       "  (575, 1.90),\n",
       "  (879, 1.90),\n",
       "  (109, 1.80),\n",
       "  (353, 1.80),\n",
       "  (777, 1.80),\n",
       "  (819, 1.80),\n",
       "  (871, 1.80),\n",
       "  (0, 1.70),\n",
       "  (107, 1.70),\n",
       "  (205, 1.70),\n",
       "  (247, 1.70),\n",
       "  (334, 1.70),\n",
       "  (440, 1.70),\n",
       "  (441, 1.70),\n",
       "  (620, 1.70),\n",
       "  (790, 1.70),\n",
       "  (987, 1.70),\n",
       "  (38, 1.60),\n",
       "  (97, 1.60),\n",
       "  (171, 1.60),\n",
       "  (360, 1.60),\n",
       "  (518, 1.60),\n",
       "  (562, 1.60),\n",
       "  (580, 1.60),\n",
       "  (692, 1.60),\n",
       "  (786, 1.60),\n",
       "  (850, 1.60),\n",
       "  (868, 1.60),\n",
       "  (96, 1.50),\n",
       "  (118, 1.50),\n",
       "  (363, 1.50),\n",
       "  (411, 1.50),\n",
       "  (498, 1.50),\n",
       "  (538, 1.50),\n",
       "  (646, 1.50),\n",
       "  (703, 1.50),\n",
       "  (837, 1.50),\n",
       "  (42, 1.40),\n",
       "  (218, 1.40),\n",
       "  (294, 1.40),\n",
       "  (340, 1.40),\n",
       "  (348, 1.40),\n",
       "  (482, 1.40),\n",
       "  (582, 1.40),\n",
       "  (638, 1.40),\n",
       "  (746, 1.40),\n",
       "  (783, 1.40),\n",
       "  (800, 1.40),\n",
       "  (892, 1.40),\n",
       "  (907, 1.40),\n",
       "  (304, 1.30),\n",
       "  (397, 1.30),\n",
       "  (424, 1.30),\n",
       "  (453, 1.30),\n",
       "  (492, 1.30),\n",
       "  (567, 1.30),\n",
       "  (655, 1.30),\n",
       "  (710, 1.30),\n",
       "  (717, 1.30),\n",
       "  (751, 1.30),\n",
       "  (791, 1.30),\n",
       "  (796, 1.30),\n",
       "  (880, 1.30),\n",
       "  (982, 1.30),\n",
       "  (996, 1.30),\n",
       "  (120, 1.20),\n",
       "  (242, 1.20),\n",
       "  (275, 1.20),\n",
       "  (398, 1.20),\n",
       "  (509, 1.20),\n",
       "  (629, 1.20),\n",
       "  (732, 1.20),\n",
       "  (788, 1.20),\n",
       "  (815, 1.20),\n",
       "  (855, 1.20),\n",
       "  (864, 1.20),\n",
       "  (881, 1.20),\n",
       "  (90, 1.10),\n",
       "  (91, 1.10),\n",
       "  (123, 1.10),\n",
       "  (274, 1.10),\n",
       "  (308, 1.10),\n",
       "  (336, 1.10),\n",
       "  (396, 1.10),\n",
       "  (491, 1.10),\n",
       "  (530, 1.10),\n",
       "  (555, 1.10),\n",
       "  (565, 1.10),\n",
       "  (570, 1.10),\n",
       "  (579, 1.10),\n",
       "  (611, 1.10),\n",
       "  (633, 1.10),\n",
       "  (698, 1.10),\n",
       "  (808, 1.10),\n",
       "  (811, 1.10),\n",
       "  (898, 1.10),\n",
       "  (934, 1.10),\n",
       "  (981, 1.10),\n",
       "  (8, 1.00),\n",
       "  (25, 1.00),\n",
       "  (28, 1.00),\n",
       "  (33, 1.00),\n",
       "  (37, 1.00),\n",
       "  (45, 1.00),\n",
       "  (52, 1.00),\n",
       "  (57, 1.00),\n",
       "  (76, 1.00),\n",
       "  (83, 1.00),\n",
       "  (86, 1.00),\n",
       "  (92, 1.00),\n",
       "  (102, 1.00),\n",
       "  (115, 1.00),\n",
       "  (128, 1.00),\n",
       "  (134, 1.00),\n",
       "  (144, 1.00),\n",
       "  (163, 1.00),\n",
       "  (164, 1.00),\n",
       "  (176, 1.00),\n",
       "  (189, 1.00),\n",
       "  (197, 1.00),\n",
       "  (198, 1.00),\n",
       "  (235, 1.00),\n",
       "  (249, 1.00),\n",
       "  (260, 1.00),\n",
       "  (281, 1.00),\n",
       "  (289, 1.00),\n",
       "  (290, 1.00),\n",
       "  (291, 1.00),\n",
       "  (293, 1.00),\n",
       "  (301, 1.00),\n",
       "  (306, 1.00),\n",
       "  (307, 1.00),\n",
       "  (314, 1.00),\n",
       "  (318, 1.00),\n",
       "  (321, 1.00),\n",
       "  (327, 1.00),\n",
       "  (347, 1.00),\n",
       "  (355, 1.00),\n",
       "  (376, 1.00),\n",
       "  (392, 1.00),\n",
       "  (393, 1.00),\n",
       "  (407, 1.00),\n",
       "  (429, 1.00),\n",
       "  (430, 1.00),\n",
       "  (433, 1.00),\n",
       "  (443, 1.00),\n",
       "  (468, 1.00),\n",
       "  (472, 1.00),\n",
       "  (474, 1.00),\n",
       "  (477, 1.00),\n",
       "  (480, 1.00),\n",
       "  (488, 1.00),\n",
       "  (506, 1.00),\n",
       "  (507, 1.00),\n",
       "  (508, 1.00),\n",
       "  (528, 1.00),\n",
       "  (533, 1.00),\n",
       "  (566, 1.00),\n",
       "  (576, 1.00),\n",
       "  (593, 1.00),\n",
       "  (595, 1.00),\n",
       "  (602, 1.00),\n",
       "  (609, 1.00),\n",
       "  (612, 1.00),\n",
       "  (625, 1.00),\n",
       "  (637, 1.00),\n",
       "  (644, 1.00),\n",
       "  (645, 1.00),\n",
       "  (654, 1.00),\n",
       "  (661, 1.00),\n",
       "  (664, 1.00),\n",
       "  (674, 1.00),\n",
       "  (684, 1.00),\n",
       "  (694, 1.00),\n",
       "  (716, 1.00),\n",
       "  (722, 1.00),\n",
       "  (728, 1.00),\n",
       "  (734, 1.00),\n",
       "  (738, 1.00),\n",
       "  (753, 1.00),\n",
       "  (759, 1.00),\n",
       "  (787, 1.00),\n",
       "  (801, 1.00),\n",
       "  (803, 1.00),\n",
       "  (816, 1.00),\n",
       "  (830, 1.00),\n",
       "  (847, 1.00),\n",
       "  (857, 1.00),\n",
       "  (858, 1.00),\n",
       "  (878, 1.00),\n",
       "  (889, 1.00),\n",
       "  (890, 1.00),\n",
       "  (923, 1.00),\n",
       "  (937, 1.00),\n",
       "  (944, 1.00),\n",
       "  (953, 1.00),\n",
       "  (957, 1.00),\n",
       "  (963, 1.00),\n",
       "  (992, 1.00),\n",
       "  (997, 1.00),\n",
       "  (138, 0.90),\n",
       "  (188, 0.90),\n",
       "  (236, 0.90),\n",
       "  (284, 0.90),\n",
       "  (316, 0.90),\n",
       "  (330, 0.90),\n",
       "  (428, 0.90),\n",
       "  (432, 0.90),\n",
       "  (515, 0.90),\n",
       "  (586, 0.90),\n",
       "  (607, 0.90),\n",
       "  (624, 0.90),\n",
       "  (729, 0.90),\n",
       "  (768, 0.90),\n",
       "  (781, 0.90),\n",
       "  (870, 0.90),\n",
       "  (873, 0.90),\n",
       "  (874, 0.90),\n",
       "  (906, 0.90),\n",
       "  (918, 0.90),\n",
       "  (920, 0.90),\n",
       "  (31, 0.80),\n",
       "  (105, 0.80),\n",
       "  (206, 0.80),\n",
       "  (219, 0.80),\n",
       "  (309, 0.80),\n",
       "  (417, 0.80),\n",
       "  (425, 0.80),\n",
       "  (527, 0.80),\n",
       "  (632, 0.80),\n",
       "  (820, 0.80),\n",
       "  (985, 0.80),\n",
       "  (17, 0.70),\n",
       "  (74, 0.70),\n",
       "  (75, 0.70),\n",
       "  (132, 0.70),\n",
       "  (136, 0.70),\n",
       "  (140, 0.70),\n",
       "  (158, 0.70),\n",
       "  (168, 0.70),\n",
       "  (173, 0.70),\n",
       "  (204, 0.70),\n",
       "  (226, 0.70),\n",
       "  (237, 0.70),\n",
       "  (256, 0.70),\n",
       "  (271, 0.70),\n",
       "  (313, 0.70),\n",
       "  (341, 0.70),\n",
       "  (342, 0.70),\n",
       "  (365, 0.70),\n",
       "  (375, 0.70),\n",
       "  (381, 0.70),\n",
       "  (387, 0.70),\n",
       "  (391, 0.70),\n",
       "  (394, 0.70),\n",
       "  (402, 0.70),\n",
       "  (442, 0.70),\n",
       "  (444, 0.70),\n",
       "  (483, 0.70),\n",
       "  (505, 0.70),\n",
       "  (517, 0.70),\n",
       "  (532, 0.70),\n",
       "  (547, 0.70),\n",
       "  (605, 0.70),\n",
       "  (614, 0.70),\n",
       "  (619, 0.70),\n",
       "  (668, 0.70),\n",
       "  (672, 0.70),\n",
       "  (727, 0.70),\n",
       "  (754, 0.70),\n",
       "  (784, 0.70),\n",
       "  (814, 0.70),\n",
       "  (829, 0.70),\n",
       "  (832, 0.70),\n",
       "  (900, 0.70),\n",
       "  (915, 0.70),\n",
       "  (984, 0.70),\n",
       "  (19, 0.60),\n",
       "  (24, 0.60),\n",
       "  (87, 0.60),\n",
       "  (141, 0.60),\n",
       "  (186, 0.60),\n",
       "  (228, 0.60),\n",
       "  (253, 0.60),\n",
       "  (331, 0.60),\n",
       "  (337, 0.60),\n",
       "  (395, 0.60),\n",
       "  (399, 0.60),\n",
       "  (423, 0.60),\n",
       "  (445, 0.60),\n",
       "  (476, 0.60),\n",
       "  (484, 0.60),\n",
       "  (537, 0.60),\n",
       "  (546, 0.60),\n",
       "  (616, 0.60),\n",
       "  (705, 0.60),\n",
       "  (748, 0.60),\n",
       "  (775, 0.60),\n",
       "  (821, 0.60),\n",
       "  (839, 0.60),\n",
       "  (852, 0.60),\n",
       "  (863, 0.60),\n",
       "  (867, 0.60),\n",
       "  (939, 0.60),\n",
       "  (976, 0.60),\n",
       "  (15, 0.50),\n",
       "  (47, 0.50),\n",
       "  (50, 0.50),\n",
       "  (178, 0.50),\n",
       "  (213, 0.50),\n",
       "  (514, 0.50),\n",
       "  (529, 0.50),\n",
       "  (535, 0.50),\n",
       "  (541, 0.50),\n",
       "  (551, 0.50),\n",
       "  (594, 0.50),\n",
       "  (627, 0.50),\n",
       "  (636, 0.50),\n",
       "  (650, 0.50),\n",
       "  (679, 0.50),\n",
       "  (680, 0.50),\n",
       "  (687, 0.50),\n",
       "  (706, 0.50),\n",
       "  (712, 0.50),\n",
       "  (719, 0.50),\n",
       "  (743, 0.50),\n",
       "  (757, 0.50),\n",
       "  (766, 0.50),\n",
       "  (886, 0.50),\n",
       "  (902, 0.50),\n",
       "  (9, 0.40),\n",
       "  (65, 0.40),\n",
       "  (72, 0.40),\n",
       "  (160, 0.40),\n",
       "  (195, 0.40),\n",
       "  (211, 0.40),\n",
       "  (231, 0.40),\n",
       "  (254, 0.40),\n",
       "  (265, 0.40),\n",
       "  (354, 0.40),\n",
       "  (389, 0.40),\n",
       "  (409, 0.40),\n",
       "  (410, 0.40),\n",
       "  (412, 0.40),\n",
       "  (495, 0.40),\n",
       "  (526, 0.40),\n",
       "  (663, 0.40),\n",
       "  (691, 0.40),\n",
       "  (699, 0.40),\n",
       "  (707, 0.40),\n",
       "  (725, 0.40),\n",
       "  (733, 0.40),\n",
       "  (848, 0.40),\n",
       "  (854, 0.40),\n",
       "  (882, 0.40),\n",
       "  (899, 0.40),\n",
       "  (41, 0.30),\n",
       "  (53, 0.30),\n",
       "  (79, 0.30),\n",
       "  (98, 0.30),\n",
       "  (149, 0.30),\n",
       "  (153, 0.30),\n",
       "  (184, 0.30),\n",
       "  (191, 0.30),\n",
       "  (201, 0.30),\n",
       "  (282, 0.30),\n",
       "  (298, 0.30),\n",
       "  (305, 0.30),\n",
       "  (343, 0.30),\n",
       "  (350, 0.30),\n",
       "  (358, 0.30),\n",
       "  (369, 0.30),\n",
       "  (388, 0.30),\n",
       "  (408, 0.30),\n",
       "  (415, 0.30),\n",
       "  (481, 0.30),\n",
       "  (558, 0.30),\n",
       "  (560, 0.30),\n",
       "  (591, 0.30),\n",
       "  (647, 0.30),\n",
       "  (752, 0.30),\n",
       "  (758, 0.30),\n",
       "  (774, 0.30),\n",
       "  (804, 0.30),\n",
       "  (817, 0.30),\n",
       "  (842, 0.30),\n",
       "  (859, 0.30),\n",
       "  (911, 0.30),\n",
       "  (926, 0.30),\n",
       "  (932, 0.30),\n",
       "  (989, 0.30),\n",
       "  (11, 0.20),\n",
       "  (21, 0.20),\n",
       "  (67, 0.20),\n",
       "  (82, 0.20),\n",
       "  (88, 0.20),\n",
       "  (110, 0.20),\n",
       "  (113, 0.20),\n",
       "  (122, 0.20),\n",
       "  (129, 0.20),\n",
       "  (156, 0.20),\n",
       "  (157, 0.20),\n",
       "  (159, 0.20),\n",
       "  (162, 0.20),\n",
       "  (172, 0.20),\n",
       "  (303, 0.20),\n",
       "  (310, 0.20),\n",
       "  (323, 0.20),\n",
       "  (328, 0.20),\n",
       "  (367, 0.20),\n",
       "  (382, 0.20),\n",
       "  (447, 0.20),\n",
       "  (454, 0.20),\n",
       "  (459, 0.20),\n",
       "  (463, 0.20),\n",
       "  (503, 0.20),\n",
       "  (544, 0.20),\n",
       "  (550, 0.20),\n",
       "  (603, 0.20),\n",
       "  (606, 0.20),\n",
       "  (642, 0.20),\n",
       "  (676, 0.20),\n",
       "  (701, 0.20),\n",
       "  (726, 0.20),\n",
       "  (737, 0.20),\n",
       "  (745, 0.20),\n",
       "  (749, 0.20),\n",
       "  (779, 0.20),\n",
       "  (809, 0.20),\n",
       "  (813, 0.20),\n",
       "  (831, 0.20),\n",
       "  (834, 0.20),\n",
       "  (875, 0.20),\n",
       "  (884, 0.20),\n",
       "  (894, 0.20),\n",
       "  (897, 0.20),\n",
       "  (917, 0.20),\n",
       "  (921, 0.20),\n",
       "  (946, 0.20),\n",
       "  (959, 0.20),\n",
       "  (56, 0.10),\n",
       "  (64, 0.10),\n",
       "  (99, 0.10),\n",
       "  (100, 0.10),\n",
       "  (112, 0.10),\n",
       "  (135, 0.10),\n",
       "  (177, 0.10),\n",
       "  (187, 0.10),\n",
       "  (192, 0.10),\n",
       "  (199, 0.10),\n",
       "  (214, 0.10),\n",
       "  (221, 0.10),\n",
       "  (224, 0.10),\n",
       "  (234, 0.10),\n",
       "  (238, 0.10),\n",
       "  (333, 0.10),\n",
       "  (344, 0.10),\n",
       "  (511, 0.10),\n",
       "  (545, 0.10),\n",
       "  (549, 0.10),\n",
       "  (554, 0.10),\n",
       "  (564, 0.10),\n",
       "  (571, 0.10),\n",
       "  (626, 0.10),\n",
       "  (635, 0.10),\n",
       "  (665, 0.10),\n",
       "  (670, 0.10),\n",
       "  (696, 0.10),\n",
       "  (764, 0.10),\n",
       "  (765, 0.10),\n",
       "  (767, 0.10),\n",
       "  (805, 0.10),\n",
       "  (818, 0.10),\n",
       "  (823, 0.10),\n",
       "  (845, 0.10),\n",
       "  (846, 0.10),\n",
       "  (849, 0.10),\n",
       "  (853, 0.10),\n",
       "  (876, 0.10),\n",
       "  (885, 0.10),\n",
       "  (888, 0.10),\n",
       "  (910, 0.10),\n",
       "  (938, 0.10),\n",
       "  (968, 0.10),\n",
       "  (990, 0.10),\n",
       "  (2, 0.00),\n",
       "  (3, 0.00),\n",
       "  (4, 0.00),\n",
       "  (5, 0.00),\n",
       "  (6, 0.00),\n",
       "  (10, 0.00),\n",
       "  (12, 0.00),\n",
       "  (13, 0.00),\n",
       "  (14, 0.00),\n",
       "  (16, 0.00),\n",
       "  (18, 0.00),\n",
       "  (20, 0.00),\n",
       "  (22, 0.00),\n",
       "  (23, 0.00),\n",
       "  (26, 0.00),\n",
       "  (27, 0.00),\n",
       "  (29, 0.00),\n",
       "  (30, 0.00),\n",
       "  (32, 0.00),\n",
       "  (34, 0.00),\n",
       "  (35, 0.00),\n",
       "  (36, 0.00),\n",
       "  (43, 0.00),\n",
       "  (44, 0.00),\n",
       "  (49, 0.00),\n",
       "  (51, 0.00),\n",
       "  (54, 0.00),\n",
       "  (58, 0.00),\n",
       "  (59, 0.00),\n",
       "  (66, 0.00),\n",
       "  (69, 0.00),\n",
       "  (70, 0.00),\n",
       "  (71, 0.00),\n",
       "  (73, 0.00),\n",
       "  (77, 0.00),\n",
       "  (78, 0.00),\n",
       "  (80, 0.00),\n",
       "  (81, 0.00),\n",
       "  (85, 0.00),\n",
       "  (89, 0.00),\n",
       "  (93, 0.00),\n",
       "  (95, 0.00),\n",
       "  (101, 0.00),\n",
       "  (103, 0.00),\n",
       "  (104, 0.00),\n",
       "  (106, 0.00),\n",
       "  (111, 0.00),\n",
       "  (114, 0.00),\n",
       "  (117, 0.00),\n",
       "  (119, 0.00),\n",
       "  (121, 0.00),\n",
       "  (125, 0.00),\n",
       "  (126, 0.00),\n",
       "  (127, 0.00),\n",
       "  (130, 0.00),\n",
       "  (131, 0.00),\n",
       "  (133, 0.00),\n",
       "  (137, 0.00),\n",
       "  (139, 0.00),\n",
       "  (142, 0.00),\n",
       "  (143, 0.00),\n",
       "  (145, 0.00),\n",
       "  (146, 0.00),\n",
       "  (147, 0.00),\n",
       "  (148, 0.00),\n",
       "  (150, 0.00),\n",
       "  (152, 0.00),\n",
       "  (154, 0.00),\n",
       "  (161, 0.00),\n",
       "  (165, 0.00),\n",
       "  (166, 0.00),\n",
       "  (167, 0.00),\n",
       "  (169, 0.00),\n",
       "  (170, 0.00),\n",
       "  (174, 0.00),\n",
       "  (175, 0.00),\n",
       "  (179, 0.00),\n",
       "  (180, 0.00),\n",
       "  (181, 0.00),\n",
       "  (182, 0.00),\n",
       "  (183, 0.00),\n",
       "  (185, 0.00),\n",
       "  (190, 0.00),\n",
       "  (194, 0.00),\n",
       "  (196, 0.00),\n",
       "  (200, 0.00),\n",
       "  (202, 0.00),\n",
       "  (203, 0.00),\n",
       "  (207, 0.00),\n",
       "  (208, 0.00),\n",
       "  (209, 0.00),\n",
       "  (210, 0.00),\n",
       "  (212, 0.00),\n",
       "  (215, 0.00),\n",
       "  (216, 0.00),\n",
       "  (217, 0.00),\n",
       "  (220, 0.00),\n",
       "  (222, 0.00),\n",
       "  (223, 0.00),\n",
       "  (225, 0.00),\n",
       "  (227, 0.00),\n",
       "  (229, 0.00),\n",
       "  (232, 0.00),\n",
       "  (233, 0.00),\n",
       "  (239, 0.00),\n",
       "  (240, 0.00),\n",
       "  (241, 0.00),\n",
       "  (243, 0.00),\n",
       "  (244, 0.00),\n",
       "  (245, 0.00),\n",
       "  (246, 0.00),\n",
       "  (248, 0.00),\n",
       "  (250, 0.00),\n",
       "  (251, 0.00),\n",
       "  (252, 0.00),\n",
       "  (255, 0.00),\n",
       "  (257, 0.00),\n",
       "  (258, 0.00),\n",
       "  (259, 0.00),\n",
       "  (261, 0.00),\n",
       "  (262, 0.00),\n",
       "  (263, 0.00),\n",
       "  (264, 0.00),\n",
       "  (266, 0.00),\n",
       "  (267, 0.00),\n",
       "  (268, 0.00),\n",
       "  (269, 0.00),\n",
       "  (270, 0.00),\n",
       "  (272, 0.00),\n",
       "  (273, 0.00),\n",
       "  (276, 0.00),\n",
       "  (277, 0.00),\n",
       "  (278, 0.00),\n",
       "  (279, 0.00),\n",
       "  (280, 0.00),\n",
       "  (283, 0.00),\n",
       "  (285, 0.00),\n",
       "  (286, 0.00),\n",
       "  (287, 0.00),\n",
       "  (288, 0.00),\n",
       "  (295, 0.00),\n",
       "  (296, 0.00),\n",
       "  (297, 0.00),\n",
       "  (299, 0.00),\n",
       "  (302, 0.00),\n",
       "  (311, 0.00),\n",
       "  (312, 0.00),\n",
       "  (315, 0.00),\n",
       "  (317, 0.00),\n",
       "  (320, 0.00),\n",
       "  (322, 0.00),\n",
       "  (324, 0.00),\n",
       "  (325, 0.00),\n",
       "  (326, 0.00),\n",
       "  (329, 0.00),\n",
       "  (332, 0.00),\n",
       "  (335, 0.00),\n",
       "  (338, 0.00),\n",
       "  (339, 0.00),\n",
       "  (345, 0.00),\n",
       "  (346, 0.00),\n",
       "  (349, 0.00),\n",
       "  (351, 0.00),\n",
       "  (352, 0.00),\n",
       "  (356, 0.00),\n",
       "  (357, 0.00),\n",
       "  (359, 0.00),\n",
       "  (361, 0.00),\n",
       "  (362, 0.00),\n",
       "  (364, 0.00),\n",
       "  (366, 0.00),\n",
       "  (368, 0.00),\n",
       "  (370, 0.00),\n",
       "  (371, 0.00),\n",
       "  (372, 0.00),\n",
       "  (373, 0.00),\n",
       "  (374, 0.00),\n",
       "  (377, 0.00),\n",
       "  (378, 0.00),\n",
       "  (379, 0.00),\n",
       "  (380, 0.00),\n",
       "  (383, 0.00),\n",
       "  (384, 0.00),\n",
       "  (385, 0.00),\n",
       "  (386, 0.00),\n",
       "  (390, 0.00),\n",
       "  (400, 0.00),\n",
       "  (403, 0.00),\n",
       "  (404, 0.00),\n",
       "  (405, 0.00),\n",
       "  (413, 0.00),\n",
       "  (416, 0.00),\n",
       "  (418, 0.00),\n",
       "  (420, 0.00),\n",
       "  (421, 0.00),\n",
       "  (422, 0.00),\n",
       "  (426, 0.00),\n",
       "  (427, 0.00),\n",
       "  (434, 0.00),\n",
       "  (435, 0.00),\n",
       "  (436, 0.00),\n",
       "  (437, 0.00),\n",
       "  (438, 0.00),\n",
       "  (439, 0.00),\n",
       "  (446, 0.00),\n",
       "  (448, 0.00),\n",
       "  (449, 0.00),\n",
       "  (450, 0.00),\n",
       "  (451, 0.00),\n",
       "  (452, 0.00),\n",
       "  (455, 0.00),\n",
       "  (456, 0.00),\n",
       "  (458, 0.00),\n",
       "  (460, 0.00),\n",
       "  (461, 0.00),\n",
       "  (462, 0.00),\n",
       "  (465, 0.00),\n",
       "  (466, 0.00),\n",
       "  (467, 0.00),\n",
       "  (469, 0.00),\n",
       "  (470, 0.00),\n",
       "  (471, 0.00),\n",
       "  (473, 0.00),\n",
       "  (475, 0.00),\n",
       "  (478, 0.00),\n",
       "  (479, 0.00),\n",
       "  (485, 0.00),\n",
       "  (486, 0.00),\n",
       "  (487, 0.00),\n",
       "  (493, 0.00),\n",
       "  (494, 0.00),\n",
       "  (497, 0.00),\n",
       "  (499, 0.00),\n",
       "  (500, 0.00),\n",
       "  (501, 0.00),\n",
       "  (502, 0.00),\n",
       "  (504, 0.00),\n",
       "  (510, 0.00),\n",
       "  (512, 0.00),\n",
       "  (513, 0.00),\n",
       "  (516, 0.00),\n",
       "  (519, 0.00),\n",
       "  (521, 0.00),\n",
       "  (522, 0.00),\n",
       "  (523, 0.00),\n",
       "  (524, 0.00),\n",
       "  (525, 0.00),\n",
       "  (531, 0.00),\n",
       "  (534, 0.00),\n",
       "  (536, 0.00),\n",
       "  (540, 0.00),\n",
       "  (542, 0.00),\n",
       "  (543, 0.00),\n",
       "  (548, 0.00),\n",
       "  (553, 0.00),\n",
       "  (557, 0.00),\n",
       "  (559, 0.00),\n",
       "  (561, 0.00),\n",
       "  (563, 0.00),\n",
       "  (568, 0.00),\n",
       "  (569, 0.00),\n",
       "  (573, 0.00),\n",
       "  (574, 0.00),\n",
       "  (577, 0.00),\n",
       "  (578, 0.00),\n",
       "  (583, 0.00),\n",
       "  (585, 0.00),\n",
       "  (587, 0.00),\n",
       "  (589, 0.00),\n",
       "  (590, 0.00),\n",
       "  (592, 0.00),\n",
       "  (596, 0.00),\n",
       "  (597, 0.00),\n",
       "  (598, 0.00),\n",
       "  (600, 0.00),\n",
       "  (601, 0.00),\n",
       "  (608, 0.00),\n",
       "  (610, 0.00),\n",
       "  (613, 0.00),\n",
       "  (615, 0.00),\n",
       "  (617, 0.00),\n",
       "  (618, 0.00),\n",
       "  (622, 0.00),\n",
       "  (623, 0.00),\n",
       "  (628, 0.00),\n",
       "  (630, 0.00),\n",
       "  (631, 0.00),\n",
       "  (634, 0.00),\n",
       "  (639, 0.00),\n",
       "  (640, 0.00),\n",
       "  (643, 0.00),\n",
       "  (648, 0.00),\n",
       "  (649, 0.00),\n",
       "  (652, 0.00),\n",
       "  (653, 0.00),\n",
       "  (656, 0.00),\n",
       "  (657, 0.00),\n",
       "  (658, 0.00),\n",
       "  (659, 0.00),\n",
       "  (660, 0.00),\n",
       "  (662, 0.00),\n",
       "  (666, 0.00),\n",
       "  (667, 0.00),\n",
       "  (671, 0.00),\n",
       "  (673, 0.00),\n",
       "  (675, 0.00),\n",
       "  (677, 0.00),\n",
       "  (678, 0.00),\n",
       "  (681, 0.00),\n",
       "  (682, 0.00),\n",
       "  (683, 0.00),\n",
       "  (685, 0.00),\n",
       "  (686, 0.00),\n",
       "  (688, 0.00),\n",
       "  (689, 0.00),\n",
       "  (690, 0.00),\n",
       "  (693, 0.00),\n",
       "  (695, 0.00),\n",
       "  (697, 0.00),\n",
       "  (700, 0.00),\n",
       "  (702, 0.00),\n",
       "  (704, 0.00),\n",
       "  (708, 0.00),\n",
       "  (713, 0.00),\n",
       "  (714, 0.00),\n",
       "  (715, 0.00),\n",
       "  (718, 0.00),\n",
       "  (720, 0.00),\n",
       "  (723, 0.00),\n",
       "  (724, 0.00),\n",
       "  (730, 0.00),\n",
       "  (731, 0.00),\n",
       "  (736, 0.00),\n",
       "  (739, 0.00),\n",
       "  (740, 0.00),\n",
       "  (742, 0.00),\n",
       "  (744, 0.00),\n",
       "  (747, 0.00),\n",
       "  (755, 0.00),\n",
       "  (756, 0.00),\n",
       "  (760, 0.00),\n",
       "  (761, 0.00),\n",
       "  (763, 0.00),\n",
       "  (769, 0.00),\n",
       "  (771, 0.00),\n",
       "  (773, 0.00),\n",
       "  (776, 0.00),\n",
       "  (778, 0.00),\n",
       "  (780, 0.00),\n",
       "  (782, 0.00),\n",
       "  (785, 0.00),\n",
       "  (789, 0.00),\n",
       "  (792, 0.00),\n",
       "  (793, 0.00),\n",
       "  (795, 0.00),\n",
       "  (797, 0.00),\n",
       "  (798, 0.00),\n",
       "  (799, 0.00),\n",
       "  (802, 0.00),\n",
       "  (807, 0.00),\n",
       "  (810, 0.00),\n",
       "  (812, 0.00),\n",
       "  (822, 0.00),\n",
       "  (825, 0.00),\n",
       "  (827, 0.00),\n",
       "  (833, 0.00),\n",
       "  (835, 0.00),\n",
       "  (836, 0.00),\n",
       "  (838, 0.00),\n",
       "  (840, 0.00),\n",
       "  (841, 0.00),\n",
       "  (844, 0.00),\n",
       "  (851, 0.00),\n",
       "  (856, 0.00),\n",
       "  (860, 0.00),\n",
       "  (861, 0.00),\n",
       "  (862, 0.00),\n",
       "  (866, 0.00),\n",
       "  (869, 0.00),\n",
       "  (877, 0.00),\n",
       "  (883, 0.00),\n",
       "  (887, 0.00),\n",
       "  (891, 0.00),\n",
       "  (895, 0.00),\n",
       "  (896, 0.00),\n",
       "  (901, 0.00),\n",
       "  (903, 0.00),\n",
       "  (905, 0.00),\n",
       "  (908, 0.00),\n",
       "  (909, 0.00),\n",
       "  (912, 0.00),\n",
       "  (913, 0.00),\n",
       "  (914, 0.00),\n",
       "  (916, 0.00),\n",
       "  (919, 0.00),\n",
       "  (922, 0.00),\n",
       "  (924, 0.00),\n",
       "  (925, 0.00),\n",
       "  (927, 0.00),\n",
       "  (928, 0.00),\n",
       "  (929, 0.00),\n",
       "  (930, 0.00),\n",
       "  (931, 0.00),\n",
       "  (933, 0.00),\n",
       "  (935, 0.00),\n",
       "  (936, 0.00),\n",
       "  (940, 0.00),\n",
       "  (941, 0.00),\n",
       "  (942, 0.00),\n",
       "  (943, 0.00),\n",
       "  (945, 0.00),\n",
       "  (947, 0.00),\n",
       "  (948, 0.00),\n",
       "  (949, 0.00),\n",
       "  (950, 0.00),\n",
       "  (951, 0.00),\n",
       "  (952, 0.00),\n",
       "  (954, 0.00),\n",
       "  (956, 0.00),\n",
       "  (958, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (962, 0.00),\n",
       "  (964, 0.00),\n",
       "  (965, 0.00),\n",
       "  (966, 0.00),\n",
       "  (967, 0.00),\n",
       "  (969, 0.00),\n",
       "  (970, 0.00),\n",
       "  (972, 0.00),\n",
       "  (974, 0.00),\n",
       "  (975, 0.00),\n",
       "  (977, 0.00),\n",
       "  (978, 0.00),\n",
       "  (979, 0.00),\n",
       "  (980, 0.00),\n",
       "  (983, 0.00),\n",
       "  (986, 0.00),\n",
       "  (988, 0.00),\n",
       "  (991, 0.00),\n",
       "  (993, 0.00),\n",
       "  (994, 0.00),\n",
       "  (995, 0.00),\n",
       "  (998, 0.00),\n",
       "  (999, 0.00)],\n",
       " [971,\n",
       "  669,\n",
       "  61,\n",
       "  55,\n",
       "  828,\n",
       "  39,\n",
       "  721,\n",
       "  401,\n",
       "  62,\n",
       "  770,\n",
       "  588,\n",
       "  904,\n",
       "  489,\n",
       "  490,\n",
       "  750,\n",
       "  63,\n",
       "  556,\n",
       "  711,\n",
       "  84,\n",
       "  599,\n",
       "  955,\n",
       "  794,\n",
       "  824,\n",
       "  419,\n",
       "  431,\n",
       "  584,\n",
       "  709,\n",
       "  464,\n",
       "  319,\n",
       "  581,\n",
       "  641,\n",
       "  116,\n",
       "  414,\n",
       "  108,\n",
       "  1,\n",
       "  48,\n",
       "  60,\n",
       "  843,\n",
       "  155,\n",
       "  604,\n",
       "  741,\n",
       "  973,\n",
       "  94,\n",
       "  539,\n",
       "  552,\n",
       "  762,\n",
       "  806,\n",
       "  46,\n",
       "  124,\n",
       "  151,\n",
       "  572,\n",
       "  893,\n",
       "  520,\n",
       "  865,\n",
       "  40,\n",
       "  406,\n",
       "  193,\n",
       "  621,\n",
       "  735,\n",
       "  772,\n",
       "  826,\n",
       "  68,\n",
       "  292,\n",
       "  300,\n",
       "  457,\n",
       "  651,\n",
       "  872,\n",
       "  7,\n",
       "  230,\n",
       "  496,\n",
       "  575,\n",
       "  879,\n",
       "  109,\n",
       "  353,\n",
       "  777,\n",
       "  819,\n",
       "  871,\n",
       "  0,\n",
       "  107,\n",
       "  205,\n",
       "  247,\n",
       "  334,\n",
       "  440,\n",
       "  441,\n",
       "  620,\n",
       "  790,\n",
       "  987,\n",
       "  38,\n",
       "  97,\n",
       "  171,\n",
       "  360,\n",
       "  518,\n",
       "  562,\n",
       "  580,\n",
       "  692,\n",
       "  786,\n",
       "  850,\n",
       "  868,\n",
       "  96,\n",
       "  118,\n",
       "  363,\n",
       "  411,\n",
       "  498,\n",
       "  538,\n",
       "  646,\n",
       "  703,\n",
       "  837,\n",
       "  42,\n",
       "  218,\n",
       "  294,\n",
       "  340,\n",
       "  348,\n",
       "  482,\n",
       "  582,\n",
       "  638,\n",
       "  746,\n",
       "  783,\n",
       "  800,\n",
       "  892,\n",
       "  907,\n",
       "  304,\n",
       "  397,\n",
       "  424,\n",
       "  453,\n",
       "  492,\n",
       "  567,\n",
       "  655,\n",
       "  710,\n",
       "  717,\n",
       "  751,\n",
       "  791,\n",
       "  796,\n",
       "  880,\n",
       "  982,\n",
       "  996,\n",
       "  120,\n",
       "  242,\n",
       "  275,\n",
       "  398,\n",
       "  509,\n",
       "  629,\n",
       "  732,\n",
       "  788,\n",
       "  815,\n",
       "  855,\n",
       "  864,\n",
       "  881,\n",
       "  90,\n",
       "  91,\n",
       "  123,\n",
       "  274,\n",
       "  308,\n",
       "  336,\n",
       "  396,\n",
       "  491,\n",
       "  530,\n",
       "  555,\n",
       "  565,\n",
       "  570,\n",
       "  579,\n",
       "  611,\n",
       "  633,\n",
       "  698,\n",
       "  808,\n",
       "  811,\n",
       "  898,\n",
       "  934,\n",
       "  981,\n",
       "  8,\n",
       "  25,\n",
       "  28,\n",
       "  33,\n",
       "  37,\n",
       "  45,\n",
       "  52,\n",
       "  57,\n",
       "  76,\n",
       "  83,\n",
       "  86,\n",
       "  92,\n",
       "  102,\n",
       "  115,\n",
       "  128,\n",
       "  134,\n",
       "  144,\n",
       "  163,\n",
       "  164,\n",
       "  176,\n",
       "  189,\n",
       "  197,\n",
       "  198,\n",
       "  235,\n",
       "  249,\n",
       "  260,\n",
       "  281,\n",
       "  289,\n",
       "  290,\n",
       "  291,\n",
       "  293,\n",
       "  301,\n",
       "  306,\n",
       "  307,\n",
       "  314,\n",
       "  318,\n",
       "  321,\n",
       "  327,\n",
       "  347,\n",
       "  355,\n",
       "  376,\n",
       "  392,\n",
       "  393,\n",
       "  407,\n",
       "  429,\n",
       "  430,\n",
       "  433,\n",
       "  443,\n",
       "  468,\n",
       "  472,\n",
       "  474,\n",
       "  477,\n",
       "  480,\n",
       "  488,\n",
       "  506,\n",
       "  507,\n",
       "  508,\n",
       "  528,\n",
       "  533,\n",
       "  566,\n",
       "  576,\n",
       "  593,\n",
       "  595,\n",
       "  602,\n",
       "  609,\n",
       "  612,\n",
       "  625,\n",
       "  637,\n",
       "  644,\n",
       "  645,\n",
       "  654,\n",
       "  661,\n",
       "  664,\n",
       "  674,\n",
       "  684,\n",
       "  694,\n",
       "  716,\n",
       "  722,\n",
       "  728,\n",
       "  734,\n",
       "  738,\n",
       "  753,\n",
       "  759,\n",
       "  787,\n",
       "  801,\n",
       "  803,\n",
       "  816,\n",
       "  830,\n",
       "  847,\n",
       "  857,\n",
       "  858,\n",
       "  878,\n",
       "  889,\n",
       "  890,\n",
       "  923,\n",
       "  937,\n",
       "  944,\n",
       "  953,\n",
       "  957,\n",
       "  963,\n",
       "  992,\n",
       "  997,\n",
       "  138,\n",
       "  188,\n",
       "  236,\n",
       "  284,\n",
       "  316,\n",
       "  330,\n",
       "  428,\n",
       "  432,\n",
       "  515,\n",
       "  586,\n",
       "  607,\n",
       "  624,\n",
       "  729,\n",
       "  768,\n",
       "  781,\n",
       "  870,\n",
       "  873,\n",
       "  874,\n",
       "  906,\n",
       "  918,\n",
       "  920,\n",
       "  31,\n",
       "  105,\n",
       "  206,\n",
       "  219,\n",
       "  309,\n",
       "  417,\n",
       "  425,\n",
       "  527,\n",
       "  632,\n",
       "  820,\n",
       "  985,\n",
       "  17,\n",
       "  74,\n",
       "  75,\n",
       "  132,\n",
       "  136,\n",
       "  140,\n",
       "  158,\n",
       "  168,\n",
       "  173,\n",
       "  204,\n",
       "  226,\n",
       "  237,\n",
       "  256,\n",
       "  271,\n",
       "  313,\n",
       "  341,\n",
       "  342,\n",
       "  365,\n",
       "  375,\n",
       "  381,\n",
       "  387,\n",
       "  391,\n",
       "  394,\n",
       "  402,\n",
       "  442,\n",
       "  444,\n",
       "  483,\n",
       "  505,\n",
       "  517,\n",
       "  532,\n",
       "  547,\n",
       "  605,\n",
       "  614,\n",
       "  619,\n",
       "  668,\n",
       "  672,\n",
       "  727,\n",
       "  754,\n",
       "  784,\n",
       "  814,\n",
       "  829,\n",
       "  832,\n",
       "  900,\n",
       "  915,\n",
       "  984,\n",
       "  19,\n",
       "  24,\n",
       "  87,\n",
       "  141,\n",
       "  186,\n",
       "  228,\n",
       "  253,\n",
       "  331,\n",
       "  337,\n",
       "  395,\n",
       "  399,\n",
       "  423,\n",
       "  445,\n",
       "  476,\n",
       "  484,\n",
       "  537,\n",
       "  546,\n",
       "  616,\n",
       "  705,\n",
       "  748,\n",
       "  775,\n",
       "  821,\n",
       "  839,\n",
       "  852,\n",
       "  863])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist, tk = diversity(learn, 10, 95)\n",
    "n, hist, tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "finished creating histogram for the 0th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 1th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 2th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 3th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 4th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 5th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 6th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 7th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 8th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 9th perturbation\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(401,\n",
       " [(971, 193.20),\n",
       "  (669, 32.90),\n",
       "  (828, 32.30),\n",
       "  (904, 26.30),\n",
       "  (794, 23.30),\n",
       "  (55, 16.90),\n",
       "  (556, 16.80),\n",
       "  (61, 15.20),\n",
       "  (39, 14.80),\n",
       "  (711, 11.60),\n",
       "  (599, 10.70),\n",
       "  (489, 10.60),\n",
       "  (588, 10.60),\n",
       "  (721, 9.50),\n",
       "  (490, 9.40),\n",
       "  (955, 9.10),\n",
       "  (84, 7.90),\n",
       "  (419, 6.70),\n",
       "  (584, 6.50),\n",
       "  (401, 6.30),\n",
       "  (604, 5.70),\n",
       "  (62, 5.30),\n",
       "  (38, 5.20),\n",
       "  (770, 5.20),\n",
       "  (1, 4.90),\n",
       "  (641, 4.70),\n",
       "  (709, 4.70),\n",
       "  (973, 4.70),\n",
       "  (750, 4.40),\n",
       "  (151, 4.00),\n",
       "  (837, 4.00),\n",
       "  (581, 3.70),\n",
       "  (824, 3.60),\n",
       "  (893, 3.60),\n",
       "  (48, 3.50),\n",
       "  (772, 3.50),\n",
       "  (651, 3.30),\n",
       "  (155, 3.00),\n",
       "  (464, 2.90),\n",
       "  (539, 2.90),\n",
       "  (63, 2.80),\n",
       "  (572, 2.80),\n",
       "  (762, 2.80),\n",
       "  (107, 2.70),\n",
       "  (108, 2.70),\n",
       "  (441, 2.70),\n",
       "  (680, 2.70),\n",
       "  (552, 2.60),\n",
       "  (843, 2.60),\n",
       "  (865, 2.50),\n",
       "  (60, 2.40),\n",
       "  (806, 2.40),\n",
       "  (826, 2.40),\n",
       "  (116, 2.30),\n",
       "  (124, 2.30),\n",
       "  (46, 2.20),\n",
       "  (300, 2.20),\n",
       "  (629, 2.20),\n",
       "  (0, 2.10),\n",
       "  (319, 2.10),\n",
       "  (397, 2.10),\n",
       "  (496, 2.10),\n",
       "  (621, 2.10),\n",
       "  (722, 2.10),\n",
       "  (746, 2.10),\n",
       "  (879, 2.10),\n",
       "  (68, 2.00),\n",
       "  (94, 2.00),\n",
       "  (292, 2.00),\n",
       "  (406, 2.00),\n",
       "  (457, 2.00),\n",
       "  (520, 2.00),\n",
       "  (605, 2.00),\n",
       "  (96, 1.90),\n",
       "  (431, 1.90),\n",
       "  (754, 1.90),\n",
       "  (855, 1.90),\n",
       "  (872, 1.90),\n",
       "  (193, 1.80),\n",
       "  (850, 1.80),\n",
       "  (109, 1.70),\n",
       "  (353, 1.70),\n",
       "  (482, 1.70),\n",
       "  (575, 1.70),\n",
       "  (741, 1.70),\n",
       "  (800, 1.70),\n",
       "  (848, 1.70),\n",
       "  (40, 1.60),\n",
       "  (230, 1.60),\n",
       "  (341, 1.60),\n",
       "  (551, 1.60),\n",
       "  (732, 1.60),\n",
       "  (815, 1.60),\n",
       "  (907, 1.60),\n",
       "  (247, 1.50),\n",
       "  (304, 1.50),\n",
       "  (611, 1.50),\n",
       "  (632, 1.50),\n",
       "  (679, 1.50),\n",
       "  (864, 1.50),\n",
       "  (996, 1.50),\n",
       "  (7, 1.40),\n",
       "  (176, 1.40),\n",
       "  (360, 1.40),\n",
       "  (398, 1.40),\n",
       "  (411, 1.40),\n",
       "  (453, 1.40),\n",
       "  (547, 1.40),\n",
       "  (602, 1.40),\n",
       "  (620, 1.40),\n",
       "  (646, 1.40),\n",
       "  (717, 1.40),\n",
       "  (748, 1.40),\n",
       "  (751, 1.40),\n",
       "  (783, 1.40),\n",
       "  (870, 1.40),\n",
       "  (97, 1.30),\n",
       "  (123, 1.30),\n",
       "  (173, 1.30),\n",
       "  (178, 1.30),\n",
       "  (334, 1.30),\n",
       "  (393, 1.30),\n",
       "  (396, 1.30),\n",
       "  (414, 1.30),\n",
       "  (440, 1.30),\n",
       "  (555, 1.30),\n",
       "  (567, 1.30),\n",
       "  (712, 1.30),\n",
       "  (777, 1.30),\n",
       "  (791, 1.30),\n",
       "  (868, 1.30),\n",
       "  (871, 1.30),\n",
       "  (892, 1.30),\n",
       "  (987, 1.30),\n",
       "  (45, 1.20),\n",
       "  (91, 1.20),\n",
       "  (118, 1.20),\n",
       "  (120, 1.20),\n",
       "  (205, 1.20),\n",
       "  (211, 1.20),\n",
       "  (218, 1.20),\n",
       "  (337, 1.20),\n",
       "  (363, 1.20),\n",
       "  (417, 1.20),\n",
       "  (424, 1.20),\n",
       "  (432, 1.20),\n",
       "  (498, 1.20),\n",
       "  (515, 1.20),\n",
       "  (538, 1.20),\n",
       "  (580, 1.20),\n",
       "  (593, 1.20),\n",
       "  (674, 1.20),\n",
       "  (786, 1.20),\n",
       "  (796, 1.20),\n",
       "  (42, 1.10),\n",
       "  (74, 1.10),\n",
       "  (90, 1.10),\n",
       "  (195, 1.10),\n",
       "  (242, 1.10),\n",
       "  (430, 1.10),\n",
       "  (474, 1.10),\n",
       "  (480, 1.10),\n",
       "  (492, 1.10),\n",
       "  (558, 1.10),\n",
       "  (613, 1.10),\n",
       "  (692, 1.10),\n",
       "  (788, 1.10),\n",
       "  (801, 1.10),\n",
       "  (805, 1.10),\n",
       "  (920, 1.10),\n",
       "  (981, 1.10),\n",
       "  (982, 1.10),\n",
       "  (8, 1.00),\n",
       "  (25, 1.00),\n",
       "  (28, 1.00),\n",
       "  (33, 1.00),\n",
       "  (37, 1.00),\n",
       "  (52, 1.00),\n",
       "  (57, 1.00),\n",
       "  (76, 1.00),\n",
       "  (115, 1.00),\n",
       "  (128, 1.00),\n",
       "  (144, 1.00),\n",
       "  (163, 1.00),\n",
       "  (164, 1.00),\n",
       "  (171, 1.00),\n",
       "  (198, 1.00),\n",
       "  (219, 1.00),\n",
       "  (235, 1.00),\n",
       "  (260, 1.00),\n",
       "  (274, 1.00),\n",
       "  (281, 1.00),\n",
       "  (289, 1.00),\n",
       "  (290, 1.00),\n",
       "  (291, 1.00),\n",
       "  (293, 1.00),\n",
       "  (294, 1.00),\n",
       "  (301, 1.00),\n",
       "  (306, 1.00),\n",
       "  (307, 1.00),\n",
       "  (314, 1.00),\n",
       "  (316, 1.00),\n",
       "  (318, 1.00),\n",
       "  (321, 1.00),\n",
       "  (327, 1.00),\n",
       "  (347, 1.00),\n",
       "  (348, 1.00),\n",
       "  (355, 1.00),\n",
       "  (376, 1.00),\n",
       "  (392, 1.00),\n",
       "  (429, 1.00),\n",
       "  (433, 1.00),\n",
       "  (443, 1.00),\n",
       "  (468, 1.00),\n",
       "  (472, 1.00),\n",
       "  (477, 1.00),\n",
       "  (488, 1.00),\n",
       "  (491, 1.00),\n",
       "  (506, 1.00),\n",
       "  (507, 1.00),\n",
       "  (508, 1.00),\n",
       "  (528, 1.00),\n",
       "  (530, 1.00),\n",
       "  (533, 1.00),\n",
       "  (535, 1.00),\n",
       "  (566, 1.00),\n",
       "  (570, 1.00),\n",
       "  (576, 1.00),\n",
       "  (586, 1.00),\n",
       "  (595, 1.00),\n",
       "  (609, 1.00),\n",
       "  (612, 1.00),\n",
       "  (625, 1.00),\n",
       "  (637, 1.00),\n",
       "  (645, 1.00),\n",
       "  (654, 1.00),\n",
       "  (661, 1.00),\n",
       "  (664, 1.00),\n",
       "  (684, 1.00),\n",
       "  (694, 1.00),\n",
       "  (703, 1.00),\n",
       "  (716, 1.00),\n",
       "  (719, 1.00),\n",
       "  (734, 1.00),\n",
       "  (735, 1.00),\n",
       "  (738, 1.00),\n",
       "  (753, 1.00),\n",
       "  (759, 1.00),\n",
       "  (787, 1.00),\n",
       "  (803, 1.00),\n",
       "  (816, 1.00),\n",
       "  (847, 1.00),\n",
       "  (857, 1.00),\n",
       "  (858, 1.00),\n",
       "  (878, 1.00),\n",
       "  (880, 1.00),\n",
       "  (889, 1.00),\n",
       "  (890, 1.00),\n",
       "  (923, 1.00),\n",
       "  (934, 1.00),\n",
       "  (937, 1.00),\n",
       "  (944, 1.00),\n",
       "  (953, 1.00),\n",
       "  (957, 1.00),\n",
       "  (992, 1.00),\n",
       "  (997, 1.00),\n",
       "  (9, 0.90),\n",
       "  (47, 0.90),\n",
       "  (86, 0.90),\n",
       "  (92, 0.90),\n",
       "  (102, 0.90),\n",
       "  (188, 0.90),\n",
       "  (206, 0.90),\n",
       "  (236, 0.90),\n",
       "  (275, 0.90),\n",
       "  (330, 0.90),\n",
       "  (336, 0.90),\n",
       "  (509, 0.90),\n",
       "  (565, 0.90),\n",
       "  (579, 0.90),\n",
       "  (607, 0.90),\n",
       "  (619, 0.90),\n",
       "  (698, 0.90),\n",
       "  (727, 0.90),\n",
       "  (781, 0.90),\n",
       "  (844, 0.90),\n",
       "  (873, 0.90),\n",
       "  (898, 0.90),\n",
       "  (17, 0.80),\n",
       "  (83, 0.80),\n",
       "  (134, 0.80),\n",
       "  (189, 0.80),\n",
       "  (197, 0.80),\n",
       "  (226, 0.80),\n",
       "  (249, 0.80),\n",
       "  (369, 0.80),\n",
       "  (382, 0.80),\n",
       "  (389, 0.80),\n",
       "  (407, 0.80),\n",
       "  (425, 0.80),\n",
       "  (428, 0.80),\n",
       "  (562, 0.80),\n",
       "  (633, 0.80),\n",
       "  (638, 0.80),\n",
       "  (706, 0.80),\n",
       "  (728, 0.80),\n",
       "  (729, 0.80),\n",
       "  (758, 0.80),\n",
       "  (819, 0.80),\n",
       "  (918, 0.80),\n",
       "  (939, 0.80),\n",
       "  (88, 0.70),\n",
       "  (105, 0.70),\n",
       "  (186, 0.70),\n",
       "  (204, 0.70),\n",
       "  (228, 0.70),\n",
       "  (231, 0.70),\n",
       "  (253, 0.70),\n",
       "  (284, 0.70),\n",
       "  (313, 0.70),\n",
       "  (350, 0.70),\n",
       "  (358, 0.70),\n",
       "  (375, 0.70),\n",
       "  (402, 0.70),\n",
       "  (408, 0.70),\n",
       "  (470, 0.70),\n",
       "  (518, 0.70),\n",
       "  (560, 0.70),\n",
       "  (644, 0.70),\n",
       "  (668, 0.70),\n",
       "  (743, 0.70),\n",
       "  (745, 0.70),\n",
       "  (768, 0.70),\n",
       "  (811, 0.70),\n",
       "  (874, 0.70),\n",
       "  (882, 0.70),\n",
       "  (910, 0.70),\n",
       "  (988, 0.70),\n",
       "  (990, 0.70),\n",
       "  (15, 0.60),\n",
       "  (75, 0.60),\n",
       "  (87, 0.60),\n",
       "  (265, 0.60),\n",
       "  (305, 0.60),\n",
       "  (381, 0.60),\n",
       "  (391, 0.60),\n",
       "  (394, 0.60),\n",
       "  (410, 0.60),\n",
       "  (412, 0.60),\n",
       "  (476, 0.60),\n",
       "  (481, 0.60),\n",
       "  (483, 0.60),\n",
       "  (495, 0.60),\n",
       "  (505, 0.60),\n",
       "  (522, 0.60),\n",
       "  (527, 0.60),\n",
       "  (545, 0.60),\n",
       "  (582, 0.60),\n",
       "  (687, 0.60),\n",
       "  (710, 0.60),\n",
       "  (731, 0.60),\n",
       "  (752, 0.60),\n",
       "  (766, 0.60),\n",
       "  (784, 0.60),\n",
       "  (808, 0.60),\n",
       "  (809, 0.60),\n",
       "  (820, 0.60),\n",
       "  (906, 0.60),\n",
       "  (963, 0.60),\n",
       "  (985, 0.60),\n",
       "  (98, 0.50),\n",
       "  (99, 0.50),\n",
       "  (117, 0.50),\n",
       "  (140, 0.50),\n",
       "  (141, 0.50),\n",
       "  (149, 0.50),\n",
       "  (156, 0.50),\n",
       "  (159, 0.50),\n",
       "  (201, 0.50),\n",
       "  (282, 0.50),\n",
       "  (308, 0.50),\n",
       "  (365, 0.50),\n",
       "  (388, 0.50),\n",
       "  (399, 0.50),\n",
       "  (415, 0.50),\n",
       "  (445, 0.50),\n",
       "  (463, 0.50),\n",
       "  (591, 0.50),\n",
       "  (606, 0.50),\n",
       "  (614, 0.50),\n",
       "  (647, 0.50),\n",
       "  (650, 0.50),\n",
       "  (757, 0.50),\n",
       "  (774, 0.50),\n",
       "  (790, 0.50),\n",
       "  (802, 0.50),\n",
       "  (804, 0.50),\n",
       "  (817, 0.50),\n",
       "  (854, 0.50),\n",
       "  (859, 0.50),\n",
       "  (902, 0.50),\n",
       "  (926, 0.50),\n",
       "  (932, 0.50),\n",
       "  (946, 0.50),\n",
       "  (959, 0.50),\n",
       "  (989, 0.50),\n",
       "  (53, 0.40),\n",
       "  (70, 0.40),\n",
       "  (95, 0.40),\n",
       "  (122, 0.40),\n",
       "  (125, 0.40),\n",
       "  (132, 0.40),\n",
       "  (136, 0.40),\n",
       "  (142, 0.40),\n",
       "  (161, 0.40),\n",
       "  (180, 0.40),\n",
       "  (187, 0.40),\n",
       "  (192, 0.40),\n",
       "  (199, 0.40),\n",
       "  (224, 0.40),\n",
       "  (238, 0.40),\n",
       "  (303, 0.40),\n",
       "  (309, 0.40),\n",
       "  (310, 0.40),\n",
       "  (328, 0.40),\n",
       "  (333, 0.40),\n",
       "  (344, 0.40),\n",
       "  (416, 0.40),\n",
       "  (423, 0.40),\n",
       "  (436, 0.40),\n",
       "  (444, 0.40),\n",
       "  (447, 0.40),\n",
       "  (459, 0.40),\n",
       "  (511, 0.40),\n",
       "  (550, 0.40),\n",
       "  (615, 0.40),\n",
       "  (616, 0.40),\n",
       "  (636, 0.40),\n",
       "  (643, 0.40),\n",
       "  (655, 0.40),\n",
       "  (696, 0.40),\n",
       "  (720, 0.40),\n",
       "  (726, 0.40),\n",
       "  (736, 0.40),\n",
       "  (779, 0.40),\n",
       "  (822, 0.40),\n",
       "  (831, 0.40),\n",
       "  (834, 0.40),\n",
       "  (875, 0.40),\n",
       "  (896, 0.40),\n",
       "  (897, 0.40),\n",
       "  (966, 0.40),\n",
       "  (64, 0.30),\n",
       "  (82, 0.30),\n",
       "  (110, 0.30),\n",
       "  (112, 0.30),\n",
       "  (113, 0.30),\n",
       "  (138, 0.30),\n",
       "  (166, 0.30),\n",
       "  (184, 0.30),\n",
       "  (191, 0.30),\n",
       "  (214, 0.30),\n",
       "  (229, 0.30),\n",
       "  (252, 0.30),\n",
       "  (256, 0.30),\n",
       "  (266, 0.30),\n",
       "  (271, 0.30),\n",
       "  (315, 0.30),\n",
       "  (340, 0.30),\n",
       "  (342, 0.30),\n",
       "  (387, 0.30),\n",
       "  (442, 0.30),\n",
       "  (484, 0.30),\n",
       "  (514, 0.30),\n",
       "  (517, 0.30),\n",
       "  (526, 0.30),\n",
       "  (534, 0.30),\n",
       "  (546, 0.30),\n",
       "  (554, 0.30),\n",
       "  (568, 0.30),\n",
       "  (577, 0.30),\n",
       "  (624, 0.30),\n",
       "  (665, 0.30),\n",
       "  (672, 0.30),\n",
       "  (676, 0.30),\n",
       "  (691, 0.30),\n",
       "  (705, 0.30),\n",
       "  (723, 0.30),\n",
       "  (755, 0.30),\n",
       "  (761, 0.30),\n",
       "  (814, 0.30),\n",
       "  (821, 0.30),\n",
       "  (829, 0.30),\n",
       "  (830, 0.30),\n",
       "  (851, 0.30),\n",
       "  (863, 0.30),\n",
       "  (866, 0.30),\n",
       "  (881, 0.30),\n",
       "  (884, 0.30),\n",
       "  (885, 0.30),\n",
       "  (900, 0.30),\n",
       "  (915, 0.30),\n",
       "  (925, 0.30),\n",
       "  (984, 0.30),\n",
       "  (11, 0.20),\n",
       "  (19, 0.20),\n",
       "  (24, 0.20),\n",
       "  (31, 0.20),\n",
       "  (41, 0.20),\n",
       "  (65, 0.20),\n",
       "  (67, 0.20),\n",
       "  (79, 0.20),\n",
       "  (93, 0.20),\n",
       "  (153, 0.20),\n",
       "  (158, 0.20),\n",
       "  (168, 0.20),\n",
       "  (183, 0.20),\n",
       "  (203, 0.20),\n",
       "  (213, 0.20),\n",
       "  (216, 0.20),\n",
       "  (237, 0.20),\n",
       "  (254, 0.20),\n",
       "  (298, 0.20),\n",
       "  (331, 0.20),\n",
       "  (409, 0.20),\n",
       "  (485, 0.20),\n",
       "  (497, 0.20),\n",
       "  (529, 0.20),\n",
       "  (532, 0.20),\n",
       "  (549, 0.20),\n",
       "  (626, 0.20),\n",
       "  (627, 0.20),\n",
       "  (725, 0.20),\n",
       "  (775, 0.20),\n",
       "  (832, 0.20),\n",
       "  (852, 0.20),\n",
       "  (867, 0.20),\n",
       "  (886, 0.20),\n",
       "  (899, 0.20),\n",
       "  (911, 0.20),\n",
       "  (912, 0.20),\n",
       "  (938, 0.20),\n",
       "  (947, 0.20),\n",
       "  (976, 0.20),\n",
       "  (12, 0.10),\n",
       "  (21, 0.10),\n",
       "  (50, 0.10),\n",
       "  (72, 0.10),\n",
       "  (100, 0.10),\n",
       "  (114, 0.10),\n",
       "  (129, 0.10),\n",
       "  (139, 0.10),\n",
       "  (160, 0.10),\n",
       "  (172, 0.10),\n",
       "  (177, 0.10),\n",
       "  (196, 0.10),\n",
       "  (209, 0.10),\n",
       "  (234, 0.10),\n",
       "  (251, 0.10),\n",
       "  (323, 0.10),\n",
       "  (343, 0.10),\n",
       "  (354, 0.10),\n",
       "  (384, 0.10),\n",
       "  (395, 0.10),\n",
       "  (422, 0.10),\n",
       "  (537, 0.10),\n",
       "  (541, 0.10),\n",
       "  (544, 0.10),\n",
       "  (563, 0.10),\n",
       "  (564, 0.10),\n",
       "  (594, 0.10),\n",
       "  (639, 0.10),\n",
       "  (642, 0.10),\n",
       "  (670, 0.10),\n",
       "  (683, 0.10),\n",
       "  (697, 0.10),\n",
       "  (699, 0.10),\n",
       "  (700, 0.10),\n",
       "  (701, 0.10),\n",
       "  (707, 0.10),\n",
       "  (737, 0.10),\n",
       "  (749, 0.10),\n",
       "  (765, 0.10),\n",
       "  (776, 0.10),\n",
       "  (813, 0.10),\n",
       "  (825, 0.10),\n",
       "  (833, 0.10),\n",
       "  (836, 0.10),\n",
       "  (838, 0.10),\n",
       "  (839, 0.10),\n",
       "  (853, 0.10),\n",
       "  (876, 0.10),\n",
       "  (894, 0.10),\n",
       "  (917, 0.10),\n",
       "  (921, 0.10),\n",
       "  (991, 0.10),\n",
       "  (2, 0.00),\n",
       "  (3, 0.00),\n",
       "  (4, 0.00),\n",
       "  (5, 0.00),\n",
       "  (6, 0.00),\n",
       "  (10, 0.00),\n",
       "  (13, 0.00),\n",
       "  (14, 0.00),\n",
       "  (16, 0.00),\n",
       "  (18, 0.00),\n",
       "  (20, 0.00),\n",
       "  (22, 0.00),\n",
       "  (23, 0.00),\n",
       "  (26, 0.00),\n",
       "  (27, 0.00),\n",
       "  (29, 0.00),\n",
       "  (30, 0.00),\n",
       "  (32, 0.00),\n",
       "  (34, 0.00),\n",
       "  (35, 0.00),\n",
       "  (36, 0.00),\n",
       "  (43, 0.00),\n",
       "  (44, 0.00),\n",
       "  (49, 0.00),\n",
       "  (51, 0.00),\n",
       "  (54, 0.00),\n",
       "  (56, 0.00),\n",
       "  (58, 0.00),\n",
       "  (59, 0.00),\n",
       "  (66, 0.00),\n",
       "  (69, 0.00),\n",
       "  (71, 0.00),\n",
       "  (73, 0.00),\n",
       "  (77, 0.00),\n",
       "  (78, 0.00),\n",
       "  (80, 0.00),\n",
       "  (81, 0.00),\n",
       "  (85, 0.00),\n",
       "  (89, 0.00),\n",
       "  (101, 0.00),\n",
       "  (103, 0.00),\n",
       "  (104, 0.00),\n",
       "  (106, 0.00),\n",
       "  (111, 0.00),\n",
       "  (119, 0.00),\n",
       "  (121, 0.00),\n",
       "  (126, 0.00),\n",
       "  (127, 0.00),\n",
       "  (130, 0.00),\n",
       "  (131, 0.00),\n",
       "  (133, 0.00),\n",
       "  (135, 0.00),\n",
       "  (137, 0.00),\n",
       "  (143, 0.00),\n",
       "  (145, 0.00),\n",
       "  (146, 0.00),\n",
       "  (147, 0.00),\n",
       "  (148, 0.00),\n",
       "  (150, 0.00),\n",
       "  (152, 0.00),\n",
       "  (154, 0.00),\n",
       "  (157, 0.00),\n",
       "  (162, 0.00),\n",
       "  (165, 0.00),\n",
       "  (167, 0.00),\n",
       "  (169, 0.00),\n",
       "  (170, 0.00),\n",
       "  (174, 0.00),\n",
       "  (175, 0.00),\n",
       "  (179, 0.00),\n",
       "  (181, 0.00),\n",
       "  (182, 0.00),\n",
       "  (185, 0.00),\n",
       "  (190, 0.00),\n",
       "  (194, 0.00),\n",
       "  (200, 0.00),\n",
       "  (202, 0.00),\n",
       "  (207, 0.00),\n",
       "  (208, 0.00),\n",
       "  (210, 0.00),\n",
       "  (212, 0.00),\n",
       "  (215, 0.00),\n",
       "  (217, 0.00),\n",
       "  (220, 0.00),\n",
       "  (221, 0.00),\n",
       "  (222, 0.00),\n",
       "  (223, 0.00),\n",
       "  (225, 0.00),\n",
       "  (227, 0.00),\n",
       "  (232, 0.00),\n",
       "  (233, 0.00),\n",
       "  (239, 0.00),\n",
       "  (240, 0.00),\n",
       "  (241, 0.00),\n",
       "  (243, 0.00),\n",
       "  (244, 0.00),\n",
       "  (245, 0.00),\n",
       "  (246, 0.00),\n",
       "  (248, 0.00),\n",
       "  (250, 0.00),\n",
       "  (255, 0.00),\n",
       "  (257, 0.00),\n",
       "  (258, 0.00),\n",
       "  (259, 0.00),\n",
       "  (261, 0.00),\n",
       "  (262, 0.00),\n",
       "  (263, 0.00),\n",
       "  (264, 0.00),\n",
       "  (267, 0.00),\n",
       "  (268, 0.00),\n",
       "  (269, 0.00),\n",
       "  (270, 0.00),\n",
       "  (272, 0.00),\n",
       "  (273, 0.00),\n",
       "  (276, 0.00),\n",
       "  (277, 0.00),\n",
       "  (278, 0.00),\n",
       "  (279, 0.00),\n",
       "  (280, 0.00),\n",
       "  (283, 0.00),\n",
       "  (285, 0.00),\n",
       "  (286, 0.00),\n",
       "  (287, 0.00),\n",
       "  (288, 0.00),\n",
       "  (295, 0.00),\n",
       "  (296, 0.00),\n",
       "  (297, 0.00),\n",
       "  (299, 0.00),\n",
       "  (302, 0.00),\n",
       "  (311, 0.00),\n",
       "  (312, 0.00),\n",
       "  (317, 0.00),\n",
       "  (320, 0.00),\n",
       "  (322, 0.00),\n",
       "  (324, 0.00),\n",
       "  (325, 0.00),\n",
       "  (326, 0.00),\n",
       "  (329, 0.00),\n",
       "  (332, 0.00),\n",
       "  (335, 0.00),\n",
       "  (338, 0.00),\n",
       "  (339, 0.00),\n",
       "  (345, 0.00),\n",
       "  (346, 0.00),\n",
       "  (349, 0.00),\n",
       "  (351, 0.00),\n",
       "  (352, 0.00),\n",
       "  (356, 0.00),\n",
       "  (357, 0.00),\n",
       "  (359, 0.00),\n",
       "  (361, 0.00),\n",
       "  (362, 0.00),\n",
       "  (364, 0.00),\n",
       "  (366, 0.00),\n",
       "  (367, 0.00),\n",
       "  (368, 0.00),\n",
       "  (370, 0.00),\n",
       "  (371, 0.00),\n",
       "  (372, 0.00),\n",
       "  (373, 0.00),\n",
       "  (374, 0.00),\n",
       "  (377, 0.00),\n",
       "  (378, 0.00),\n",
       "  (379, 0.00),\n",
       "  (380, 0.00),\n",
       "  (383, 0.00),\n",
       "  (385, 0.00),\n",
       "  (386, 0.00),\n",
       "  (390, 0.00),\n",
       "  (400, 0.00),\n",
       "  (403, 0.00),\n",
       "  (404, 0.00),\n",
       "  (405, 0.00),\n",
       "  (413, 0.00),\n",
       "  (418, 0.00),\n",
       "  (420, 0.00),\n",
       "  (421, 0.00),\n",
       "  (426, 0.00),\n",
       "  (427, 0.00),\n",
       "  (434, 0.00),\n",
       "  (435, 0.00),\n",
       "  (437, 0.00),\n",
       "  (438, 0.00),\n",
       "  (439, 0.00),\n",
       "  (446, 0.00),\n",
       "  (448, 0.00),\n",
       "  (449, 0.00),\n",
       "  (450, 0.00),\n",
       "  (451, 0.00),\n",
       "  (452, 0.00),\n",
       "  (454, 0.00),\n",
       "  (455, 0.00),\n",
       "  (456, 0.00),\n",
       "  (458, 0.00),\n",
       "  (460, 0.00),\n",
       "  (461, 0.00),\n",
       "  (462, 0.00),\n",
       "  (465, 0.00),\n",
       "  (466, 0.00),\n",
       "  (467, 0.00),\n",
       "  (469, 0.00),\n",
       "  (471, 0.00),\n",
       "  (473, 0.00),\n",
       "  (475, 0.00),\n",
       "  (478, 0.00),\n",
       "  (479, 0.00),\n",
       "  (486, 0.00),\n",
       "  (487, 0.00),\n",
       "  (493, 0.00),\n",
       "  (494, 0.00),\n",
       "  (499, 0.00),\n",
       "  (500, 0.00),\n",
       "  (501, 0.00),\n",
       "  (502, 0.00),\n",
       "  (503, 0.00),\n",
       "  (504, 0.00),\n",
       "  (510, 0.00),\n",
       "  (512, 0.00),\n",
       "  (513, 0.00),\n",
       "  (516, 0.00),\n",
       "  (519, 0.00),\n",
       "  (521, 0.00),\n",
       "  (523, 0.00),\n",
       "  (524, 0.00),\n",
       "  (525, 0.00),\n",
       "  (531, 0.00),\n",
       "  (536, 0.00),\n",
       "  (540, 0.00),\n",
       "  (542, 0.00),\n",
       "  (543, 0.00),\n",
       "  (548, 0.00),\n",
       "  (553, 0.00),\n",
       "  (557, 0.00),\n",
       "  (559, 0.00),\n",
       "  (561, 0.00),\n",
       "  (569, 0.00),\n",
       "  (571, 0.00),\n",
       "  (573, 0.00),\n",
       "  (574, 0.00),\n",
       "  (578, 0.00),\n",
       "  (583, 0.00),\n",
       "  (585, 0.00),\n",
       "  (587, 0.00),\n",
       "  (589, 0.00),\n",
       "  (590, 0.00),\n",
       "  (592, 0.00),\n",
       "  (596, 0.00),\n",
       "  (597, 0.00),\n",
       "  (598, 0.00),\n",
       "  (600, 0.00),\n",
       "  (601, 0.00),\n",
       "  (603, 0.00),\n",
       "  (608, 0.00),\n",
       "  (610, 0.00),\n",
       "  (617, 0.00),\n",
       "  (618, 0.00),\n",
       "  (622, 0.00),\n",
       "  (623, 0.00),\n",
       "  (628, 0.00),\n",
       "  (630, 0.00),\n",
       "  (631, 0.00),\n",
       "  (634, 0.00),\n",
       "  (635, 0.00),\n",
       "  (640, 0.00),\n",
       "  (648, 0.00),\n",
       "  (649, 0.00),\n",
       "  (652, 0.00),\n",
       "  (653, 0.00),\n",
       "  (656, 0.00),\n",
       "  (657, 0.00),\n",
       "  (658, 0.00),\n",
       "  (659, 0.00),\n",
       "  (660, 0.00),\n",
       "  (662, 0.00),\n",
       "  (663, 0.00),\n",
       "  (666, 0.00),\n",
       "  (667, 0.00),\n",
       "  (671, 0.00),\n",
       "  (673, 0.00),\n",
       "  (675, 0.00),\n",
       "  (677, 0.00),\n",
       "  (678, 0.00),\n",
       "  (681, 0.00),\n",
       "  (682, 0.00),\n",
       "  (685, 0.00),\n",
       "  (686, 0.00),\n",
       "  (688, 0.00),\n",
       "  (689, 0.00),\n",
       "  (690, 0.00),\n",
       "  (693, 0.00),\n",
       "  (695, 0.00),\n",
       "  (702, 0.00),\n",
       "  (704, 0.00),\n",
       "  (708, 0.00),\n",
       "  (713, 0.00),\n",
       "  (714, 0.00),\n",
       "  (715, 0.00),\n",
       "  (718, 0.00),\n",
       "  (724, 0.00),\n",
       "  (730, 0.00),\n",
       "  (733, 0.00),\n",
       "  (739, 0.00),\n",
       "  (740, 0.00),\n",
       "  (742, 0.00),\n",
       "  (744, 0.00),\n",
       "  (747, 0.00),\n",
       "  (756, 0.00),\n",
       "  (760, 0.00),\n",
       "  (763, 0.00),\n",
       "  (764, 0.00),\n",
       "  (767, 0.00),\n",
       "  (769, 0.00),\n",
       "  (771, 0.00),\n",
       "  (773, 0.00),\n",
       "  (778, 0.00),\n",
       "  (780, 0.00),\n",
       "  (782, 0.00),\n",
       "  (785, 0.00),\n",
       "  (789, 0.00),\n",
       "  (792, 0.00),\n",
       "  (793, 0.00),\n",
       "  (795, 0.00),\n",
       "  (797, 0.00),\n",
       "  (798, 0.00),\n",
       "  (799, 0.00),\n",
       "  (807, 0.00),\n",
       "  (810, 0.00),\n",
       "  (812, 0.00),\n",
       "  (818, 0.00),\n",
       "  (823, 0.00),\n",
       "  (827, 0.00),\n",
       "  (835, 0.00),\n",
       "  (840, 0.00),\n",
       "  (841, 0.00),\n",
       "  (842, 0.00),\n",
       "  (845, 0.00),\n",
       "  (846, 0.00),\n",
       "  (849, 0.00),\n",
       "  (856, 0.00),\n",
       "  (860, 0.00),\n",
       "  (861, 0.00),\n",
       "  (862, 0.00),\n",
       "  (869, 0.00),\n",
       "  (877, 0.00),\n",
       "  (883, 0.00),\n",
       "  (887, 0.00),\n",
       "  (888, 0.00),\n",
       "  (891, 0.00),\n",
       "  (895, 0.00),\n",
       "  (901, 0.00),\n",
       "  (903, 0.00),\n",
       "  (905, 0.00),\n",
       "  (908, 0.00),\n",
       "  (909, 0.00),\n",
       "  (913, 0.00),\n",
       "  (914, 0.00),\n",
       "  (916, 0.00),\n",
       "  (919, 0.00),\n",
       "  (922, 0.00),\n",
       "  (924, 0.00),\n",
       "  (927, 0.00),\n",
       "  (928, 0.00),\n",
       "  (929, 0.00),\n",
       "  (930, 0.00),\n",
       "  (931, 0.00),\n",
       "  (933, 0.00),\n",
       "  (935, 0.00),\n",
       "  (936, 0.00),\n",
       "  (940, 0.00),\n",
       "  (941, 0.00),\n",
       "  (942, 0.00),\n",
       "  (943, 0.00),\n",
       "  (945, 0.00),\n",
       "  (948, 0.00),\n",
       "  (949, 0.00),\n",
       "  (950, 0.00),\n",
       "  (951, 0.00),\n",
       "  (952, 0.00),\n",
       "  (954, 0.00),\n",
       "  (956, 0.00),\n",
       "  (958, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (962, 0.00),\n",
       "  (964, 0.00),\n",
       "  (965, 0.00),\n",
       "  (967, 0.00),\n",
       "  (968, 0.00),\n",
       "  (969, 0.00),\n",
       "  (970, 0.00),\n",
       "  (972, 0.00),\n",
       "  (974, 0.00),\n",
       "  (975, 0.00),\n",
       "  (977, 0.00),\n",
       "  (978, 0.00),\n",
       "  (979, 0.00),\n",
       "  (980, 0.00),\n",
       "  (983, 0.00),\n",
       "  (986, 0.00),\n",
       "  (993, 0.00),\n",
       "  (994, 0.00),\n",
       "  (995, 0.00),\n",
       "  (998, 0.00),\n",
       "  (999, 0.00)],\n",
       " [971,\n",
       "  669,\n",
       "  828,\n",
       "  904,\n",
       "  794,\n",
       "  55,\n",
       "  556,\n",
       "  61,\n",
       "  39,\n",
       "  711,\n",
       "  599,\n",
       "  489,\n",
       "  588,\n",
       "  721,\n",
       "  490,\n",
       "  955,\n",
       "  84,\n",
       "  419,\n",
       "  584,\n",
       "  401,\n",
       "  604,\n",
       "  62,\n",
       "  38,\n",
       "  770,\n",
       "  1,\n",
       "  641,\n",
       "  709,\n",
       "  973,\n",
       "  750,\n",
       "  151,\n",
       "  837,\n",
       "  581,\n",
       "  824,\n",
       "  893,\n",
       "  48,\n",
       "  772,\n",
       "  651,\n",
       "  155,\n",
       "  464,\n",
       "  539,\n",
       "  63,\n",
       "  572,\n",
       "  762,\n",
       "  107,\n",
       "  108,\n",
       "  441,\n",
       "  680,\n",
       "  552,\n",
       "  843,\n",
       "  865,\n",
       "  60,\n",
       "  806,\n",
       "  826,\n",
       "  116,\n",
       "  124,\n",
       "  46,\n",
       "  300,\n",
       "  629,\n",
       "  0,\n",
       "  319,\n",
       "  397,\n",
       "  496,\n",
       "  621,\n",
       "  722,\n",
       "  746,\n",
       "  879,\n",
       "  68,\n",
       "  94,\n",
       "  292,\n",
       "  406,\n",
       "  457,\n",
       "  520,\n",
       "  605,\n",
       "  96,\n",
       "  431,\n",
       "  754,\n",
       "  855,\n",
       "  872,\n",
       "  193,\n",
       "  850,\n",
       "  109,\n",
       "  353,\n",
       "  482,\n",
       "  575,\n",
       "  741,\n",
       "  800,\n",
       "  848,\n",
       "  40,\n",
       "  230,\n",
       "  341,\n",
       "  551,\n",
       "  732,\n",
       "  815,\n",
       "  907,\n",
       "  247,\n",
       "  304,\n",
       "  611,\n",
       "  632,\n",
       "  679,\n",
       "  864,\n",
       "  996,\n",
       "  7,\n",
       "  176,\n",
       "  360,\n",
       "  398,\n",
       "  411,\n",
       "  453,\n",
       "  547,\n",
       "  602,\n",
       "  620,\n",
       "  646,\n",
       "  717,\n",
       "  748,\n",
       "  751,\n",
       "  783,\n",
       "  870,\n",
       "  97,\n",
       "  123,\n",
       "  173,\n",
       "  178,\n",
       "  334,\n",
       "  393,\n",
       "  396,\n",
       "  414,\n",
       "  440,\n",
       "  555,\n",
       "  567,\n",
       "  712,\n",
       "  777,\n",
       "  791,\n",
       "  868,\n",
       "  871,\n",
       "  892,\n",
       "  987,\n",
       "  45,\n",
       "  91,\n",
       "  118,\n",
       "  120,\n",
       "  205,\n",
       "  211,\n",
       "  218,\n",
       "  337,\n",
       "  363,\n",
       "  417,\n",
       "  424,\n",
       "  432,\n",
       "  498,\n",
       "  515,\n",
       "  538,\n",
       "  580,\n",
       "  593,\n",
       "  674,\n",
       "  786,\n",
       "  796,\n",
       "  42,\n",
       "  74,\n",
       "  90,\n",
       "  195,\n",
       "  242,\n",
       "  430,\n",
       "  474,\n",
       "  480,\n",
       "  492,\n",
       "  558,\n",
       "  613,\n",
       "  692,\n",
       "  788,\n",
       "  801,\n",
       "  805,\n",
       "  920,\n",
       "  981,\n",
       "  982,\n",
       "  8,\n",
       "  25,\n",
       "  28,\n",
       "  33,\n",
       "  37,\n",
       "  52,\n",
       "  57,\n",
       "  76,\n",
       "  115,\n",
       "  128,\n",
       "  144,\n",
       "  163,\n",
       "  164,\n",
       "  171,\n",
       "  198,\n",
       "  219,\n",
       "  235,\n",
       "  260,\n",
       "  274,\n",
       "  281,\n",
       "  289,\n",
       "  290,\n",
       "  291,\n",
       "  293,\n",
       "  294,\n",
       "  301,\n",
       "  306,\n",
       "  307,\n",
       "  314,\n",
       "  316,\n",
       "  318,\n",
       "  321,\n",
       "  327,\n",
       "  347,\n",
       "  348,\n",
       "  355,\n",
       "  376,\n",
       "  392,\n",
       "  429,\n",
       "  433,\n",
       "  443,\n",
       "  468,\n",
       "  472,\n",
       "  477,\n",
       "  488,\n",
       "  491,\n",
       "  506,\n",
       "  507,\n",
       "  508,\n",
       "  528,\n",
       "  530,\n",
       "  533,\n",
       "  535,\n",
       "  566,\n",
       "  570,\n",
       "  576,\n",
       "  586,\n",
       "  595,\n",
       "  609,\n",
       "  612,\n",
       "  625,\n",
       "  637,\n",
       "  645,\n",
       "  654,\n",
       "  661,\n",
       "  664,\n",
       "  684,\n",
       "  694,\n",
       "  703,\n",
       "  716,\n",
       "  719,\n",
       "  734,\n",
       "  735,\n",
       "  738,\n",
       "  753,\n",
       "  759,\n",
       "  787,\n",
       "  803,\n",
       "  816,\n",
       "  847,\n",
       "  857,\n",
       "  858,\n",
       "  878,\n",
       "  880,\n",
       "  889,\n",
       "  890,\n",
       "  923,\n",
       "  934,\n",
       "  937,\n",
       "  944,\n",
       "  953,\n",
       "  957,\n",
       "  992,\n",
       "  997,\n",
       "  9,\n",
       "  47,\n",
       "  86,\n",
       "  92,\n",
       "  102,\n",
       "  188,\n",
       "  206,\n",
       "  236,\n",
       "  275,\n",
       "  330,\n",
       "  336,\n",
       "  509,\n",
       "  565,\n",
       "  579,\n",
       "  607,\n",
       "  619,\n",
       "  698,\n",
       "  727,\n",
       "  781,\n",
       "  844,\n",
       "  873,\n",
       "  898,\n",
       "  17,\n",
       "  83,\n",
       "  134,\n",
       "  189,\n",
       "  197,\n",
       "  226,\n",
       "  249,\n",
       "  369,\n",
       "  382,\n",
       "  389,\n",
       "  407,\n",
       "  425,\n",
       "  428,\n",
       "  562,\n",
       "  633,\n",
       "  638,\n",
       "  706,\n",
       "  728,\n",
       "  729,\n",
       "  758,\n",
       "  819,\n",
       "  918,\n",
       "  939,\n",
       "  88,\n",
       "  105,\n",
       "  186,\n",
       "  204,\n",
       "  228,\n",
       "  231,\n",
       "  253,\n",
       "  284,\n",
       "  313,\n",
       "  350,\n",
       "  358,\n",
       "  375,\n",
       "  402,\n",
       "  408,\n",
       "  470,\n",
       "  518,\n",
       "  560,\n",
       "  644,\n",
       "  668,\n",
       "  743,\n",
       "  745,\n",
       "  768,\n",
       "  811,\n",
       "  874,\n",
       "  882,\n",
       "  910,\n",
       "  988,\n",
       "  990,\n",
       "  15,\n",
       "  75,\n",
       "  87,\n",
       "  265,\n",
       "  305,\n",
       "  381,\n",
       "  391,\n",
       "  394,\n",
       "  410,\n",
       "  412,\n",
       "  476,\n",
       "  481,\n",
       "  483,\n",
       "  495,\n",
       "  505,\n",
       "  522,\n",
       "  527,\n",
       "  545,\n",
       "  582,\n",
       "  687,\n",
       "  710,\n",
       "  731,\n",
       "  752,\n",
       "  766,\n",
       "  784,\n",
       "  808,\n",
       "  809,\n",
       "  820,\n",
       "  906,\n",
       "  963,\n",
       "  985,\n",
       "  98,\n",
       "  99,\n",
       "  117,\n",
       "  140,\n",
       "  141,\n",
       "  149,\n",
       "  156,\n",
       "  159,\n",
       "  201,\n",
       "  282,\n",
       "  308,\n",
       "  365,\n",
       "  388,\n",
       "  399,\n",
       "  415,\n",
       "  445,\n",
       "  463,\n",
       "  591,\n",
       "  606,\n",
       "  614,\n",
       "  647,\n",
       "  650,\n",
       "  757,\n",
       "  774,\n",
       "  790,\n",
       "  802,\n",
       "  804,\n",
       "  817,\n",
       "  854,\n",
       "  859,\n",
       "  902])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist, tk = diversity(learn, 10, 95)\n",
    "n, hist, tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f348433e710>]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_hist = sorted(hist, key=lambda x: x[0], reverse = False)\n",
    "values = [elem[1] for elem in sorted_hist]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.18)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.74)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_from_uniform(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "finished creating histogram for the 0th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 1th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 2th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 3th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 4th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 5th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 6th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 7th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 8th perturbation\n",
      "at batch no 0\n",
      "finished creating histogram for the 9th perturbation\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(645,\n",
       " [(904, 92.00),\n",
       "  (646, 33.70),\n",
       "  (611, 14.80),\n",
       "  (750, 13.50),\n",
       "  (506, 12.10),\n",
       "  (971, 10.70),\n",
       "  (741, 10.20),\n",
       "  (808, 10.10),\n",
       "  (916, 9.20),\n",
       "  (721, 7.70),\n",
       "  (824, 7.40),\n",
       "  (769, 6.60),\n",
       "  (489, 6.50),\n",
       "  (580, 5.50),\n",
       "  (703, 5.40),\n",
       "  (893, 5.20),\n",
       "  (692, 5.10),\n",
       "  (582, 5.00),\n",
       "  (496, 4.50),\n",
       "  (905, 4.40),\n",
       "  (411, 4.20),\n",
       "  (806, 4.20),\n",
       "  (922, 3.90),\n",
       "  (84, 3.70),\n",
       "  (752, 3.70),\n",
       "  (565, 3.60),\n",
       "  (754, 3.60),\n",
       "  (963, 3.60),\n",
       "  (621, 3.40),\n",
       "  (55, 3.30),\n",
       "  (911, 3.30),\n",
       "  (982, 3.30),\n",
       "  (549, 3.20),\n",
       "  (94, 3.10),\n",
       "  (709, 3.10),\n",
       "  (879, 3.10),\n",
       "  (151, 3.00),\n",
       "  (128, 2.80),\n",
       "  (273, 2.80),\n",
       "  (562, 2.80),\n",
       "  (850, 2.80),\n",
       "  (890, 2.80),\n",
       "  (230, 2.70),\n",
       "  (987, 2.70),\n",
       "  (313, 2.50),\n",
       "  (454, 2.50),\n",
       "  (474, 2.50),\n",
       "  (134, 2.40),\n",
       "  (417, 2.40),\n",
       "  (533, 2.40),\n",
       "  (46, 2.30),\n",
       "  (595, 2.30),\n",
       "  (641, 2.30),\n",
       "  (645, 2.30),\n",
       "  (854, 2.30),\n",
       "  (864, 2.30),\n",
       "  (310, 2.20),\n",
       "  (472, 2.20),\n",
       "  (783, 2.20),\n",
       "  (826, 2.20),\n",
       "  (284, 2.10),\n",
       "  (292, 2.10),\n",
       "  (539, 2.10),\n",
       "  (616, 2.10),\n",
       "  (633, 2.10),\n",
       "  (805, 2.10),\n",
       "  (872, 2.10),\n",
       "  (981, 2.10),\n",
       "  (60, 2.00),\n",
       "  (63, 2.00),\n",
       "  (334, 2.00),\n",
       "  (424, 2.00),\n",
       "  (425, 2.00),\n",
       "  (464, 2.00),\n",
       "  (481, 2.00),\n",
       "  (791, 2.00),\n",
       "  (72, 1.90),\n",
       "  (109, 1.90),\n",
       "  (427, 1.90),\n",
       "  (445, 1.90),\n",
       "  (518, 1.90),\n",
       "  (584, 1.90),\n",
       "  (735, 1.90),\n",
       "  (762, 1.90),\n",
       "  (781, 1.90),\n",
       "  (868, 1.90),\n",
       "  (870, 1.90),\n",
       "  (898, 1.90),\n",
       "  (48, 1.80),\n",
       "  (260, 1.80),\n",
       "  (289, 1.80),\n",
       "  (319, 1.80),\n",
       "  (331, 1.80),\n",
       "  (341, 1.80),\n",
       "  (401, 1.80),\n",
       "  (558, 1.80),\n",
       "  (614, 1.80),\n",
       "  (815, 1.80),\n",
       "  (828, 1.80),\n",
       "  (61, 1.70),\n",
       "  (93, 1.70),\n",
       "  (96, 1.70),\n",
       "  (113, 1.70),\n",
       "  (238, 1.70),\n",
       "  (363, 1.70),\n",
       "  (374, 1.70),\n",
       "  (388, 1.70),\n",
       "  (457, 1.70),\n",
       "  (591, 1.70),\n",
       "  (906, 1.70),\n",
       "  (955, 1.70),\n",
       "  (57, 1.60),\n",
       "  (124, 1.60),\n",
       "  (189, 1.60),\n",
       "  (307, 1.60),\n",
       "  (315, 1.60),\n",
       "  (502, 1.60),\n",
       "  (538, 1.60),\n",
       "  (701, 1.60),\n",
       "  (730, 1.60),\n",
       "  (772, 1.60),\n",
       "  (775, 1.60),\n",
       "  (782, 1.60),\n",
       "  (790, 1.60),\n",
       "  (800, 1.60),\n",
       "  (809, 1.60),\n",
       "  (944, 1.60),\n",
       "  (108, 1.50),\n",
       "  (222, 1.50),\n",
       "  (254, 1.50),\n",
       "  (376, 1.50),\n",
       "  (377, 1.50),\n",
       "  (381, 1.50),\n",
       "  (440, 1.50),\n",
       "  (443, 1.50),\n",
       "  (478, 1.50),\n",
       "  (491, 1.50),\n",
       "  (492, 1.50),\n",
       "  (508, 1.50),\n",
       "  (509, 1.50),\n",
       "  (570, 1.50),\n",
       "  (572, 1.50),\n",
       "  (581, 1.50),\n",
       "  (609, 1.50),\n",
       "  (743, 1.50),\n",
       "  (847, 1.50),\n",
       "  (907, 1.50),\n",
       "  (47, 1.40),\n",
       "  (68, 1.40),\n",
       "  (235, 1.40),\n",
       "  (332, 1.40),\n",
       "  (365, 1.40),\n",
       "  (434, 1.40),\n",
       "  (488, 1.40),\n",
       "  (517, 1.40),\n",
       "  (636, 1.40),\n",
       "  (640, 1.40),\n",
       "  (696, 1.40),\n",
       "  (698, 1.40),\n",
       "  (770, 1.40),\n",
       "  (821, 1.40),\n",
       "  (829, 1.40),\n",
       "  (832, 1.40),\n",
       "  (880, 1.40),\n",
       "  (31, 1.30),\n",
       "  (39, 1.30),\n",
       "  (182, 1.30),\n",
       "  (242, 1.30),\n",
       "  (274, 1.30),\n",
       "  (300, 1.30),\n",
       "  (304, 1.30),\n",
       "  (318, 1.30),\n",
       "  (348, 1.30),\n",
       "  (354, 1.30),\n",
       "  (369, 1.30),\n",
       "  (378, 1.30),\n",
       "  (452, 1.30),\n",
       "  (497, 1.30),\n",
       "  (522, 1.30),\n",
       "  (579, 1.30),\n",
       "  (619, 1.30),\n",
       "  (620, 1.30),\n",
       "  (635, 1.30),\n",
       "  (674, 1.30),\n",
       "  (679, 1.30),\n",
       "  (697, 1.30),\n",
       "  (751, 1.30),\n",
       "  (774, 1.30),\n",
       "  (787, 1.30),\n",
       "  (798, 1.30),\n",
       "  (825, 1.30),\n",
       "  (842, 1.30),\n",
       "  (852, 1.30),\n",
       "  (948, 1.30),\n",
       "  (87, 1.20),\n",
       "  (90, 1.20),\n",
       "  (163, 1.20),\n",
       "  (171, 1.20),\n",
       "  (186, 1.20),\n",
       "  (195, 1.20),\n",
       "  (197, 1.20),\n",
       "  (247, 1.20),\n",
       "  (275, 1.20),\n",
       "  (342, 1.20),\n",
       "  (360, 1.20),\n",
       "  (389, 1.20),\n",
       "  (406, 1.20),\n",
       "  (420, 1.20),\n",
       "  (436, 1.20),\n",
       "  (463, 1.20),\n",
       "  (588, 1.20),\n",
       "  (665, 1.20),\n",
       "  (711, 1.20),\n",
       "  (722, 1.20),\n",
       "  (725, 1.20),\n",
       "  (786, 1.20),\n",
       "  (858, 1.20),\n",
       "  (865, 1.20),\n",
       "  (892, 1.20),\n",
       "  (910, 1.20),\n",
       "  (8, 1.10),\n",
       "  (83, 1.10),\n",
       "  (92, 1.10),\n",
       "  (125, 1.10),\n",
       "  (144, 1.10),\n",
       "  (149, 1.10),\n",
       "  (164, 1.10),\n",
       "  (188, 1.10),\n",
       "  (229, 1.10),\n",
       "  (271, 1.10),\n",
       "  (281, 1.10),\n",
       "  (291, 1.10),\n",
       "  (294, 1.10),\n",
       "  (295, 1.10),\n",
       "  (302, 1.10),\n",
       "  (314, 1.10),\n",
       "  (336, 1.10),\n",
       "  (343, 1.10),\n",
       "  (350, 1.10),\n",
       "  (361, 1.10),\n",
       "  (375, 1.10),\n",
       "  (383, 1.10),\n",
       "  (393, 1.10),\n",
       "  (429, 1.10),\n",
       "  (468, 1.10),\n",
       "  (507, 1.10),\n",
       "  (528, 1.10),\n",
       "  (530, 1.10),\n",
       "  (592, 1.10),\n",
       "  (618, 1.10),\n",
       "  (624, 1.10),\n",
       "  (710, 1.10),\n",
       "  (727, 1.10),\n",
       "  (738, 1.10),\n",
       "  (746, 1.10),\n",
       "  (759, 1.10),\n",
       "  (816, 1.10),\n",
       "  (819, 1.10),\n",
       "  (831, 1.10),\n",
       "  (871, 1.10),\n",
       "  (919, 1.10),\n",
       "  (938, 1.10),\n",
       "  (939, 1.10),\n",
       "  (973, 1.10),\n",
       "  (1, 1.00),\n",
       "  (7, 1.00),\n",
       "  (17, 1.00),\n",
       "  (25, 1.00),\n",
       "  (28, 1.00),\n",
       "  (33, 1.00),\n",
       "  (37, 1.00),\n",
       "  (42, 1.00),\n",
       "  (50, 1.00),\n",
       "  (52, 1.00),\n",
       "  (62, 1.00),\n",
       "  (65, 1.00),\n",
       "  (77, 1.00),\n",
       "  (86, 1.00),\n",
       "  (91, 1.00),\n",
       "  (97, 1.00),\n",
       "  (100, 1.00),\n",
       "  (115, 1.00),\n",
       "  (118, 1.00),\n",
       "  (120, 1.00),\n",
       "  (123, 1.00),\n",
       "  (131, 1.00),\n",
       "  (140, 1.00),\n",
       "  (142, 1.00),\n",
       "  (155, 1.00),\n",
       "  (192, 1.00),\n",
       "  (198, 1.00),\n",
       "  (236, 1.00),\n",
       "  (276, 1.00),\n",
       "  (290, 1.00),\n",
       "  (293, 1.00),\n",
       "  (301, 1.00),\n",
       "  (306, 1.00),\n",
       "  (316, 1.00),\n",
       "  (321, 1.00),\n",
       "  (335, 1.00),\n",
       "  (347, 1.00),\n",
       "  (355, 1.00),\n",
       "  (384, 1.00),\n",
       "  (392, 1.00),\n",
       "  (395, 1.00),\n",
       "  (398, 1.00),\n",
       "  (408, 1.00),\n",
       "  (413, 1.00),\n",
       "  (432, 1.00),\n",
       "  (448, 1.00),\n",
       "  (480, 1.00),\n",
       "  (498, 1.00),\n",
       "  (511, 1.00),\n",
       "  (556, 1.00),\n",
       "  (566, 1.00),\n",
       "  (575, 1.00),\n",
       "  (593, 1.00),\n",
       "  (612, 1.00),\n",
       "  (625, 1.00),\n",
       "  (637, 1.00),\n",
       "  (654, 1.00),\n",
       "  (661, 1.00),\n",
       "  (684, 1.00),\n",
       "  (694, 1.00),\n",
       "  (716, 1.00),\n",
       "  (756, 1.00),\n",
       "  (758, 1.00),\n",
       "  (823, 1.00),\n",
       "  (843, 1.00),\n",
       "  (853, 1.00),\n",
       "  (867, 1.00),\n",
       "  (873, 1.00),\n",
       "  (877, 1.00),\n",
       "  (882, 1.00),\n",
       "  (918, 1.00),\n",
       "  (920, 1.00),\n",
       "  (923, 1.00),\n",
       "  (932, 1.00),\n",
       "  (934, 1.00),\n",
       "  (937, 1.00),\n",
       "  (953, 1.00),\n",
       "  (989, 1.00),\n",
       "  (992, 1.00),\n",
       "  (996, 1.00),\n",
       "  (15, 0.90),\n",
       "  (21, 0.90),\n",
       "  (22, 0.90),\n",
       "  (44, 0.90),\n",
       "  (88, 0.90),\n",
       "  (99, 0.90),\n",
       "  (141, 0.90),\n",
       "  (193, 0.90),\n",
       "  (200, 0.90),\n",
       "  (205, 0.90),\n",
       "  (214, 0.90),\n",
       "  (217, 0.90),\n",
       "  (218, 0.90),\n",
       "  (224, 0.90),\n",
       "  (225, 0.90),\n",
       "  (261, 0.90),\n",
       "  (364, 0.90),\n",
       "  (366, 0.90),\n",
       "  (387, 0.90),\n",
       "  (442, 0.90),\n",
       "  (444, 0.90),\n",
       "  (519, 0.90),\n",
       "  (536, 0.90),\n",
       "  (577, 0.90),\n",
       "  (606, 0.90),\n",
       "  (608, 0.90),\n",
       "  (639, 0.90),\n",
       "  (700, 0.90),\n",
       "  (753, 0.90),\n",
       "  (768, 0.90),\n",
       "  (817, 0.90),\n",
       "  (820, 0.90),\n",
       "  (834, 0.90),\n",
       "  (874, 0.90),\n",
       "  (901, 0.90),\n",
       "  (921, 0.90),\n",
       "  (962, 0.90),\n",
       "  (984, 0.90),\n",
       "  (991, 0.90),\n",
       "  (997, 0.90),\n",
       "  (24, 0.80),\n",
       "  (70, 0.80),\n",
       "  (102, 0.80),\n",
       "  (116, 0.80),\n",
       "  (146, 0.80),\n",
       "  (160, 0.80),\n",
       "  (161, 0.80),\n",
       "  (168, 0.80),\n",
       "  (170, 0.80),\n",
       "  (228, 0.80),\n",
       "  (282, 0.80),\n",
       "  (296, 0.80),\n",
       "  (305, 0.80),\n",
       "  (327, 0.80),\n",
       "  (337, 0.80),\n",
       "  (353, 0.80),\n",
       "  (372, 0.80),\n",
       "  (382, 0.80),\n",
       "  (433, 0.80),\n",
       "  (459, 0.80),\n",
       "  (467, 0.80),\n",
       "  (483, 0.80),\n",
       "  (495, 0.80),\n",
       "  (520, 0.80),\n",
       "  (547, 0.80),\n",
       "  (555, 0.80),\n",
       "  (561, 0.80),\n",
       "  (574, 0.80),\n",
       "  (585, 0.80),\n",
       "  (603, 0.80),\n",
       "  (615, 0.80),\n",
       "  (629, 0.80),\n",
       "  (647, 0.80),\n",
       "  (652, 0.80),\n",
       "  (656, 0.80),\n",
       "  (712, 0.80),\n",
       "  (723, 0.80),\n",
       "  (734, 0.80),\n",
       "  (755, 0.80),\n",
       "  (767, 0.80),\n",
       "  (863, 0.80),\n",
       "  (897, 0.80),\n",
       "  (900, 0.80),\n",
       "  (9, 0.70),\n",
       "  (10, 0.70),\n",
       "  (32, 0.70),\n",
       "  (36, 0.70),\n",
       "  (45, 0.70),\n",
       "  (75, 0.70),\n",
       "  (114, 0.70),\n",
       "  (136, 0.70),\n",
       "  (139, 0.70),\n",
       "  (169, 0.70),\n",
       "  (173, 0.70),\n",
       "  (176, 0.70),\n",
       "  (178, 0.70),\n",
       "  (206, 0.70),\n",
       "  (208, 0.70),\n",
       "  (211, 0.70),\n",
       "  (216, 0.70),\n",
       "  (231, 0.70),\n",
       "  (232, 0.70),\n",
       "  (249, 0.70),\n",
       "  (253, 0.70),\n",
       "  (269, 0.70),\n",
       "  (270, 0.70),\n",
       "  (317, 0.70),\n",
       "  (326, 0.70),\n",
       "  (344, 0.70),\n",
       "  (396, 0.70),\n",
       "  (397, 0.70),\n",
       "  (403, 0.70),\n",
       "  (416, 0.70),\n",
       "  (421, 0.70),\n",
       "  (447, 0.70),\n",
       "  (476, 0.70),\n",
       "  (484, 0.70),\n",
       "  (546, 0.70),\n",
       "  (554, 0.70),\n",
       "  (586, 0.70),\n",
       "  (644, 0.70),\n",
       "  (664, 0.70),\n",
       "  (667, 0.70),\n",
       "  (669, 0.70),\n",
       "  (672, 0.70),\n",
       "  (688, 0.70),\n",
       "  (695, 0.70),\n",
       "  (704, 0.70),\n",
       "  (737, 0.70),\n",
       "  (764, 0.70),\n",
       "  (765, 0.70),\n",
       "  (833, 0.70),\n",
       "  (845, 0.70),\n",
       "  (891, 0.70),\n",
       "  (936, 0.70),\n",
       "  (946, 0.70),\n",
       "  (952, 0.70),\n",
       "  (954, 0.70),\n",
       "  (956, 0.70),\n",
       "  (959, 0.70),\n",
       "  (972, 0.70),\n",
       "  (985, 0.70),\n",
       "  (16, 0.60),\n",
       "  (30, 0.60),\n",
       "  (41, 0.60),\n",
       "  (49, 0.60),\n",
       "  (89, 0.60),\n",
       "  (98, 0.60),\n",
       "  (105, 0.60),\n",
       "  (132, 0.60),\n",
       "  (183, 0.60),\n",
       "  (202, 0.60),\n",
       "  (226, 0.60),\n",
       "  (245, 0.60),\n",
       "  (246, 0.60),\n",
       "  (263, 0.60),\n",
       "  (267, 0.60),\n",
       "  (308, 0.60),\n",
       "  (312, 0.60),\n",
       "  (320, 0.60),\n",
       "  (323, 0.60),\n",
       "  (379, 0.60),\n",
       "  (415, 0.60),\n",
       "  (455, 0.60),\n",
       "  (490, 0.60),\n",
       "  (505, 0.60),\n",
       "  (527, 0.60),\n",
       "  (529, 0.60),\n",
       "  (540, 0.60),\n",
       "  (545, 0.60),\n",
       "  (552, 0.60),\n",
       "  (576, 0.60),\n",
       "  (659, 0.60),\n",
       "  (729, 0.60),\n",
       "  (733, 0.60),\n",
       "  (757, 0.60),\n",
       "  (763, 0.60),\n",
       "  (777, 0.60),\n",
       "  (778, 0.60),\n",
       "  (779, 0.60),\n",
       "  (796, 0.60),\n",
       "  (803, 0.60),\n",
       "  (814, 0.60),\n",
       "  (822, 0.60),\n",
       "  (846, 0.60),\n",
       "  (857, 0.60),\n",
       "  (951, 0.60),\n",
       "  (979, 0.60),\n",
       "  (5, 0.50),\n",
       "  (18, 0.50),\n",
       "  (19, 0.50),\n",
       "  (20, 0.50),\n",
       "  (76, 0.50),\n",
       "  (82, 0.50),\n",
       "  (95, 0.50),\n",
       "  (110, 0.50),\n",
       "  (117, 0.50),\n",
       "  (122, 0.50),\n",
       "  (137, 0.50),\n",
       "  (157, 0.50),\n",
       "  (179, 0.50),\n",
       "  (190, 0.50),\n",
       "  (204, 0.50),\n",
       "  (252, 0.50),\n",
       "  (255, 0.50),\n",
       "  (258, 0.50),\n",
       "  (268, 0.50),\n",
       "  (278, 0.50),\n",
       "  (279, 0.50),\n",
       "  (280, 0.50),\n",
       "  (285, 0.50),\n",
       "  (311, 0.50),\n",
       "  (345, 0.50),\n",
       "  (358, 0.50),\n",
       "  (390, 0.50),\n",
       "  (391, 0.50),\n",
       "  (399, 0.50),\n",
       "  (402, 0.50),\n",
       "  (407, 0.50),\n",
       "  (409, 0.50),\n",
       "  (428, 0.50),\n",
       "  (446, 0.50),\n",
       "  (450, 0.50),\n",
       "  (486, 0.50),\n",
       "  (503, 0.50),\n",
       "  (568, 0.50),\n",
       "  (638, 0.50),\n",
       "  (643, 0.50),\n",
       "  (650, 0.50),\n",
       "  (658, 0.50),\n",
       "  (668, 0.50),\n",
       "  (671, 0.50),\n",
       "  (683, 0.50),\n",
       "  (724, 0.50),\n",
       "  (748, 0.50),\n",
       "  (793, 0.50),\n",
       "  (799, 0.50),\n",
       "  (802, 0.50),\n",
       "  (818, 0.50),\n",
       "  (830, 0.50),\n",
       "  (837, 0.50),\n",
       "  (839, 0.50),\n",
       "  (849, 0.50),\n",
       "  (866, 0.50),\n",
       "  (881, 0.50),\n",
       "  (883, 0.50),\n",
       "  (888, 0.50),\n",
       "  (889, 0.50),\n",
       "  (950, 0.50),\n",
       "  (999, 0.50),\n",
       "  (2, 0.40),\n",
       "  (12, 0.40),\n",
       "  (14, 0.40),\n",
       "  (23, 0.40),\n",
       "  (35, 0.40),\n",
       "  (40, 0.40),\n",
       "  (53, 0.40),\n",
       "  (59, 0.40),\n",
       "  (79, 0.40),\n",
       "  (107, 0.40),\n",
       "  (121, 0.40),\n",
       "  (130, 0.40),\n",
       "  (143, 0.40),\n",
       "  (148, 0.40),\n",
       "  (184, 0.40),\n",
       "  (203, 0.40),\n",
       "  (234, 0.40),\n",
       "  (243, 0.40),\n",
       "  (250, 0.40),\n",
       "  (266, 0.40),\n",
       "  (297, 0.40),\n",
       "  (298, 0.40),\n",
       "  (322, 0.40),\n",
       "  (328, 0.40),\n",
       "  (330, 0.40),\n",
       "  (385, 0.40),\n",
       "  (386, 0.40),\n",
       "  (423, 0.40),\n",
       "  (431, 0.40),\n",
       "  (438, 0.40),\n",
       "  (451, 0.40),\n",
       "  (453, 0.40),\n",
       "  (456, 0.40),\n",
       "  (458, 0.40),\n",
       "  (477, 0.40),\n",
       "  (482, 0.40),\n",
       "  (501, 0.40),\n",
       "  (531, 0.40),\n",
       "  (535, 0.40),\n",
       "  (550, 0.40),\n",
       "  (567, 0.40),\n",
       "  (590, 0.40),\n",
       "  (594, 0.40),\n",
       "  (599, 0.40),\n",
       "  (602, 0.40),\n",
       "  (604, 0.40),\n",
       "  (607, 0.40),\n",
       "  (632, 0.40),\n",
       "  (699, 0.40),\n",
       "  (702, 0.40),\n",
       "  (708, 0.40),\n",
       "  (718, 0.40),\n",
       "  (732, 0.40),\n",
       "  (749, 0.40),\n",
       "  (761, 0.40),\n",
       "  (766, 0.40),\n",
       "  (801, 0.40),\n",
       "  (840, 0.40),\n",
       "  (878, 0.40),\n",
       "  (884, 0.40),\n",
       "  (902, 0.40),\n",
       "  (903, 0.40),\n",
       "  (917, 0.40),\n",
       "  (924, 0.40),\n",
       "  (958, 0.40),\n",
       "  (978, 0.40),\n",
       "  (0, 0.30),\n",
       "  (4, 0.30),\n",
       "  (34, 0.30),\n",
       "  (58, 0.30),\n",
       "  (69, 0.30),\n",
       "  (71, 0.30),\n",
       "  (73, 0.30),\n",
       "  (112, 0.30),\n",
       "  (135, 0.30),\n",
       "  (145, 0.30),\n",
       "  (150, 0.30),\n",
       "  (158, 0.30),\n",
       "  (166, 0.30),\n",
       "  (175, 0.30),\n",
       "  (177, 0.30),\n",
       "  (180, 0.30),\n",
       "  (196, 0.30),\n",
       "  (207, 0.30),\n",
       "  (213, 0.30),\n",
       "  (221, 0.30),\n",
       "  (223, 0.30),\n",
       "  (256, 0.30),\n",
       "  (324, 0.30),\n",
       "  (338, 0.30),\n",
       "  (351, 0.30),\n",
       "  (359, 0.30),\n",
       "  (367, 0.30),\n",
       "  (394, 0.30),\n",
       "  (410, 0.30),\n",
       "  (412, 0.30),\n",
       "  (462, 0.30),\n",
       "  (470, 0.30),\n",
       "  (512, 0.30),\n",
       "  (514, 0.30),\n",
       "  (515, 0.30),\n",
       "  (537, 0.30),\n",
       "  (560, 0.30),\n",
       "  (563, 0.30),\n",
       "  (589, 0.30),\n",
       "  (597, 0.30),\n",
       "  (622, 0.30),\n",
       "  (623, 0.30),\n",
       "  (670, 0.30),\n",
       "  (681, 0.30),\n",
       "  (715, 0.30),\n",
       "  (717, 0.30),\n",
       "  (720, 0.30),\n",
       "  (736, 0.30),\n",
       "  (742, 0.30),\n",
       "  (745, 0.30),\n",
       "  (784, 0.30),\n",
       "  (788, 0.30),\n",
       "  (789, 0.30),\n",
       "  (794, 0.30),\n",
       "  (797, 0.30),\n",
       "  (807, 0.30),\n",
       "  (836, 0.30),\n",
       "  (848, 0.30),\n",
       "  (862, 0.30),\n",
       "  (886, 0.30),\n",
       "  (915, 0.30),\n",
       "  (961, 0.30),\n",
       "  (969, 0.30),\n",
       "  (986, 0.30),\n",
       "  (988, 0.30),\n",
       "  (998, 0.30),\n",
       "  (51, 0.20),\n",
       "  (66, 0.20),\n",
       "  (74, 0.20),\n",
       "  (78, 0.20),\n",
       "  (81, 0.20),\n",
       "  (106, 0.20),\n",
       "  (119, 0.20),\n",
       "  (126, 0.20),\n",
       "  (138, 0.20),\n",
       "  (162, 0.20),\n",
       "  (172, 0.20),\n",
       "  (185, 0.20),\n",
       "  (187, 0.20),\n",
       "  (191, 0.20),\n",
       "  (194, 0.20),\n",
       "  (199, 0.20),\n",
       "  (201, 0.20),\n",
       "  (219, 0.20),\n",
       "  (227, 0.20),\n",
       "  (237, 0.20),\n",
       "  (239, 0.20),\n",
       "  (257, 0.20),\n",
       "  (259, 0.20),\n",
       "  (264, 0.20),\n",
       "  (272, 0.20),\n",
       "  (283, 0.20),\n",
       "  (309, 0.20),\n",
       "  (325, 0.20),\n",
       "  (329, 0.20),\n",
       "  (339, 0.20),\n",
       "  (340, 0.20),\n",
       "  (346, 0.20),\n",
       "  (352, 0.20),\n",
       "  (357, 0.20),\n",
       "  (362, 0.20),\n",
       "  (380, 0.20),\n",
       "  (404, 0.20),\n",
       "  (422, 0.20),\n",
       "  (430, 0.20),\n",
       "  (449, 0.20),\n",
       "  (461, 0.20),\n",
       "  (475, 0.20),\n",
       "  (487, 0.20),\n",
       "  (500, 0.20),\n",
       "  (516, 0.20),\n",
       "  (521, 0.20),\n",
       "  (534, 0.20),\n",
       "  (542, 0.20),\n",
       "  (564, 0.20),\n",
       "  (571, 0.20),\n",
       "  (628, 0.20),\n",
       "  (649, 0.20),\n",
       "  (651, 0.20),\n",
       "  (676, 0.20),\n",
       "  (685, 0.20),\n",
       "  (687, 0.20),\n",
       "  (689, 0.20),\n",
       "  (705, 0.20),\n",
       "  (706, 0.20),\n",
       "  (707, 0.20),\n",
       "  (714, 0.20),\n",
       "  (726, 0.20),\n",
       "  (728, 0.20),\n",
       "  (731, 0.20),\n",
       "  (776, 0.20),\n",
       "  (780, 0.20),\n",
       "  (838, 0.20),\n",
       "  (844, 0.20),\n",
       "  (861, 0.20),\n",
       "  (885, 0.20),\n",
       "  (895, 0.20),\n",
       "  (912, 0.20),\n",
       "  (913, 0.20),\n",
       "  (925, 0.20),\n",
       "  (930, 0.20),\n",
       "  (933, 0.20),\n",
       "  (943, 0.20),\n",
       "  (947, 0.20),\n",
       "  (949, 0.20),\n",
       "  (957, 0.20),\n",
       "  (983, 0.20),\n",
       "  (990, 0.20),\n",
       "  (13, 0.10),\n",
       "  (38, 0.10),\n",
       "  (67, 0.10),\n",
       "  (104, 0.10),\n",
       "  (127, 0.10),\n",
       "  (133, 0.10),\n",
       "  (147, 0.10),\n",
       "  (153, 0.10),\n",
       "  (154, 0.10),\n",
       "  (174, 0.10),\n",
       "  (181, 0.10),\n",
       "  (209, 0.10),\n",
       "  (210, 0.10),\n",
       "  (233, 0.10),\n",
       "  (241, 0.10),\n",
       "  (248, 0.10),\n",
       "  (251, 0.10),\n",
       "  (265, 0.10),\n",
       "  (277, 0.10),\n",
       "  (303, 0.10),\n",
       "  (356, 0.10),\n",
       "  (405, 0.10),\n",
       "  (414, 0.10),\n",
       "  (435, 0.10),\n",
       "  (439, 0.10),\n",
       "  (465, 0.10),\n",
       "  (466, 0.10),\n",
       "  (473, 0.10),\n",
       "  (479, 0.10),\n",
       "  (499, 0.10),\n",
       "  (504, 0.10),\n",
       "  (510, 0.10),\n",
       "  (523, 0.10),\n",
       "  (524, 0.10),\n",
       "  (526, 0.10),\n",
       "  (532, 0.10),\n",
       "  (543, 0.10),\n",
       "  (544, 0.10),\n",
       "  (551, 0.10),\n",
       "  (553, 0.10),\n",
       "  (557, 0.10),\n",
       "  (569, 0.10),\n",
       "  (573, 0.10),\n",
       "  (578, 0.10),\n",
       "  (600, 0.10),\n",
       "  (617, 0.10),\n",
       "  (626, 0.10),\n",
       "  (653, 0.10),\n",
       "  (655, 0.10),\n",
       "  (673, 0.10),\n",
       "  (682, 0.10),\n",
       "  (686, 0.10),\n",
       "  (713, 0.10),\n",
       "  (739, 0.10),\n",
       "  (747, 0.10),\n",
       "  (773, 0.10),\n",
       "  (785, 0.10),\n",
       "  (792, 0.10),\n",
       "  (804, 0.10),\n",
       "  (811, 0.10),\n",
       "  (812, 0.10),\n",
       "  (827, 0.10),\n",
       "  (835, 0.10),\n",
       "  (841, 0.10),\n",
       "  (855, 0.10),\n",
       "  (860, 0.10),\n",
       "  (869, 0.10),\n",
       "  (894, 0.10),\n",
       "  (896, 0.10),\n",
       "  (899, 0.10),\n",
       "  (914, 0.10),\n",
       "  (940, 0.10),\n",
       "  (966, 0.10),\n",
       "  (968, 0.10),\n",
       "  (975, 0.10),\n",
       "  (976, 0.10),\n",
       "  (995, 0.10),\n",
       "  (3, 0.00),\n",
       "  (6, 0.00),\n",
       "  (11, 0.00),\n",
       "  (26, 0.00),\n",
       "  (27, 0.00),\n",
       "  (29, 0.00),\n",
       "  (43, 0.00),\n",
       "  (54, 0.00),\n",
       "  (56, 0.00),\n",
       "  (64, 0.00),\n",
       "  (80, 0.00),\n",
       "  (85, 0.00),\n",
       "  (101, 0.00),\n",
       "  (103, 0.00),\n",
       "  (111, 0.00),\n",
       "  (129, 0.00),\n",
       "  (152, 0.00),\n",
       "  (156, 0.00),\n",
       "  (159, 0.00),\n",
       "  (165, 0.00),\n",
       "  (167, 0.00),\n",
       "  (212, 0.00),\n",
       "  (215, 0.00),\n",
       "  (220, 0.00),\n",
       "  (240, 0.00),\n",
       "  (244, 0.00),\n",
       "  (262, 0.00),\n",
       "  (286, 0.00),\n",
       "  (287, 0.00),\n",
       "  (288, 0.00),\n",
       "  (299, 0.00),\n",
       "  (333, 0.00),\n",
       "  (349, 0.00),\n",
       "  (368, 0.00),\n",
       "  (370, 0.00),\n",
       "  (371, 0.00),\n",
       "  (373, 0.00),\n",
       "  (400, 0.00),\n",
       "  (418, 0.00),\n",
       "  (419, 0.00),\n",
       "  (426, 0.00),\n",
       "  (437, 0.00),\n",
       "  (441, 0.00),\n",
       "  (460, 0.00),\n",
       "  (469, 0.00),\n",
       "  (471, 0.00),\n",
       "  (485, 0.00),\n",
       "  (493, 0.00),\n",
       "  (494, 0.00),\n",
       "  (513, 0.00),\n",
       "  (525, 0.00),\n",
       "  (541, 0.00),\n",
       "  (548, 0.00),\n",
       "  (559, 0.00),\n",
       "  (583, 0.00),\n",
       "  (587, 0.00),\n",
       "  (596, 0.00),\n",
       "  (598, 0.00),\n",
       "  (601, 0.00),\n",
       "  (605, 0.00),\n",
       "  (610, 0.00),\n",
       "  (613, 0.00),\n",
       "  (627, 0.00),\n",
       "  (630, 0.00),\n",
       "  (631, 0.00),\n",
       "  (634, 0.00),\n",
       "  (642, 0.00),\n",
       "  (648, 0.00),\n",
       "  (657, 0.00),\n",
       "  (660, 0.00),\n",
       "  (662, 0.00),\n",
       "  (663, 0.00),\n",
       "  (666, 0.00),\n",
       "  (675, 0.00),\n",
       "  (677, 0.00),\n",
       "  (678, 0.00),\n",
       "  (680, 0.00),\n",
       "  (690, 0.00),\n",
       "  (691, 0.00),\n",
       "  (693, 0.00),\n",
       "  (719, 0.00),\n",
       "  (740, 0.00),\n",
       "  (744, 0.00),\n",
       "  (760, 0.00),\n",
       "  (771, 0.00),\n",
       "  (795, 0.00),\n",
       "  (810, 0.00),\n",
       "  (813, 0.00),\n",
       "  (851, 0.00),\n",
       "  (856, 0.00),\n",
       "  (859, 0.00),\n",
       "  (875, 0.00),\n",
       "  (876, 0.00),\n",
       "  (887, 0.00),\n",
       "  (908, 0.00),\n",
       "  (909, 0.00),\n",
       "  (926, 0.00),\n",
       "  (927, 0.00),\n",
       "  (928, 0.00),\n",
       "  (929, 0.00),\n",
       "  (931, 0.00),\n",
       "  (935, 0.00),\n",
       "  (941, 0.00),\n",
       "  (942, 0.00),\n",
       "  (945, 0.00),\n",
       "  (960, 0.00),\n",
       "  (964, 0.00),\n",
       "  (965, 0.00),\n",
       "  (967, 0.00),\n",
       "  (970, 0.00),\n",
       "  (974, 0.00),\n",
       "  (977, 0.00),\n",
       "  (980, 0.00),\n",
       "  (993, 0.00),\n",
       "  (994, 0.00)],\n",
       " [904,\n",
       "  646,\n",
       "  611,\n",
       "  750,\n",
       "  506,\n",
       "  971,\n",
       "  741,\n",
       "  808,\n",
       "  916,\n",
       "  721,\n",
       "  824,\n",
       "  769,\n",
       "  489,\n",
       "  580,\n",
       "  703,\n",
       "  893,\n",
       "  692,\n",
       "  582,\n",
       "  496,\n",
       "  905,\n",
       "  411,\n",
       "  806,\n",
       "  922,\n",
       "  84,\n",
       "  752,\n",
       "  565,\n",
       "  754,\n",
       "  963,\n",
       "  621,\n",
       "  55,\n",
       "  911,\n",
       "  982,\n",
       "  549,\n",
       "  94,\n",
       "  709,\n",
       "  879,\n",
       "  151,\n",
       "  128,\n",
       "  273,\n",
       "  562,\n",
       "  850,\n",
       "  890,\n",
       "  230,\n",
       "  987,\n",
       "  313,\n",
       "  454,\n",
       "  474,\n",
       "  134,\n",
       "  417,\n",
       "  533,\n",
       "  46,\n",
       "  595,\n",
       "  641,\n",
       "  645,\n",
       "  854,\n",
       "  864,\n",
       "  310,\n",
       "  472,\n",
       "  783,\n",
       "  826,\n",
       "  284,\n",
       "  292,\n",
       "  539,\n",
       "  616,\n",
       "  633,\n",
       "  805,\n",
       "  872,\n",
       "  981,\n",
       "  60,\n",
       "  63,\n",
       "  334,\n",
       "  424,\n",
       "  425,\n",
       "  464,\n",
       "  481,\n",
       "  791,\n",
       "  72,\n",
       "  109,\n",
       "  427,\n",
       "  445,\n",
       "  518,\n",
       "  584,\n",
       "  735,\n",
       "  762,\n",
       "  781,\n",
       "  868,\n",
       "  870,\n",
       "  898,\n",
       "  48,\n",
       "  260,\n",
       "  289,\n",
       "  319,\n",
       "  331,\n",
       "  341,\n",
       "  401,\n",
       "  558,\n",
       "  614,\n",
       "  815,\n",
       "  828,\n",
       "  61,\n",
       "  93,\n",
       "  96,\n",
       "  113,\n",
       "  238,\n",
       "  363,\n",
       "  374,\n",
       "  388,\n",
       "  457,\n",
       "  591,\n",
       "  906,\n",
       "  955,\n",
       "  57,\n",
       "  124,\n",
       "  189,\n",
       "  307,\n",
       "  315,\n",
       "  502,\n",
       "  538,\n",
       "  701,\n",
       "  730,\n",
       "  772,\n",
       "  775,\n",
       "  782,\n",
       "  790,\n",
       "  800,\n",
       "  809,\n",
       "  944,\n",
       "  108,\n",
       "  222,\n",
       "  254,\n",
       "  376,\n",
       "  377,\n",
       "  381,\n",
       "  440,\n",
       "  443,\n",
       "  478,\n",
       "  491,\n",
       "  492,\n",
       "  508,\n",
       "  509,\n",
       "  570,\n",
       "  572,\n",
       "  581,\n",
       "  609,\n",
       "  743,\n",
       "  847,\n",
       "  907,\n",
       "  47,\n",
       "  68,\n",
       "  235,\n",
       "  332,\n",
       "  365,\n",
       "  434,\n",
       "  488,\n",
       "  517,\n",
       "  636,\n",
       "  640,\n",
       "  696,\n",
       "  698,\n",
       "  770,\n",
       "  821,\n",
       "  829,\n",
       "  832,\n",
       "  880,\n",
       "  31,\n",
       "  39,\n",
       "  182,\n",
       "  242,\n",
       "  274,\n",
       "  300,\n",
       "  304,\n",
       "  318,\n",
       "  348,\n",
       "  354,\n",
       "  369,\n",
       "  378,\n",
       "  452,\n",
       "  497,\n",
       "  522,\n",
       "  579,\n",
       "  619,\n",
       "  620,\n",
       "  635,\n",
       "  674,\n",
       "  679,\n",
       "  697,\n",
       "  751,\n",
       "  774,\n",
       "  787,\n",
       "  798,\n",
       "  825,\n",
       "  842,\n",
       "  852,\n",
       "  948,\n",
       "  87,\n",
       "  90,\n",
       "  163,\n",
       "  171,\n",
       "  186,\n",
       "  195,\n",
       "  197,\n",
       "  247,\n",
       "  275,\n",
       "  342,\n",
       "  360,\n",
       "  389,\n",
       "  406,\n",
       "  420,\n",
       "  436,\n",
       "  463,\n",
       "  588,\n",
       "  665,\n",
       "  711,\n",
       "  722,\n",
       "  725,\n",
       "  786,\n",
       "  858,\n",
       "  865,\n",
       "  892,\n",
       "  910,\n",
       "  8,\n",
       "  83,\n",
       "  92,\n",
       "  125,\n",
       "  144,\n",
       "  149,\n",
       "  164,\n",
       "  188,\n",
       "  229,\n",
       "  271,\n",
       "  281,\n",
       "  291,\n",
       "  294,\n",
       "  295,\n",
       "  302,\n",
       "  314,\n",
       "  336,\n",
       "  343,\n",
       "  350,\n",
       "  361,\n",
       "  375,\n",
       "  383,\n",
       "  393,\n",
       "  429,\n",
       "  468,\n",
       "  507,\n",
       "  528,\n",
       "  530,\n",
       "  592,\n",
       "  618,\n",
       "  624,\n",
       "  710,\n",
       "  727,\n",
       "  738,\n",
       "  746,\n",
       "  759,\n",
       "  816,\n",
       "  819,\n",
       "  831,\n",
       "  871,\n",
       "  919,\n",
       "  938,\n",
       "  939,\n",
       "  973,\n",
       "  1,\n",
       "  7,\n",
       "  17,\n",
       "  25,\n",
       "  28,\n",
       "  33,\n",
       "  37,\n",
       "  42,\n",
       "  50,\n",
       "  52,\n",
       "  62,\n",
       "  65,\n",
       "  77,\n",
       "  86,\n",
       "  91,\n",
       "  97,\n",
       "  100,\n",
       "  115,\n",
       "  118,\n",
       "  120,\n",
       "  123,\n",
       "  131,\n",
       "  140,\n",
       "  142,\n",
       "  155,\n",
       "  192,\n",
       "  198,\n",
       "  236,\n",
       "  276,\n",
       "  290,\n",
       "  293,\n",
       "  301,\n",
       "  306,\n",
       "  316,\n",
       "  321,\n",
       "  335,\n",
       "  347,\n",
       "  355,\n",
       "  384,\n",
       "  392,\n",
       "  395,\n",
       "  398,\n",
       "  408,\n",
       "  413,\n",
       "  432,\n",
       "  448,\n",
       "  480,\n",
       "  498,\n",
       "  511,\n",
       "  556,\n",
       "  566,\n",
       "  575,\n",
       "  593,\n",
       "  612,\n",
       "  625,\n",
       "  637,\n",
       "  654,\n",
       "  661,\n",
       "  684,\n",
       "  694,\n",
       "  716,\n",
       "  756,\n",
       "  758,\n",
       "  823,\n",
       "  843,\n",
       "  853,\n",
       "  867,\n",
       "  873,\n",
       "  877,\n",
       "  882,\n",
       "  918,\n",
       "  920,\n",
       "  923,\n",
       "  932,\n",
       "  934,\n",
       "  937,\n",
       "  953,\n",
       "  989,\n",
       "  992,\n",
       "  996,\n",
       "  15,\n",
       "  21,\n",
       "  22,\n",
       "  44,\n",
       "  88,\n",
       "  99,\n",
       "  141,\n",
       "  193,\n",
       "  200,\n",
       "  205,\n",
       "  214,\n",
       "  217,\n",
       "  218,\n",
       "  224,\n",
       "  225,\n",
       "  261,\n",
       "  364,\n",
       "  366,\n",
       "  387,\n",
       "  442,\n",
       "  444,\n",
       "  519,\n",
       "  536,\n",
       "  577,\n",
       "  606,\n",
       "  608,\n",
       "  639,\n",
       "  700,\n",
       "  753,\n",
       "  768,\n",
       "  817,\n",
       "  820,\n",
       "  834,\n",
       "  874,\n",
       "  901,\n",
       "  921,\n",
       "  962,\n",
       "  984,\n",
       "  991,\n",
       "  997,\n",
       "  24,\n",
       "  70,\n",
       "  102,\n",
       "  116,\n",
       "  146,\n",
       "  160,\n",
       "  161,\n",
       "  168,\n",
       "  170,\n",
       "  228,\n",
       "  282,\n",
       "  296,\n",
       "  305,\n",
       "  327,\n",
       "  337,\n",
       "  353,\n",
       "  372,\n",
       "  382,\n",
       "  433,\n",
       "  459,\n",
       "  467,\n",
       "  483,\n",
       "  495,\n",
       "  520,\n",
       "  547,\n",
       "  555,\n",
       "  561,\n",
       "  574,\n",
       "  585,\n",
       "  603,\n",
       "  615,\n",
       "  629,\n",
       "  647,\n",
       "  652,\n",
       "  656,\n",
       "  712,\n",
       "  723,\n",
       "  734,\n",
       "  755,\n",
       "  767,\n",
       "  863,\n",
       "  897,\n",
       "  900,\n",
       "  9,\n",
       "  10,\n",
       "  32,\n",
       "  36,\n",
       "  45,\n",
       "  75,\n",
       "  114,\n",
       "  136,\n",
       "  139,\n",
       "  169,\n",
       "  173,\n",
       "  176,\n",
       "  178,\n",
       "  206,\n",
       "  208,\n",
       "  211,\n",
       "  216,\n",
       "  231,\n",
       "  232,\n",
       "  249,\n",
       "  253,\n",
       "  269,\n",
       "  270,\n",
       "  317,\n",
       "  326,\n",
       "  344,\n",
       "  396,\n",
       "  397,\n",
       "  403,\n",
       "  416,\n",
       "  421,\n",
       "  447,\n",
       "  476,\n",
       "  484,\n",
       "  546,\n",
       "  554,\n",
       "  586,\n",
       "  644,\n",
       "  664,\n",
       "  667,\n",
       "  669,\n",
       "  672,\n",
       "  688,\n",
       "  695,\n",
       "  704,\n",
       "  737,\n",
       "  764,\n",
       "  765,\n",
       "  833,\n",
       "  845,\n",
       "  891,\n",
       "  936,\n",
       "  946,\n",
       "  952,\n",
       "  954,\n",
       "  956,\n",
       "  959,\n",
       "  972,\n",
       "  985,\n",
       "  16,\n",
       "  30,\n",
       "  41,\n",
       "  49,\n",
       "  89,\n",
       "  98,\n",
       "  105,\n",
       "  132,\n",
       "  183,\n",
       "  202,\n",
       "  226,\n",
       "  245,\n",
       "  246,\n",
       "  263,\n",
       "  267,\n",
       "  308,\n",
       "  312,\n",
       "  320,\n",
       "  323,\n",
       "  379,\n",
       "  415,\n",
       "  455,\n",
       "  490,\n",
       "  505,\n",
       "  527,\n",
       "  529,\n",
       "  540,\n",
       "  545,\n",
       "  552,\n",
       "  576,\n",
       "  659,\n",
       "  729,\n",
       "  733,\n",
       "  757,\n",
       "  763,\n",
       "  777,\n",
       "  778,\n",
       "  779,\n",
       "  796,\n",
       "  803,\n",
       "  814,\n",
       "  822,\n",
       "  846,\n",
       "  857,\n",
       "  951,\n",
       "  979,\n",
       "  5,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  76,\n",
       "  82,\n",
       "  95,\n",
       "  110,\n",
       "  117,\n",
       "  122,\n",
       "  137,\n",
       "  157,\n",
       "  179,\n",
       "  190,\n",
       "  204,\n",
       "  252,\n",
       "  255,\n",
       "  258,\n",
       "  268,\n",
       "  278,\n",
       "  279,\n",
       "  280,\n",
       "  285,\n",
       "  311,\n",
       "  345,\n",
       "  358,\n",
       "  390,\n",
       "  391,\n",
       "  399,\n",
       "  402,\n",
       "  407,\n",
       "  409,\n",
       "  428,\n",
       "  446,\n",
       "  450,\n",
       "  486,\n",
       "  503,\n",
       "  568,\n",
       "  638,\n",
       "  643,\n",
       "  650,\n",
       "  658,\n",
       "  668,\n",
       "  671,\n",
       "  683,\n",
       "  724,\n",
       "  748,\n",
       "  793,\n",
       "  799,\n",
       "  802,\n",
       "  818,\n",
       "  830,\n",
       "  837,\n",
       "  839,\n",
       "  849,\n",
       "  866,\n",
       "  881,\n",
       "  883,\n",
       "  888,\n",
       "  889,\n",
       "  950,\n",
       "  999,\n",
       "  2,\n",
       "  12,\n",
       "  14,\n",
       "  23,\n",
       "  35,\n",
       "  40,\n",
       "  53,\n",
       "  59,\n",
       "  79,\n",
       "  107,\n",
       "  121,\n",
       "  130,\n",
       "  143,\n",
       "  148,\n",
       "  184,\n",
       "  203,\n",
       "  234,\n",
       "  243,\n",
       "  250,\n",
       "  266,\n",
       "  297,\n",
       "  298,\n",
       "  322,\n",
       "  328,\n",
       "  330,\n",
       "  385,\n",
       "  386,\n",
       "  423,\n",
       "  431,\n",
       "  438,\n",
       "  451,\n",
       "  453,\n",
       "  456,\n",
       "  458,\n",
       "  477,\n",
       "  482,\n",
       "  501,\n",
       "  531,\n",
       "  535,\n",
       "  550,\n",
       "  567,\n",
       "  590,\n",
       "  594,\n",
       "  599,\n",
       "  602,\n",
       "  604,\n",
       "  607,\n",
       "  632,\n",
       "  699,\n",
       "  702,\n",
       "  708])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#resnet33\n",
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist, tk = diversity(learn, 10, 95)\n",
    "n, hist, tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7efe0bcfba90>]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8VNX9//HXhyQQQCAsAdkDCCLggsYF61Zx39uvtlbb8uvX1lr7s7bab136tfbb1rbW/rRaW5VKXahf9wW3goiiLAqGRSCsCUtI2BLIQvbt/P6YmzAJCTNJJpnkzvv5eOSR3DtnZs69d/K+555z71xzziEiIl1ft2hXQEREIkOBLiLiEwp0ERGfUKCLiPiEAl1ExCcU6CIiPqFAFxHxCQW6iIhPKNBFRHwiviPfbNCgQS4lJaUj31JEpMtbsWJFnnMuOVS5Dg30lJQU0tLSOvItRUS6PDPbEU45dbmIiPiEAl1ExCcU6CIiPqFAFxHxCQW6iIhPKNBFRHxCgS4i4hMKdBGJGVU1tbyStpPaWn/eerNDLywSEYmmJxZm8vD8zSTEGV+bOiLa1Yk4tdBFJGbsL64AoLC0Kso1aR8KdBERn1Cgi4j4hAJdRMQnFOgiIj6hQBcR8QkFuoiITyjQRUR8QoEuIjHHn9eJKtBFRHxDgS4iMceiXYF2okAXEfEJBbqIiE8o0EUk5mhQVEREOjUFuojEHA2KiohIp6ZAFxHxCQW6iMQcDYqKiEinpkAXEfGJsALdzH5mZulmts7MXjSzRDMbY2bLzGyLmb1sZt3bu7IiItK8kIFuZsOBnwCpzrkpQBxwPfAg8IhzbjyQD9zUnhUVEYmUWD9tMR7oaWbxQC9gN3A+8Jr3+HPANZGvnohI5MXsoKhzLgf4M5BFIMgLgRVAgXOu2iuWDQxvr0qKiEho4XS59AeuBsYAw4DewKVNFG1yp2dmN5tZmpml5ebmtqWuIiJyBOF0uVwAbHPO5TrnqoA3gDOBJK8LBmAEsKupJzvnZjrnUp1zqcnJyRGptIiIHC6cQM8CzjCzXmZmwHRgPfAxcK1XZgYwp32qKCIi4QinD30ZgcHPlcBa7zkzgbuAO8wsAxgIzGrHeoqISAjxoYuAc+5+4P5Gs7cCp0W8RiIi0iq6UlRExCcU6CISMwLDgP6lQBeRmOGcXy8pClCgi4j4hAJdRMQnFOgiIj6hQBeRmKFBURER6RIU6CIiPqFAF5GYodMWRUSkS1Cgi0jM0KCoiIh0CQp0ERGfUKCLSMzQoKiIiHQJCnQRiRkaFBURkS5BgS4i4hMKdBERn1Cgi4j4hAJdRMQnFOgiIj6hQBcR8QkFuoiITyjQRUR8QoEuIuITCnQREZ9QoIuI+IQCXUTEJxToIiI+oUAXkZjj1/tcKNBFRHxCgS4iMcev97kIK9DNLMnMXjOzjWa2wcymmdkAM5tvZlu83/3bu7IiItK8cFvojwJznXMTgROBDcDdwALn3HhggTctIiJREjLQzawvcA4wC8A5V+mcKwCuBp7zij0HXNNelRQRiaRYHhQdC+QCz5jZKjN72sx6A0Occ7sBvN+D27GeIiISQjiBHg+cDDzhnJsKlNCC7hUzu9nM0swsLTc3t5XVFBGJnFgeFM0Gsp1zy7zp1wgE/F4zGwrg/d7X1JOdczOdc6nOudTk5ORI1FlERJoQMtCdc3uAnWZ2rDdrOrAeeBuY4c2bAcxplxqKiEhY4sMsdxvwgpl1B7YC3yOwM3jFzG4CsoDr2qeKIiKR5ddB0bAC3Tm3Gkht4qHpka2OiIi0lq4UFZGYE8uDoiIi0gUo0EVEfEKBLiIxx6+Dogp0ERGfUKCLiPiEAl1ExCcU6CISc3TaooiIT2hQVEREOjUFuoiITyjQRUR8QoEuIuITCnQREZ9QoIuI+IQCXUTEJxToIiI+oUAXEfEJBbqIiE8o0EVEfEKBLiLiEwp0ERGfUKCLiPiEAl1ExCcU6CIiPqFAFxHxCQW6iIhPKNBFRHxCgS4i4hMKdBERn1Cgi4j4hAJdRMQnFOgiIj6hQBcR8YmwA93M4sxslZm9602PMbNlZrbFzF42s+7tV00REQmlJS3024ENQdMPAo8458YD+cBNkayYiIi0TFiBbmYjgMuBp71pA84HXvOKPAdc0x4VFBGR8ITbQv8L8Aug1pseCBQ456q96WxgeITrJiIiLRAy0M3sCmCfc25F8Owmirpmnn+zmaWZWVpubm4rqykiIqGE00L/CnCVmW0HXiLQ1fIXIMnM4r0yI4BdTT3ZOTfTOZfqnEtNTk6OQJVFRKQpIQPdOXePc26Ecy4FuB74yDl3I/AxcK1XbAYwp91qKSIiIbXlPPS7gDvMLINAn/qsyFRJRERaIz50kUOccwuBhd7fW4HTIl8lERFpDV0pKiLiEwp0ERGfUKCLiPiEAl1EYk6TF834gAJdRMQnFOgiEnOautTdDxToIiI+oUAX6YL2F1dQVlkT7WpIJ6NAF+mCTvndh1z1+OJoV6PL0qCoiHQqW/YVR7sK0sko0EUk5mhQVEREOjUFuoiITyjQRSTmaFBUREQ6NQW6iIhPKNBFRHxCgS4iMUenLYqI+IQGRUVEpFNToIuI+IQCXURihvm189yjQBeRmOH82nnuUaCLSMxxPk12BbqIiE8o0EVEfEKBLiIxQ4OiIiI+4dOu83oKdBGJOX4NdgW6iIhPKNBFJOY4n36biwJdRGKGBkVFpFPx60UxHcmvq1CBLtLF+DWMpO1CBrqZjTSzj81sg5mlm9nt3vwBZjbfzLZ4v/u3f3VFRHneen7fGYbTQq8G7nTOHQecAfzYzCYBdwMLnHPjgQXetIi0M3W5tJ1f12DIQHfO7XbOrfT+PghsAIYDVwPPecWeA65pr0qKyCF+DaOOoEHRIGaWAkwFlgFDnHO7IRD6wOBIV05EDqcGetv5dR2GHehmdhTwOvBT51xRC553s5mlmVlabm5ua+ooIkFq/ZpG0mZhBbqZJRAI8xecc294s/ea2VDv8aHAvqae65yb6ZxLdc6lJicnR6LOIiKt4vd9YThnuRgwC9jgnHs46KG3gRne3zOAOZGvnog05vdQ6gh+vVI0PowyXwG+A6w1s9XevHuBPwKvmNlNQBZwXftUUUSC+TWMOoLfB0VDBrpzbjHQ3GqYHtnqiEgoaqG3nV/Xoa4UFelifJpFEgEKdJEuRme5SHMU6CJdjPJcmqNAF+lqFOjSDAW6SBejs1zazq/fh6NAF+kkNuwu4qXlWSHL+TSLJALCOQ9dRDrApY8uAuD600YdsZzyvO38ulNUC12ki/Frd4G0nQJdpIupjXKe55dUcuVfF7PzQGl0KyKHUaCLdDHRHhR9Z80u1uYU8tSnmVGtR1v49RhHgS7S1fg1jaTNFOginrnrdnPbi6uiXY2QfeTRznM/fL+VX4chFOginlv+tZJ3vtwV7WqE7CP3axhJ2ynQRTqZUN/VEu0+dD/w6zpUoIt0MjUhmujRPsuljo4UAn79djqvpu2MdjUABbrIYaJ9nnfIFnqU6vfI/M0s27o/YneJ2HmglLnr9kTktaLp2aXb+a/X1kS7GoACXeQwoVrI7a2z9qE/umAL35z5ecRe77LHFnHLv1ZE7PVawq9HFwp0kUZqovzfHu0dSkc5WF4d7SqwJruAf6/dHe1qRIy+y0WkkWgHasjTFjtJ3keqGs45rINv9llX96seXwLA9j9e3qHv317UQhdpJNqBHur9o3GGRnv221d34PruLDvD9qJAF2mktja67x+qy6e982/1zgJKKhp2h7TnTi7aO1A/UaCLNFId5UQP1Ypsz9bywfIqrvnbEn7S6IrZ9hxXCG6hf7h+L89/tr3d3queT5vq6kMXaaSzD4q2Z+0qqwM7s9U7CxrMb6pOker1rq45tAP9/vNpAHx3WkqEXj22qIUuMWn2Z9v5x6dbm3ws6l0uoQK9HRPdNfpdp6k6NZ7z+opsUu5+j+KKlp290pF96HX82T5XoEuMum9OOg+8v6HJx6LdQg/99u3fn924Wyc40Jtrmf99YQYAuwvKWvWeHSHS3VW1naz/X4Eu0khNTeT/SWct3kbGvoPhvX8HnbaYV1zB8m0HGsyr8ro/Gr9Fe7aio9FCj5Ro7/wbU6CLNBLpf9Lqmlp+++56Lnj407BaiO3Zh/7wB5t437uQ5htPfsY3nvqswePVzezMwmmJ1p1L3tL6Bfeht4dfvPYlZ//pIyCoS6lRJVvb0u5sZ+go0CWm7dhfwuIteQ3mRfqftLz6UGCt310Usnyo0A/1XS9H8thHGdz6wkoAtuaVAA0Dte4Mn8ZvEdyKbi4U67pimtspNKe9W+ivpGWz88CRu4FauxNvy7ZoDwp06ZQ+2riXG5/+vN2/iOrchxby7VnLGsyLeKBX1bTotTuqy6VOZVCgV9WE7kMP1ZqtDKPFHTxwGo1WbuN1XFBaRWlly7+KoLN1FynQPVU1tdzx8mq25hZHuyoC/OD5FSzJ2E9FdcefctLWgMnYd5D31hz6fpDgZegWxiXutbWB0AzeEQSLeKAH1a+51nXwOqn7+8XlWWTtP3Sj6LpFqwxjmy3JOHRUVNXEDqC9Qr5u3ZU2OhPn1Ac+5KwHPw77dUorq3l60dYWH420t5gJ9MsfW8Tsz7Y3+/jKHfm8sSqHu14PfA3m+2t3U1Ba2ab3LCitJPdgRZteI1bVxV5zoRZpDVqgLUjMjH3F/PHfGxu0aC94+FN+/L8r66eDlyGc1651jv+es46J981t8ggl0pf+r95ZwMqsfACqapseFA1u0QYvw9efWFL/t3lbLZxAD3XU0lTIh2tJRh479pccsczO/DKm/uaDBvMOlIT///7neZv53XsbeK/RF3u9tDyLK/66KPzKRlhMBLpzjvRdRdw3J73ZMnUfqrhuxs4Dpdz6wkrufOXLkK/9xMJMHpy7scnHpv52Pqc+8GHrKh2GtO0HeGtVToueM3/9Xm59IfyvLHXOse9geUur1mZ1/+LlVeH/Y9fWOvKKm96BhlqGsqCAaclh9Pef+4InP8lkT1Hg9T/ZnNugPgAVQcsQTtjV1Dr+d1lW4LlNlG9tC725rpL/88wXfP3vS4GgFnqjok210AHyig+FYH0LvSb0Tjh4uZpa38112zy2YAsPz998xNe+8ellnPvQQoAGR0rBO8ePNu4jv7QqZD2bU/c5K2vUTXP3G2tZl1PkndXU8Uf7vg70N1dl8/SirQ0+HCUV1VRUH/6Bq/tQJcR1Y7+3p677Jz2SB+du5ImFmZz//xbym3fWN3isvcdLrn3yM3768uoWPecHz6fx/to99S2kFTvyqa117CooI6fR+cOV1bVMf/gTTntgAZv3Hn7KXUV1Tbv3f97+0iquenwxzjUf1nWe/DST1N99yC5vOeqW8bYXV3HaAwtYsSO/2RZ/cP9pc8v0xfYDpNz9Hhn7DrIupxA4FNB1QXjTs1/Uly/xXrM86PNWF2Qb9xRx9+trmnyvdbsKg+oVeO72vBLyiivYtOcgP381dEOjKeH0bVc3c9piwyOYEO/jLWNNreOZJdsoKq+isrq2Qau7IkQLPbuZQcyH52/msQVb+O27gf+11TsL+OWba3HOsS6nkLLKhts3+EipqsZF7Ogm1DjHb99dz7VPLo3Ie7WEry/9/9nLgQ/+yaP718+bfP88ALb94TLufXMtiQlx3H/l5Pp/9LhuRp7XTZK+69AZCW+tyuHF5VlclzqSr08dTrduRnb+of7DrbklbM3dxq+unNTq+mbmFpMysDdx3Vp2UbVzjjdX5XDFCcPoHt9wH52+q5B56Xv52QXjG3xFaV5xBdn5ZVw/83OmjR3IZ1v3Aw2/RvRXc9axNTdw6PrkwkxGDezFTy+YAMBzS7dz/9vpXHPSMKaNG0i/nt25ZMrRh9Xt7S93cf7EwRzVI5412QVU1ThOHpWEmbG3qJwvdxZw0eTA81bvLGBJRh4//uox9c9f5p0n/ciHW3hswRYG9u7OivsurH+8sKyKqx9fzMPfPIn56/cC8PxnOxg/+CjufPVLFv3iq/U3fn7ny138xxNL+dsNJx9Wz+z8QwFSFzy3zF7B2RMGcePpowF44fMdQKBLBWDT7y6pX6fFFdXMS9/ToLVZXFFNn8SEBi30++asY+Z3TuGSvwQOy3903jhGD+zdoC6/fHNd/d8lFdUM6N2d8/68kJ4JcST1SmB3YeuOmMIZj6gKurDIOUdmbglPL9pK1oFDn/XG3UYvLc/i9ZXZ9dPrcoqYMrwfN/xjGVkHStlTVM57a3bTMyGO+XecS3Z+KblBLfutucXMXbeHey6dWD/vsscWkfn7y5r9X5i1eBv3XDqRW2avYE9ROS94RzQXThrS7LJN+8MCkvv0CLkOGtueV8KI/j2Jj+vG2uxCzA4d7Rwp1wvacATQWm0KdDO7BHgUiAOeds79MSK1akaglVbZ5EYprqjmyYWZ3PrVcXy8MZf+vRPqH/vVnHWHlf/h7BV84AXA/VdOrh91/3RzboMNsbuwjKH9eta3hJdtO0Ctc3x96vBmB1GKyqvoHnf4wc/ybQdIGdSLmlrHvqIKThyZRGllNU8szCQxIY6H5m3inksnkhDXjVNTBnD8iH5NLueU++fx+68dXz9vzD3vA/DAexuY+d1UKqprqKpxzPjn8voy3502mkFHHVpvm/YcJNMbAK4L87rlnbVoG0sz9zc4xe4Nr2vn5nPG0qt7PPe/Hei+emv1Lt5aHQjMZ793KlkHSklMiGN4Uk+yDpRyzxtrAbjt/GP460cZDZZl6qgkVmUVMO+n5zA2uTfX/C3QH3viiKTDWm3PLtkGwP6SSj7ZnMuv306nb2I8X2YHWrOPzN/MqqzA9488+Ukm45IDIVnXNwyBW4UBzE0//LZndV0OANfP/JzP75nO3PQ9zE3fQ1LP7pw1fhA78xu2GvOKK+u7GUoqqvnh7IZdWdP+EDj3+e83HtqBbM0tqd8hwKHWbHOD8XuLyhk5oBcQ6BYqKzxyd8aiLbmcMro/ZZU1/OvzLMYk92ZInx6cPnZgg+6elLvfO+y5BaWV1Hh96CWVNdz1+hpeScs+rFzj73m529vGdev88Y8zePzjQ9v6qU+2Nnhu3Xauc9frgefnNFq/4+59nyV3n8/gPj149MMth10E9dbqXcTHNQz8up16U/aXVNYffYdj38Fy9hVVcMVfFzO0XyKP33Ay//FE4HNSt+NoSZdgR7DWnhZmZnHAZuBCIBv4AviWc259c89JTU11aWlpLX6vTzbncucrX3Jd6gieWJjJb6+Zwn1vBUJ61IBeDVoPrXXD6aPq+y0bu/eyifz+/ab7yRtL7tPjsIHQ/r0S+ME5Y/nT3E0N5s/8zim8tTqH99c2fV/FG04fRd7BCi6YNISisiqe+nQrcWZhdQWNGdSbbXmHBoae/8/TOFheXX8IOjyp52FdLOG48sRhTBnWlz/8O7z10VVdeeKw+pZ9c166+Qyu927JdvVJw5iz+sjlm/L6j6axeW9x/Y6vKTefM5aZTXzvjBmcmjKA5dsOcNclE+vHcpJ6JVBb6ygKuiNQ/14J3D59PL9+p9l/z04pIc74102nt+rWdw98bUqDo51wZDxwKd+etYzPtx44Yrmh/RLZXVjOD84ewz8WBRobb9x6ZoOGAQSO+I/um8gvLjmWq08a3rIFCGJmK5xzqSHLtSHQpwG/ds5d7E3fA+Cc+0Nzz2ltoF/518WszSkMXVDaZPTAXuzY3/ado0hX9eatZ/K1v4ff9z194mAWbNwHQDc78vjCez85i8nDDj/qDke4gd6WQdHhwM6g6WxvXsT165nQ5PyLJw/h+OFHXkGPXn9Sg+mh/RJDvt+7t53F7JtO497LJh6x3PjBR4V8ra5ixrTRvPrDaQ3m9UyIAwKtyNW/upCrThzW7PNPHJkU1vuc6HUjpQzsxSmj+/PdaYH+6QG9u7em2gBcfsLQVj2vg+961kBw91dLXH1S89ugJe69bCK9usfVT48Z1PsIpZu2/Y+Xc+eFE+qnz52Q3Or6nDK6P7++chKzbzqtft7lx4e3Xa9s4nM5PKln/d9nHTOowWMXNdHPvvDn59EnMT5kmDcef6kLczhymB87pA/jkts/L9rSQr8OuNg5931v+jvAac652xqVuxm4GWDUqFGn7Nixo8Xv5Zzj2aXb+SB9L988dSSvpO3kzosmcMroAQC8tiKbB95bz99uOJkzjxnEvPQ9JCbEsfNAKd8+YzQbdhfx81e/5PpTR3Jd6kj+5510po7sz3WpI8jOLyOvuIJXV2QzeVhfhiX15KvHDq5/74c/2MSijDz++/JJrM0u4JjBfThj7AByiys4um8izy3dzuZ9xZw+ZgBnj0/GOcfAo3qwNDOPHvFxvLUqh5VZ+YxLPorLjh/KvW+u5aFrT8AssKMa0jeRbXkl9O/VnaP7JVJYVkW/ngmUVFQT180Y0b8Xb3+5i1mLt3F03x5cOOloap3jqhOHUVhWRX5pJSUV1ezYX8rSzP0k9Uxg0rC+VNcGymTmFrNoSx57i8q55dxxdI/rhhkszsjjoklH8+91u5m/fi/fP3ssJ41MYmlmHutyCkkZ2JtzJiRTVFbF4L6HdoIHvTMWCsuqeHF5FrdNH0/W/lKOG9qXFTvymb9+DzPOTGFNdiFnjR9EcXk12fll/HB2GvFx3Xjtlmn07B5H38QEEhPiGmznl7/IonePeBLiunHRpCHsLixn+bYDLM7I45LJR7O7sIzRA3szZXg/nvwkk0825XLc0D786doTKSitpG/PwGuu2HGApRn7GTf4KD5cv5d31uziN1dPYfKwviQmxPHJplwG9+3BuOSjeHbpdq4/dSQ1tY7c4grOmZDM7M92sK+onFvOG0dZZQ0vf7GT4f17cu6EZFbvLODy44cya/E20nbkM23sQHYXltG7RzyDjurB2uxC7rxoAglx3fhg/R7MjJNHJVFQWsWiLXnUOse0cQM5fcxAbnz6c9JzivjW6aMwg7GDevPxxlwunDSE44b25dij+/DWqhxeWLYjsE4mH833zkzhoQ82ccUJQ3k1LZvjhvbhg/S9lFRWs2nPQaYM78eMaSmMTe7NN576nP0lFYwa0Iv/uvhYdheU8/rKbC6aNIQ7LjoWgJyCMkorqknu04Nl2w6QEGfk5JfxjVNHkrGvmMSEOEYN6MWewnKeXrSVuG7d6NczgR+cM4Ze3eOprK7loXkb+f7ZYxnSN5E5q3NYmrGfH503ju7x3aipdWzNK+GkEUk8szTQNXH79PFs3HMQM/hiez7rdxXxva+kMGFIHyAwvgOBI8a56/YwckBP9hZVkJoSGBt4Y2UOcd2M9F2FfCN1JNOPG8LnW/czckAvnlyYyTkTkrlw0hAWbNjLF9vzuePCCTyzZBtJvRJITRnA2EG9+d17GxiW1JMBvRO4dMpQEhPiWJqZx7NLtrNoSx4XTx7C5ScMY2lmHlv2FnPW+EF8fepwBvdNZP2uIl5dsZOK6lrSdxWR0M2YNm4guwrKOVhexQ/PHccpo/uzLa+EJxdm8pXxg7jyhKFtum+qr7pcRERiWUd0uXwBjDezMWbWHbgeeLsNryciIm3Q6tMWnXPVZvZ/gXkETlv8p3Ou+UsxRUSkXbXpPHTn3PvA+xGqi4iItIGvL/0XEYklCnQREZ9QoIuI+IQCXUTEJxToIiI+0eoLi1r1Zma5QMsvFQ0YBOSFLOUvWubYoGWODW1Z5tHOuZDfrdChgd4WZpYWzpVSfqJljg1a5tjQEcusLhcREZ9QoIuI+ERXCvSZ0a5AFGiZY4OWOTa0+zJ3mT50ERE5sq7UQhcRkSPoEoFuZpeY2SYzyzCzu6Ndn0gws5Fm9rGZbTCzdDO73Zs/wMzmm9kW73d/b76Z2WPeOlhjZoffur6LMLM4M1tlZu9602PMbJm3zC97X8eMmfXwpjO8x1OiWe/WMrMkM3vNzDZ623ua37ezmf3M+1yvM7MXzSzRb9vZzP5pZvvMbF3QvBZvVzOb4ZXfYmYz2lKnTh/o3s2o/wZcCkwCvmVmk6Jbq4ioBu50zh0HnAH82Fuuu4EFzrnxwAJvGgLLP977uRl4ouOrHDG3AxuCph8EHvGWOR+4yZt/E5DvnDsGeMQr1xU9Csx1zk0ETiSw7L7dzmY2HPgJkOqcm0Lg67Wvx3/b+VngkkbzWrRdzWwAcD9wOnAacH/dTqBVnHOd+geYBswLmr4HuCfa9WqH5ZwDXAhsAoZ684YCm7y/nwK+FVS+vlxX+gFGeB/084F3ASNwsUV84+1N4Lv2p3l/x3vlLNrL0MLl7Qtsa1xvP29nDt1veIC33d4FLvbjdgZSgHWt3a7At4CnguY3KNfSn07fQqcDb0YdLd4h5lRgGTDEObcbwPtdd4NTv6yHvwC/AGq96YFAgXOu2psOXq76ZfYeL/TKdyVjgVzgGa+b6Wkz642Pt7NzLgf4M5AF7Caw3Vbg7+1cp6XbNaLbuysEelN3VvXNqTlmdhTwOvBT51zRkYo2Ma9LrQczuwLY55xbETy7iaIujMe6injgZOAJ59xUoIRDh+FN6fLL7HUZXA2MAYYBvQl0OTTmp+0cSnPLGNFl7wqBng2MDJoeAeyKUl0iyswSCIT5C865N7zZe81sqPf4UGCfN98P6+ErwFVmth14iUC3y1+AJDOru3tW8HLVL7P3eD/gQEdWOAKygWzn3DJv+jUCAe/n7XwBsM05l+ucqwLeAM7E39u5Tku3a0S3d1cIdF/ejNrMDJgFbHDOPRz00NtA3Uj3DAJ963Xzv+uNlp8BFNYd2nUVzrl7nHMjnHMpBLbjR865G4GPgWu9Yo2XuW5dXOuV71ItN+fcHmCnmR3rzZoOrMfH25lAV8sZZtbL+5zXLbNvt3OQlm7XecBFZtbfO7K5yJvXOtEeVAhz4OEyYDOQCfwy2vWJ0DKdReDQag2w2vu5jEDf4QJgi/d7gFfeCJztkwmsJXAGQdSXow3Lfx7wrvf3WGA5kAG8CvTw5id60xne42OjXe9WLutJQJq3rd8C+vt9OwP/A2wE1gGzgR5+287AiwTGCKoItLRvas12Bf7TW/YM4HttqZOuFBUR8Ymu0OXcQ9kBAAAAMElEQVQiIiJhUKCLiPiEAl1ExCcU6CIiPqFAFxHxCQW6iIhPKNBFRHxCgS4i4hP/H5I+UKpviHgCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_hist = sorted(hist, key=lambda x: x[0], reverse = False)\n",
    "values = [elem[1] for elem in sorted_hist]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.62)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fCvFG0VMKts"
   },
   "outputs": [],
   "source": [
    "def make_triplet_samples(z, margin, r2, r3):\n",
    "  positive_sample = z + random_vector_volume(z.shape, 0, margin).cuda() \n",
    "  negative_sample = z + random_vector_volume(z.shape, r2, r3).cuda()\n",
    "  return positive_sample, negative_sample\n",
    "\n",
    "def random_vector_surface(shape, r = 1.):\n",
    "  mat = torch.randn(size=shape).cuda()\n",
    "  norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "  return (mat/norm) * r\n",
    "\n",
    "def random_vector_volume(shape, inner_r, outer_r):\n",
    "  fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "  fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "  fraction.unsqueeze_(-1)\n",
    "  return random_vector_surface(shape, 1) * fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PizmBkGqMKtu"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_frequent(x):\n",
    "  return Counter(x).most_common(1)[0]\n",
    "\n",
    "def preds_around(center, radius, n_preds, model, dummy_img):\n",
    "  z_s = random_vector_volume([n_preds, 10], radius, radius + 0.01) + center[None]\n",
    "  noises = model.forward_z(z_s)\n",
    "  perturbed_imgs = noises + dummy_img \n",
    "  return torch.argmax(arch(perturbed_imgs), 1)\n",
    "  \n",
    "def most_freq_pred_around(center, radius, n_preds, model, dummy_img):\n",
    "  preds = preds_around(center, radius, n_preds, model, dummy_img)\n",
    "  most_freq = most_frequent(preds.tolist())\n",
    "  return (class_index_to_label(most_freq[0]), most_freq[1]/n_preds)\n",
    "\n",
    "def investigate_neighborhood(z, step, model, dummy_img):\n",
    "  with torch.no_grad():\n",
    "    result = []\n",
    "    for radius in np.arange(0.1, 6., step):\n",
    "#       print(\"creating {} more preds\".format(int(10 + 5 * (radius ** 2))))\n",
    "      most_freq_pred = most_freq_pred_around(z, radius, int(10 + 5 * (radius ** 2)), model, dummy_img)\n",
    "      result.append((radius, most_freq_pred))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-btRW4qMKtw",
    "outputId": "90e81f6a-1b9e-45a4-ae82-bda370319bd9"
   },
   "outputs": [],
   "source": [
    "#experiment 1\n",
    "\n",
    "z = torch.tensor([0.5] * 10).cuda()\n",
    "# z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "# z_s = z[None]\n",
    "\n",
    "model = learn.model.eval()\n",
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "  \n",
    "for i in range(6):\n",
    "  z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "  print(\"investigation for: \", z)\n",
    "  for elem in investigate_neighborhood(z, 0.5, model, x_img):\n",
    "    print(elem)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 1-1: modified investigate_z\n",
    "z_investigate_path = '/root/Derakhshani/adversarial/textual_notes/investigate_z_{}.txt'.format(env.save_filename)\n",
    "if Path(z_investigate_path).exists(): raise FileExistsError(\"file already exists\")\n",
    "file = open(str(z_investigate_path), 'w')\n",
    "        \n",
    "for i, (z, noise) in enumerate(zip(pruned_z_s, pruned_noises)):\n",
    "  hist = compute_prediction_histogram(learn, noise)\n",
    "  indexed_hist = [(i, val) for i, val in enumerate(hist)]\n",
    "  sorted_hist = sorted(indexed_hist, key=lambda x: x[1], reverse=True)\n",
    "  labeled_hist = [(class_index_to_label(i), count) for i, count in sorted_hist]\n",
    "  print(\"result {}:\".format(i))\n",
    "  print(big_vector_to_str(z))\n",
    "  print(labeled_hist[:6])\n",
    "  print(\"\\n\\n\")\n",
    "  \n",
    "  file.write(\"result {}:\\n\".format(i))\n",
    "  file.write(big_vector_to_str(z) + \"\\n\")\n",
    "  file.write(str(labeled_hist[:6]))\n",
    "  file.write(\"\\n\\n\\n\")\n",
    "  file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bp6YOnipMKtz"
   },
   "outputs": [],
   "source": [
    "#experiment 2\n",
    "import itertools\n",
    "z_s = [torch.tensor(t).cuda() for t in itertools.product( *([[-0.33, 0.33]] * 10) )]\n",
    "model = learn.model.eval()\n",
    "noises = []\n",
    "with torch.no_grad():\n",
    "  for z in z_s:\n",
    "    noises.append(model.forward_single_z(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "55lErWDyMKt1",
    "outputId": "93d8fb71-3fd5-44a5-d3ec-8013e13f17ba"
   },
   "outputs": [],
   "source": [
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "\n",
    "preds = []\n",
    "for noise in noises:\n",
    "  perturbed_img = x_img + noise\n",
    "  preds.append(torch.argmax(arch(perturbed_img[None]), 1)[0].item())\n",
    "\n",
    "from collections import Counter\n",
    "result = [(class_index_to_label(index), count) for index, count in Counter(preds).most_common(5)]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4WSg-wBFMKt5"
   },
   "outputs": [],
   "source": [
    "#experiment 3\n",
    "import itertools\n",
    "dimension_values = [[-0.9, 0.9]] * z_dim\n",
    "for i in range(z_dim):\n",
    "  if i % 100 != 0:\n",
    "    dimension_values[i] = [0.]\n",
    "# dimension_values[0] = [0.]\n",
    "# dimension_values[3] = [0.]\n",
    "# dimension_values[6] = [0.]\n",
    "# dimension_values[9] = [0.]\n",
    "pruned_z_s = [torch.tensor(t).cuda() for t in itertools.product(*dimension_values)]\n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 3: for the targeted-attack case\n",
    "pruned_z_s = []\n",
    "for i in range(z_dim):\n",
    "  new_z = torch.zeros(z_dim).cuda()\n",
    "  new_z[i] = 1.\n",
    "  pruned_z_s.append(new_z)\n",
    "  \n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 3-1: noises for \n",
    "pruned_z_s = []\n",
    "# for i in range(z_dim):\n",
    "#   new_z = torch.empty(z_dim).uniform_(0,1).cuda().detach()\n",
    "#   pruned_z_s.append(new_z)\n",
    "\n",
    "for i in range(z_dim):\n",
    "  new_z = torch.zeros(z_dim).cuda().detach()\n",
    "  new_z[i] = 1.\n",
    "  pruned_z_s.append(new_z)\n",
    "  \n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise in pruned_noises[0:200]:\n",
    "  img = noise_to_image(noise)\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider web\n",
    "z_values = [\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33],\n",
    "  [-0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33,  0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33,  0.33,  0.33],\n",
    "  [-0.33,  0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_values = [\n",
    "  # window screen\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuAVZzmKMKt9",
    "outputId": "7e6643e0-ce55-438b-e0ae-79bde3ee4cef"
   },
   "outputs": [],
   "source": [
    "#vgg-16_12 most repeated labels:\n",
    "l = [(611, 215.0),\n",
    "  (474, 194.1),\n",
    "  (398, 120.3),\n",
    "  (721, 79.6),\n",
    "  (741, 73.5),\n",
    "  (510, 62.5)]\n",
    "\n",
    "[(class_index_to_label(index), count) for index, count in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAgk-YyWc3rG"
   },
   "outputs": [],
   "source": [
    "# learn.recorder.plot_losses()\n",
    "# learn.recorder.plot_lr()\n",
    "# learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTHG4Bt7VDYp"
   },
   "outputs": [],
   "source": [
    "fooling_rates = []\n",
    "model = learn.model.eval()\n",
    "learn.metrics = [validation_single_perturbation]\n",
    "for i in range(10):\n",
    "  global_perturbations = model(torch.rand(1, 3, 224, 244).cuda())[0]\n",
    "  nag_util.global_perturbations = global_perturbations\n",
    "  fooling_rates.append(learn.validate()[1].cpu().item())\n",
    "  print(\"%d : %f\"%(i, fooling_rates[-1]))\n",
    "\n",
    "mean = np.mean(fooling_rates)\n",
    "stddev = np.std(fooling_rates)\n",
    "print(mean, stddev); print(fooling_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "OFCjzI7UaY3C",
    "outputId": "740185b4-dd54-46f4-b0af-79ee452568e1"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[200][0]\n",
    "x = normalize(x_img.data.cuda())\n",
    "z = torch.tensor([-0.33,  0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33], dtype=torch.float32).cuda()\n",
    "# z = torch.empty(z_dim).uniform_(-1,1).cuda()\n",
    "p = model.forward_single_z(z).detach()\n",
    "\n",
    "p_x = x + p\n",
    "# print(\"img range, noise range\")\n",
    "# print_range(x); print_range(p)\n",
    "adv_label = class_index_to_label(arch(p_x[None]).argmax(1).item())\n",
    "print_big_vector(arch(p_x[None])[0])\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0., 1.])\n",
    "p_img = Image(p)\n",
    "x_img.show()\n",
    "p_img.show()\n",
    "p_x_img.show()\n",
    "\n",
    "\n",
    "# print_range(p)\n",
    "# print_range(denormalize(x))\n",
    "# print_range(p_x)\n",
    "\n",
    "benign_label = class_index_to_label(arch(x[None]).argmax(1).item())\n",
    "\n",
    "print_big_vector(arch(x[None])[0])\n",
    "print(benign_label, adv_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzwsI2P1ZANz"
   },
   "outputs": [],
   "source": [
    "z1 = torch.tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p1 = model.forward_single_z(z1)\n",
    "\n",
    "z2 = torch.tensor([1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p2 = model.forward_single_z(z2)\n",
    "\n",
    "z3 = torch.tensor([1, 1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p3 = model.forward_single_z(z3)\n",
    "\n",
    "l2_distance(p1, p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eroI82OKSnAL"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[4][0]\n",
    "x = x_img.data[None].cuda()\n",
    "p = model(x)[0].squeeze().detach() \n",
    "x = x.squeeze()\n",
    "x = normalize(x)\n",
    "\n",
    "p_x = x + p\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0.,1.])\n",
    "p_img = Image(p)\n",
    "# x_img.show()\n",
    "p_img.show()\n",
    "# p_x_img.show()\n",
    "\n",
    "print_range(p)\n",
    "print_range(x)\n",
    "print_range(p_x)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NAG-tripletLossExperiment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
