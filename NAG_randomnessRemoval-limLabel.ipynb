{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ahmad-PH/nag-notebooks/blob/master/NAG_tripletLossExperiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqeZpz16do4y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up-to-date.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os; import subprocess\n",
    "\n",
    "def detect_env():\n",
    "    return 'colab' if 'content' in os.listdir('/') else 'IBM'\n",
    "  \n",
    "def run_shell_command(cmd):\n",
    "  p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "  print(str(p.communicate()[0], 'utf-8'))\n",
    "  \n",
    "if detect_env() == 'colab': root_folder = '/content'\n",
    "elif detect_env() == 'IBM' : root_folder = '/root/Derakhshani/adversarial'\n",
    "python_files_path = root_folder + '/nag-public'\n",
    "\n",
    "if os.path.isdir(python_files_path):\n",
    "  initial_dir = os.getcwd()\n",
    "  os.chdir(python_files_path)\n",
    "  run_shell_command('git pull')\n",
    "  os.chdir(initial_dir)\n",
    "else:\n",
    "  os.chdir('/root/Derakhshani/adversarial')\n",
    "  run_shell_command('git clone https://github.com/ahmad-PH/nag-public.git')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(python_files_path + '/NAG-11May-beforeDenoiser')\n",
    "\n",
    "from nag_util import *\n",
    "import nag_util\n",
    "from environment import *\n",
    "from visualization import *\n",
    "\n",
    "env = create_env()\n",
    "env.setup(cuda_index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Ev7jcRKoARg"
   },
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.imports import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import sys; import os; import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_arch = \"targeted\"\n",
    "# gen_arch = \"non-targeted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralRelu(nn.Module):\n",
    "  def __init__(self, leak=None, sub=None, maxv=None):\n",
    "    super().__init__()\n",
    "    self.leak,self.sub,self.maxv = leak,sub,maxv\n",
    "\n",
    "  def forward(self, x): \n",
    "    x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n",
    "    if self.sub is not None: x.sub_(self.sub)\n",
    "    if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "    return x\n",
    "  \n",
    "class deconv_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k_size = (4,4), s = (2,2), pad = (1,1), b = True, activation = True):\n",
    "        super(deconv_layer, self).__init__()\n",
    "\n",
    "        self.CT2d = nn.ConvTranspose2d(in_channels = in_ch,\n",
    "                                  out_channels = out_ch,\n",
    "                                  kernel_size = k_size,\n",
    "                                  stride = s, \n",
    "                                  padding = pad,\n",
    "                                  bias = b)\n",
    "        self.BN2d = nn.BatchNorm2d(out_ch)\n",
    "        \n",
    "        self.activation = activation\n",
    "        if self.activation:\n",
    "            self.relu = GeneralRelu(0, 0.2, 5)\n",
    "        \n",
    "        self.weight_init()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.activation:\n",
    "            return self.relu(self.BN2d(self.CT2d(input)))\n",
    "        else:\n",
    "            return self.BN2d(self.CT2d(input))\n",
    "\n",
    "    def weight_init(self):\n",
    "        self.CT2d.weight.data.normal_(mean = 0, std = 0.02)\n",
    "        self.CT2d.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deconv_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k_size = (4,4), s = (2,2), pad = (1,1), b = True, activation = True):\n",
    "        super(deconv_layer, self).__init__()\n",
    "\n",
    "        self.CT2d = nn.ConvTranspose2d(in_channels = in_ch,\n",
    "                                  out_channels = out_ch,\n",
    "                                  kernel_size = k_size,\n",
    "                                  stride = s, \n",
    "                                  padding = pad,\n",
    "                                  bias = b)\n",
    "        self.BN2d = nn.BatchNorm2d(out_ch)\n",
    "        self.activation = activation\n",
    "\n",
    "        self.weight_init()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.activation:\n",
    "            return F.relu(self.BN2d(self.CT2d(input)), inplace=True)\n",
    "        else:\n",
    "            return self.BN2d(self.CT2d(input))\n",
    "\n",
    "    def weight_init(self):\n",
    "        self.CT2d.weight.data.normal_(mean = 0, std = 0.02)\n",
    "        self.CT2d.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gen_arch == \"targeted\":\n",
    "  class Gen(nn.Module):\n",
    "    def __init__(self, z_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "      super(Gen, self).__init__()\n",
    "\n",
    "      self.bs = None\n",
    "      self.z_dim = z_dim\n",
    "      self.gf_dim = gf_dim\n",
    "      self.y_dim = y_dim\n",
    "      self.df_dim = df_dim\n",
    "      self.image_shape = image_shape\n",
    "      self.n_active_labels = 10\n",
    "      \n",
    "#       self.n_units_coeffs = [7, 4, 2, 1, 1, 1, 1]\n",
    "      self.n_unit_coeffs = [10, 7, 4, 2, 1, 1, 1]\n",
    "      self.n_units = [coeff * self.gf_dim for coeff in self.n_unit_coeffs]\n",
    "      \n",
    "      self.z_ = nn.Linear(self.z_dim, self.n_units[0] * 4 * 4, bias=True)\n",
    "      self.z_.bias.data.fill_(0)\n",
    "      self.BN_ = nn.BatchNorm2d(self.n_units[0])\n",
    "\n",
    "      self.half = max(self.gf_dim // 2, 1) \n",
    "      self.quarter = max(self.gf_dim // 4, 1)\n",
    "      self.eighth = max(self.gf_dim // 8, 1)\n",
    "      # sixteenth = max(self.gf_dim // 16, 1)\n",
    "\n",
    "      self.CT2d_1 = deconv_layer(self.n_units[0], self.n_units[1], k_size = (5,5), pad = (2,2))\n",
    "      self.CT2d_2 = deconv_layer(self.n_units[1], self.n_units[2])    \n",
    "      self.CT2d_3 = deconv_layer(self.n_units[2], self.n_units[3])\n",
    "      self.CT2d_4 = deconv_layer(self.n_units[3], self.n_units[4])\n",
    "      self.CT2d_5 = deconv_layer(self.n_units[4], self.n_units[5])\n",
    "      self.CT2d_6 = deconv_layer(self.n_units[5], self.n_units[6])\n",
    "      self.CT2d_7 = deconv_layer(self.n_units[6], 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "    def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "      h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "      h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "      output = deconv_layer(h_input)\n",
    "      assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "              \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "              \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "      return output\n",
    "\n",
    "    def forward_z(self, z):\n",
    "      self.bs = z.shape[0]\n",
    "\n",
    "      h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "      assert h0.shape[2:] == (4, 4), \"Unexpected shape, it shoud be (4,4)\"\n",
    "\n",
    "      h1 = self.CT2d_1(h0)\n",
    "      h2 = self.CT2d_2(h1)\n",
    "      h3 = self.CT2d_3(h2)\n",
    "      h4 = self.CT2d_4(h3)\n",
    "      h5 = self.CT2d_5(h4)\n",
    "      h6 = self.CT2d_6(h5)\n",
    "      h7 = self.CT2d_7(h6)\n",
    "      \n",
    "      \n",
    "#       h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "#       h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "#       h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "#       h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "#       h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "#       h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "#       h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "      ksi = 10.0\n",
    "      output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "      # this coeff scales the output to be appropriate for images that are \n",
    "      # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "      # interval)\n",
    "      return output_coeff * torch.tanh(h7)\n",
    "\n",
    "  #   # blind-selection\n",
    "    def forward(self, inputs):\n",
    "      self.bs = inputs.shape[0]\n",
    "\n",
    "      benign_preds_onehot = arch(inputs)\n",
    "      benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "\n",
    "      z = torch.zeros([self.bs, 1000]).cuda()\n",
    "      for i in range(self.bs):\n",
    "        random_label = self.randint(0, self.n_active_labels, exclude = benign_preds[i].item())\n",
    "        z[i][random_label] = 1.\n",
    "\n",
    "      z_out = self.forward_z(z)\n",
    "\n",
    "      return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "\n",
    "  #   #second-best selection: made validation so much worse\n",
    "  #   def forward(self, inputs):\n",
    "  #     self.bs = inputs.shape[0]\n",
    "\n",
    "  #     benign_preds_onehot = arch(inputs)\n",
    "  #     target_preds = torch.topk(benign_preds_onehot, 2, dim = 1).indices[:, 1:]\n",
    "\n",
    "  #     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "  #     for i in range(self.bs):\n",
    "  #       z[i][target_preds[i]] = 1.\n",
    "\n",
    "  #     z_out = self.forward_z(z)\n",
    "\n",
    "  #     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "\n",
    "  #    def forward(self, inputs):\n",
    "  #     self.bs = inputs.shape[0]\n",
    "\n",
    "  #     benign_preds_onehot = arch(inputs)\n",
    "  #     benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "\n",
    "  #     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "  #     random_label = self.randint(0,1000, exclude = benign_preds.tolist())\n",
    "  #     for i in range(self.bs):\n",
    "  #       z[i][random_label] = 1.\n",
    "\n",
    "  #     z_out = self.forward_z(z)\n",
    "\n",
    "  #     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "\n",
    "    @staticmethod\n",
    "    def randint(low, high, exclude):\n",
    "      if exclude >= low and exclude < high:\n",
    "        temp = np.random.randint(low, high - 1)\n",
    "        if temp >= exclude:\n",
    "          temp = temp + 1\n",
    "        return temp\n",
    "      else:\n",
    "        return np.random.randint(low, high)\n",
    "\n",
    "    def forward_single_z(self, z):\n",
    "      return self.forward_z(z[None]).squeeze()\n",
    "\n",
    "    def generate_single_noise(self):\n",
    "      z = torch.empty(self.z_dim).uniform_(-1,1).cuda()\n",
    "      return self.forward_single_z(z)         \n",
    "\n",
    "    def make_triplet_samples(self, z, margin, r2, r3):\n",
    "      positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "      negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "      return positive_sample, negative_sample\n",
    "\n",
    "    def random_vector_surface(self, shape, r = 1.):\n",
    "      mat = torch.randn(size=shape).cuda()\n",
    "      norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "      return (mat/norm) * r\n",
    "\n",
    "\n",
    "    def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "      fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "      fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "      fraction.unsqueeze_(-1)\n",
    "      return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "    def make_z(self, in_shape):\n",
    "      return torch.empty(in_shape).cuda().uniform_(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = Gen(z_dim = 1000).cuda()\n",
    "# t = torch.empty(1000).uniform_().cuda()\n",
    "# g.forward_single_z(t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gen_arch == \"non-targeted\":\n",
    "  class Gen(nn.Module):\n",
    "    def __init__(self, z_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "      super(Gen, self).__init__()\n",
    "\n",
    "      self.bs = None\n",
    "      self.z_dim = z_dim\n",
    "      self.gf_dim = gf_dim\n",
    "      self.y_dim = y_dim\n",
    "      self.df_dim = df_dim\n",
    "      self.image_shape = image_shape\n",
    "\n",
    "      self.z_ = nn.Linear(self.z_dim, self.gf_dim * 7 * 4 * 4, bias=True)\n",
    "      self.z_.bias.data.fill_(0)\n",
    "      self.BN_ = nn.BatchNorm2d(self.gf_dim * 7)\n",
    "\n",
    "      self.half = max(self.gf_dim // 2, 1) \n",
    "      self.quarter = max(self.gf_dim // 4, 1)\n",
    "      self.eighth = max(self.gf_dim // 8, 1)\n",
    "      # sixteenth = max(self.gf_dim // 16, 1)\n",
    "\n",
    "      self.CT2d_1 = deconv_layer(self.gf_dim * 8, self.gf_dim * 4, k_size = (5,5), pad = (2,2))\n",
    "      self.CT2d_2 = deconv_layer(self.gf_dim * 5, self.gf_dim * 2)    \n",
    "      self.CT2d_3 = deconv_layer(self.gf_dim * 2 + self.half, self.gf_dim * 1)\n",
    "      self.CT2d_4 = deconv_layer(self.gf_dim * 1 + self.quarter, self.gf_dim * 1)\n",
    "      self.CT2d_5 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "      self.CT2d_6 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "      self.CT2d_7 = deconv_layer(self.gf_dim * 1 + self.eighth, 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "    def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "      h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "      h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "      output = deconv_layer(h_input)\n",
    "      assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "              \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "              \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "      return output\n",
    "\n",
    "    def forward_z(self, z):\n",
    "      self.bs = z.shape[0]\n",
    "\n",
    "      h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "      assert h0.shape[2:] == (4, 4), \"Non-expected shape, it shoud be (4,4)\"\n",
    "\n",
    "      h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "      h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "      h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "      h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "      h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "      h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "      h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "      ksi = 10.0\n",
    "      output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "      # this coeff scales the output to be appropriate for images that are \n",
    "      # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "      # interval)\n",
    "      return output_coeff * torch.tanh(h7)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "      self.bs = inputs.shape[0]\n",
    "      z = inputs.new_empty([self.bs, self.z_dim]).uniform_(-1,1).cuda()\n",
    "      p, n = self.make_triplet_samples(z, 0.1, 0.1, 2.)\n",
    "\n",
    "      z_out = self.forward_z(z)\n",
    "#       p_out = self.forward_z(p)\n",
    "#       n_out = self.forward_z(n)\n",
    "\n",
    "#       return z_out, p_out, n_out, inputs, z\n",
    "      return z_out, None, None, inputs, z\n",
    "\n",
    "    def forward_single_z(self, z):\n",
    "      return self.forward_z(z[None]).squeeze()\n",
    "\n",
    "    def generate_single_noise(self):\n",
    "      z = torch.empty(self.z_dim).uniform_(-1,1).cuda()\n",
    "      return self.forward_single_z(z)\n",
    "\n",
    "\n",
    "    def make_triplet_samples(self, z, margin, r2, r3):\n",
    "      positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "      negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "      return positive_sample, negative_sample\n",
    "\n",
    "    def random_vector_surface(self, shape, r = 1.):\n",
    "      mat = torch.randn(size=shape).cuda()\n",
    "      norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "      return (mat/norm) * r\n",
    "\n",
    "\n",
    "    def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "      fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "      fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "      fraction.unsqueeze_(-1)\n",
    "      return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "    def make_z(self, in_shape):\n",
    "      return torch.empty(in_shape).cuda().uniform_(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def js_distance(x1, x2):\n",
    "  m = 0.5 * (x1 + x2)\n",
    "  return 0.5 * (F.kl_div(x1, m) + F.kl_div(x2, m))\n",
    "\n",
    "def kl_distance(x1, x2):\n",
    "  inp = torch.log(x1)\n",
    "  target = x2\n",
    "  return F.kl_div(inp, target, reduction='batchmean')\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  x1 = tensorify(x1)\n",
    "  x2 = tensorify(x2)\n",
    "  x1 = x1 / torch.sum(x1)\n",
    "  x2 = x2 / torch.sum(x2)\n",
    "  return kl_distance(x1[None], x2[None])\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  if not isinstance(x1, torch.Tensor): x1 = torch.tensor(x1)\n",
    "  if not isinstance(x2, torch.Tensor): x2 = torch.tensor(x2)\n",
    "  x1 = x1 * 100. / torch.sum(x1)\n",
    "  x2 = x2 * 100. / torch.sum(x2)\n",
    "  return torch.norm(x1 - x2, 2)\n",
    "\n",
    "def distance_from_uniform(x):\n",
    "  return distrib_distance(x, [1.] * len(x))\n",
    "\n",
    "def wasserstein_distance(x1, x2):\n",
    "  return torch.mean(x1 - x2)\n",
    "\n",
    "def l1_distance(x1, x2):\n",
    "  return F.l1_loss(x1, x2)\n",
    "\n",
    "def l2_distance(x1, x2):\n",
    "  return F.mse_loss(x1 * 10, x2 * 10)\n",
    "\n",
    "def mse_loss(x1, x2):\n",
    "  return F.mse_loss(x1, x2)\n",
    "\n",
    "def cos_distance(x1, x2, dim = 1):\n",
    "  return -1 * torch.mean(F.cosine_similarity(x1, x2, dim=dim))\n",
    "\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, distance_func, margin):\n",
    "  # max distance when using l1_distance is 2\n",
    "  # max distacne when using l2-distance is sqrt(2)\n",
    "  ap_dist = distance_func(anchor, positive)\n",
    "  an_dist = distance_func(anchor, negative)\n",
    "\n",
    "  triplet_loss.call_count += 1\n",
    "  if triplet_loss.call_count % 200 == 0 : #and anchor.shape[1] == 1000:\n",
    "#     print(\"a: \", end=\"\"); print_big_vector(anchor[0])\n",
    "#     print(\"p: \", end=\"\"); print_big_vector(positive[0])\n",
    "#     print(\"n: \", end=\"\"); print_big_vector(negative[0])\n",
    "    print(\"func:{}, ap_dist: {}, an_dist: {}\".format(distance_func.__name__, ap_dist, an_dist))\n",
    "    \n",
    "  return torch.mean(F.relu(ap_dist - an_dist + margin))\n",
    "\n",
    "triplet_loss.call_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def diversity_loss(embeddings, z_s):\n",
    "#   size = z_s.shape[0]\n",
    "#   result = 0\n",
    "#   for i in range(size):\n",
    "#     for j in range(i+1, size):\n",
    "# #       a = F.cosine_similarity(embeddings[i], embeddings[j], dim = 0)\n",
    "# #       b = torch.norm(z_s[i] - z_s[j], 2, dim = 0)\n",
    "# #       print('embeddings: ')\n",
    "# #       print_big_vector(embeddings[i])\n",
    "# #       print_big_vector(embeddings[j])\n",
    "# #       print(f'a: {a}, b:{b}, multiple: {a*b}')\n",
    "#       result += F.cosine_similarity(embeddings[i], embeddings[j], dim = 0) * \\\n",
    "#                 torch.norm(z_s[i] - z_s[j], 2, dim = 0)\n",
    "#   n_pairs = (size * (size - 1)) / 2\n",
    "#   mean = result / n_pairs \n",
    "# #   print(f'result {result}, n_pairs {n_pairs}, mean {mean}')\n",
    "#   return mean\n",
    "  \n",
    "\n",
    "# # normalized with shuffling\n",
    "# def diversity_loss(embeddings, deranged_embeddings, z_s, deranged_z_s):\n",
    "#     cos_similarity = F.cosine_similarity(embeddings, deranged_embeddings)\n",
    "#     z_distance = torch.norm(z_s - deranged_z_s, 2, dim = 1)\n",
    "#     return torch.mean(cos_similarity * z_distance)\n",
    "  \n",
    "# # normalized with shuffling\n",
    "# def diversity_loss(embeddings, z_s):\n",
    "#     deranged_embeddings, deranged_z_s = derange(embeddings, z_s)\n",
    "#     cos_similarity = F.cosine_similarity(embeddings, deranged_embeddings)\n",
    "#     z_distance = torch.norm(z_s - deranged_z_s, dim = 1)\n",
    "#     max_possible_z_distance = 6.3246\n",
    "#     return torch.mean(cos_similarity * (z_distance/max_possible_z_distance))\n",
    "\n",
    "\n",
    "def diversity_loss(input, target):\n",
    "#   return -1 * torch.mean(torch.pow(f_x_a-f_x_s,2))\n",
    "  if input.shape[0] != batch_size:\n",
    "    print(\"input shape: \", input.shape)\n",
    "    print(\"target shape: \", target.shape, \"\\n\\n\")\n",
    "  return torch.mean(F.cosine_similarity(\n",
    "    input.view([batch_size, -1]),\n",
    "    target.view([batch_size, -1]), \n",
    "  ))\n",
    "\n",
    "\n",
    "if gen_arch == 'non-targeted':\n",
    "  def fool_loss(input, target):\n",
    "    true_class = torch.argmax(target, dim=1).view(-1,1).long().cuda()\n",
    "    target_probabilities = input.gather(1, true_class)\n",
    "    epsilon = 1e-10\n",
    "    result =  torch.mean(-1 * torch.log(1 - target_probabilities + epsilon))\n",
    "\n",
    "    fool_loss.call_count += 1\n",
    "    if fool_loss.call_count % 200 == 0:\n",
    "      print(\"target probs {}, loss: {}: \".format(target_probabilities, result))\n",
    "\n",
    "    return result\n",
    "\n",
    "  fool_loss.call_count = 0\n",
    "\n",
    "if gen_arch == 'targeted':\n",
    "  def fool_loss(model_output, target_labels):\n",
    "    target_labels = target_labels.view(-1, 1).long().cuda()\n",
    "    target_probabilities = model_output.gather(1, target_labels)\n",
    "    epsilon = 1e-10\n",
    "    # highest possible fool_loss is - log(1e-10) == 23\n",
    "    result = torch.mean(-1 * torch.log(target_probabilities + epsilon))\n",
    "\n",
    "    fool_loss.call_count += 1\n",
    "    if fool_loss.call_count % 200 == 0:\n",
    "      print(\"target probs {}, loss: {}: \".format(target_probabilities, result))\n",
    "\n",
    "    return result\n",
    "\n",
    "  fool_loss.call_count = 0\n",
    "\n",
    "\n",
    "def targeted_validation(gen_output, target):\n",
    "  perturbations, _, _, clean_images, _, z = gen_output\n",
    "  perturbed_images = clean_images + perturbations\n",
    "  target_labels = torch.argmax(z, 1)\n",
    "  adversary_preds = torch.argmax(arch(perturbed_images), 1)\n",
    "#   print('adv preds: ', adversary_preds.shape, adversary_preds)\n",
    "#   print('target_labels: ', target_labels.shape, target_labels)\n",
    "#   print('eq: ', (adversary_preds == target_labels))\n",
    "  return (adversary_preds == target_labels).float().mean()\n",
    "  \n",
    "\n",
    "# # targeted \n",
    "# def validation(gen_output, target):\n",
    "#   perturbations, _, _, clean_images, _, _ = gen_output\n",
    "#   return validation_(perturbations, clean_images)\n",
    "\n",
    "# # non-targeted\n",
    "# def validation(gen_output, target):\n",
    "#   perturbations, _, _, clean_images, _ = gen_output\n",
    "#   return validation_(perturbations, clean_images)\n",
    "\n",
    "# # general\n",
    "def validation(gen_output, target):\n",
    "  perturbations = gen_output[0]\n",
    "  clean_images = gen_output[3]\n",
    "  return validation_(perturbations, clean_images)\n",
    "\n",
    "unfooled_histogram = np.array([0.] * 1000)\n",
    "fooled_histogram = np.array([0.] * 1000)\n",
    "valid_cnt = 0\n",
    "\n",
    "def print_hist(unfooled, fooled):\n",
    "  indexed = [(i, u) for i, u in enumerate(unfooled)]\n",
    "  summarized = list(filter(lambda x: x[1] > 0.0, indexed))\n",
    "  total = fooled + unfooled\n",
    "\n",
    "  percent_total = [(i, 100. * u / (total[i] + 1e-10), total[i]) for i, u in enumerate(unfooled)]\n",
    "  sorted_percent_total = sorted(percent_total, key =lambda x: x[1], reverse = True)\n",
    "\n",
    "  print('\\npercent_total: ')\n",
    "  print(list(filter(lambda x: x[1] > 0.0, sorted_percent_total)))\n",
    "  print('\\n')\n",
    "  \n",
    "  return sorted_percent_total\n",
    "\n",
    "def validation_(perturbations, clean_images):\n",
    "  perturbed_images = clean_images + perturbations\n",
    "  benign_preds = torch.argmax(arch(clean_images), 1)\n",
    "  adversary_preds = torch.argmax(arch(perturbed_images), 1)\n",
    "\n",
    "  is_unfooled = (benign_preds == adversary_preds)\n",
    "  for i , unfooled in enumerate(is_unfooled):\n",
    "    if unfooled == 1:\n",
    "      unfooled_histogram[benign_preds[i]] += 1\n",
    "    else:\n",
    "      fooled_histogram[benign_preds[i]] += 1\n",
    "  \n",
    "  global valid_cnt\n",
    "  valid_cnt += 1\n",
    "  if valid_cnt % 10 == 0:\n",
    "    print_hist(unfooled_histogram, fooled_histogram)\n",
    "    \n",
    "  return (benign_preds != adversary_preds).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gen_arch == 'targeted':\n",
    "  class FeatureLoss(nn.Module):\n",
    "      def __name__(self):\n",
    "        return \"feature_loss\"\n",
    "\n",
    "      def __init__(self, dis, layers, layer_weights):\n",
    "          super().__init__()\n",
    "\n",
    "          # define generator here \n",
    "          self.dis = dis\n",
    "          self.diversity_layers = layers\n",
    "          self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "          self.weights = layer_weights\n",
    "          self.metric_names = [\"fool_loss\"] #+ [f\"div_loss_{i}\" for i in range(len(layers))] #maybe Gram\n",
    "  #         self.triplet_hooks = hook_outputs([arch.m.features[4]], detach=False)\n",
    "\n",
    "      def make_features(self, x, clone=False):\n",
    "          y = self.dis(x)\n",
    "          return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "\n",
    "      def forward(self, inp, target):\n",
    "        sigma_B, _, _, X_B, B_Y, z = inp\n",
    "\n",
    "        X_A = X_B + sigma_B\n",
    "  #       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "        A_Y, A_feat = self.make_features(X_A)\n",
    "  #       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "        chosen_labels = z.argmax(dim=1)\n",
    "        fooling_loss =  fool_loss(A_Y, chosen_labels)\n",
    "\n",
    "  #       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "  #       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "  #       self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "  #       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "        self.losses = [fooling_loss]\n",
    "        self.metrics = dict(zip(self.metric_names, [fooling_loss]))\n",
    "\n",
    "        return sum(self.losses)\n",
    "\n",
    "      def add_perturbation_shuffled(self, inp, perturbation):\n",
    "  #       j = torch.randperm(inp.shape[0])\n",
    "          j = derangement(inp.shape[0])\n",
    "          return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derange(*args):\n",
    "  if len(args) == 0: raise ValueError('shuffle function needs atleast one argument')\n",
    "  deranged_indexes = derangement(args[0].shape[0])\n",
    "  if not all([args[0].shape[0] == arg.shape[0] for arg in args]): \n",
    "    raise ValueError('inputs to shuffle must all have the same 0th dimension')\n",
    "  return [arg[deranged_indexes] for arg in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gen_arch == 'non-targeted':\n",
    "  class FeatureLoss(nn.Module):\n",
    "      def __name__(self):\n",
    "        return \"feature_loss\"\n",
    "\n",
    "      def __init__(self, dis, layers, layer_weights):\n",
    "          super().__init__()\n",
    "\n",
    "          self.dis = dis\n",
    "          self.diversity_layers = layers\n",
    "          self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "          self.weights = layer_weights\n",
    "\n",
    "  #         self.metric_names = [\"fool_loss\"] + [f\"div_loss_{i}\" for i in range(len(layers))] + ['triplet_loss']# Maybe Gram\n",
    "  #         self.metric_names = [\"div_loss\"] + ['triplet_loss']# Maybe Gram\n",
    "          self.metric_names = [\"fool_loss\"] + ['div_loss']# Maybe Gram\n",
    "          self.triplet_weight = 4.\n",
    "          self.div_weight = 1.\n",
    "          self.fooling_weight = 1.\n",
    "\n",
    "      def make_features(self, x, clone=False):\n",
    "          y = self.dis(x)\n",
    "          return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "\n",
    "      # contrastive loss\n",
    "      def forward(self, inp, target):\n",
    "          sigma_B, sigma_pos, sigma_neg, X_B, z_B = inp\n",
    "\n",
    "          deranged_perturbations, deranged_z_s = derange(sigma_B, z_B)\n",
    "\n",
    "          X_A = X_B + sigma_B\n",
    "          X_S = X_B + deranged_perturbations\n",
    "#           X_A_pos = X_B + sigma_pos\n",
    "#           X_A_neg = X_B + sigma_neg\n",
    "\n",
    "          B_Y, _ = self.make_features(X_B)\n",
    "          A_Y, A_feat = self.make_features(X_A)\n",
    "          _, S_feat = self.make_features(X_S)\n",
    "#           pos_softmax, _ = self.make_features(X_A_pos)\n",
    "#           neg_softmax, _ = self.make_features(X_A_neg)\n",
    "\n",
    "          raw_fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "          weighted_fooling_loss = self.fooling_weight * raw_fooling_loss\n",
    "\n",
    "#           raw_diversity_loss = diversity_loss(A_feat[0], S_feat[0], z_B, deranged_z_s)\n",
    "          raw_diversity_loss = diversity_loss(A_feat[0], S_feat[0])\n",
    "          weighted_diversity_loss = raw_diversity_loss * self.div_weight\n",
    "\n",
    "#           raw_triplet_loss = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "#           weighted_triplet_loss = raw_triplet_loss * self.triplet_weight\n",
    "\n",
    "  #         self.losses = weighted_diversity_losses + [weighted_triplet_loss]\n",
    "  #         raw_losses = raw_diversity_losses + [raw_triplet_loss]\n",
    "\n",
    "          self.losses = [weighted_fooling_loss] + [weighted_diversity_loss] #+ [weighted_triplet_loss]\n",
    "          raw_losses = [raw_fooling_loss] + [raw_diversity_loss] #+ [raw_triplet_loss]\n",
    "\n",
    "  #         self.losses = [fooling_loss] + [weighted_triplet_loss]\n",
    "  #         self.metrics = dict(zip(self.metric_names, [fooling_loss] + [raw_triplet_loss]))\n",
    "\n",
    "          if len(self.metric_names) != len(raw_losses):\n",
    "            raise Exception(\"length of metric names unequals length of losses\")\n",
    "\n",
    "          self.metrics = dict(zip(self.metric_names, raw_losses))\n",
    "          return sum(self.losses)\n",
    "\n",
    "\n",
    "\n",
    "  # #     triplet loss\n",
    "  #     def forward(self, inp, target):\n",
    "  #         sigma_B, sigma_pos, sigma_neg, X_B, _ = inp\n",
    "\n",
    "  #         X_A = self.add_perturbation(X_B, sigma_B) \n",
    "  #         X_A_pos = self.add_perturbation(X_B, sigma_pos)\n",
    "  #         X_A_neg = self.add_perturbation(X_B, sigma_neg) \n",
    "  #         X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "  # #         B_Y, _ = self.make_features(X_B)\n",
    "  #         A_Y, A_feat = self.make_features(X_A)\n",
    "  # #         _, S_feat = self.make_features(X_S)\n",
    "  #         pos_softmax, _ = self.make_features(X_A_pos)\n",
    "  #         neg_softmax, _ = self.make_features(X_A_neg)\n",
    "\n",
    "  # #         raw_fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "  # #         weighted_fooling_loss = self.fooling_weight * raw_fooling_loss\n",
    "\n",
    "  #         raw_diversity_losses = [diversity_loss(a_f, s_f, sigma_B, ) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "  #         weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "  #         raw_triplet_loss = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "  #         weighted_triplet_loss = raw_triplet_loss * self.triplet_weight\n",
    "\n",
    "  #         self.losses = weighted_diversity_losses + [weighted_triplet_loss]\n",
    "  #         raw_losses = raw_diversity_losses + [raw_triplet_loss]\n",
    "\n",
    "  # #         self.losses = [weighted_fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss]\n",
    "  # #         raw_losses = [raw_fooling_loss] + raw_diversity_losses + [raw_triplet_loss]\n",
    "\n",
    "  # #         self.losses = [fooling_loss] + [weighted_triplet_loss]\n",
    "  # #         self.metrics = dict(zip(self.metric_names, [fooling_loss] + [raw_triplet_loss]))\n",
    "\n",
    "  #         if len(self.metric_names) != len(raw_losses):\n",
    "  #           raise Exception(\"length of metric names unequals length of losses\")\n",
    "\n",
    "  #         self.metrics = dict(zip(self.metric_names, raw_losses))\n",
    "  #         return sum(self.losses)\n",
    "\n",
    "\n",
    "  #     #use two types of triplet losses\n",
    "  #     def forward(self, inp, target):\n",
    "  #       sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "  #       X_A = self.add_perturbation(X_B, sigma_B) \n",
    "  #       X_A_pos = self.add_perturbation(X_B, sigma_pos)\n",
    "  #       X_A_neg = self.add_perturbation(X_B, sigma_neg) \n",
    "\n",
    "  #       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "  #       B_Y, _ = self.make_features(X_B)\n",
    "  #       A_Y, A_feat = self.make_features(X_A)\n",
    "  #       _, S_feat = self.make_features(X_S)\n",
    "  #       pos_softmax, _ = self.make_features(X_A_pos)\n",
    "  #       neg_softmax, _ = self.make_features(X_A_neg)\n",
    "\n",
    "  #       fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "  #       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "  #       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "  #       raw_triplet_loss_sm = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "  #       weighted_triplet_loss_sm = raw_triplet_loss_sm * self.triplet_weight_sm\n",
    "\n",
    "  #       raw_triplet_loss_noise = triplet_loss(sigma_B, sigma_pos, sigma_neg, l2_distance, 5.)\n",
    "  #       weighted_triplet_loss_noise = raw_triplet_loss_noise * self.triplet_weight_noise\n",
    "\n",
    "  #       self.losses = [fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss_sm, weighted_triplet_loss_noise] \n",
    "  #       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses + [weighted_triplet_loss_sm, weighted_triplet_loss_noise]))\n",
    "\n",
    "  #       return sum(self.losses)\n",
    "\n",
    "  #     # just fooling and diversity\n",
    "  #     def forward(self, inp, target):\n",
    "  #       sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "  #       X_A = self.add_perturbation(X_B, sigma_B) \n",
    "\n",
    "  #       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "  #       B_Y, _ = self.make_features(X_B)\n",
    "  #       A_Y, A_feat = self.make_features(X_A)\n",
    "  #       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "  #       fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "  #       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "  #       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "  #       self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "  #       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "  #       return sum(self.losses)\n",
    "\n",
    "\n",
    "      def add_perturbation_shuffled(self, inp, perturbation):\n",
    "        j = derangement(inp.shape[0])\n",
    "        return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def produce_summary(root_folder, n_files):\n",
    "  def writeline(file, values, fmt_string):\n",
    "    file.write(', '.join(fmt_string.format(v) for v in values) + '\\n')\n",
    "  \n",
    "  last_rows = []\n",
    "  for i in range(n_files):\n",
    "    prefix = '/root/Derakhshani/adversarial/textual_notes/CSVs'\n",
    "    df = pd.read_csv(\"{}/{}/{}.csv\".format(prefix, root_folder, i))\n",
    "    last_rows.append(df.iloc[-1][1:-1].values.tolist())\n",
    "  \n",
    "  last_rows = np.array(last_rows)\n",
    "  \n",
    "  labels = list(df.columns[1:-1])\n",
    "  means = np.mean(last_rows, axis=0).tolist()\n",
    "  outfile = open('{}/{}/summary.txt'.format(prefix, root_folder), 'w+')\n",
    "  outfile.write('means: \\n')\n",
    "  writeline(outfile, labels, '{: >20}')\n",
    "  writeline(outfile, means, '{: >20.3}')\n",
    "  outfile.write('\\n')\n",
    "      \n",
    "  operations = []\n",
    "  for column in df.columns[1:-1]:\n",
    "    if column in ['train_loss', 'valid_loss', 'fool_loss', 'triplet_loss'] or column[:8] == 'div_loss':\n",
    "      operations.append('min')\n",
    "    elif column in ['validation', 'targeted_validation', 'div_metric', 'entropy']:\n",
    "      operations.append('max')\n",
    "    else:\n",
    "      raise ValueError('column {} is not recognized'.format(column))\n",
    "    \n",
    "  results = []\n",
    "  indexes = []\n",
    "  \n",
    "  for i in range(len(operations)):\n",
    "    values = last_rows[:, i]\n",
    "    if operations[i] == 'max': operation = np.max\n",
    "    elif operations[i] == 'min': operation = np.min\n",
    "    result = operation(values)\n",
    "    results.append(result)\n",
    "    indexes.append(values.tolist().index(result))\n",
    "  \n",
    "  outfile.write('bests: \\n')\n",
    "  writeline(outfile, labels, '{: >20}')\n",
    "  writeline(outfile, operations, '{: >20}')\n",
    "  writeline(outfile, results, '{: >20.3}')\n",
    "  writeline(outfile, indexes, '{: >20}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils import dir_util \n",
    "\n",
    "def investigate_initial_settings(n_settings, n_epochs, lr, wd, results_dir):\n",
    "  os.mkdir(env.get_csv_dir() + results_dir)\n",
    "  os.mkdir(env.get_models_dir() + results_dir)\n",
    "  \n",
    "  for setting_ind in range(n_settings):\n",
    "    print(f\"investigation no: {setting_ind}\")\n",
    "    learn = None; gen = None; gc.collect()\n",
    "    gen = Gen(z_dim = z_dim)\n",
    "    init_cnn(gen, True)\n",
    "    \n",
    "    tmp_csv_filename =  env.temp_csv_path + '/' + results_dir + '/' + str(setting_ind)\n",
    "    csv_logger = partial(ImmediateCSVLogger, filename=tmp_csv_filename)\n",
    "\n",
    "    if gen_arch == 'non-targeted':\n",
    "      metrics = [validation]\n",
    "    elif gen_arch == 'targeted':\n",
    "      metrics = [validation, targeted_validation]\n",
    "      \n",
    "    learn = Learner(data, gen, loss_func = feat_loss, metrics=metrics, \n",
    "                    model_dir = env.get_learner_models_dir(), \n",
    "                    callback_fns=[DiversityMetric, LossMetrics, csv_logger])\n",
    "    \n",
    "    saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=model.__name__ + \"-best\")\n",
    "    saver_every_epoch = SaveModelCallback(learn, every='epoch', name=model.__name__)\n",
    "\n",
    "    learn.fit(n_epochs, lr=lr, wd = wd, callbacks=[saver_best, saver_every_epoch])\n",
    "    \n",
    "    shutil.copyfile(tmp_csv_filename + \".csv\", env.get_csv_dir() + results_dir + '/' + str(setting_ind) + '.csv')\n",
    "    \n",
    "    model_dest = env.get_models_dir() + results_dir + '/' + str(setting_ind)\n",
    "    os.mkdir(model_dest)\n",
    "    dir_util.copy_tree(env.data_path/env.get_learner_models_dir(), model_dest)\n",
    "    shutil.rmtree(env.data_path/env.get_learner_models_dir())  \n",
    "    \n",
    "  produce_summary(results_dir, n_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_perturbations(learn, n_perturbations):\n",
    "  initial_training_mode = learn.model.training\n",
    "  gen = learn.model.eval()\n",
    "  perturbations = [gen.generate_single_noise() for _ in range(n_perturbations)]\n",
    "  learn.model.train(initial_training_mode)  \n",
    "  return perturbations\n",
    "\n",
    "def compute_prediction_histogram(learn, perturbation, verbose=False):\n",
    "  pred_hist = [0] * 1000\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 and verbose: print (\"at batch no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbation[None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      pred_hist[pred] += 1\n",
    "  return pred_hist\n",
    "\n",
    "\n",
    "def compute_mean_prediction_histogram(learn, perturbations, verbose=False):\n",
    "  pred_hist = torch.tensor([0] * 1000).detach_()\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 5 == 0 and verbose: print(f\"at batch no {batch_no}\")\n",
    "    for j, perturbation in enumerate(perturbations):\n",
    "      perturbed_batch = batch + perturbation[None]\n",
    "      preds = arch(perturbed_batch).argmax(1)\n",
    "      for pred in preds:\n",
    "        pred_hist[pred] += 1\n",
    "  pred_hist = pred_hist.float() / len(perturbations)\n",
    "  return pred_hist.tolist()\n",
    "\n",
    "\n",
    "def classes_needed_to_reach(percentage, hist):\n",
    "  hist_sum = np.sum(hist)\n",
    "  indexed_hist = [(i, hist_element) for i,hist_element in  \n",
    "                          enumerate(hist)]\n",
    "  sorted_hist = sorted(indexed_hist, key=lambda x: x[1], reverse = True)\n",
    "  \n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = sorted_hist[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / hist_sum) * 100.\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, sorted_hist\n",
    "\n",
    "def diversity(learn, n_perturbations, percentage = 95, verbose = True):\n",
    "  pred_histogram = compute_mean_prediction_histogram(\n",
    "      learn, generate_perturbations(learn, n_perturbations), verbose\n",
    "  )\n",
    "  print(\"finished creating the prediction histogram\")\n",
    "\n",
    "  return classes_needed_to_reach(95, pred_histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiversityMetric(LearnerCallback):\n",
    "  _order = -20 # Needs to run before the recorder\n",
    "  \n",
    "  def __init__(self, learn):\n",
    "    super().__init__(learn)\n",
    "    self.average_over = 4\n",
    "    self.n_perturbations = 10\n",
    "    self.percentage = 95\n",
    "  \n",
    "  def on_train_begin(self, **kwargs):\n",
    "    self.learn.recorder.add_metric_names(['div_metric', 'entropy'])\n",
    "    \n",
    "  def on_epoch_begin(self, **kwargs):\n",
    "    global learn\n",
    "    self.perturbations_list = [generate_perturbations(self.learn, self.n_perturbations) \\\n",
    "                          for _ in range(self.average_over)]\n",
    "    self.pred_hist_list = [torch.tensor([0] * 1000).detach_() for _ in range(self.average_over)]\n",
    "    \n",
    "  def on_batch_end(self, last_output, train, **kwargs):\n",
    "    if not train:\n",
    "      images = last_output[3]; assert(images.shape[1:] == (3,224, 224))\n",
    "      for perturbations, pred_hist in zip(self.perturbations_list, self.pred_hist_list):\n",
    "        for j, perturbation in enumerate(perturbations):\n",
    "          perturbed_batch = images + perturbation[None]\n",
    "          preds = arch(perturbed_batch).argmax(1)\n",
    "          for pred in preds:\n",
    "            pred_hist[pred] += 1\n",
    "  \n",
    "  def on_epoch_end(self, last_metrics, **kwargs):\n",
    "    for i in range(len(self.pred_hist_list)):\n",
    "      self.pred_hist_list[i] = (self.pred_hist_list[i].float() / self.n_perturbations).tolist()\n",
    "    \n",
    "    div_metric_list = [classes_needed_to_reach(self.percentage, pred_hist)[0] \\\n",
    "                          for pred_hist in self.pred_hist_list]\n",
    "    entropy_list = [entropy(pred_hist) for pred_hist in self.pred_hist_list]\n",
    "    return add_metrics(last_metrics, [np.mean(div_metric_list), np.mean(entropy_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetedDiversityMetric(DiversityMetric):\n",
    "    def __init__(self, n_perturbations, percentage):\n",
    "      super().__init__(n_perturbations, percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoolingWeightScheduler(LearnerCallback):\n",
    "  def __init__(self, learn: Learner):\n",
    "    super().__init__(learn)\n",
    "    self.weights_history = []\n",
    "    self.fooling_loss_history = []\n",
    "  \n",
    "  def get_metric_value(self, metric_name):\n",
    "    for value, name in zip(self.learn.recorder.metrics[-1],self.learn.recorder.names[3:-1]):\n",
    "      if name == metric_name:\n",
    "        return value\n",
    "    raise ValueError('Could not find {} metric.'.format(metric_name))\n",
    "  \n",
    "  def on_epoch_end(self, last_metrics, **kwargs):\n",
    "    # history keeping\n",
    "    self.weights_history.append((kwargs['epoch'], self.learn.loss_func.fooling_weight))\n",
    "    \n",
    "    # the actual functionality\n",
    "    fooling_loss = self.get_metric_value('fool_loss')\n",
    "    self.fooling_loss_history.append(fooling_loss)\n",
    "    \n",
    "    if len(self.weights_history) < 2:\n",
    "      return\n",
    "    \n",
    "    if self.fooling_loss_history[-1] > self.fooling_loss_history[-2]:\n",
    "      self.learn.loss_func.fooling_weight += 0.3    \n",
    "      print('fooling weight increased to {} at the end of epoch {}'.format(\n",
    "        self.learn.loss_func.fooling_weight, kwargs['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tltucTv2ep9-"
   },
   "outputs": [],
   "source": [
    "# mode = 'sanity_check'\n",
    "mode = 'normal'\n",
    "# mode = 'div_metric_calc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.resnet50\n",
    "# model = models.resnet152\n",
    "# model = models.vgg16_bn\n",
    "model = torchvision.models.googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SO1h55obXzOv",
    "outputId": "54414cc5-84d5-4f45-ecab-87374a58dd33"
   },
   "outputs": [],
   "source": [
    "if mode == \"normal\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "elif mode == \"sanity_check\":\n",
    "  env.load_dataset('dataset_sanity_check_small', 'dataset_sanity_check_small')  \n",
    "  env.set_data_path('dataset_sanity_check_small')\n",
    "elif mode == \"div_metric_calc\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "  env.load_test_dataset(str(env.data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "koaQZmjMom7w"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "gpu_flag = True\n",
    "nag_util.batch_size = batch_size; nag_util.gpu_flag = gpu_flag;\n",
    "tfms = get_transforms(do_flip=False, max_rotate=0)\n",
    "data = (ImageList.from_folder(env.data_path)\n",
    "        .split_by_folder(valid=('test' if mode == 'div_metric_calc' else 'valid'))\n",
    "        .label_from_folder()\n",
    "        .transform(tfms, size=224)\n",
    "        .databunch(bs=batch_size, num_workers=1)\n",
    "        .normalize(imagenet_stats))\n",
    "\n",
    "# data.show_batch(rows=2, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDBkRV8yovwV"
   },
   "outputs": [],
   "source": [
    "if gen_arch == \"non-targeted\":\n",
    "  z_dim = 10\n",
    "elif gen_arch == \"targeted\":\n",
    "  z_dim = 1000\n",
    "  \n",
    "arch = SoftmaxWrapper(model(pretrained=True).cuda().eval())\n",
    "nag_util.arch = arch\n",
    "requires_grad(arch, False)\n",
    "\n",
    "# vgg:\n",
    "# layers = []\n",
    "# blocks = [i-1 for i,o in enumerate(children(arch.features)) if isinstance(o, nn.MaxPool2d)]\n",
    "# layers = [arch.features[i] for i in blocks]\n",
    "# layer_weights = [1] * len(layers)\n",
    "\n",
    "layers = [\n",
    "    arch.softmax\n",
    "]\n",
    "\n",
    "layer_weights = [1.] * len(layers)\n",
    "\n",
    "# inception:\n",
    "# layers = [\n",
    "#     arch.Conv2d_1a_3x3,\n",
    "#     arch.Mixed_6e,\n",
    "#     arch.Mixed_7a,\n",
    "#     arch.fc    \n",
    "# ]\n",
    "# layer_weights = [1.0/4.0] * len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qd9gXUy_ovww"
   },
   "outputs": [],
   "source": [
    "feat_loss = FeatureLoss(arch, layers, layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRAnneal(LearnerCallback):\n",
    "  _order = -20 # Needs to run before the recorder\n",
    "  \n",
    "  def __init__(self, learn, final_value):\n",
    "    super().__init__(learn)\n",
    "    self.final_value = final_value\n",
    "  \n",
    "  def on_train_begin(self, **kwargs):\n",
    "    self.initial_value = self.opt.lr\n",
    "    self.learn.recorder.add_metric_names(['lr'])\n",
    "  \n",
    "  def on_epoch_end(self, epoch, n_epochs, last_metrics, **kwargs):\n",
    "    self.opt.lr = annealing_linear(self.initial_value, self.final_value, float(epoch) / n_epochs)\n",
    "    return add_metrics(last_metrics, self.opt.lr)\n",
    "  \n",
    "# class LRMonitor(LearnerCallBack):\n",
    "#   def __init__(self, learn):\n",
    "#     super().__init__(learn)\n",
    "#     self.name = 'lr'\n",
    "    \n",
    "#   def on_epoch_end(self, last_metrics, **kwargs):\n",
    "#     return add_metrics(last_metrics, self.opt.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; import time\n",
    "\n",
    "class FileControl(LearnerCallback):\n",
    "    def __init__(self, learn, root_folder, gen):\n",
    "      super().__init__(learn)\n",
    "      self.root_folder = root_folder\n",
    "      self.gen = gen\n",
    "      \n",
    "    def on_epoch_end(self, epoch, **kwargs):\n",
    "      with open(self.root_folder + '/ctrl.txt', 'r') as control_file:\n",
    "        control_data = json.loads(control_file.read())\n",
    "      \n",
    "      if str(epoch) in control_data:\n",
    "        action = control_data[str(epoch)]\n",
    "        self.epoch = epoch\n",
    "        return self.perform_action(action)\n",
    "        \n",
    "    def perform_action(self, action):\n",
    "      if action == 'stop':\n",
    "        return {'stop_training': True}\n",
    "      elif action == 'ask':\n",
    "        print('prompted to ask for action at epoch {}:'.format(self.epoch))\n",
    "        new_action = input()\n",
    "        self.perform_action(new_action)\n",
    "      elif action == 'ask_file':\n",
    "        return self.ask_from_file()\n",
    "      elif action == 'double_labels':\n",
    "        self.gen.n_active_labels = min(self.gen.n_active_labels * 2, 1000)\n",
    "        print('increased n_active_labels to {} at end of epoch {}'.format(self.gen.n_active_labels ,self.epoch))\n",
    "      elif action == 'continue':\n",
    "        return \n",
    "      else:\n",
    "        print('invalid action: \\\"{}\\\". please enter a valid action:'.format(action))\n",
    "        return self.perform_action(input())\n",
    "    \n",
    "    def ask_from_file(self):\n",
    "      wait_file = open(self.root_folder + '/wait.txt', 'w')\n",
    "      while True:\n",
    "        if not os.path.isfile(self.root_folder + '/answer.txt'):\n",
    "          open(self.root_folder + '/answer.txt', 'x')\n",
    "        answers_file = open(self.root_folder + '/answer.txt', 'r')\n",
    "        action = answers_file.read().strip()\n",
    "        if action in ['stop', 'double_labels', 'continue']:\n",
    "          print('action read: \\\"{}\\\"'.format(action))\n",
    "          wait_file.close()\n",
    "          os.remove(self.root_folder + '/wait.txt')\n",
    "          return self.perform_action(action)\n",
    "        else:\n",
    "          wait_file.truncate(0)\n",
    "          wait_file.write('invalid action \\\"{}\\\"\\n'.format(action))\n",
    "          wait_file.flush()\n",
    "          time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Object:\n",
    "#   pass\n",
    "\n",
    "# g = Object()\n",
    "# g.n_active_labels = 2\n",
    "\n",
    "# f = FileControl(learn, '/root/Derakhshani/adversarial/ctrl', g)\n",
    "\n",
    "# f.on_epoch_end(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.save_filename = 'resnet50_65' #resnet50_64\n",
    "# env.save_filename = 'resnet50_17'\n",
    "env.save_filename = 'googlenet_25_limlablesx'\n",
    "\n",
    "if Path(env.get_csv_path() + '.csv').exists(): raise FileExistsError(\"csv_path already exists\")\n",
    "if Path(env.get_models_path()).exists(): raise FileExistsError(\"models_path already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9J20CBLS8S9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models_directory returned is:  models/532\n"
     ]
    }
   ],
   "source": [
    "learn = None; gen = None; gc.collect()\n",
    "csv_logger = partial(ImmediateCSVLogger, filename= env.temp_csv_path + '/' + env.save_filename)\n",
    "gen = Gen(z_dim=z_dim)\n",
    "init_cnn(gen, True)\n",
    "\n",
    "if gen_arch == 'non-targeted':\n",
    "  metrics = [validation]\n",
    "elif gen_arch == 'targeted':\n",
    "  metrics = [validation, targeted_validation]\n",
    "    \n",
    "learn = Learner(data, gen, loss_func = feat_loss, model_dir = env.get_learner_models_dir(),\n",
    "                metrics=metrics, callback_fns=[DiversityMetric, LossMetrics, csv_logger])\n",
    "\n",
    "# learn = Learner(data, Gen(z_dim=10), loss_func = feat_loss, metrics=[validation], callback_fns=LossMetrics, opt_func = optim.SGD)\n",
    "# learn = Learner(data, Gen(z_dim=z_dim), loss_func = feat_loss, metrics=[validation], callback_fns=[LossMetrics, DiversityWeightsScheduler])\n",
    "\n",
    "# load_starting_point(learn, model.__name__, z_dim)\n",
    "# random_seed(42, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wOZYzOHDEdB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type FeatureLoss. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Gen. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type deconv_layer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (9000 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "n02454379,n02454379,n02454379,n02454379,n02454379\n",
       "Path: /root/Derakhshani/adversarial/datasets/dataset;\n",
       "\n",
       "Valid: LabelList (1000 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "n02454379,n02397096,n02090379,n01729977,n02268853\n",
       "Path: /root/Derakhshani/adversarial/datasets/dataset;\n",
       "\n",
       "Test: None, model=Gen(\n",
       "  (z_): Linear(in_features=1000, out_features=10240, bias=True)\n",
       "  (BN_): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (CT2d_1): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(640, 448, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (BN2d): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_2): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(448, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_3): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_4): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_5): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_6): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (CT2d_7): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(64, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (BN2d): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FeatureLoss(\n",
       "  (dis): SoftmaxWrapper(\n",
       "    (m): GoogLeNet(\n",
       "      (conv1): BasicConv2d(\n",
       "        (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      (conv2): BasicConv2d(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv3): BasicConv2d(\n",
       "        (conv): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      (inception3a): Inception(\n",
       "        (branch1): BasicConv2d(\n",
       "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch3): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch4): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (inception3b): Inception(\n",
       "        (branch1): BasicConv2d(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch3): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch4): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (maxpool3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      (inception4a): Inception(\n",
       "        (branch1): BasicConv2d(\n",
       "          (conv): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(208, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch3): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch4): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (inception4b): Inception(\n",
       "        (branch1): BasicConv2d(\n",
       "          (conv): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch3): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch4): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (inception4c): Inception(\n",
       "        (branch1): BasicConv2d(\n",
       "          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch3): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch4): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (inception4d): Inception(\n",
       "        (branch1): BasicConv2d(\n",
       "          (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch3): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch4): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (inception4e): Inception(\n",
       "        (branch1): BasicConv2d(\n",
       "          (conv): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch3): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch4): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      (inception5a): Inception(\n",
       "        (branch1): BasicConv2d(\n",
       "          (conv): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch3): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch4): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (inception5b): Inception(\n",
       "        (branch1): BasicConv2d(\n",
       "          (conv): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch3): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch4): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (dropout): Dropout(p=0.2)\n",
       "      (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax()\n",
       "  )\n",
       "), metrics=[<function validation at 0x7f0034a550d0>, <function targeted_validation at 0x7f0034068840>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/Derakhshani/adversarial/datasets/dataset'), model_dir='models/532', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), <class '__main__.DiversityMetric'>, <class 'fastai.callbacks.loss_metrics.LossMetrics'>, functools.partial(<class 'nag_util.ImmediateCSVLogger'>, filename='/root/Derakhshani/adversarial/temp/googlenet_25_limlablesx')], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): Linear(in_features=1000, out_features=10240, bias=True)\n",
       "  (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ConvTranspose2d(640, 448, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): ConvTranspose2d(448, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (8): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (12): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ConvTranspose2d(64, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (15): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")], add_time=True, silent=None)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !cp \"/content/gdrive/My Drive/DL/models/vgg16_12-last.pth\"  \"/content/\"\n",
    "# learn.load('/content/vgg16_12-last')\n",
    "\n",
    "# load_filename = 'googlenet_13_attempt5/googlenet_13_attempt5_29'\n",
    "# load_filename = 'investigate_googlenet_3/0/googlenet_1'\n",
    "# load_filename = 'vgg16_30/vgg16_30_69'\n",
    "load_filename = 'googlenet_25_limlables/googlenet_25_limlables_209'\n",
    "# load_filename = None\n",
    "\n",
    "learn.load('/root/Derakhshani/adversarial/models/' + load_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find(1e-6, 100)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dir = 'investigate_googlenet_3'\n",
    "# investigate_initial_settings(4, 2, lr = 1e-2, wd = 0.0, results_dir = results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree(env.get_models_dir() + results_dir)\n",
    "# shutil.rmtree(env.get_csv_dir() + results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WeightTuner(LearnerCallback):\n",
    "#   def __init__(self, learn: Learner):\n",
    "#     super().__init__(learn)\n",
    "#     self.fooling_weight = learn.loss_func.fooling_weight\n",
    "    \n",
    "#   def on_epoch_begin(self, **kwargs):\n",
    "#     fooling_rate = \n",
    "#     print(\"by how much to increase the fooling_weight? (current value: {})\".format(fooling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicalLRScheduler(LearnerCallback):\n",
    "  def __init__(self, learn, max_lr, min_lr, cycle_len):\n",
    "    super().__init__(learn)\n",
    "    self.max_lr = max_lr\n",
    "    self.min_lr = min_lr\n",
    "    self.cycle_len = cycle_len\n",
    "    \n",
    "  def on_train_begin(self, **kwargs):\n",
    "    self.n_iter_per_epoch = len(self.learn.data.train_dl)\n",
    "    self.cycle_len_iters = self.cycle_len * self.n_iter_per_epoch\n",
    "    self.learn.opt.lr = self.min_lr\n",
    "    \n",
    "    \n",
    "  def on_batch_end(self, iteration, train, **kwargs):\n",
    "    if train:\n",
    "      cycle_index = iteration % self.cycle_len_iters\n",
    "      half_cycle_len = self.cycle_len_iters / 2\n",
    "\n",
    "      if cycle_index < half_cycle_len:\n",
    "        new_lr = float(self.max_lr - self.min_lr) / half_cycle_len * cycle_index + self.min_lr\n",
    "      else:\n",
    "        new_lr = float(self.min_lr - self.max_lr) / half_cycle_len * (cycle_index - half_cycle_len) + self.max_lr\n",
    "\n",
    "#       print('iter: {}, lr: {}'.format(iteration, new_lr))\n",
    "      self.opt.lr = new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the selected settings are : \n",
      "\tmode: normal \n",
      "\tnetw-under-attack: googlenet \n",
      "\tload filename: googlenet_25_limlables/googlenet_25_limlables_209 \n",
      "      \tsave filename: googlenet_25_limlablesx\n",
      "\tmetric names: ['fool_loss']\n",
      "\tgen arch: targeted\n",
      "\n",
      "please MAKE SURE that the config is correct.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  load_filename\n",
    "except NameError:\n",
    "  load_filename = None\n",
    "\n",
    "print(\"the selected settings are : \")\n",
    "print('''\\tmode: {} \\n\\tnetw-under-attack: {} \\n\\tload filename: {} \n",
    "      \\tsave filename: {}\\n\\tmetric names: {}\\n\\tgen arch: {}\\n'''.format(\n",
    "      mode, model.__name__, load_filename , env.save_filename, feat_loss.metric_names,\n",
    "      gen_arch\n",
    "))\n",
    "print(\"please MAKE SURE that the config is correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find(1e-6, 100)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "percent_total: \n",
      "[(293, 99.9999999988889, 9.0), (340, 89.9999999991, 10.0), (109, 87.49999999890625, 8.0), (640, 83.33333333263889, 12.0), (479, 79.9999999984, 5.0), (387, 77.77777777691358, 9.0), (953, 77.77777777691358, 9.0), (290, 76.92307692248521, 13.0), (476, 74.999999999375, 12.0), (288, 74.9999999990625, 8.0), (396, 74.9999999990625, 8.0), (8, 72.72727272661157, 11.0), (57, 72.72727272661157, 11.0), (67, 72.72727272661157, 11.0), (82, 72.72727272661157, 11.0), (123, 72.72727272661157, 11.0), (276, 72.72727272661157, 11.0), (955, 72.72727272661157, 11.0), (997, 72.72727272661157, 11.0), (509, 71.42857142755102, 7.0), (580, 70.58823529370243, 17.0), (84, 69.9999999993, 10.0), (91, 69.9999999993, 10.0), (99, 69.9999999993, 10.0), (287, 69.9999999993, 10.0), (454, 69.9999999993, 10.0), (611, 69.9999999993, 10.0), (646, 69.9999999993, 10.0), (671, 69.9999999993, 10.0), (289, 69.23076923023669, 13.0), (56, 66.66666666611111, 12.0), (300, 66.66666666592593, 9.0), (444, 66.66666666592593, 9.0), (298, 66.66666666555555, 6.0), (37, 63.63636363578512, 11.0), (330, 63.63636363578512, 11.0), (334, 63.63636363578512, 11.0), (7, 62.49999999921875, 8.0), (39, 62.49999999921875, 8.0), (129, 62.49999999921875, 8.0), (131, 62.49999999921875, 8.0), (331, 62.49999999921875, 8.0), (138, 61.53846153798816, 13.0), (415, 61.53846153798816, 13.0), (612, 61.53846153798816, 13.0), (721, 61.53846153798816, 13.0), (182, 59.9999999996, 15.0), (865, 59.9999999996, 15.0), (28, 59.9999999994, 10.0), (116, 59.9999999994, 10.0), (174, 59.9999999994, 10.0), (243, 59.9999999994, 10.0), (322, 59.9999999994, 10.0), (327, 59.9999999994, 10.0), (328, 59.9999999994, 10.0), (364, 59.9999999994, 10.0), (393, 59.9999999994, 10.0), (394, 59.9999999994, 10.0), (918, 59.9999999994, 10.0), (443, 59.9999999988, 5.0), (696, 59.9999999988, 5.0), (281, 58.333333332847225, 12.0), (292, 58.333333332847225, 12.0), (490, 58.333333332847225, 12.0), (645, 58.333333332847225, 12.0), (825, 58.333333332847225, 12.0), (917, 58.333333332847225, 12.0), (992, 58.333333332847225, 12.0), (48, 57.14285714244898, 14.0), (652, 57.14285714244898, 14.0), (104, 57.14285714204082, 7.0), (120, 57.14285714204082, 7.0), (581, 57.14285714204082, 7.0), (735, 57.14285714204082, 7.0), (878, 57.14285714204082, 7.0), (269, 56.249999999648445, 16.0), (41, 55.55555555493827, 9.0), (247, 55.55555555493827, 9.0), (360, 55.55555555493827, 9.0), (382, 55.55555555493827, 9.0), (383, 55.55555555493827, 9.0), (398, 55.55555555493827, 9.0), (506, 55.55555555493827, 9.0), (665, 55.55555555493827, 9.0), (741, 55.55555555493827, 9.0), (879, 55.55555555493827, 9.0), (973, 55.55555555493827, 9.0), (15, 54.54545454495868, 11.0), (45, 54.54545454495868, 11.0), (81, 54.54545454495868, 11.0), (119, 54.54545454495868, 11.0), (177, 54.54545454495868, 11.0), (275, 54.54545454495868, 11.0), (342, 54.54545454495868, 11.0), (406, 54.54545454495868, 11.0), (603, 54.54545454495868, 11.0), (946, 54.54545454495868, 11.0), (355, 53.84615384573964, 13.0), (410, 53.84615384573964, 13.0), (547, 53.33333333297778, 15.0), (489, 49.9999999996875, 16.0), (25, 49.99999999958333, 12.0), (575, 49.99999999958333, 12.0), (586, 49.99999999958333, 12.0), (788, 49.99999999958333, 12.0), (847, 49.99999999958333, 12.0), (857, 49.99999999958333, 12.0), (949, 49.99999999958333, 12.0), (956, 49.99999999958333, 12.0), (963, 49.99999999958333, 12.0), (61, 49.9999999995, 10.0), (65, 49.9999999995, 10.0), (110, 49.9999999995, 10.0), (113, 49.9999999995, 10.0), (192, 49.9999999995, 10.0), (260, 49.9999999995, 10.0), (294, 49.9999999995, 10.0), (348, 49.9999999995, 10.0), (349, 49.9999999995, 10.0), (401, 49.9999999995, 10.0), (483, 49.9999999995, 10.0), (565, 49.9999999995, 10.0), (707, 49.9999999995, 10.0), (717, 49.9999999995, 10.0), (730, 49.9999999995, 10.0), (835, 49.9999999995, 10.0), (866, 49.9999999995, 10.0), (880, 49.9999999995, 10.0), (944, 49.9999999995, 10.0), (947, 49.9999999995, 10.0), (53, 49.999999999375, 8.0), (97, 49.999999999375, 8.0), (118, 49.999999999375, 8.0), (163, 49.999999999375, 8.0), (291, 49.999999999375, 8.0), (329, 49.999999999375, 8.0), (381, 49.999999999375, 8.0), (694, 49.999999999375, 8.0), (762, 49.999999999375, 8.0), (62, 49.99999999916667, 6.0), (539, 49.99999999916667, 6.0), (787, 49.99999999916667, 6.0), (282, 49.9999999975, 2.0), (218, 46.66666666635555, 15.0), (621, 46.15384615349112, 13.0), (800, 46.15384615349112, 13.0), (198, 45.45454545413223, 11.0), (363, 45.45454545413223, 11.0), (376, 45.45454545413223, 11.0), (407, 45.45454545413223, 11.0), (582, 45.45454545413223, 11.0), (791, 45.45454545413223, 11.0), (858, 45.45454545413223, 11.0), (922, 45.45454545413223, 11.0), (33, 44.444444443950616, 9.0), (66, 44.444444443950616, 9.0), (85, 44.444444443950616, 9.0), (133, 44.444444443950616, 9.0), (156, 44.444444443950616, 9.0), (173, 44.444444443950616, 9.0), (214, 44.444444443950616, 9.0), (240, 44.444444443950616, 9.0), (264, 44.444444443950616, 9.0), (278, 44.444444443950616, 9.0), (412, 44.444444443950616, 9.0), (467, 44.444444443950616, 9.0), (808, 44.444444443950616, 9.0), (853, 44.444444443950616, 9.0), (936, 44.444444443950616, 9.0), (937, 44.444444443950616, 9.0), (54, 42.857142856530615, 7.0), (76, 42.857142856530615, 7.0), (353, 42.857142856530615, 7.0), (588, 42.857142856530615, 7.0), (820, 42.857142856530615, 7.0), (839, 42.857142856530615, 7.0), (886, 42.857142856530615, 7.0), (904, 42.857142856530615, 7.0), (24, 41.66666666631944, 12.0), (125, 41.66666666631944, 12.0), (128, 41.66666666631944, 12.0), (193, 41.66666666631944, 12.0), (211, 41.66666666631944, 12.0), (538, 41.66666666631944, 12.0), (765, 41.66666666631944, 12.0), (0, 39.9999999996, 10.0), (30, 39.9999999996, 10.0), (52, 39.9999999996, 10.0), (79, 39.9999999996, 10.0), (88, 39.9999999996, 10.0), (90, 39.9999999996, 10.0), (102, 39.9999999996, 10.0), (108, 39.9999999996, 10.0), (124, 39.9999999996, 10.0), (137, 39.9999999996, 10.0), (191, 39.9999999996, 10.0), (305, 39.9999999996, 10.0), (321, 39.9999999996, 10.0), (336, 39.9999999996, 10.0), (430, 39.9999999996, 10.0), (779, 39.9999999996, 10.0), (781, 39.9999999996, 10.0), (783, 39.9999999996, 10.0), (806, 39.9999999996, 10.0), (959, 39.9999999996, 10.0), (461, 39.9999999992, 5.0), (217, 38.461538461242604, 13.0), (458, 38.461538461242604, 13.0), (468, 38.461538461242604, 13.0), (805, 38.461538461242604, 13.0), (38, 37.49999999953125, 8.0), (127, 37.49999999953125, 8.0), (170, 37.49999999953125, 8.0), (189, 37.49999999953125, 8.0), (226, 37.49999999953125, 8.0), (271, 37.49999999953125, 8.0), (302, 37.49999999953125, 8.0), (362, 37.49999999953125, 8.0), (555, 37.49999999953125, 8.0), (614, 37.49999999953125, 8.0), (637, 37.49999999953125, 8.0), (932, 37.49999999953125, 8.0), (958, 37.49999999953125, 8.0), (987, 37.49999999953125, 8.0), (87, 36.36363636330579, 11.0), (96, 36.36363636330579, 11.0), (98, 36.36363636330579, 11.0), (254, 36.36363636330579, 11.0), (372, 36.36363636330579, 11.0), (429, 36.36363636330579, 11.0), (495, 36.36363636330579, 11.0), (634, 36.36363636330579, 11.0), (658, 36.36363636330579, 11.0), (670, 36.36363636330579, 11.0), (692, 36.36363636330579, 11.0), (822, 36.36363636330579, 11.0), (993, 36.36363636330579, 11.0), (474, 35.71428571403061, 14.0), (706, 35.71428571403061, 14.0), (533, 33.33333333311111, 15.0), (568, 33.33333333311111, 15.0), (221, 33.333333333055556, 12.0), (235, 33.333333333055556, 12.0), (388, 33.333333333055556, 12.0), (436, 33.333333333055556, 12.0), (448, 33.333333333055556, 12.0), (624, 33.333333333055556, 12.0), (687, 33.333333333055556, 12.0), (737, 33.333333333055556, 12.0), (763, 33.333333333055556, 12.0), (71, 33.333333332962965, 9.0), (100, 33.333333332962965, 9.0), (126, 33.333333332962965, 9.0), (155, 33.333333332962965, 9.0), (160, 33.333333332962965, 9.0), (172, 33.333333332962965, 9.0), (251, 33.333333332962965, 9.0), (272, 33.333333332962965, 9.0), (277, 33.333333332962965, 9.0), (284, 33.333333332962965, 9.0), (285, 33.333333332962965, 9.0), (299, 33.333333332962965, 9.0), (319, 33.333333332962965, 9.0), (371, 33.333333332962965, 9.0), (384, 33.333333332962965, 9.0), (389, 33.333333332962965, 9.0), (417, 33.333333332962965, 9.0), (471, 33.333333332962965, 9.0), (723, 33.333333332962965, 9.0), (746, 33.333333332962965, 9.0), (768, 33.333333332962965, 9.0), (848, 33.333333332962965, 9.0), (863, 33.333333332962965, 9.0), (884, 33.333333332962965, 9.0), (934, 33.333333332962965, 9.0), (995, 33.333333332962965, 9.0), (185, 33.33333333277778, 6.0), (885, 33.333333332222224, 3.0), (77, 31.24999999980469, 16.0), (44, 30.76923076899408, 13.0), (236, 30.76923076899408, 13.0), (47, 29.9999999997, 10.0), (105, 29.9999999997, 10.0), (121, 29.9999999997, 10.0), (134, 29.9999999997, 10.0), (136, 29.9999999997, 10.0), (140, 29.9999999997, 10.0), (213, 29.9999999997, 10.0), (280, 29.9999999997, 10.0), (316, 29.9999999997, 10.0), (343, 29.9999999997, 10.0), (352, 29.9999999997, 10.0), (390, 29.9999999997, 10.0), (397, 29.9999999997, 10.0), (428, 29.9999999997, 10.0), (496, 29.9999999997, 10.0), (502, 29.9999999997, 10.0), (535, 29.9999999997, 10.0), (561, 29.9999999997, 10.0), (594, 29.9999999997, 10.0), (616, 29.9999999997, 10.0), (703, 29.9999999997, 10.0), (828, 29.9999999997, 10.0), (864, 29.9999999997, 10.0), (873, 29.9999999997, 10.0), (889, 29.9999999997, 10.0), (984, 29.9999999997, 10.0), (202, 28.57142857122449, 14.0), (216, 28.57142857122449, 14.0), (562, 28.57142857122449, 14.0), (649, 28.57142857122449, 14.0), (939, 28.57142857122449, 14.0), (962, 28.57142857122449, 14.0), (144, 28.57142857102041, 7.0), (184, 28.57142857102041, 7.0), (212, 28.57142857102041, 7.0), (231, 28.57142857102041, 7.0), (295, 28.57142857102041, 7.0), (303, 28.57142857102041, 7.0), (309, 28.57142857102041, 7.0), (427, 28.57142857102041, 7.0), (599, 28.57142857102041, 7.0), (792, 28.57142857102041, 7.0), (911, 28.57142857102041, 7.0), (50, 27.27272727247934, 11.0), (51, 27.27272727247934, 11.0), (69, 27.27272727247934, 11.0), (83, 27.27272727247934, 11.0), (135, 27.27272727247934, 11.0), (187, 27.27272727247934, 11.0), (197, 27.27272727247934, 11.0), (242, 27.27272727247934, 11.0), (301, 27.27272727247934, 11.0), (365, 27.27272727247934, 11.0), (369, 27.27272727247934, 11.0), (395, 27.27272727247934, 11.0), (477, 27.27272727247934, 11.0), (520, 27.27272727247934, 11.0), (522, 27.27272727247934, 11.0), (544, 27.27272727247934, 11.0), (577, 27.27272727247934, 11.0), (642, 27.27272727247934, 11.0), (690, 27.27272727247934, 11.0), (716, 27.27272727247934, 11.0), (748, 27.27272727247934, 11.0), (784, 27.27272727247934, 11.0), (795, 27.27272727247934, 11.0), (890, 27.27272727247934, 11.0), (938, 27.27272727247934, 11.0), (991, 27.27272727247934, 11.0), (210, 24.999999999791665, 12.0), (233, 24.999999999791665, 12.0), (314, 24.999999999791665, 12.0), (350, 24.999999999791665, 12.0), (351, 24.999999999791665, 12.0), (361, 24.999999999791665, 12.0), (563, 24.999999999791665, 12.0), (609, 24.999999999791665, 12.0), (675, 24.999999999791665, 12.0), (761, 24.999999999791665, 12.0), (793, 24.999999999791665, 12.0), (821, 24.999999999791665, 12.0), (849, 24.999999999791665, 12.0), (870, 24.999999999791665, 12.0), (35, 24.9999999996875, 8.0), (43, 24.9999999996875, 8.0), (68, 24.9999999996875, 8.0), (122, 24.9999999996875, 8.0), (139, 24.9999999996875, 8.0), (219, 24.9999999996875, 8.0), (230, 24.9999999996875, 8.0), (248, 24.9999999996875, 8.0), (250, 24.9999999996875, 8.0), (268, 24.9999999996875, 8.0), (273, 24.9999999996875, 8.0), (311, 24.9999999996875, 8.0), (370, 24.9999999996875, 8.0), (464, 24.9999999996875, 8.0), (597, 24.9999999996875, 8.0), (620, 24.9999999996875, 8.0), (647, 24.9999999996875, 8.0), (688, 24.9999999996875, 8.0), (727, 24.9999999996875, 8.0), (794, 24.9999999996875, 8.0), (907, 24.9999999996875, 8.0), (914, 24.9999999996875, 8.0), (924, 24.9999999996875, 8.0), (951, 24.9999999996875, 8.0), (711, 24.999999999375, 4.0), (201, 23.07692307674556, 13.0), (472, 23.07692307674556, 13.0), (498, 23.07692307674556, 13.0), (532, 23.07692307674556, 13.0), (777, 23.07692307674556, 13.0), (829, 23.07692307674556, 13.0), (926, 23.07692307674556, 13.0), (933, 23.07692307674556, 13.0), (14, 22.222222221975308, 9.0), (17, 22.222222221975308, 9.0), (34, 22.222222221975308, 9.0), (49, 22.222222221975308, 9.0), (86, 22.222222221975308, 9.0), (132, 22.222222221975308, 9.0), (145, 22.222222221975308, 9.0), (165, 22.222222221975308, 9.0), (171, 22.222222221975308, 9.0), (188, 22.222222221975308, 9.0), (199, 22.222222221975308, 9.0), (206, 22.222222221975308, 9.0), (222, 22.222222221975308, 9.0), (246, 22.222222221975308, 9.0), (255, 22.222222221975308, 9.0), (279, 22.222222221975308, 9.0), (308, 22.222222221975308, 9.0), (338, 22.222222221975308, 9.0), (374, 22.222222221975308, 9.0), (386, 22.222222221975308, 9.0), (392, 22.222222221975308, 9.0), (439, 22.222222221975308, 9.0), (453, 22.222222221975308, 9.0), (455, 22.222222221975308, 9.0), (528, 22.222222221975308, 9.0), (546, 22.222222221975308, 9.0), (560, 22.222222221975308, 9.0), (608, 22.222222221975308, 9.0), (677, 22.222222221975308, 9.0), (850, 22.222222221975308, 9.0), (919, 22.222222221975308, 9.0), (945, 22.222222221975308, 9.0), (981, 22.222222221975308, 9.0), (519, 21.42857142841837, 14.0), (263, 19.999999999866667, 15.0), (572, 19.999999999866667, 15.0), (9, 19.9999999998, 10.0), (58, 19.9999999998, 10.0), (93, 19.9999999998, 10.0), (161, 19.9999999998, 10.0), (180, 19.9999999998, 10.0), (227, 19.9999999998, 10.0), (229, 19.9999999998, 10.0), (323, 19.9999999998, 10.0), (366, 19.9999999998, 10.0), (433, 19.9999999998, 10.0), (466, 19.9999999998, 10.0), (475, 19.9999999998, 10.0), (488, 19.9999999998, 10.0), (491, 19.9999999998, 10.0), (500, 19.9999999998, 10.0), (521, 19.9999999998, 10.0), (574, 19.9999999998, 10.0), (595, 19.9999999998, 10.0), (625, 19.9999999998, 10.0), (628, 19.9999999998, 10.0), (641, 19.9999999998, 10.0), (643, 19.9999999998, 10.0), (682, 19.9999999998, 10.0), (685, 19.9999999998, 10.0), (695, 19.9999999998, 10.0), (715, 19.9999999998, 10.0), (738, 19.9999999998, 10.0), (743, 19.9999999998, 10.0), (776, 19.9999999998, 10.0), (819, 19.9999999998, 10.0), (826, 19.9999999998, 10.0), (856, 19.9999999998, 10.0), (874, 19.9999999998, 10.0), (887, 19.9999999998, 10.0), (916, 19.9999999998, 10.0), (950, 19.9999999998, 10.0), (954, 19.9999999998, 10.0), (733, 19.9999999996, 5.0), (882, 19.9999999996, 5.0), (906, 19.9999999996, 5.0), (912, 19.9999999996, 5.0), (20, 18.181818181652893, 11.0), (150, 18.181818181652893, 11.0), (194, 18.181818181652893, 11.0), (228, 18.181818181652893, 11.0), (245, 18.181818181652893, 11.0), (335, 18.181818181652893, 11.0), (337, 18.181818181652893, 11.0), (413, 18.181818181652893, 11.0), (508, 18.181818181652893, 11.0), (510, 18.181818181652893, 11.0), (559, 18.181818181652893, 11.0), (571, 18.181818181652893, 11.0), (576, 18.181818181652893, 11.0), (607, 18.181818181652893, 11.0), (661, 18.181818181652893, 11.0), (684, 18.181818181652893, 11.0), (698, 18.181818181652893, 11.0), (729, 18.181818181652893, 11.0), (740, 18.181818181652893, 11.0), (754, 18.181818181652893, 11.0), (766, 18.181818181652893, 11.0), (796, 18.181818181652893, 11.0), (888, 18.181818181652893, 11.0), (929, 18.181818181652893, 11.0), (988, 18.181818181652893, 11.0), (982, 16.666666666597223, 24.0), (27, 16.666666666527778, 12.0), (114, 16.666666666527778, 12.0), (186, 16.666666666527778, 12.0), (232, 16.666666666527778, 12.0), (244, 16.666666666527778, 12.0), (249, 16.666666666527778, 12.0), (258, 16.666666666527778, 12.0), (283, 16.666666666527778, 12.0), (297, 16.666666666527778, 12.0), (347, 16.666666666527778, 12.0), (409, 16.666666666527778, 12.0), (492, 16.666666666527778, 12.0), (511, 16.666666666527778, 12.0), (566, 16.666666666527778, 12.0), (590, 16.666666666527778, 12.0), (790, 16.666666666527778, 12.0), (802, 16.666666666527778, 12.0), (803, 16.666666666527778, 12.0), (167, 16.66666666638889, 6.0), (176, 16.66666666638889, 6.0), (266, 16.66666666638889, 6.0), (345, 16.66666666638889, 6.0), (497, 16.66666666638889, 6.0), (663, 16.66666666638889, 6.0), (686, 16.66666666638889, 6.0), (705, 16.66666666638889, 6.0), (734, 16.66666666638889, 6.0), (898, 16.66666666638889, 6.0), (157, 15.38461538449704, 13.0), (274, 15.38461538449704, 13.0), (339, 15.38461538449704, 13.0), (375, 15.38461538449704, 13.0), (379, 15.38461538449704, 13.0), (667, 15.38461538449704, 13.0), (679, 15.38461538449704, 13.0), (755, 15.38461538449704, 13.0), (756, 15.38461538449704, 13.0), (905, 15.38461538449704, 13.0), (990, 15.38461538449704, 13.0), (209, 14.285714285612245, 14.0), (411, 14.285714285612245, 14.0), (431, 14.285714285612245, 14.0), (524, 14.285714285612245, 14.0), (573, 14.285714285612245, 14.0), (830, 14.285714285612245, 14.0), (843, 14.285714285612245, 14.0), (952, 14.285714285612245, 14.0), (196, 14.285714285510204, 7.0), (200, 14.285714285510204, 7.0), (253, 14.285714285510204, 7.0), (356, 14.285714285510204, 7.0), (358, 14.285714285510204, 7.0), (445, 14.285714285510204, 7.0), (513, 14.285714285510204, 7.0), (514, 14.285714285510204, 7.0), (545, 14.285714285510204, 7.0), (551, 14.285714285510204, 7.0), (587, 14.285714285510204, 7.0), (600, 14.285714285510204, 7.0), (772, 14.285714285510204, 7.0), (798, 14.285714285510204, 7.0), (893, 14.285714285510204, 7.0), (909, 14.285714285510204, 7.0), (910, 14.285714285510204, 7.0), (969, 14.285714285510204, 7.0), (480, 13.333333333244445, 15.0), (648, 13.333333333244445, 15.0), (18, 12.49999999984375, 8.0), (26, 12.49999999984375, 8.0), (64, 12.49999999984375, 8.0), (106, 12.49999999984375, 8.0), (164, 12.49999999984375, 8.0), (175, 12.49999999984375, 8.0), (179, 12.49999999984375, 8.0), (224, 12.49999999984375, 8.0), (234, 12.49999999984375, 8.0), (259, 12.49999999984375, 8.0), (318, 12.49999999984375, 8.0), (341, 12.49999999984375, 8.0), (368, 12.49999999984375, 8.0), (419, 12.49999999984375, 8.0), (441, 12.49999999984375, 8.0), (449, 12.49999999984375, 8.0), (469, 12.49999999984375, 8.0), (627, 12.49999999984375, 8.0), (656, 12.49999999984375, 8.0), (699, 12.49999999984375, 8.0), (704, 12.49999999984375, 8.0), (753, 12.49999999984375, 8.0), (757, 12.49999999984375, 8.0), (891, 12.49999999984375, 8.0), (901, 12.49999999984375, 8.0), (943, 12.49999999984375, 8.0), (994, 12.49999999984375, 8.0), (996, 12.49999999984375, 8.0), (1, 11.111111110987654, 9.0), (13, 11.111111110987654, 9.0), (32, 11.111111110987654, 9.0), (36, 11.111111110987654, 9.0), (78, 11.111111110987654, 9.0), (95, 11.111111110987654, 9.0), (107, 11.111111110987654, 9.0), (115, 11.111111110987654, 9.0), (181, 11.111111110987654, 9.0), (204, 11.111111110987654, 9.0), (207, 11.111111110987654, 9.0), (220, 11.111111110987654, 9.0), (261, 11.111111110987654, 9.0), (270, 11.111111110987654, 9.0), (385, 11.111111110987654, 9.0), (402, 11.111111110987654, 9.0), (418, 11.111111110987654, 9.0), (420, 11.111111110987654, 9.0), (447, 11.111111110987654, 9.0), (451, 11.111111110987654, 9.0), (452, 11.111111110987654, 9.0), (462, 11.111111110987654, 9.0), (470, 11.111111110987654, 9.0), (487, 11.111111110987654, 9.0), (507, 11.111111110987654, 9.0), (518, 11.111111110987654, 9.0), (541, 11.111111110987654, 9.0), (584, 11.111111110987654, 9.0), (604, 11.111111110987654, 9.0), (632, 11.111111110987654, 9.0), (639, 11.111111110987654, 9.0), (666, 11.111111110987654, 9.0), (668, 11.111111110987654, 9.0), (719, 11.111111110987654, 9.0), (751, 11.111111110987654, 9.0), (810, 11.111111110987654, 9.0), (816, 11.111111110987654, 9.0), (827, 11.111111110987654, 9.0), (941, 11.111111110987654, 9.0), (6, 9.9999999999, 10.0), (11, 9.9999999999, 10.0), (22, 9.9999999999, 10.0), (63, 9.9999999999, 10.0), (75, 9.9999999999, 10.0), (89, 9.9999999999, 10.0), (178, 9.9999999999, 10.0), (195, 9.9999999999, 10.0), (239, 9.9999999999, 10.0), (252, 9.9999999999, 10.0), (262, 9.9999999999, 10.0), (265, 9.9999999999, 10.0), (286, 9.9999999999, 10.0), (313, 9.9999999999, 10.0), (391, 9.9999999999, 10.0), (408, 9.9999999999, 10.0), (423, 9.9999999999, 10.0), (456, 9.9999999999, 10.0), (530, 9.9999999999, 10.0), (569, 9.9999999999, 10.0), (579, 9.9999999999, 10.0), (606, 9.9999999999, 10.0), (664, 9.9999999999, 10.0), (683, 9.9999999999, 10.0), (728, 9.9999999999, 10.0), (732, 9.9999999999, 10.0), (736, 9.9999999999, 10.0), (739, 9.9999999999, 10.0), (752, 9.9999999999, 10.0), (759, 9.9999999999, 10.0), (760, 9.9999999999, 10.0), (767, 9.9999999999, 10.0), (797, 9.9999999999, 10.0), (801, 9.9999999999, 10.0), (813, 9.9999999999, 10.0), (814, 9.9999999999, 10.0), (867, 9.9999999999, 10.0), (872, 9.9999999999, 10.0), (915, 9.9999999999, 10.0), (920, 9.9999999999, 10.0), (927, 9.9999999999, 10.0), (957, 9.9999999999, 10.0), (971, 9.9999999999, 10.0), (526, 9.523809523764173, 21.0), (10, 9.090909090826447, 11.0), (16, 9.090909090826447, 11.0), (29, 9.090909090826447, 11.0), (42, 9.090909090826447, 11.0), (205, 9.090909090826447, 11.0), (225, 9.090909090826447, 11.0), (257, 9.090909090826447, 11.0), (310, 9.090909090826447, 11.0), (324, 9.090909090826447, 11.0), (344, 9.090909090826447, 11.0), (377, 9.090909090826447, 11.0), (516, 9.090909090826447, 11.0), (630, 9.090909090826447, 11.0), (672, 9.090909090826447, 11.0), (799, 9.090909090826447, 11.0), (817, 9.090909090826447, 11.0), (834, 9.090909090826447, 11.0), (840, 9.090909090826447, 11.0), (842, 9.090909090826447, 11.0), (852, 9.090909090826447, 11.0), (855, 9.090909090826447, 11.0), (860, 9.090909090826447, 11.0), (862, 9.090909090826447, 11.0), (868, 9.090909090826447, 11.0), (881, 9.090909090826447, 11.0), (935, 9.090909090826447, 11.0), (964, 9.090909090826447, 11.0), (966, 9.090909090826447, 11.0), (967, 9.090909090826447, 11.0), (979, 9.090909090826447, 11.0), (2, 8.333333333263889, 12.0), (55, 8.333333333263889, 12.0), (151, 8.333333333263889, 12.0), (315, 8.333333333263889, 12.0), (416, 8.333333333263889, 12.0), (425, 8.333333333263889, 12.0), (432, 8.333333333263889, 12.0), (440, 8.333333333263889, 12.0), (481, 8.333333333263889, 12.0), (523, 8.333333333263889, 12.0), (525, 8.333333333263889, 12.0), (548, 8.333333333263889, 12.0), (593, 8.333333333263889, 12.0), (654, 8.333333333263889, 12.0), (710, 8.333333333263889, 12.0), (714, 8.333333333263889, 12.0), (785, 8.333333333263889, 12.0), (786, 8.333333333263889, 12.0), (789, 8.333333333263889, 12.0), (833, 8.333333333263889, 12.0), (921, 8.333333333263889, 12.0), (931, 8.333333333263889, 12.0), (998, 8.333333333263889, 12.0), (154, 7.69230769224852, 13.0), (159, 7.69230769224852, 13.0), (162, 7.69230769224852, 13.0), (183, 7.69230769224852, 13.0), (237, 7.69230769224852, 13.0), (238, 7.69230769224852, 13.0), (304, 7.69230769224852, 13.0), (312, 7.69230769224852, 13.0), (359, 7.69230769224852, 13.0), (404, 7.69230769224852, 13.0), (442, 7.69230769224852, 13.0), (505, 7.69230769224852, 13.0), (543, 7.69230769224852, 13.0), (591, 7.69230769224852, 13.0), (968, 7.69230769224852, 13.0), (450, 7.142857142806123, 14.0), (708, 7.142857142806123, 14.0), (644, 6.6666666666222225, 15.0), (769, 6.6666666666222225, 15.0), (831, 6.6666666666222225, 15.0), (877, 6.6666666666222225, 15.0), (424, 6.249999999960938, 16.0), (484, 6.249999999960938, 16.0), (564, 6.249999999960938, 16.0), (655, 6.249999999960938, 16.0)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.validate(learn.data.train_dl)\n",
    "print_hist(unfooled_histogram, fooled_histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "percent_total: \n",
      "[(293, 99.9999999988889, 9.0), (340, 89.9999999991, 10.0), (109, 87.49999999890625, 8.0), (640, 83.33333333263889, 12.0), (479, 79.9999999984, 5.0), (387, 77.77777777691358, 9.0), (953, 77.77777777691358, 9.0), (290, 76.92307692248521, 13.0), (476, 74.999999999375, 12.0), (288, 74.9999999990625, 8.0), (396, 74.9999999990625, 8.0), (8, 72.72727272661157, 11.0), (57, 72.72727272661157, 11.0), (67, 72.72727272661157, 11.0), (82, 72.72727272661157, 11.0), (123, 72.72727272661157, 11.0), (276, 72.72727272661157, 11.0), (955, 72.72727272661157, 11.0), (997, 72.72727272661157, 11.0), (509, 71.42857142755102, 7.0), (580, 70.58823529370243, 17.0), (84, 69.9999999993, 10.0), (91, 69.9999999993, 10.0), (99, 69.9999999993, 10.0), (287, 69.9999999993, 10.0), (454, 69.9999999993, 10.0), (611, 69.9999999993, 10.0), (646, 69.9999999993, 10.0), (671, 69.9999999993, 10.0), (289, 69.23076923023669, 13.0), (56, 66.66666666611111, 12.0), (300, 66.66666666592593, 9.0), (444, 66.66666666592593, 9.0), (298, 66.66666666555555, 6.0), (37, 63.63636363578512, 11.0), (330, 63.63636363578512, 11.0), (334, 63.63636363578512, 11.0), (7, 62.49999999921875, 8.0), (39, 62.49999999921875, 8.0), (129, 62.49999999921875, 8.0), (131, 62.49999999921875, 8.0), (331, 62.49999999921875, 8.0), (138, 61.53846153798816, 13.0), (415, 61.53846153798816, 13.0), (612, 61.53846153798816, 13.0), (721, 61.53846153798816, 13.0), (182, 59.9999999996, 15.0), (865, 59.9999999996, 15.0), (28, 59.9999999994, 10.0), (116, 59.9999999994, 10.0), (174, 59.9999999994, 10.0), (243, 59.9999999994, 10.0), (322, 59.9999999994, 10.0), (327, 59.9999999994, 10.0), (328, 59.9999999994, 10.0), (364, 59.9999999994, 10.0), (393, 59.9999999994, 10.0), (394, 59.9999999994, 10.0), (918, 59.9999999994, 10.0), (443, 59.9999999988, 5.0), (696, 59.9999999988, 5.0), (281, 58.333333332847225, 12.0), (292, 58.333333332847225, 12.0), (490, 58.333333332847225, 12.0), (645, 58.333333332847225, 12.0), (825, 58.333333332847225, 12.0), (917, 58.333333332847225, 12.0), (992, 58.333333332847225, 12.0), (48, 57.14285714244898, 14.0), (652, 57.14285714244898, 14.0), (104, 57.14285714204082, 7.0), (120, 57.14285714204082, 7.0), (581, 57.14285714204082, 7.0), (735, 57.14285714204082, 7.0), (878, 57.14285714204082, 7.0), (269, 56.249999999648445, 16.0), (41, 55.55555555493827, 9.0), (247, 55.55555555493827, 9.0), (360, 55.55555555493827, 9.0), (382, 55.55555555493827, 9.0), (383, 55.55555555493827, 9.0), (398, 55.55555555493827, 9.0), (506, 55.55555555493827, 9.0), (665, 55.55555555493827, 9.0), (741, 55.55555555493827, 9.0), (879, 55.55555555493827, 9.0), (973, 55.55555555493827, 9.0), (15, 54.54545454495868, 11.0), (45, 54.54545454495868, 11.0), (81, 54.54545454495868, 11.0), (119, 54.54545454495868, 11.0), (177, 54.54545454495868, 11.0), (275, 54.54545454495868, 11.0), (342, 54.54545454495868, 11.0), (406, 54.54545454495868, 11.0), (603, 54.54545454495868, 11.0), (946, 54.54545454495868, 11.0), (355, 53.84615384573964, 13.0), (410, 53.84615384573964, 13.0), (547, 53.33333333297778, 15.0), (489, 49.9999999996875, 16.0), (25, 49.99999999958333, 12.0), (575, 49.99999999958333, 12.0), (586, 49.99999999958333, 12.0), (788, 49.99999999958333, 12.0), (847, 49.99999999958333, 12.0), (857, 49.99999999958333, 12.0), (949, 49.99999999958333, 12.0), (956, 49.99999999958333, 12.0), (963, 49.99999999958333, 12.0), (61, 49.9999999995, 10.0), (65, 49.9999999995, 10.0), (110, 49.9999999995, 10.0), (113, 49.9999999995, 10.0), (192, 49.9999999995, 10.0), (260, 49.9999999995, 10.0), (294, 49.9999999995, 10.0), (348, 49.9999999995, 10.0), (349, 49.9999999995, 10.0), (401, 49.9999999995, 10.0), (483, 49.9999999995, 10.0), (565, 49.9999999995, 10.0), (707, 49.9999999995, 10.0), (717, 49.9999999995, 10.0), (730, 49.9999999995, 10.0), (835, 49.9999999995, 10.0), (866, 49.9999999995, 10.0), (880, 49.9999999995, 10.0), (944, 49.9999999995, 10.0), (947, 49.9999999995, 10.0), (53, 49.999999999375, 8.0), (97, 49.999999999375, 8.0), (118, 49.999999999375, 8.0), (163, 49.999999999375, 8.0), (291, 49.999999999375, 8.0), (329, 49.999999999375, 8.0), (381, 49.999999999375, 8.0), (694, 49.999999999375, 8.0), (762, 49.999999999375, 8.0), (62, 49.99999999916667, 6.0), (539, 49.99999999916667, 6.0), (787, 49.99999999916667, 6.0), (282, 49.9999999975, 2.0), (218, 46.66666666635555, 15.0), (621, 46.15384615349112, 13.0), (800, 46.15384615349112, 13.0), (198, 45.45454545413223, 11.0), (363, 45.45454545413223, 11.0), (376, 45.45454545413223, 11.0), (407, 45.45454545413223, 11.0), (582, 45.45454545413223, 11.0), (791, 45.45454545413223, 11.0), (858, 45.45454545413223, 11.0), (922, 45.45454545413223, 11.0), (33, 44.444444443950616, 9.0), (66, 44.444444443950616, 9.0), (85, 44.444444443950616, 9.0), (133, 44.444444443950616, 9.0), (156, 44.444444443950616, 9.0), (173, 44.444444443950616, 9.0), (214, 44.444444443950616, 9.0), (240, 44.444444443950616, 9.0), (264, 44.444444443950616, 9.0), (278, 44.444444443950616, 9.0), (412, 44.444444443950616, 9.0), (467, 44.444444443950616, 9.0), (808, 44.444444443950616, 9.0), (853, 44.444444443950616, 9.0), (936, 44.444444443950616, 9.0), (937, 44.444444443950616, 9.0), (54, 42.857142856530615, 7.0), (76, 42.857142856530615, 7.0), (353, 42.857142856530615, 7.0), (588, 42.857142856530615, 7.0), (820, 42.857142856530615, 7.0), (839, 42.857142856530615, 7.0), (886, 42.857142856530615, 7.0), (904, 42.857142856530615, 7.0), (24, 41.66666666631944, 12.0), (125, 41.66666666631944, 12.0), (128, 41.66666666631944, 12.0), (193, 41.66666666631944, 12.0), (211, 41.66666666631944, 12.0), (538, 41.66666666631944, 12.0), (765, 41.66666666631944, 12.0), (0, 39.9999999996, 10.0), (30, 39.9999999996, 10.0), (52, 39.9999999996, 10.0), (79, 39.9999999996, 10.0), (88, 39.9999999996, 10.0), (90, 39.9999999996, 10.0), (102, 39.9999999996, 10.0), (108, 39.9999999996, 10.0), (124, 39.9999999996, 10.0), (137, 39.9999999996, 10.0), (191, 39.9999999996, 10.0), (305, 39.9999999996, 10.0), (321, 39.9999999996, 10.0), (336, 39.9999999996, 10.0), (430, 39.9999999996, 10.0), (779, 39.9999999996, 10.0), (781, 39.9999999996, 10.0), (783, 39.9999999996, 10.0), (806, 39.9999999996, 10.0), (959, 39.9999999996, 10.0), (461, 39.9999999992, 5.0), (217, 38.461538461242604, 13.0), (458, 38.461538461242604, 13.0), (468, 38.461538461242604, 13.0), (805, 38.461538461242604, 13.0), (38, 37.49999999953125, 8.0), (127, 37.49999999953125, 8.0), (170, 37.49999999953125, 8.0), (189, 37.49999999953125, 8.0), (226, 37.49999999953125, 8.0), (271, 37.49999999953125, 8.0), (302, 37.49999999953125, 8.0), (362, 37.49999999953125, 8.0), (555, 37.49999999953125, 8.0), (614, 37.49999999953125, 8.0), (637, 37.49999999953125, 8.0), (932, 37.49999999953125, 8.0), (958, 37.49999999953125, 8.0), (987, 37.49999999953125, 8.0), (87, 36.36363636330579, 11.0), (96, 36.36363636330579, 11.0), (98, 36.36363636330579, 11.0), (254, 36.36363636330579, 11.0), (372, 36.36363636330579, 11.0), (429, 36.36363636330579, 11.0), (495, 36.36363636330579, 11.0), (634, 36.36363636330579, 11.0), (658, 36.36363636330579, 11.0), (670, 36.36363636330579, 11.0), (692, 36.36363636330579, 11.0), (822, 36.36363636330579, 11.0), (993, 36.36363636330579, 11.0), (474, 35.71428571403061, 14.0), (706, 35.71428571403061, 14.0), (533, 33.33333333311111, 15.0), (568, 33.33333333311111, 15.0), (221, 33.333333333055556, 12.0), (235, 33.333333333055556, 12.0), (388, 33.333333333055556, 12.0), (436, 33.333333333055556, 12.0), (448, 33.333333333055556, 12.0), (624, 33.333333333055556, 12.0), (687, 33.333333333055556, 12.0), (737, 33.333333333055556, 12.0), (763, 33.333333333055556, 12.0), (71, 33.333333332962965, 9.0), (100, 33.333333332962965, 9.0), (126, 33.333333332962965, 9.0), (155, 33.333333332962965, 9.0), (160, 33.333333332962965, 9.0), (172, 33.333333332962965, 9.0), (251, 33.333333332962965, 9.0), (272, 33.333333332962965, 9.0), (277, 33.333333332962965, 9.0), (284, 33.333333332962965, 9.0), (285, 33.333333332962965, 9.0), (299, 33.333333332962965, 9.0), (319, 33.333333332962965, 9.0), (371, 33.333333332962965, 9.0), (384, 33.333333332962965, 9.0), (389, 33.333333332962965, 9.0), (417, 33.333333332962965, 9.0), (471, 33.333333332962965, 9.0), (723, 33.333333332962965, 9.0), (746, 33.333333332962965, 9.0), (768, 33.333333332962965, 9.0), (848, 33.333333332962965, 9.0), (863, 33.333333332962965, 9.0), (884, 33.333333332962965, 9.0), (934, 33.333333332962965, 9.0), (995, 33.333333332962965, 9.0), (185, 33.33333333277778, 6.0), (885, 33.333333332222224, 3.0), (77, 31.24999999980469, 16.0), (44, 30.76923076899408, 13.0), (236, 30.76923076899408, 13.0), (47, 29.9999999997, 10.0), (105, 29.9999999997, 10.0), (121, 29.9999999997, 10.0), (134, 29.9999999997, 10.0), (136, 29.9999999997, 10.0), (140, 29.9999999997, 10.0), (213, 29.9999999997, 10.0), (280, 29.9999999997, 10.0), (316, 29.9999999997, 10.0), (343, 29.9999999997, 10.0), (352, 29.9999999997, 10.0), (390, 29.9999999997, 10.0), (397, 29.9999999997, 10.0), (428, 29.9999999997, 10.0), (496, 29.9999999997, 10.0), (502, 29.9999999997, 10.0), (535, 29.9999999997, 10.0), (561, 29.9999999997, 10.0), (594, 29.9999999997, 10.0), (616, 29.9999999997, 10.0), (703, 29.9999999997, 10.0), (828, 29.9999999997, 10.0), (864, 29.9999999997, 10.0), (873, 29.9999999997, 10.0), (889, 29.9999999997, 10.0), (984, 29.9999999997, 10.0), (202, 28.57142857122449, 14.0), (216, 28.57142857122449, 14.0), (562, 28.57142857122449, 14.0), (649, 28.57142857122449, 14.0), (939, 28.57142857122449, 14.0), (962, 28.57142857122449, 14.0), (144, 28.57142857102041, 7.0), (184, 28.57142857102041, 7.0), (212, 28.57142857102041, 7.0), (231, 28.57142857102041, 7.0), (295, 28.57142857102041, 7.0), (303, 28.57142857102041, 7.0), (309, 28.57142857102041, 7.0), (427, 28.57142857102041, 7.0), (599, 28.57142857102041, 7.0), (792, 28.57142857102041, 7.0), (911, 28.57142857102041, 7.0), (50, 27.27272727247934, 11.0), (51, 27.27272727247934, 11.0), (69, 27.27272727247934, 11.0), (83, 27.27272727247934, 11.0), (135, 27.27272727247934, 11.0), (187, 27.27272727247934, 11.0), (197, 27.27272727247934, 11.0), (242, 27.27272727247934, 11.0), (301, 27.27272727247934, 11.0), (365, 27.27272727247934, 11.0), (369, 27.27272727247934, 11.0), (395, 27.27272727247934, 11.0), (477, 27.27272727247934, 11.0), (520, 27.27272727247934, 11.0), (522, 27.27272727247934, 11.0), (544, 27.27272727247934, 11.0), (577, 27.27272727247934, 11.0), (642, 27.27272727247934, 11.0), (690, 27.27272727247934, 11.0), (716, 27.27272727247934, 11.0), (748, 27.27272727247934, 11.0), (784, 27.27272727247934, 11.0), (795, 27.27272727247934, 11.0), (890, 27.27272727247934, 11.0), (938, 27.27272727247934, 11.0), (991, 27.27272727247934, 11.0), (210, 24.999999999791665, 12.0), (233, 24.999999999791665, 12.0), (314, 24.999999999791665, 12.0), (350, 24.999999999791665, 12.0), (351, 24.999999999791665, 12.0), (361, 24.999999999791665, 12.0), (563, 24.999999999791665, 12.0), (609, 24.999999999791665, 12.0), (675, 24.999999999791665, 12.0), (761, 24.999999999791665, 12.0), (793, 24.999999999791665, 12.0), (821, 24.999999999791665, 12.0), (849, 24.999999999791665, 12.0), (870, 24.999999999791665, 12.0), (35, 24.9999999996875, 8.0), (43, 24.9999999996875, 8.0), (68, 24.9999999996875, 8.0), (122, 24.9999999996875, 8.0), (139, 24.9999999996875, 8.0), (219, 24.9999999996875, 8.0), (230, 24.9999999996875, 8.0), (248, 24.9999999996875, 8.0), (250, 24.9999999996875, 8.0), (268, 24.9999999996875, 8.0), (273, 24.9999999996875, 8.0), (311, 24.9999999996875, 8.0), (370, 24.9999999996875, 8.0), (464, 24.9999999996875, 8.0), (597, 24.9999999996875, 8.0), (620, 24.9999999996875, 8.0), (647, 24.9999999996875, 8.0), (688, 24.9999999996875, 8.0), (727, 24.9999999996875, 8.0), (794, 24.9999999996875, 8.0), (907, 24.9999999996875, 8.0), (914, 24.9999999996875, 8.0), (924, 24.9999999996875, 8.0), (951, 24.9999999996875, 8.0), (711, 24.999999999375, 4.0), (201, 23.07692307674556, 13.0), (472, 23.07692307674556, 13.0), (498, 23.07692307674556, 13.0), (532, 23.07692307674556, 13.0), (777, 23.07692307674556, 13.0), (829, 23.07692307674556, 13.0), (926, 23.07692307674556, 13.0), (933, 23.07692307674556, 13.0), (14, 22.222222221975308, 9.0), (17, 22.222222221975308, 9.0), (34, 22.222222221975308, 9.0), (49, 22.222222221975308, 9.0), (86, 22.222222221975308, 9.0), (132, 22.222222221975308, 9.0), (145, 22.222222221975308, 9.0), (165, 22.222222221975308, 9.0), (171, 22.222222221975308, 9.0), (188, 22.222222221975308, 9.0), (199, 22.222222221975308, 9.0), (206, 22.222222221975308, 9.0), (222, 22.222222221975308, 9.0), (246, 22.222222221975308, 9.0), (255, 22.222222221975308, 9.0), (279, 22.222222221975308, 9.0), (308, 22.222222221975308, 9.0), (338, 22.222222221975308, 9.0), (374, 22.222222221975308, 9.0), (386, 22.222222221975308, 9.0), (392, 22.222222221975308, 9.0), (439, 22.222222221975308, 9.0), (453, 22.222222221975308, 9.0), (455, 22.222222221975308, 9.0), (528, 22.222222221975308, 9.0), (546, 22.222222221975308, 9.0), (560, 22.222222221975308, 9.0), (608, 22.222222221975308, 9.0), (677, 22.222222221975308, 9.0), (850, 22.222222221975308, 9.0), (919, 22.222222221975308, 9.0), (945, 22.222222221975308, 9.0), (981, 22.222222221975308, 9.0), (519, 21.42857142841837, 14.0), (263, 19.999999999866667, 15.0), (572, 19.999999999866667, 15.0), (9, 19.9999999998, 10.0), (58, 19.9999999998, 10.0), (93, 19.9999999998, 10.0), (161, 19.9999999998, 10.0), (180, 19.9999999998, 10.0), (227, 19.9999999998, 10.0), (229, 19.9999999998, 10.0), (323, 19.9999999998, 10.0), (366, 19.9999999998, 10.0), (433, 19.9999999998, 10.0), (466, 19.9999999998, 10.0), (475, 19.9999999998, 10.0), (488, 19.9999999998, 10.0), (491, 19.9999999998, 10.0), (500, 19.9999999998, 10.0), (521, 19.9999999998, 10.0), (574, 19.9999999998, 10.0), (595, 19.9999999998, 10.0), (625, 19.9999999998, 10.0), (628, 19.9999999998, 10.0), (641, 19.9999999998, 10.0), (643, 19.9999999998, 10.0), (682, 19.9999999998, 10.0), (685, 19.9999999998, 10.0), (695, 19.9999999998, 10.0), (715, 19.9999999998, 10.0), (738, 19.9999999998, 10.0), (743, 19.9999999998, 10.0), (776, 19.9999999998, 10.0), (819, 19.9999999998, 10.0), (826, 19.9999999998, 10.0), (856, 19.9999999998, 10.0), (874, 19.9999999998, 10.0), (887, 19.9999999998, 10.0), (916, 19.9999999998, 10.0), (950, 19.9999999998, 10.0), (954, 19.9999999998, 10.0), (733, 19.9999999996, 5.0), (882, 19.9999999996, 5.0), (906, 19.9999999996, 5.0), (912, 19.9999999996, 5.0), (20, 18.181818181652893, 11.0), (150, 18.181818181652893, 11.0), (194, 18.181818181652893, 11.0), (228, 18.181818181652893, 11.0), (245, 18.181818181652893, 11.0), (335, 18.181818181652893, 11.0), (337, 18.181818181652893, 11.0), (413, 18.181818181652893, 11.0), (508, 18.181818181652893, 11.0), (510, 18.181818181652893, 11.0), (559, 18.181818181652893, 11.0), (571, 18.181818181652893, 11.0), (576, 18.181818181652893, 11.0), (607, 18.181818181652893, 11.0), (661, 18.181818181652893, 11.0), (684, 18.181818181652893, 11.0), (698, 18.181818181652893, 11.0), (729, 18.181818181652893, 11.0), (740, 18.181818181652893, 11.0), (754, 18.181818181652893, 11.0), (766, 18.181818181652893, 11.0), (796, 18.181818181652893, 11.0), (888, 18.181818181652893, 11.0), (929, 18.181818181652893, 11.0), (988, 18.181818181652893, 11.0), (982, 16.666666666597223, 24.0), (27, 16.666666666527778, 12.0), (114, 16.666666666527778, 12.0), (186, 16.666666666527778, 12.0), (232, 16.666666666527778, 12.0), (244, 16.666666666527778, 12.0), (249, 16.666666666527778, 12.0), (258, 16.666666666527778, 12.0), (283, 16.666666666527778, 12.0), (297, 16.666666666527778, 12.0), (347, 16.666666666527778, 12.0), (409, 16.666666666527778, 12.0), (492, 16.666666666527778, 12.0), (511, 16.666666666527778, 12.0), (566, 16.666666666527778, 12.0), (590, 16.666666666527778, 12.0), (790, 16.666666666527778, 12.0), (802, 16.666666666527778, 12.0), (803, 16.666666666527778, 12.0), (167, 16.66666666638889, 6.0), (176, 16.66666666638889, 6.0), (266, 16.66666666638889, 6.0), (345, 16.66666666638889, 6.0), (497, 16.66666666638889, 6.0), (663, 16.66666666638889, 6.0), (686, 16.66666666638889, 6.0), (705, 16.66666666638889, 6.0), (734, 16.66666666638889, 6.0), (898, 16.66666666638889, 6.0), (157, 15.38461538449704, 13.0), (274, 15.38461538449704, 13.0), (339, 15.38461538449704, 13.0), (375, 15.38461538449704, 13.0), (379, 15.38461538449704, 13.0), (667, 15.38461538449704, 13.0), (679, 15.38461538449704, 13.0), (755, 15.38461538449704, 13.0), (756, 15.38461538449704, 13.0), (905, 15.38461538449704, 13.0), (990, 15.38461538449704, 13.0), (209, 14.285714285612245, 14.0), (411, 14.285714285612245, 14.0), (431, 14.285714285612245, 14.0), (524, 14.285714285612245, 14.0), (573, 14.285714285612245, 14.0), (830, 14.285714285612245, 14.0), (843, 14.285714285612245, 14.0), (952, 14.285714285612245, 14.0), (196, 14.285714285510204, 7.0), (200, 14.285714285510204, 7.0), (253, 14.285714285510204, 7.0), (356, 14.285714285510204, 7.0), (358, 14.285714285510204, 7.0), (445, 14.285714285510204, 7.0), (513, 14.285714285510204, 7.0), (514, 14.285714285510204, 7.0), (545, 14.285714285510204, 7.0), (551, 14.285714285510204, 7.0), (587, 14.285714285510204, 7.0), (600, 14.285714285510204, 7.0), (772, 14.285714285510204, 7.0), (798, 14.285714285510204, 7.0), (893, 14.285714285510204, 7.0), (909, 14.285714285510204, 7.0), (910, 14.285714285510204, 7.0), (969, 14.285714285510204, 7.0), (480, 13.333333333244445, 15.0), (648, 13.333333333244445, 15.0), (18, 12.49999999984375, 8.0), (26, 12.49999999984375, 8.0), (64, 12.49999999984375, 8.0), (106, 12.49999999984375, 8.0), (164, 12.49999999984375, 8.0), (175, 12.49999999984375, 8.0), (179, 12.49999999984375, 8.0), (224, 12.49999999984375, 8.0), (234, 12.49999999984375, 8.0), (259, 12.49999999984375, 8.0), (318, 12.49999999984375, 8.0), (341, 12.49999999984375, 8.0), (368, 12.49999999984375, 8.0), (419, 12.49999999984375, 8.0), (441, 12.49999999984375, 8.0), (449, 12.49999999984375, 8.0), (469, 12.49999999984375, 8.0), (627, 12.49999999984375, 8.0), (656, 12.49999999984375, 8.0), (699, 12.49999999984375, 8.0), (704, 12.49999999984375, 8.0), (753, 12.49999999984375, 8.0), (757, 12.49999999984375, 8.0), (891, 12.49999999984375, 8.0), (901, 12.49999999984375, 8.0), (943, 12.49999999984375, 8.0), (994, 12.49999999984375, 8.0), (996, 12.49999999984375, 8.0), (1, 11.111111110987654, 9.0), (13, 11.111111110987654, 9.0), (32, 11.111111110987654, 9.0), (36, 11.111111110987654, 9.0), (78, 11.111111110987654, 9.0), (95, 11.111111110987654, 9.0), (107, 11.111111110987654, 9.0), (115, 11.111111110987654, 9.0), (181, 11.111111110987654, 9.0), (204, 11.111111110987654, 9.0), (207, 11.111111110987654, 9.0), (220, 11.111111110987654, 9.0), (261, 11.111111110987654, 9.0), (270, 11.111111110987654, 9.0), (385, 11.111111110987654, 9.0), (402, 11.111111110987654, 9.0), (418, 11.111111110987654, 9.0), (420, 11.111111110987654, 9.0), (447, 11.111111110987654, 9.0), (451, 11.111111110987654, 9.0), (452, 11.111111110987654, 9.0), (462, 11.111111110987654, 9.0), (470, 11.111111110987654, 9.0), (487, 11.111111110987654, 9.0), (507, 11.111111110987654, 9.0), (518, 11.111111110987654, 9.0), (541, 11.111111110987654, 9.0), (584, 11.111111110987654, 9.0), (604, 11.111111110987654, 9.0), (632, 11.111111110987654, 9.0), (639, 11.111111110987654, 9.0), (666, 11.111111110987654, 9.0), (668, 11.111111110987654, 9.0), (719, 11.111111110987654, 9.0), (751, 11.111111110987654, 9.0), (810, 11.111111110987654, 9.0), (816, 11.111111110987654, 9.0), (827, 11.111111110987654, 9.0), (941, 11.111111110987654, 9.0), (6, 9.9999999999, 10.0), (11, 9.9999999999, 10.0), (22, 9.9999999999, 10.0), (63, 9.9999999999, 10.0), (75, 9.9999999999, 10.0), (89, 9.9999999999, 10.0), (178, 9.9999999999, 10.0), (195, 9.9999999999, 10.0), (239, 9.9999999999, 10.0), (252, 9.9999999999, 10.0), (262, 9.9999999999, 10.0), (265, 9.9999999999, 10.0), (286, 9.9999999999, 10.0), (313, 9.9999999999, 10.0), (391, 9.9999999999, 10.0), (408, 9.9999999999, 10.0), (423, 9.9999999999, 10.0), (456, 9.9999999999, 10.0), (530, 9.9999999999, 10.0), (569, 9.9999999999, 10.0), (579, 9.9999999999, 10.0), (606, 9.9999999999, 10.0), (664, 9.9999999999, 10.0), (683, 9.9999999999, 10.0), (728, 9.9999999999, 10.0), (732, 9.9999999999, 10.0), (736, 9.9999999999, 10.0), (739, 9.9999999999, 10.0), (752, 9.9999999999, 10.0), (759, 9.9999999999, 10.0), (760, 9.9999999999, 10.0), (767, 9.9999999999, 10.0), (797, 9.9999999999, 10.0), (801, 9.9999999999, 10.0), (813, 9.9999999999, 10.0), (814, 9.9999999999, 10.0), (867, 9.9999999999, 10.0), (872, 9.9999999999, 10.0), (915, 9.9999999999, 10.0), (920, 9.9999999999, 10.0), (927, 9.9999999999, 10.0), (957, 9.9999999999, 10.0), (971, 9.9999999999, 10.0), (526, 9.523809523764173, 21.0), (10, 9.090909090826447, 11.0), (16, 9.090909090826447, 11.0), (29, 9.090909090826447, 11.0), (42, 9.090909090826447, 11.0), (205, 9.090909090826447, 11.0), (225, 9.090909090826447, 11.0), (257, 9.090909090826447, 11.0), (310, 9.090909090826447, 11.0), (324, 9.090909090826447, 11.0), (344, 9.090909090826447, 11.0), (377, 9.090909090826447, 11.0), (516, 9.090909090826447, 11.0), (630, 9.090909090826447, 11.0), (672, 9.090909090826447, 11.0), (799, 9.090909090826447, 11.0), (817, 9.090909090826447, 11.0), (834, 9.090909090826447, 11.0), (840, 9.090909090826447, 11.0), (842, 9.090909090826447, 11.0), (852, 9.090909090826447, 11.0), (855, 9.090909090826447, 11.0), (860, 9.090909090826447, 11.0), (862, 9.090909090826447, 11.0), (868, 9.090909090826447, 11.0), (881, 9.090909090826447, 11.0), (935, 9.090909090826447, 11.0), (964, 9.090909090826447, 11.0), (966, 9.090909090826447, 11.0), (967, 9.090909090826447, 11.0), (979, 9.090909090826447, 11.0), (2, 8.333333333263889, 12.0), (55, 8.333333333263889, 12.0), (151, 8.333333333263889, 12.0), (315, 8.333333333263889, 12.0), (416, 8.333333333263889, 12.0), (425, 8.333333333263889, 12.0), (432, 8.333333333263889, 12.0), (440, 8.333333333263889, 12.0), (481, 8.333333333263889, 12.0), (523, 8.333333333263889, 12.0), (525, 8.333333333263889, 12.0), (548, 8.333333333263889, 12.0), (593, 8.333333333263889, 12.0), (654, 8.333333333263889, 12.0), (710, 8.333333333263889, 12.0), (714, 8.333333333263889, 12.0), (785, 8.333333333263889, 12.0), (786, 8.333333333263889, 12.0), (789, 8.333333333263889, 12.0), (833, 8.333333333263889, 12.0), (921, 8.333333333263889, 12.0), (931, 8.333333333263889, 12.0), (998, 8.333333333263889, 12.0), (154, 7.69230769224852, 13.0), (159, 7.69230769224852, 13.0), (162, 7.69230769224852, 13.0), (183, 7.69230769224852, 13.0), (237, 7.69230769224852, 13.0), (238, 7.69230769224852, 13.0), (304, 7.69230769224852, 13.0), (312, 7.69230769224852, 13.0), (359, 7.69230769224852, 13.0), (404, 7.69230769224852, 13.0), (442, 7.69230769224852, 13.0), (505, 7.69230769224852, 13.0), (543, 7.69230769224852, 13.0), (591, 7.69230769224852, 13.0), (968, 7.69230769224852, 13.0), (450, 7.142857142806123, 14.0), (708, 7.142857142806123, 14.0), (644, 6.6666666666222225, 15.0), (769, 6.6666666666222225, 15.0), (831, 6.6666666666222225, 15.0), (877, 6.6666666666222225, 15.0), (424, 6.249999999960938, 16.0), (484, 6.249999999960938, 16.0), (564, 6.249999999960938, 16.0), (655, 6.249999999960938, 16.0)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(293, 99.9999999988889, 9.0),\n",
       " (340, 89.9999999991, 10.0),\n",
       " (109, 87.49999999890625, 8.0),\n",
       " (640, 83.33333333263889, 12.0),\n",
       " (479, 79.9999999984, 5.0),\n",
       " (387, 77.77777777691358, 9.0),\n",
       " (953, 77.77777777691358, 9.0),\n",
       " (290, 76.92307692248521, 13.0),\n",
       " (476, 74.999999999375, 12.0),\n",
       " (288, 74.9999999990625, 8.0),\n",
       " (396, 74.9999999990625, 8.0),\n",
       " (8, 72.72727272661157, 11.0),\n",
       " (57, 72.72727272661157, 11.0),\n",
       " (67, 72.72727272661157, 11.0),\n",
       " (82, 72.72727272661157, 11.0),\n",
       " (123, 72.72727272661157, 11.0),\n",
       " (276, 72.72727272661157, 11.0),\n",
       " (955, 72.72727272661157, 11.0),\n",
       " (997, 72.72727272661157, 11.0),\n",
       " (509, 71.42857142755102, 7.0),\n",
       " (580, 70.58823529370243, 17.0),\n",
       " (84, 69.9999999993, 10.0),\n",
       " (91, 69.9999999993, 10.0),\n",
       " (99, 69.9999999993, 10.0),\n",
       " (287, 69.9999999993, 10.0),\n",
       " (454, 69.9999999993, 10.0),\n",
       " (611, 69.9999999993, 10.0),\n",
       " (646, 69.9999999993, 10.0),\n",
       " (671, 69.9999999993, 10.0),\n",
       " (289, 69.23076923023669, 13.0),\n",
       " (56, 66.66666666611111, 12.0),\n",
       " (300, 66.66666666592593, 9.0),\n",
       " (444, 66.66666666592593, 9.0),\n",
       " (298, 66.66666666555555, 6.0),\n",
       " (37, 63.63636363578512, 11.0),\n",
       " (330, 63.63636363578512, 11.0),\n",
       " (334, 63.63636363578512, 11.0),\n",
       " (7, 62.49999999921875, 8.0),\n",
       " (39, 62.49999999921875, 8.0),\n",
       " (129, 62.49999999921875, 8.0),\n",
       " (131, 62.49999999921875, 8.0),\n",
       " (331, 62.49999999921875, 8.0),\n",
       " (138, 61.53846153798816, 13.0),\n",
       " (415, 61.53846153798816, 13.0),\n",
       " (612, 61.53846153798816, 13.0),\n",
       " (721, 61.53846153798816, 13.0),\n",
       " (182, 59.9999999996, 15.0),\n",
       " (865, 59.9999999996, 15.0),\n",
       " (28, 59.9999999994, 10.0),\n",
       " (116, 59.9999999994, 10.0),\n",
       " (174, 59.9999999994, 10.0),\n",
       " (243, 59.9999999994, 10.0),\n",
       " (322, 59.9999999994, 10.0),\n",
       " (327, 59.9999999994, 10.0),\n",
       " (328, 59.9999999994, 10.0),\n",
       " (364, 59.9999999994, 10.0),\n",
       " (393, 59.9999999994, 10.0),\n",
       " (394, 59.9999999994, 10.0),\n",
       " (918, 59.9999999994, 10.0),\n",
       " (443, 59.9999999988, 5.0),\n",
       " (696, 59.9999999988, 5.0),\n",
       " (281, 58.333333332847225, 12.0),\n",
       " (292, 58.333333332847225, 12.0),\n",
       " (490, 58.333333332847225, 12.0),\n",
       " (645, 58.333333332847225, 12.0),\n",
       " (825, 58.333333332847225, 12.0),\n",
       " (917, 58.333333332847225, 12.0),\n",
       " (992, 58.333333332847225, 12.0),\n",
       " (48, 57.14285714244898, 14.0),\n",
       " (652, 57.14285714244898, 14.0),\n",
       " (104, 57.14285714204082, 7.0),\n",
       " (120, 57.14285714204082, 7.0),\n",
       " (581, 57.14285714204082, 7.0),\n",
       " (735, 57.14285714204082, 7.0),\n",
       " (878, 57.14285714204082, 7.0),\n",
       " (269, 56.249999999648445, 16.0),\n",
       " (41, 55.55555555493827, 9.0),\n",
       " (247, 55.55555555493827, 9.0),\n",
       " (360, 55.55555555493827, 9.0),\n",
       " (382, 55.55555555493827, 9.0),\n",
       " (383, 55.55555555493827, 9.0),\n",
       " (398, 55.55555555493827, 9.0),\n",
       " (506, 55.55555555493827, 9.0),\n",
       " (665, 55.55555555493827, 9.0),\n",
       " (741, 55.55555555493827, 9.0),\n",
       " (879, 55.55555555493827, 9.0),\n",
       " (973, 55.55555555493827, 9.0),\n",
       " (15, 54.54545454495868, 11.0),\n",
       " (45, 54.54545454495868, 11.0),\n",
       " (81, 54.54545454495868, 11.0),\n",
       " (119, 54.54545454495868, 11.0),\n",
       " (177, 54.54545454495868, 11.0),\n",
       " (275, 54.54545454495868, 11.0),\n",
       " (342, 54.54545454495868, 11.0),\n",
       " (406, 54.54545454495868, 11.0),\n",
       " (603, 54.54545454495868, 11.0),\n",
       " (946, 54.54545454495868, 11.0),\n",
       " (355, 53.84615384573964, 13.0),\n",
       " (410, 53.84615384573964, 13.0),\n",
       " (547, 53.33333333297778, 15.0),\n",
       " (489, 49.9999999996875, 16.0),\n",
       " (25, 49.99999999958333, 12.0),\n",
       " (575, 49.99999999958333, 12.0),\n",
       " (586, 49.99999999958333, 12.0),\n",
       " (788, 49.99999999958333, 12.0),\n",
       " (847, 49.99999999958333, 12.0),\n",
       " (857, 49.99999999958333, 12.0),\n",
       " (949, 49.99999999958333, 12.0),\n",
       " (956, 49.99999999958333, 12.0),\n",
       " (963, 49.99999999958333, 12.0),\n",
       " (61, 49.9999999995, 10.0),\n",
       " (65, 49.9999999995, 10.0),\n",
       " (110, 49.9999999995, 10.0),\n",
       " (113, 49.9999999995, 10.0),\n",
       " (192, 49.9999999995, 10.0),\n",
       " (260, 49.9999999995, 10.0),\n",
       " (294, 49.9999999995, 10.0),\n",
       " (348, 49.9999999995, 10.0),\n",
       " (349, 49.9999999995, 10.0),\n",
       " (401, 49.9999999995, 10.0),\n",
       " (483, 49.9999999995, 10.0),\n",
       " (565, 49.9999999995, 10.0),\n",
       " (707, 49.9999999995, 10.0),\n",
       " (717, 49.9999999995, 10.0),\n",
       " (730, 49.9999999995, 10.0),\n",
       " (835, 49.9999999995, 10.0),\n",
       " (866, 49.9999999995, 10.0),\n",
       " (880, 49.9999999995, 10.0),\n",
       " (944, 49.9999999995, 10.0),\n",
       " (947, 49.9999999995, 10.0),\n",
       " (53, 49.999999999375, 8.0),\n",
       " (97, 49.999999999375, 8.0),\n",
       " (118, 49.999999999375, 8.0),\n",
       " (163, 49.999999999375, 8.0),\n",
       " (291, 49.999999999375, 8.0),\n",
       " (329, 49.999999999375, 8.0),\n",
       " (381, 49.999999999375, 8.0),\n",
       " (694, 49.999999999375, 8.0),\n",
       " (762, 49.999999999375, 8.0),\n",
       " (62, 49.99999999916667, 6.0),\n",
       " (539, 49.99999999916667, 6.0),\n",
       " (787, 49.99999999916667, 6.0),\n",
       " (282, 49.9999999975, 2.0),\n",
       " (218, 46.66666666635555, 15.0),\n",
       " (621, 46.15384615349112, 13.0),\n",
       " (800, 46.15384615349112, 13.0),\n",
       " (198, 45.45454545413223, 11.0),\n",
       " (363, 45.45454545413223, 11.0),\n",
       " (376, 45.45454545413223, 11.0),\n",
       " (407, 45.45454545413223, 11.0),\n",
       " (582, 45.45454545413223, 11.0),\n",
       " (791, 45.45454545413223, 11.0),\n",
       " (858, 45.45454545413223, 11.0),\n",
       " (922, 45.45454545413223, 11.0),\n",
       " (33, 44.444444443950616, 9.0),\n",
       " (66, 44.444444443950616, 9.0),\n",
       " (85, 44.444444443950616, 9.0),\n",
       " (133, 44.444444443950616, 9.0),\n",
       " (156, 44.444444443950616, 9.0),\n",
       " (173, 44.444444443950616, 9.0),\n",
       " (214, 44.444444443950616, 9.0),\n",
       " (240, 44.444444443950616, 9.0),\n",
       " (264, 44.444444443950616, 9.0),\n",
       " (278, 44.444444443950616, 9.0),\n",
       " (412, 44.444444443950616, 9.0),\n",
       " (467, 44.444444443950616, 9.0),\n",
       " (808, 44.444444443950616, 9.0),\n",
       " (853, 44.444444443950616, 9.0),\n",
       " (936, 44.444444443950616, 9.0),\n",
       " (937, 44.444444443950616, 9.0),\n",
       " (54, 42.857142856530615, 7.0),\n",
       " (76, 42.857142856530615, 7.0),\n",
       " (353, 42.857142856530615, 7.0),\n",
       " (588, 42.857142856530615, 7.0),\n",
       " (820, 42.857142856530615, 7.0),\n",
       " (839, 42.857142856530615, 7.0),\n",
       " (886, 42.857142856530615, 7.0),\n",
       " (904, 42.857142856530615, 7.0),\n",
       " (24, 41.66666666631944, 12.0),\n",
       " (125, 41.66666666631944, 12.0),\n",
       " (128, 41.66666666631944, 12.0),\n",
       " (193, 41.66666666631944, 12.0),\n",
       " (211, 41.66666666631944, 12.0),\n",
       " (538, 41.66666666631944, 12.0),\n",
       " (765, 41.66666666631944, 12.0),\n",
       " (0, 39.9999999996, 10.0),\n",
       " (30, 39.9999999996, 10.0),\n",
       " (52, 39.9999999996, 10.0),\n",
       " (79, 39.9999999996, 10.0),\n",
       " (88, 39.9999999996, 10.0),\n",
       " (90, 39.9999999996, 10.0),\n",
       " (102, 39.9999999996, 10.0),\n",
       " (108, 39.9999999996, 10.0),\n",
       " (124, 39.9999999996, 10.0),\n",
       " (137, 39.9999999996, 10.0),\n",
       " (191, 39.9999999996, 10.0),\n",
       " (305, 39.9999999996, 10.0),\n",
       " (321, 39.9999999996, 10.0),\n",
       " (336, 39.9999999996, 10.0),\n",
       " (430, 39.9999999996, 10.0),\n",
       " (779, 39.9999999996, 10.0),\n",
       " (781, 39.9999999996, 10.0),\n",
       " (783, 39.9999999996, 10.0),\n",
       " (806, 39.9999999996, 10.0),\n",
       " (959, 39.9999999996, 10.0),\n",
       " (461, 39.9999999992, 5.0),\n",
       " (217, 38.461538461242604, 13.0),\n",
       " (458, 38.461538461242604, 13.0),\n",
       " (468, 38.461538461242604, 13.0),\n",
       " (805, 38.461538461242604, 13.0),\n",
       " (38, 37.49999999953125, 8.0),\n",
       " (127, 37.49999999953125, 8.0),\n",
       " (170, 37.49999999953125, 8.0),\n",
       " (189, 37.49999999953125, 8.0),\n",
       " (226, 37.49999999953125, 8.0),\n",
       " (271, 37.49999999953125, 8.0),\n",
       " (302, 37.49999999953125, 8.0),\n",
       " (362, 37.49999999953125, 8.0),\n",
       " (555, 37.49999999953125, 8.0),\n",
       " (614, 37.49999999953125, 8.0),\n",
       " (637, 37.49999999953125, 8.0),\n",
       " (932, 37.49999999953125, 8.0),\n",
       " (958, 37.49999999953125, 8.0),\n",
       " (987, 37.49999999953125, 8.0),\n",
       " (87, 36.36363636330579, 11.0),\n",
       " (96, 36.36363636330579, 11.0),\n",
       " (98, 36.36363636330579, 11.0),\n",
       " (254, 36.36363636330579, 11.0),\n",
       " (372, 36.36363636330579, 11.0),\n",
       " (429, 36.36363636330579, 11.0),\n",
       " (495, 36.36363636330579, 11.0),\n",
       " (634, 36.36363636330579, 11.0),\n",
       " (658, 36.36363636330579, 11.0),\n",
       " (670, 36.36363636330579, 11.0),\n",
       " (692, 36.36363636330579, 11.0),\n",
       " (822, 36.36363636330579, 11.0),\n",
       " (993, 36.36363636330579, 11.0),\n",
       " (474, 35.71428571403061, 14.0),\n",
       " (706, 35.71428571403061, 14.0),\n",
       " (533, 33.33333333311111, 15.0),\n",
       " (568, 33.33333333311111, 15.0),\n",
       " (221, 33.333333333055556, 12.0),\n",
       " (235, 33.333333333055556, 12.0),\n",
       " (388, 33.333333333055556, 12.0),\n",
       " (436, 33.333333333055556, 12.0),\n",
       " (448, 33.333333333055556, 12.0),\n",
       " (624, 33.333333333055556, 12.0),\n",
       " (687, 33.333333333055556, 12.0),\n",
       " (737, 33.333333333055556, 12.0),\n",
       " (763, 33.333333333055556, 12.0),\n",
       " (71, 33.333333332962965, 9.0),\n",
       " (100, 33.333333332962965, 9.0),\n",
       " (126, 33.333333332962965, 9.0),\n",
       " (155, 33.333333332962965, 9.0),\n",
       " (160, 33.333333332962965, 9.0),\n",
       " (172, 33.333333332962965, 9.0),\n",
       " (251, 33.333333332962965, 9.0),\n",
       " (272, 33.333333332962965, 9.0),\n",
       " (277, 33.333333332962965, 9.0),\n",
       " (284, 33.333333332962965, 9.0),\n",
       " (285, 33.333333332962965, 9.0),\n",
       " (299, 33.333333332962965, 9.0),\n",
       " (319, 33.333333332962965, 9.0),\n",
       " (371, 33.333333332962965, 9.0),\n",
       " (384, 33.333333332962965, 9.0),\n",
       " (389, 33.333333332962965, 9.0),\n",
       " (417, 33.333333332962965, 9.0),\n",
       " (471, 33.333333332962965, 9.0),\n",
       " (723, 33.333333332962965, 9.0),\n",
       " (746, 33.333333332962965, 9.0),\n",
       " (768, 33.333333332962965, 9.0),\n",
       " (848, 33.333333332962965, 9.0),\n",
       " (863, 33.333333332962965, 9.0),\n",
       " (884, 33.333333332962965, 9.0),\n",
       " (934, 33.333333332962965, 9.0),\n",
       " (995, 33.333333332962965, 9.0),\n",
       " (185, 33.33333333277778, 6.0),\n",
       " (885, 33.333333332222224, 3.0),\n",
       " (77, 31.24999999980469, 16.0),\n",
       " (44, 30.76923076899408, 13.0),\n",
       " (236, 30.76923076899408, 13.0),\n",
       " (47, 29.9999999997, 10.0),\n",
       " (105, 29.9999999997, 10.0),\n",
       " (121, 29.9999999997, 10.0),\n",
       " (134, 29.9999999997, 10.0),\n",
       " (136, 29.9999999997, 10.0),\n",
       " (140, 29.9999999997, 10.0),\n",
       " (213, 29.9999999997, 10.0),\n",
       " (280, 29.9999999997, 10.0),\n",
       " (316, 29.9999999997, 10.0),\n",
       " (343, 29.9999999997, 10.0),\n",
       " (352, 29.9999999997, 10.0),\n",
       " (390, 29.9999999997, 10.0),\n",
       " (397, 29.9999999997, 10.0),\n",
       " (428, 29.9999999997, 10.0),\n",
       " (496, 29.9999999997, 10.0),\n",
       " (502, 29.9999999997, 10.0),\n",
       " (535, 29.9999999997, 10.0),\n",
       " (561, 29.9999999997, 10.0),\n",
       " (594, 29.9999999997, 10.0),\n",
       " (616, 29.9999999997, 10.0),\n",
       " (703, 29.9999999997, 10.0),\n",
       " (828, 29.9999999997, 10.0),\n",
       " (864, 29.9999999997, 10.0),\n",
       " (873, 29.9999999997, 10.0),\n",
       " (889, 29.9999999997, 10.0),\n",
       " (984, 29.9999999997, 10.0),\n",
       " (202, 28.57142857122449, 14.0),\n",
       " (216, 28.57142857122449, 14.0),\n",
       " (562, 28.57142857122449, 14.0),\n",
       " (649, 28.57142857122449, 14.0),\n",
       " (939, 28.57142857122449, 14.0),\n",
       " (962, 28.57142857122449, 14.0),\n",
       " (144, 28.57142857102041, 7.0),\n",
       " (184, 28.57142857102041, 7.0),\n",
       " (212, 28.57142857102041, 7.0),\n",
       " (231, 28.57142857102041, 7.0),\n",
       " (295, 28.57142857102041, 7.0),\n",
       " (303, 28.57142857102041, 7.0),\n",
       " (309, 28.57142857102041, 7.0),\n",
       " (427, 28.57142857102041, 7.0),\n",
       " (599, 28.57142857102041, 7.0),\n",
       " (792, 28.57142857102041, 7.0),\n",
       " (911, 28.57142857102041, 7.0),\n",
       " (50, 27.27272727247934, 11.0),\n",
       " (51, 27.27272727247934, 11.0),\n",
       " (69, 27.27272727247934, 11.0),\n",
       " (83, 27.27272727247934, 11.0),\n",
       " (135, 27.27272727247934, 11.0),\n",
       " (187, 27.27272727247934, 11.0),\n",
       " (197, 27.27272727247934, 11.0),\n",
       " (242, 27.27272727247934, 11.0),\n",
       " (301, 27.27272727247934, 11.0),\n",
       " (365, 27.27272727247934, 11.0),\n",
       " (369, 27.27272727247934, 11.0),\n",
       " (395, 27.27272727247934, 11.0),\n",
       " (477, 27.27272727247934, 11.0),\n",
       " (520, 27.27272727247934, 11.0),\n",
       " (522, 27.27272727247934, 11.0),\n",
       " (544, 27.27272727247934, 11.0),\n",
       " (577, 27.27272727247934, 11.0),\n",
       " (642, 27.27272727247934, 11.0),\n",
       " (690, 27.27272727247934, 11.0),\n",
       " (716, 27.27272727247934, 11.0),\n",
       " (748, 27.27272727247934, 11.0),\n",
       " (784, 27.27272727247934, 11.0),\n",
       " (795, 27.27272727247934, 11.0),\n",
       " (890, 27.27272727247934, 11.0),\n",
       " (938, 27.27272727247934, 11.0),\n",
       " (991, 27.27272727247934, 11.0),\n",
       " (210, 24.999999999791665, 12.0),\n",
       " (233, 24.999999999791665, 12.0),\n",
       " (314, 24.999999999791665, 12.0),\n",
       " (350, 24.999999999791665, 12.0),\n",
       " (351, 24.999999999791665, 12.0),\n",
       " (361, 24.999999999791665, 12.0),\n",
       " (563, 24.999999999791665, 12.0),\n",
       " (609, 24.999999999791665, 12.0),\n",
       " (675, 24.999999999791665, 12.0),\n",
       " (761, 24.999999999791665, 12.0),\n",
       " (793, 24.999999999791665, 12.0),\n",
       " (821, 24.999999999791665, 12.0),\n",
       " (849, 24.999999999791665, 12.0),\n",
       " (870, 24.999999999791665, 12.0),\n",
       " (35, 24.9999999996875, 8.0),\n",
       " (43, 24.9999999996875, 8.0),\n",
       " (68, 24.9999999996875, 8.0),\n",
       " (122, 24.9999999996875, 8.0),\n",
       " (139, 24.9999999996875, 8.0),\n",
       " (219, 24.9999999996875, 8.0),\n",
       " (230, 24.9999999996875, 8.0),\n",
       " (248, 24.9999999996875, 8.0),\n",
       " (250, 24.9999999996875, 8.0),\n",
       " (268, 24.9999999996875, 8.0),\n",
       " (273, 24.9999999996875, 8.0),\n",
       " (311, 24.9999999996875, 8.0),\n",
       " (370, 24.9999999996875, 8.0),\n",
       " (464, 24.9999999996875, 8.0),\n",
       " (597, 24.9999999996875, 8.0),\n",
       " (620, 24.9999999996875, 8.0),\n",
       " (647, 24.9999999996875, 8.0),\n",
       " (688, 24.9999999996875, 8.0),\n",
       " (727, 24.9999999996875, 8.0),\n",
       " (794, 24.9999999996875, 8.0),\n",
       " (907, 24.9999999996875, 8.0),\n",
       " (914, 24.9999999996875, 8.0),\n",
       " (924, 24.9999999996875, 8.0),\n",
       " (951, 24.9999999996875, 8.0),\n",
       " (711, 24.999999999375, 4.0),\n",
       " (201, 23.07692307674556, 13.0),\n",
       " (472, 23.07692307674556, 13.0),\n",
       " (498, 23.07692307674556, 13.0),\n",
       " (532, 23.07692307674556, 13.0),\n",
       " (777, 23.07692307674556, 13.0),\n",
       " (829, 23.07692307674556, 13.0),\n",
       " (926, 23.07692307674556, 13.0),\n",
       " (933, 23.07692307674556, 13.0),\n",
       " (14, 22.222222221975308, 9.0),\n",
       " (17, 22.222222221975308, 9.0),\n",
       " (34, 22.222222221975308, 9.0),\n",
       " (49, 22.222222221975308, 9.0),\n",
       " (86, 22.222222221975308, 9.0),\n",
       " (132, 22.222222221975308, 9.0),\n",
       " (145, 22.222222221975308, 9.0),\n",
       " (165, 22.222222221975308, 9.0),\n",
       " (171, 22.222222221975308, 9.0),\n",
       " (188, 22.222222221975308, 9.0),\n",
       " (199, 22.222222221975308, 9.0),\n",
       " (206, 22.222222221975308, 9.0),\n",
       " (222, 22.222222221975308, 9.0),\n",
       " (246, 22.222222221975308, 9.0),\n",
       " (255, 22.222222221975308, 9.0),\n",
       " (279, 22.222222221975308, 9.0),\n",
       " (308, 22.222222221975308, 9.0),\n",
       " (338, 22.222222221975308, 9.0),\n",
       " (374, 22.222222221975308, 9.0),\n",
       " (386, 22.222222221975308, 9.0),\n",
       " (392, 22.222222221975308, 9.0),\n",
       " (439, 22.222222221975308, 9.0),\n",
       " (453, 22.222222221975308, 9.0),\n",
       " (455, 22.222222221975308, 9.0),\n",
       " (528, 22.222222221975308, 9.0),\n",
       " (546, 22.222222221975308, 9.0),\n",
       " (560, 22.222222221975308, 9.0),\n",
       " (608, 22.222222221975308, 9.0),\n",
       " (677, 22.222222221975308, 9.0),\n",
       " (850, 22.222222221975308, 9.0),\n",
       " (919, 22.222222221975308, 9.0),\n",
       " (945, 22.222222221975308, 9.0),\n",
       " (981, 22.222222221975308, 9.0),\n",
       " (519, 21.42857142841837, 14.0),\n",
       " (263, 19.999999999866667, 15.0),\n",
       " (572, 19.999999999866667, 15.0),\n",
       " (9, 19.9999999998, 10.0),\n",
       " (58, 19.9999999998, 10.0),\n",
       " (93, 19.9999999998, 10.0),\n",
       " (161, 19.9999999998, 10.0),\n",
       " (180, 19.9999999998, 10.0),\n",
       " (227, 19.9999999998, 10.0),\n",
       " (229, 19.9999999998, 10.0),\n",
       " (323, 19.9999999998, 10.0),\n",
       " (366, 19.9999999998, 10.0),\n",
       " (433, 19.9999999998, 10.0),\n",
       " (466, 19.9999999998, 10.0),\n",
       " (475, 19.9999999998, 10.0),\n",
       " (488, 19.9999999998, 10.0),\n",
       " (491, 19.9999999998, 10.0),\n",
       " (500, 19.9999999998, 10.0),\n",
       " (521, 19.9999999998, 10.0),\n",
       " (574, 19.9999999998, 10.0),\n",
       " (595, 19.9999999998, 10.0),\n",
       " (625, 19.9999999998, 10.0),\n",
       " (628, 19.9999999998, 10.0),\n",
       " (641, 19.9999999998, 10.0),\n",
       " (643, 19.9999999998, 10.0),\n",
       " (682, 19.9999999998, 10.0),\n",
       " (685, 19.9999999998, 10.0),\n",
       " (695, 19.9999999998, 10.0),\n",
       " (715, 19.9999999998, 10.0),\n",
       " (738, 19.9999999998, 10.0),\n",
       " (743, 19.9999999998, 10.0),\n",
       " (776, 19.9999999998, 10.0),\n",
       " (819, 19.9999999998, 10.0),\n",
       " (826, 19.9999999998, 10.0),\n",
       " (856, 19.9999999998, 10.0),\n",
       " (874, 19.9999999998, 10.0),\n",
       " (887, 19.9999999998, 10.0),\n",
       " (916, 19.9999999998, 10.0),\n",
       " (950, 19.9999999998, 10.0),\n",
       " (954, 19.9999999998, 10.0),\n",
       " (733, 19.9999999996, 5.0),\n",
       " (882, 19.9999999996, 5.0),\n",
       " (906, 19.9999999996, 5.0),\n",
       " (912, 19.9999999996, 5.0),\n",
       " (20, 18.181818181652893, 11.0),\n",
       " (150, 18.181818181652893, 11.0),\n",
       " (194, 18.181818181652893, 11.0),\n",
       " (228, 18.181818181652893, 11.0),\n",
       " (245, 18.181818181652893, 11.0),\n",
       " (335, 18.181818181652893, 11.0),\n",
       " (337, 18.181818181652893, 11.0),\n",
       " (413, 18.181818181652893, 11.0),\n",
       " (508, 18.181818181652893, 11.0),\n",
       " (510, 18.181818181652893, 11.0),\n",
       " (559, 18.181818181652893, 11.0),\n",
       " (571, 18.181818181652893, 11.0),\n",
       " (576, 18.181818181652893, 11.0),\n",
       " (607, 18.181818181652893, 11.0),\n",
       " (661, 18.181818181652893, 11.0),\n",
       " (684, 18.181818181652893, 11.0),\n",
       " (698, 18.181818181652893, 11.0),\n",
       " (729, 18.181818181652893, 11.0),\n",
       " (740, 18.181818181652893, 11.0),\n",
       " (754, 18.181818181652893, 11.0),\n",
       " (766, 18.181818181652893, 11.0),\n",
       " (796, 18.181818181652893, 11.0),\n",
       " (888, 18.181818181652893, 11.0),\n",
       " (929, 18.181818181652893, 11.0),\n",
       " (988, 18.181818181652893, 11.0),\n",
       " (982, 16.666666666597223, 24.0),\n",
       " (27, 16.666666666527778, 12.0),\n",
       " (114, 16.666666666527778, 12.0),\n",
       " (186, 16.666666666527778, 12.0),\n",
       " (232, 16.666666666527778, 12.0),\n",
       " (244, 16.666666666527778, 12.0),\n",
       " (249, 16.666666666527778, 12.0),\n",
       " (258, 16.666666666527778, 12.0),\n",
       " (283, 16.666666666527778, 12.0),\n",
       " (297, 16.666666666527778, 12.0),\n",
       " (347, 16.666666666527778, 12.0),\n",
       " (409, 16.666666666527778, 12.0),\n",
       " (492, 16.666666666527778, 12.0),\n",
       " (511, 16.666666666527778, 12.0),\n",
       " (566, 16.666666666527778, 12.0),\n",
       " (590, 16.666666666527778, 12.0),\n",
       " (790, 16.666666666527778, 12.0),\n",
       " (802, 16.666666666527778, 12.0),\n",
       " (803, 16.666666666527778, 12.0),\n",
       " (167, 16.66666666638889, 6.0),\n",
       " (176, 16.66666666638889, 6.0),\n",
       " (266, 16.66666666638889, 6.0),\n",
       " (345, 16.66666666638889, 6.0),\n",
       " (497, 16.66666666638889, 6.0),\n",
       " (663, 16.66666666638889, 6.0),\n",
       " (686, 16.66666666638889, 6.0),\n",
       " (705, 16.66666666638889, 6.0),\n",
       " (734, 16.66666666638889, 6.0),\n",
       " (898, 16.66666666638889, 6.0),\n",
       " (157, 15.38461538449704, 13.0),\n",
       " (274, 15.38461538449704, 13.0),\n",
       " (339, 15.38461538449704, 13.0),\n",
       " (375, 15.38461538449704, 13.0),\n",
       " (379, 15.38461538449704, 13.0),\n",
       " (667, 15.38461538449704, 13.0),\n",
       " (679, 15.38461538449704, 13.0),\n",
       " (755, 15.38461538449704, 13.0),\n",
       " (756, 15.38461538449704, 13.0),\n",
       " (905, 15.38461538449704, 13.0),\n",
       " (990, 15.38461538449704, 13.0),\n",
       " (209, 14.285714285612245, 14.0),\n",
       " (411, 14.285714285612245, 14.0),\n",
       " (431, 14.285714285612245, 14.0),\n",
       " (524, 14.285714285612245, 14.0),\n",
       " (573, 14.285714285612245, 14.0),\n",
       " (830, 14.285714285612245, 14.0),\n",
       " (843, 14.285714285612245, 14.0),\n",
       " (952, 14.285714285612245, 14.0),\n",
       " (196, 14.285714285510204, 7.0),\n",
       " (200, 14.285714285510204, 7.0),\n",
       " (253, 14.285714285510204, 7.0),\n",
       " (356, 14.285714285510204, 7.0),\n",
       " (358, 14.285714285510204, 7.0),\n",
       " (445, 14.285714285510204, 7.0),\n",
       " (513, 14.285714285510204, 7.0),\n",
       " (514, 14.285714285510204, 7.0),\n",
       " (545, 14.285714285510204, 7.0),\n",
       " (551, 14.285714285510204, 7.0),\n",
       " (587, 14.285714285510204, 7.0),\n",
       " (600, 14.285714285510204, 7.0),\n",
       " (772, 14.285714285510204, 7.0),\n",
       " (798, 14.285714285510204, 7.0),\n",
       " (893, 14.285714285510204, 7.0),\n",
       " (909, 14.285714285510204, 7.0),\n",
       " (910, 14.285714285510204, 7.0),\n",
       " (969, 14.285714285510204, 7.0),\n",
       " (480, 13.333333333244445, 15.0),\n",
       " (648, 13.333333333244445, 15.0),\n",
       " (18, 12.49999999984375, 8.0),\n",
       " (26, 12.49999999984375, 8.0),\n",
       " (64, 12.49999999984375, 8.0),\n",
       " (106, 12.49999999984375, 8.0),\n",
       " (164, 12.49999999984375, 8.0),\n",
       " (175, 12.49999999984375, 8.0),\n",
       " (179, 12.49999999984375, 8.0),\n",
       " (224, 12.49999999984375, 8.0),\n",
       " (234, 12.49999999984375, 8.0),\n",
       " (259, 12.49999999984375, 8.0),\n",
       " (318, 12.49999999984375, 8.0),\n",
       " (341, 12.49999999984375, 8.0),\n",
       " (368, 12.49999999984375, 8.0),\n",
       " (419, 12.49999999984375, 8.0),\n",
       " (441, 12.49999999984375, 8.0),\n",
       " (449, 12.49999999984375, 8.0),\n",
       " (469, 12.49999999984375, 8.0),\n",
       " (627, 12.49999999984375, 8.0),\n",
       " (656, 12.49999999984375, 8.0),\n",
       " (699, 12.49999999984375, 8.0),\n",
       " (704, 12.49999999984375, 8.0),\n",
       " (753, 12.49999999984375, 8.0),\n",
       " (757, 12.49999999984375, 8.0),\n",
       " (891, 12.49999999984375, 8.0),\n",
       " (901, 12.49999999984375, 8.0),\n",
       " (943, 12.49999999984375, 8.0),\n",
       " (994, 12.49999999984375, 8.0),\n",
       " (996, 12.49999999984375, 8.0),\n",
       " (1, 11.111111110987654, 9.0),\n",
       " (13, 11.111111110987654, 9.0),\n",
       " (32, 11.111111110987654, 9.0),\n",
       " (36, 11.111111110987654, 9.0),\n",
       " (78, 11.111111110987654, 9.0),\n",
       " (95, 11.111111110987654, 9.0),\n",
       " (107, 11.111111110987654, 9.0),\n",
       " (115, 11.111111110987654, 9.0),\n",
       " (181, 11.111111110987654, 9.0),\n",
       " (204, 11.111111110987654, 9.0),\n",
       " (207, 11.111111110987654, 9.0),\n",
       " (220, 11.111111110987654, 9.0),\n",
       " (261, 11.111111110987654, 9.0),\n",
       " (270, 11.111111110987654, 9.0),\n",
       " (385, 11.111111110987654, 9.0),\n",
       " (402, 11.111111110987654, 9.0),\n",
       " (418, 11.111111110987654, 9.0),\n",
       " (420, 11.111111110987654, 9.0),\n",
       " (447, 11.111111110987654, 9.0),\n",
       " (451, 11.111111110987654, 9.0),\n",
       " (452, 11.111111110987654, 9.0),\n",
       " (462, 11.111111110987654, 9.0),\n",
       " (470, 11.111111110987654, 9.0),\n",
       " (487, 11.111111110987654, 9.0),\n",
       " (507, 11.111111110987654, 9.0),\n",
       " (518, 11.111111110987654, 9.0),\n",
       " (541, 11.111111110987654, 9.0),\n",
       " (584, 11.111111110987654, 9.0),\n",
       " (604, 11.111111110987654, 9.0),\n",
       " (632, 11.111111110987654, 9.0),\n",
       " (639, 11.111111110987654, 9.0),\n",
       " (666, 11.111111110987654, 9.0),\n",
       " (668, 11.111111110987654, 9.0),\n",
       " (719, 11.111111110987654, 9.0),\n",
       " (751, 11.111111110987654, 9.0),\n",
       " (810, 11.111111110987654, 9.0),\n",
       " (816, 11.111111110987654, 9.0),\n",
       " (827, 11.111111110987654, 9.0),\n",
       " (941, 11.111111110987654, 9.0),\n",
       " (6, 9.9999999999, 10.0),\n",
       " (11, 9.9999999999, 10.0),\n",
       " (22, 9.9999999999, 10.0),\n",
       " (63, 9.9999999999, 10.0),\n",
       " (75, 9.9999999999, 10.0),\n",
       " (89, 9.9999999999, 10.0),\n",
       " (178, 9.9999999999, 10.0),\n",
       " (195, 9.9999999999, 10.0),\n",
       " (239, 9.9999999999, 10.0),\n",
       " (252, 9.9999999999, 10.0),\n",
       " (262, 9.9999999999, 10.0),\n",
       " (265, 9.9999999999, 10.0),\n",
       " (286, 9.9999999999, 10.0),\n",
       " (313, 9.9999999999, 10.0),\n",
       " (391, 9.9999999999, 10.0),\n",
       " (408, 9.9999999999, 10.0),\n",
       " (423, 9.9999999999, 10.0),\n",
       " (456, 9.9999999999, 10.0),\n",
       " (530, 9.9999999999, 10.0),\n",
       " (569, 9.9999999999, 10.0),\n",
       " (579, 9.9999999999, 10.0),\n",
       " (606, 9.9999999999, 10.0),\n",
       " (664, 9.9999999999, 10.0),\n",
       " (683, 9.9999999999, 10.0),\n",
       " (728, 9.9999999999, 10.0),\n",
       " (732, 9.9999999999, 10.0),\n",
       " (736, 9.9999999999, 10.0),\n",
       " (739, 9.9999999999, 10.0),\n",
       " (752, 9.9999999999, 10.0),\n",
       " (759, 9.9999999999, 10.0),\n",
       " (760, 9.9999999999, 10.0),\n",
       " (767, 9.9999999999, 10.0),\n",
       " (797, 9.9999999999, 10.0),\n",
       " (801, 9.9999999999, 10.0),\n",
       " (813, 9.9999999999, 10.0),\n",
       " (814, 9.9999999999, 10.0),\n",
       " (867, 9.9999999999, 10.0),\n",
       " (872, 9.9999999999, 10.0),\n",
       " (915, 9.9999999999, 10.0),\n",
       " (920, 9.9999999999, 10.0),\n",
       " (927, 9.9999999999, 10.0),\n",
       " (957, 9.9999999999, 10.0),\n",
       " (971, 9.9999999999, 10.0),\n",
       " (526, 9.523809523764173, 21.0),\n",
       " (10, 9.090909090826447, 11.0),\n",
       " (16, 9.090909090826447, 11.0),\n",
       " (29, 9.090909090826447, 11.0),\n",
       " (42, 9.090909090826447, 11.0),\n",
       " (205, 9.090909090826447, 11.0),\n",
       " (225, 9.090909090826447, 11.0),\n",
       " (257, 9.090909090826447, 11.0),\n",
       " (310, 9.090909090826447, 11.0),\n",
       " (324, 9.090909090826447, 11.0),\n",
       " (344, 9.090909090826447, 11.0),\n",
       " (377, 9.090909090826447, 11.0),\n",
       " (516, 9.090909090826447, 11.0),\n",
       " (630, 9.090909090826447, 11.0),\n",
       " (672, 9.090909090826447, 11.0),\n",
       " (799, 9.090909090826447, 11.0),\n",
       " (817, 9.090909090826447, 11.0),\n",
       " (834, 9.090909090826447, 11.0),\n",
       " (840, 9.090909090826447, 11.0),\n",
       " (842, 9.090909090826447, 11.0),\n",
       " (852, 9.090909090826447, 11.0),\n",
       " (855, 9.090909090826447, 11.0),\n",
       " (860, 9.090909090826447, 11.0),\n",
       " (862, 9.090909090826447, 11.0),\n",
       " (868, 9.090909090826447, 11.0),\n",
       " (881, 9.090909090826447, 11.0),\n",
       " (935, 9.090909090826447, 11.0),\n",
       " (964, 9.090909090826447, 11.0),\n",
       " (966, 9.090909090826447, 11.0),\n",
       " (967, 9.090909090826447, 11.0),\n",
       " (979, 9.090909090826447, 11.0),\n",
       " (2, 8.333333333263889, 12.0),\n",
       " (55, 8.333333333263889, 12.0),\n",
       " (151, 8.333333333263889, 12.0),\n",
       " (315, 8.333333333263889, 12.0),\n",
       " (416, 8.333333333263889, 12.0),\n",
       " (425, 8.333333333263889, 12.0),\n",
       " (432, 8.333333333263889, 12.0),\n",
       " (440, 8.333333333263889, 12.0),\n",
       " (481, 8.333333333263889, 12.0),\n",
       " (523, 8.333333333263889, 12.0),\n",
       " (525, 8.333333333263889, 12.0),\n",
       " (548, 8.333333333263889, 12.0),\n",
       " (593, 8.333333333263889, 12.0),\n",
       " (654, 8.333333333263889, 12.0),\n",
       " (710, 8.333333333263889, 12.0),\n",
       " (714, 8.333333333263889, 12.0),\n",
       " (785, 8.333333333263889, 12.0),\n",
       " (786, 8.333333333263889, 12.0),\n",
       " (789, 8.333333333263889, 12.0),\n",
       " (833, 8.333333333263889, 12.0),\n",
       " (921, 8.333333333263889, 12.0),\n",
       " (931, 8.333333333263889, 12.0),\n",
       " (998, 8.333333333263889, 12.0),\n",
       " (154, 7.69230769224852, 13.0),\n",
       " (159, 7.69230769224852, 13.0),\n",
       " (162, 7.69230769224852, 13.0),\n",
       " (183, 7.69230769224852, 13.0),\n",
       " (237, 7.69230769224852, 13.0),\n",
       " (238, 7.69230769224852, 13.0),\n",
       " (304, 7.69230769224852, 13.0),\n",
       " (312, 7.69230769224852, 13.0),\n",
       " (359, 7.69230769224852, 13.0),\n",
       " (404, 7.69230769224852, 13.0),\n",
       " (442, 7.69230769224852, 13.0),\n",
       " (505, 7.69230769224852, 13.0),\n",
       " (543, 7.69230769224852, 13.0),\n",
       " (591, 7.69230769224852, 13.0),\n",
       " (968, 7.69230769224852, 13.0),\n",
       " (450, 7.142857142806123, 14.0),\n",
       " (708, 7.142857142806123, 14.0),\n",
       " (644, 6.6666666666222225, 15.0),\n",
       " (769, 6.6666666666222225, 15.0),\n",
       " (831, 6.6666666666222225, 15.0),\n",
       " (877, 6.6666666666222225, 15.0),\n",
       " (424, 6.249999999960938, 16.0),\n",
       " (484, 6.249999999960938, 16.0),\n",
       " (564, 6.249999999960938, 16.0),\n",
       " (655, 6.249999999960938, 16.0),\n",
       " (3, 0.0, 8.0),\n",
       " (4, 0.0, 10.0),\n",
       " (5, 0.0, 8.0),\n",
       " (12, 0.0, 9.0),\n",
       " (19, 0.0, 9.0),\n",
       " (21, 0.0, 11.0),\n",
       " (23, 0.0, 8.0),\n",
       " (31, 0.0, 14.0),\n",
       " (40, 0.0, 11.0),\n",
       " (46, 0.0, 7.0),\n",
       " (59, 0.0, 8.0),\n",
       " (60, 0.0, 6.0),\n",
       " (70, 0.0, 8.0),\n",
       " (72, 0.0, 11.0),\n",
       " (73, 0.0, 7.0),\n",
       " (74, 0.0, 7.0),\n",
       " (80, 0.0, 6.0),\n",
       " (92, 0.0, 10.0),\n",
       " (94, 0.0, 9.0),\n",
       " (101, 0.0, 12.0),\n",
       " (103, 0.0, 8.0),\n",
       " (111, 0.0, 12.0),\n",
       " (112, 0.0, 8.0),\n",
       " (117, 0.0, 10.0),\n",
       " (130, 0.0, 11.0),\n",
       " (141, 0.0, 11.0),\n",
       " (142, 0.0, 11.0),\n",
       " (143, 0.0, 12.0),\n",
       " (146, 0.0, 10.0),\n",
       " (147, 0.0, 12.0),\n",
       " (148, 0.0, 10.0),\n",
       " (149, 0.0, 9.0),\n",
       " (152, 0.0, 7.0),\n",
       " (153, 0.0, 13.0),\n",
       " (158, 0.0, 6.0),\n",
       " (166, 0.0, 12.0),\n",
       " (168, 0.0, 8.0),\n",
       " (169, 0.0, 13.0),\n",
       " (190, 0.0, 8.0),\n",
       " (203, 0.0, 12.0),\n",
       " (208, 0.0, 12.0),\n",
       " (215, 0.0, 7.0),\n",
       " (223, 0.0, 11.0),\n",
       " (241, 0.0, 9.0),\n",
       " (256, 0.0, 13.0),\n",
       " (267, 0.0, 9.0),\n",
       " (296, 0.0, 11.0),\n",
       " (306, 0.0, 10.0),\n",
       " (307, 0.0, 9.0),\n",
       " (317, 0.0, 9.0),\n",
       " (320, 0.0, 11.0),\n",
       " (325, 0.0, 9.0),\n",
       " (326, 0.0, 10.0),\n",
       " (332, 0.0, 12.0),\n",
       " (333, 0.0, 9.0),\n",
       " (346, 0.0, 9.0),\n",
       " (354, 0.0, 9.0),\n",
       " (357, 0.0, 7.0),\n",
       " (367, 0.0, 10.0),\n",
       " (373, 0.0, 10.0),\n",
       " (378, 0.0, 13.0),\n",
       " (380, 0.0, 6.0),\n",
       " (399, 0.0, 9.0),\n",
       " (400, 0.0, 10.0),\n",
       " (403, 0.0, 11.0),\n",
       " (405, 0.0, 11.0),\n",
       " (414, 0.0, 10.0),\n",
       " (421, 0.0, 11.0),\n",
       " (422, 0.0, 7.0),\n",
       " (426, 0.0, 10.0),\n",
       " (434, 0.0, 11.0),\n",
       " (435, 0.0, 19.0),\n",
       " (437, 0.0, 13.0),\n",
       " (438, 0.0, 9.0),\n",
       " (446, 0.0, 10.0),\n",
       " (457, 0.0, 12.0),\n",
       " (459, 0.0, 6.0),\n",
       " (460, 0.0, 12.0),\n",
       " (463, 0.0, 11.0),\n",
       " (465, 0.0, 7.0),\n",
       " (473, 0.0, 4.0),\n",
       " (478, 0.0, 7.0),\n",
       " (482, 0.0, 8.0),\n",
       " (485, 0.0, 10.0),\n",
       " (486, 0.0, 7.0),\n",
       " (493, 0.0, 8.0),\n",
       " (494, 0.0, 10.0),\n",
       " (499, 0.0, 6.0),\n",
       " (501, 0.0, 6.0),\n",
       " (503, 0.0, 6.0),\n",
       " (504, 0.0, 9.0),\n",
       " (512, 0.0, 10.0),\n",
       " (515, 0.0, 14.0),\n",
       " (517, 0.0, 10.0),\n",
       " (527, 0.0, 12.0),\n",
       " (529, 0.0, 13.0),\n",
       " (531, 0.0, 10.0),\n",
       " (534, 0.0, 11.0),\n",
       " (536, 0.0, 3.0),\n",
       " (537, 0.0, 9.0),\n",
       " (540, 0.0, 9.0),\n",
       " (542, 0.0, 10.0),\n",
       " (549, 0.0, 10.0),\n",
       " (550, 0.0, 8.0),\n",
       " (552, 0.0, 5.0),\n",
       " (553, 0.0, 10.0),\n",
       " (554, 0.0, 10.0),\n",
       " (556, 0.0, 10.0),\n",
       " (557, 0.0, 10.0),\n",
       " (558, 0.0, 11.0),\n",
       " (567, 0.0, 7.0),\n",
       " (570, 0.0, 10.0),\n",
       " (578, 0.0, 15.0),\n",
       " (583, 0.0, 10.0),\n",
       " (585, 0.0, 7.0),\n",
       " (589, 0.0, 11.0),\n",
       " (592, 0.0, 11.0),\n",
       " (596, 0.0, 5.0),\n",
       " (598, 0.0, 10.0),\n",
       " (601, 0.0, 7.0),\n",
       " (602, 0.0, 10.0),\n",
       " (605, 0.0, 10.0),\n",
       " (610, 0.0, 6.0),\n",
       " (613, 0.0, 6.0),\n",
       " (615, 0.0, 9.0),\n",
       " (617, 0.0, 13.0),\n",
       " (618, 0.0, 7.0),\n",
       " (619, 0.0, 8.0),\n",
       " (622, 0.0, 9.0),\n",
       " (623, 0.0, 7.0),\n",
       " (626, 0.0, 11.0),\n",
       " (629, 0.0, 7.0),\n",
       " (631, 0.0, 13.0),\n",
       " (633, 0.0, 9.0),\n",
       " (635, 0.0, 8.0),\n",
       " (636, 0.0, 8.0),\n",
       " (638, 0.0, 7.0),\n",
       " (650, 0.0, 7.0),\n",
       " (651, 0.0, 14.0),\n",
       " (653, 0.0, 12.0),\n",
       " (657, 0.0, 14.0),\n",
       " (659, 0.0, 11.0),\n",
       " (660, 0.0, 10.0),\n",
       " (662, 0.0, 16.0),\n",
       " (669, 0.0, 8.0),\n",
       " (673, 0.0, 12.0),\n",
       " (674, 0.0, 13.0),\n",
       " (676, 0.0, 8.0),\n",
       " (678, 0.0, 4.0),\n",
       " (680, 0.0, 10.0),\n",
       " (681, 0.0, 7.0),\n",
       " (689, 0.0, 9.0),\n",
       " (691, 0.0, 8.0),\n",
       " (693, 0.0, 7.0),\n",
       " (697, 0.0, 7.0),\n",
       " (700, 0.0, 7.0),\n",
       " (701, 0.0, 10.0),\n",
       " (702, 0.0, 15.0),\n",
       " (709, 0.0, 10.0),\n",
       " (712, 0.0, 10.0),\n",
       " (713, 0.0, 9.0),\n",
       " (718, 0.0, 15.0),\n",
       " (720, 0.0, 8.0),\n",
       " (722, 0.0, 7.0),\n",
       " (724, 0.0, 12.0),\n",
       " (725, 0.0, 7.0),\n",
       " (726, 0.0, 11.0),\n",
       " (731, 0.0, 8.0),\n",
       " (742, 0.0, 7.0),\n",
       " (744, 0.0, 7.0),\n",
       " (745, 0.0, 9.0),\n",
       " (747, 0.0, 8.0),\n",
       " (749, 0.0, 7.0),\n",
       " (750, 0.0, 6.0),\n",
       " (758, 0.0, 10.0),\n",
       " (764, 0.0, 10.0),\n",
       " (770, 0.0, 10.0),\n",
       " (771, 0.0, 8.0),\n",
       " (773, 0.0, 6.0),\n",
       " (774, 0.0, 12.0),\n",
       " (775, 0.0, 13.0),\n",
       " (778, 0.0, 6.0),\n",
       " (780, 0.0, 10.0),\n",
       " (782, 0.0, 7.0),\n",
       " (804, 0.0, 7.0),\n",
       " (807, 0.0, 8.0),\n",
       " (809, 0.0, 8.0),\n",
       " (811, 0.0, 4.0),\n",
       " (812, 0.0, 8.0),\n",
       " (815, 0.0, 8.0),\n",
       " (818, 0.0, 11.0),\n",
       " (823, 0.0, 14.0),\n",
       " (824, 0.0, 4.0),\n",
       " (832, 0.0, 9.0),\n",
       " (836, 0.0, 5.0),\n",
       " (837, 0.0, 13.0),\n",
       " (838, 0.0, 14.0),\n",
       " (841, 0.0, 10.0),\n",
       " (844, 0.0, 11.0),\n",
       " (845, 0.0, 7.0),\n",
       " (846, 0.0, 12.0),\n",
       " (851, 0.0, 10.0),\n",
       " (854, 0.0, 8.0),\n",
       " (859, 0.0, 13.0),\n",
       " (861, 0.0, 12.0),\n",
       " (869, 0.0, 9.0),\n",
       " (871, 0.0, 5.0),\n",
       " (875, 0.0, 12.0),\n",
       " (876, 0.0, 4.0),\n",
       " (883, 0.0, 12.0),\n",
       " (892, 0.0, 4.0),\n",
       " (894, 0.0, 13.0),\n",
       " (895, 0.0, 12.0),\n",
       " (896, 0.0, 11.0),\n",
       " (897, 0.0, 8.0),\n",
       " (899, 0.0, 6.0),\n",
       " (900, 0.0, 9.0),\n",
       " (902, 0.0, 9.0),\n",
       " (903, 0.0, 13.0),\n",
       " (908, 0.0, 7.0),\n",
       " (913, 0.0, 13.0),\n",
       " (923, 0.0, 18.0),\n",
       " (925, 0.0, 6.0),\n",
       " (928, 0.0, 9.0),\n",
       " (930, 0.0, 7.0),\n",
       " (940, 0.0, 9.0),\n",
       " (942, 0.0, 7.0),\n",
       " (948, 0.0, 8.0),\n",
       " (960, 0.0, 11.0),\n",
       " (961, 0.0, 11.0),\n",
       " (965, 0.0, 9.0),\n",
       " (970, 0.0, 11.0),\n",
       " (972, 0.0, 13.0),\n",
       " (974, 0.0, 9.0),\n",
       " (975, 0.0, 11.0),\n",
       " (976, 0.0, 7.0),\n",
       " (977, 0.0, 8.0),\n",
       " (978, 0.0, 7.0),\n",
       " (980, 0.0, 12.0),\n",
       " (983, 0.0, 10.0),\n",
       " (985, 0.0, 13.0),\n",
       " (986, 0.0, 9.0),\n",
       " (989, 0.0, 9.0),\n",
       " (999, 0.0, 5.0)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spt = print_hist(unfooled_histogram, fooled_histogram)\n",
    "spt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "spt_2 = [(37, 99.999999999, 10.0),\n",
    " (340, 99.9999999988889, 9.0),\n",
    " (76, 99.99999999833334, 6.0),\n",
    " (290, 90.90909090826446, 11.0),\n",
    " (84, 88.88888888790123, 9.0),\n",
    " (444, 87.49999999890625, 8.0),\n",
    " (580, 85.71428571367348, 14.0),\n",
    " (109, 85.71428571306123, 7.0),\n",
    " (116, 85.71428571306123, 7.0),\n",
    " (509, 85.71428571306123, 7.0),\n",
    " (545, 85.71428571306123, 7.0),\n",
    " (671, 83.33333333263889, 12.0),\n",
    " (581, 83.33333333194444, 6.0),\n",
    " (99, 79.9999999992, 10.0),\n",
    " (135, 79.9999999992, 10.0),\n",
    " (281, 79.9999999992, 10.0),\n",
    " (640, 79.9999999992, 10.0),\n",
    " (996, 79.9999999984, 5.0),\n",
    " (69, 77.77777777691358, 9.0),\n",
    " (323, 77.77777777691358, 9.0),\n",
    " (765, 77.77777777691358, 9.0),\n",
    " (39, 74.9999999990625, 8.0),\n",
    " (288, 74.9999999990625, 8.0),\n",
    " (300, 74.9999999990625, 8.0),\n",
    " (393, 74.9999999990625, 8.0),\n",
    " (735, 74.9999999990625, 8.0),\n",
    " (898, 74.999999998125, 4.0),\n",
    " (8, 72.72727272661157, 11.0),\n",
    " (123, 72.72727272661157, 11.0),\n",
    " (213, 72.72727272661157, 11.0),\n",
    " (415, 72.72727272661157, 11.0),\n",
    " (646, 72.72727272661157, 11.0),\n",
    " (353, 71.42857142755102, 7.0),\n",
    " (757, 71.42857142755102, 7.0),\n",
    " (878, 71.42857142755102, 7.0),\n",
    " (958, 71.42857142755102, 7.0),\n",
    " (67, 69.9999999993, 10.0),\n",
    " (287, 69.9999999993, 10.0),\n",
    " (327, 69.9999999993, 10.0),\n",
    " (476, 69.9999999993, 10.0),\n",
    " (490, 69.9999999993, 10.0),\n",
    " (721, 69.9999999993, 10.0),\n",
    " (953, 69.9999999993, 10.0),\n",
    " (612, 69.23076923023669, 13.0),\n",
    " (56, 66.66666666611111, 12.0),\n",
    " (687, 66.66666666611111, 12.0),\n",
    " (45, 66.66666666592593, 9.0),\n",
    " (90, 66.66666666592593, 9.0),\n",
    " (177, 66.66666666592593, 9.0),\n",
    " (182, 66.66666666592593, 9.0),\n",
    " (293, 66.66666666592593, 9.0),\n",
    " (294, 66.66666666592593, 9.0),\n",
    " (330, 66.66666666592593, 9.0),\n",
    " (363, 66.66666666592593, 9.0),\n",
    " (396, 66.66666666592593, 9.0),\n",
    " (611, 66.66666666592593, 9.0),\n",
    " (791, 66.66666666592593, 9.0),\n",
    " (863, 66.66666666592593, 9.0),\n",
    " (946, 66.66666666592593, 9.0),\n",
    " (582, 66.66666666555555, 6.0),\n",
    " (654, 66.66666666555555, 6.0),\n",
    " (25, 63.63636363578512, 11.0),\n",
    " (82, 63.63636363578512, 11.0),\n",
    " (110, 63.63636363578512, 11.0),\n",
    " (698, 63.63636363578512, 11.0),\n",
    " (737, 63.63636363578512, 11.0),\n",
    " (917, 63.63636363578512, 11.0),\n",
    " (36, 62.49999999921875, 8.0),\n",
    " (41, 62.49999999921875, 8.0),\n",
    " (790, 62.49999999921875, 8.0),\n",
    " (832, 62.49999999921875, 8.0),\n",
    " (218, 61.53846153798816, 13.0),\n",
    " (50, 59.9999999994, 10.0),\n",
    " (83, 59.9999999994, 10.0),\n",
    " (251, 59.9999999994, 10.0),\n",
    " (320, 59.9999999994, 10.0),\n",
    " (455, 59.9999999994, 10.0),\n",
    " (621, 59.9999999994, 10.0),\n",
    " (645, 59.9999999994, 10.0),\n",
    " (984, 59.9999999994, 10.0),\n",
    " (992, 59.9999999994, 10.0),\n",
    " (112, 59.9999999988, 5.0),\n",
    " (298, 59.9999999988, 5.0),\n",
    " (44, 58.333333332847225, 12.0),\n",
    " (800, 58.333333332847225, 12.0),\n",
    " (198, 57.14285714204082, 7.0),\n",
    " (334, 57.14285714204082, 7.0),\n",
    " (613, 57.14285714204082, 7.0),\n",
    " (741, 57.14285714204082, 7.0),\n",
    " (762, 57.14285714204082, 7.0),\n",
    " (808, 57.14285714204082, 7.0),\n",
    " (884, 57.14285714204082, 7.0),\n",
    " (65, 55.55555555493827, 9.0),\n",
    " (81, 55.55555555493827, 9.0),\n",
    " (85, 55.55555555493827, 9.0),\n",
    " (102, 55.55555555493827, 9.0),\n",
    " (299, 55.55555555493827, 9.0),\n",
    " (321, 55.55555555493827, 9.0),\n",
    " (328, 55.55555555493827, 9.0),\n",
    " (412, 55.55555555493827, 9.0),\n",
    " (458, 55.55555555493827, 9.0),\n",
    " (498, 55.55555555493827, 9.0),\n",
    " (538, 55.55555555493827, 9.0),\n",
    " (565, 55.55555555493827, 9.0),\n",
    " (643, 55.55555555493827, 9.0),\n",
    " (766, 55.55555555493827, 9.0),\n",
    " (779, 55.55555555493827, 9.0),\n",
    " (853, 55.55555555493827, 9.0),\n",
    " (858, 55.55555555493827, 9.0),\n",
    " (47, 54.54545454495868, 11.0),\n",
    " (217, 54.54545454495868, 11.0),\n",
    " (289, 54.54545454495868, 11.0),\n",
    " (355, 54.54545454495868, 11.0),\n",
    " (547, 54.54545454495868, 11.0),\n",
    " (575, 54.54545454495868, 11.0),\n",
    " (665, 53.84615384573964, 13.0),\n",
    " (489, 53.33333333297778, 15.0),\n",
    " (495, 49.99999999964286, 14.0),\n",
    " (269, 49.99999999958333, 12.0),\n",
    " (342, 49.99999999958333, 12.0),\n",
    " (48, 49.9999999995, 10.0),\n",
    " (87, 49.9999999995, 10.0),\n",
    " (138, 49.9999999995, 10.0),\n",
    " (276, 49.9999999995, 10.0),\n",
    " (292, 49.9999999995, 10.0),\n",
    " (352, 49.9999999995, 10.0),\n",
    " (398, 49.9999999995, 10.0),\n",
    " (425, 49.9999999995, 10.0),\n",
    " (430, 49.9999999995, 10.0),\n",
    " (454, 49.9999999995, 10.0),\n",
    " (624, 49.9999999995, 10.0),\n",
    " (825, 49.9999999995, 10.0),\n",
    " (847, 49.9999999995, 10.0),\n",
    " (947, 49.9999999995, 10.0),\n",
    " (991, 49.9999999995, 10.0),\n",
    " (100, 49.999999999375, 8.0),\n",
    " (219, 49.999999999375, 8.0),\n",
    " (240, 49.999999999375, 8.0),\n",
    " (247, 49.999999999375, 8.0),\n",
    " (277, 49.999999999375, 8.0),\n",
    " (284, 49.999999999375, 8.0),\n",
    " (302, 49.999999999375, 8.0),\n",
    " (364, 49.999999999375, 8.0),\n",
    " (417, 49.999999999375, 8.0),\n",
    " (522, 49.999999999375, 8.0),\n",
    " (748, 49.999999999375, 8.0),\n",
    " (801, 49.999999999375, 8.0),\n",
    " (815, 49.999999999375, 8.0),\n",
    " (821, 49.999999999375, 8.0),\n",
    " (854, 49.999999999375, 8.0),\n",
    " (921, 49.999999999375, 8.0),\n",
    " (17, 49.99999999916667, 6.0),\n",
    " (46, 49.99999999916667, 6.0),\n",
    " (54, 49.99999999916667, 6.0),\n",
    " (362, 49.99999999916667, 6.0),\n",
    " (694, 49.99999999916667, 6.0),\n",
    " (711, 49.99999999916667, 6.0),\n",
    " (807, 49.99999999916667, 6.0),\n",
    " (824, 49.99999999916667, 6.0),\n",
    " (839, 49.99999999916667, 6.0),\n",
    " (873, 49.99999999916667, 6.0),\n",
    " (904, 49.99999999916667, 6.0),\n",
    " (911, 49.99999999916667, 6.0),\n",
    " (536, 49.99999999875, 4.0),\n",
    " (474, 46.66666666635555, 15.0),\n",
    " (410, 46.15384615349112, 13.0),\n",
    " (849, 46.15384615349112, 13.0),\n",
    " (955, 46.15384615349112, 13.0),\n",
    " (57, 45.45454545413223, 11.0),\n",
    " (191, 45.45454545413223, 11.0),\n",
    " (243, 45.45454545413223, 11.0),\n",
    " (386, 45.45454545413223, 11.0),\n",
    " (524, 45.45454545413223, 11.0),\n",
    " (609, 45.45454545413223, 11.0),\n",
    " (716, 45.45454545413223, 11.0),\n",
    " (729, 45.45454545413223, 11.0),\n",
    " (918, 45.45454545413223, 11.0),\n",
    " (24, 44.444444443950616, 9.0),\n",
    " (93, 44.444444443950616, 9.0),\n",
    " (129, 44.444444443950616, 9.0),\n",
    " (133, 44.444444443950616, 9.0),\n",
    " (174, 44.444444443950616, 9.0),\n",
    " (196, 44.444444443950616, 9.0),\n",
    " (210, 44.444444443950616, 9.0),\n",
    " (270, 44.444444443950616, 9.0),\n",
    " (275, 44.444444443950616, 9.0),\n",
    " (314, 44.444444443950616, 9.0),\n",
    " (322, 44.444444443950616, 9.0),\n",
    " (383, 44.444444443950616, 9.0),\n",
    " (462, 44.444444443950616, 9.0),\n",
    " (506, 44.444444443950616, 9.0),\n",
    " (566, 44.444444443950616, 9.0),\n",
    " (594, 44.444444443950616, 9.0),\n",
    " (706, 44.444444443950616, 9.0),\n",
    " (752, 44.444444443950616, 9.0),\n",
    " (993, 44.444444443950616, 9.0),\n",
    " (406, 42.85714285683674, 14.0),\n",
    " (35, 42.857142856530615, 7.0),\n",
    " (62, 42.857142856530615, 7.0),\n",
    " (79, 42.857142856530615, 7.0),\n",
    " (97, 42.857142856530615, 7.0),\n",
    " (120, 42.857142856530615, 7.0),\n",
    " (139, 42.857142856530615, 7.0),\n",
    " (343, 42.857142856530615, 7.0),\n",
    " (360, 42.857142856530615, 7.0),\n",
    " (371, 42.857142856530615, 7.0),\n",
    " (389, 42.857142856530615, 7.0),\n",
    " (401, 42.857142856530615, 7.0),\n",
    " (599, 42.857142856530615, 7.0),\n",
    " (635, 42.857142856530615, 7.0),\n",
    " (746, 42.857142856530615, 7.0),\n",
    " (768, 42.857142856530615, 7.0),\n",
    " (912, 42.857142856530615, 7.0),\n",
    " (221, 41.66666666631944, 12.0),\n",
    " (407, 41.66666666631944, 12.0),\n",
    " (562, 41.66666666631944, 12.0),\n",
    " (794, 41.66666666631944, 12.0),\n",
    " (865, 41.66666666631944, 12.0),\n",
    " (905, 41.66666666631944, 12.0),\n",
    " (533, 39.99999999973333, 15.0),\n",
    " (68, 39.9999999996, 10.0),\n",
    " (71, 39.9999999996, 10.0),\n",
    " (108, 39.9999999996, 10.0),\n",
    " (156, 39.9999999996, 10.0),\n",
    " (351, 39.9999999996, 10.0),\n",
    " (388, 39.9999999996, 10.0),\n",
    " (468, 39.9999999996, 10.0),\n",
    " (652, 39.9999999996, 10.0),\n",
    " (730, 39.9999999996, 10.0),\n",
    " (987, 39.9999999996, 10.0),\n",
    " (38, 39.9999999992, 5.0),\n",
    " (427, 39.9999999992, 5.0),\n",
    " (449, 39.9999999992, 5.0),\n",
    " (254, 38.461538461242604, 13.0),\n",
    " (866, 38.461538461242604, 13.0),\n",
    " (788, 37.49999999976563, 16.0),\n",
    " (12, 37.49999999953125, 8.0),\n",
    " (14, 37.49999999953125, 8.0),\n",
    " (43, 37.49999999953125, 8.0),\n",
    " (52, 37.49999999953125, 8.0),\n",
    " (63, 37.49999999953125, 8.0),\n",
    " (88, 37.49999999953125, 8.0),\n",
    " (91, 37.49999999953125, 8.0),\n",
    " (113, 37.49999999953125, 8.0),\n",
    " (140, 37.49999999953125, 8.0),\n",
    " (161, 37.49999999953125, 8.0),\n",
    " (195, 37.49999999953125, 8.0),\n",
    " (201, 37.49999999953125, 8.0),\n",
    " (203, 37.49999999953125, 8.0),\n",
    " (273, 37.49999999953125, 8.0),\n",
    " (280, 37.49999999953125, 8.0),\n",
    " (307, 37.49999999953125, 8.0),\n",
    " (426, 37.49999999953125, 8.0),\n",
    " (467, 37.49999999953125, 8.0),\n",
    " (571, 37.49999999953125, 8.0),\n",
    " (595, 37.49999999953125, 8.0),\n",
    " (670, 37.49999999953125, 8.0),\n",
    " (695, 37.49999999953125, 8.0),\n",
    " (723, 37.49999999953125, 8.0),\n",
    " (860, 37.49999999953125, 8.0),\n",
    " (886, 37.49999999953125, 8.0),\n",
    " (900, 37.49999999953125, 8.0),\n",
    " (128, 36.36363636330579, 11.0),\n",
    " (382, 36.36363636330579, 11.0),\n",
    " (387, 36.36363636330579, 11.0),\n",
    " (408, 36.36363636330579, 11.0),\n",
    " (414, 36.36363636330579, 11.0),\n",
    " (573, 36.36363636330579, 11.0),\n",
    " (483, 33.333333333055556, 12.0),\n",
    " (523, 33.333333333055556, 12.0),\n",
    " (919, 33.333333333055556, 12.0),\n",
    " (7, 33.333333332962965, 9.0),\n",
    " (11, 33.333333332962965, 9.0),\n",
    " (28, 33.333333332962965, 9.0),\n",
    " (61, 33.333333332962965, 9.0),\n",
    " (96, 33.333333332962965, 9.0),\n",
    " (126, 33.333333332962965, 9.0),\n",
    " (131, 33.333333332962965, 9.0),\n",
    " (136, 33.333333332962965, 9.0),\n",
    " (142, 33.333333332962965, 9.0),\n",
    " (162, 33.333333332962965, 9.0),\n",
    " (173, 33.333333332962965, 9.0),\n",
    " (186, 33.333333332962965, 9.0),\n",
    " (192, 33.333333332962965, 9.0),\n",
    " (193, 33.333333332962965, 9.0),\n",
    " (197, 33.333333332962965, 9.0),\n",
    " (204, 33.333333332962965, 9.0),\n",
    " (229, 33.333333332962965, 9.0),\n",
    " (305, 33.333333332962965, 9.0),\n",
    " (312, 33.333333332962965, 9.0),\n",
    " (365, 33.333333332962965, 9.0),\n",
    " (440, 33.333333332962965, 9.0),\n",
    " (472, 33.333333332962965, 9.0),\n",
    " (487, 33.333333332962965, 9.0),\n",
    " (491, 33.333333332962965, 9.0),\n",
    " (531, 33.333333332962965, 9.0),\n",
    " (556, 33.333333332962965, 9.0),\n",
    " (614, 33.333333332962965, 9.0),\n",
    " (625, 33.333333332962965, 9.0),\n",
    " (642, 33.333333332962965, 9.0),\n",
    " (661, 33.333333332962965, 9.0),\n",
    " (753, 33.333333332962965, 9.0),\n",
    " (783, 33.333333332962965, 9.0),\n",
    " (820, 33.333333332962965, 9.0),\n",
    " (857, 33.333333332962965, 9.0),\n",
    " (870, 33.333333332962965, 9.0),\n",
    " (922, 33.333333332962965, 9.0),\n",
    " (956, 33.333333332962965, 9.0),\n",
    " (970, 33.333333332962965, 9.0),\n",
    " (988, 33.333333332962965, 9.0),\n",
    " (86, 33.33333333277778, 6.0),\n",
    " (172, 33.33333333277778, 6.0),\n",
    " (373, 33.33333333277778, 6.0),\n",
    " (470, 33.33333333277778, 6.0),\n",
    " (514, 33.33333333277778, 6.0),\n",
    " (601, 33.33333333277778, 6.0),\n",
    " (637, 33.33333333277778, 6.0),\n",
    " (664, 33.33333333277778, 6.0),\n",
    " (734, 33.33333333277778, 6.0),\n",
    " (772, 33.33333333277778, 6.0),\n",
    " (792, 33.33333333277778, 6.0),\n",
    " (826, 33.33333333277778, 6.0),\n",
    " (250, 33.333333332222224, 3.0),\n",
    " (282, 33.333333332222224, 3.0),\n",
    " (663, 33.333333332222224, 3.0),\n",
    " (572, 30.76923076899408, 13.0),\n",
    " (660, 30.76923076899408, 13.0),\n",
    " (952, 30.76923076899408, 13.0),\n",
    " (72, 29.9999999997, 10.0),\n",
    " (119, 29.9999999997, 10.0),\n",
    " (134, 29.9999999997, 10.0),\n",
    " (137, 29.9999999997, 10.0),\n",
    " (216, 29.9999999997, 10.0),\n",
    " (235, 29.9999999997, 10.0),\n",
    " (237, 29.9999999997, 10.0),\n",
    " (272, 29.9999999997, 10.0),\n",
    " (316, 29.9999999997, 10.0),\n",
    " (335, 29.9999999997, 10.0),\n",
    " (337, 29.9999999997, 10.0),\n",
    " (372, 29.9999999997, 10.0),\n",
    " (375, 29.9999999997, 10.0),\n",
    " (394, 29.9999999997, 10.0),\n",
    " (436, 29.9999999997, 10.0),\n",
    " (448, 29.9999999997, 10.0),\n",
    " (525, 29.9999999997, 10.0),\n",
    " (576, 29.9999999997, 10.0),\n",
    " (775, 29.9999999997, 10.0),\n",
    " (806, 29.9999999997, 10.0),\n",
    " (828, 29.9999999997, 10.0),\n",
    " (877, 29.9999999997, 10.0),\n",
    " (903, 29.9999999997, 10.0),\n",
    " (944, 29.9999999997, 10.0),\n",
    " (949, 29.9999999997, 10.0),\n",
    " (963, 29.9999999997, 10.0),\n",
    " (409, 28.57142857122449, 14.0),\n",
    " (692, 28.57142857122449, 14.0),\n",
    " (49, 28.57142857102041, 7.0),\n",
    " (70, 28.57142857102041, 7.0),\n",
    " (144, 28.57142857102041, 7.0),\n",
    " (164, 28.57142857102041, 7.0),\n",
    " (170, 28.57142857102041, 7.0),\n",
    " (171, 28.57142857102041, 7.0),\n",
    " (185, 28.57142857102041, 7.0),\n",
    " (230, 28.57142857102041, 7.0),\n",
    " (242, 28.57142857102041, 7.0),\n",
    " (264, 28.57142857102041, 7.0),\n",
    " (271, 28.57142857102041, 7.0),\n",
    " (350, 28.57142857102041, 7.0),\n",
    " (453, 28.57142857102041, 7.0),\n",
    " (469, 28.57142857102041, 7.0),\n",
    " (497, 28.57142857102041, 7.0),\n",
    " (604, 28.57142857102041, 7.0),\n",
    " (787, 28.57142857102041, 7.0),\n",
    " (856, 28.57142857102041, 7.0),\n",
    " (880, 28.57142857102041, 7.0),\n",
    " (936, 28.57142857102041, 7.0),\n",
    " (0, 27.27272727247934, 11.0),\n",
    " (16, 27.27272727247934, 11.0),\n",
    " (55, 27.27272727247934, 11.0),\n",
    " (189, 27.27272727247934, 11.0),\n",
    " (234, 27.27272727247934, 11.0),\n",
    " (349, 27.27272727247934, 11.0),\n",
    " (563, 27.27272727247934, 11.0),\n",
    " (588, 27.27272727247934, 11.0),\n",
    " (634, 27.27272727247934, 11.0),\n",
    " (656, 27.27272727247934, 11.0),\n",
    " (763, 27.27272727247934, 11.0),\n",
    " (915, 27.27272727247934, 11.0),\n",
    " (939, 27.27272727247934, 11.0),\n",
    " (981, 27.27272727247934, 11.0),\n",
    " (985, 27.27272727247934, 11.0),\n",
    " (77, 26.66666666648889, 15.0),\n",
    " (559, 26.66666666648889, 15.0),\n",
    " (157, 24.999999999791665, 12.0),\n",
    " (236, 24.999999999791665, 12.0),\n",
    " (263, 24.999999999791665, 12.0),\n",
    " (347, 24.999999999791665, 12.0),\n",
    " (603, 24.999999999791665, 12.0),\n",
    " (888, 24.999999999791665, 12.0),\n",
    " (51, 24.9999999996875, 8.0),\n",
    " (89, 24.9999999996875, 8.0),\n",
    " (121, 24.9999999996875, 8.0),\n",
    " (127, 24.9999999996875, 8.0),\n",
    " (155, 24.9999999996875, 8.0),\n",
    " (160, 24.9999999996875, 8.0),\n",
    " (178, 24.9999999996875, 8.0),\n",
    " (181, 24.9999999996875, 8.0),\n",
    " (232, 24.9999999996875, 8.0),\n",
    " (239, 24.9999999996875, 8.0),\n",
    " (257, 24.9999999996875, 8.0),\n",
    " (285, 24.9999999996875, 8.0),\n",
    " (295, 24.9999999996875, 8.0),\n",
    " (319, 24.9999999996875, 8.0),\n",
    " (325, 24.9999999996875, 8.0),\n",
    " (331, 24.9999999996875, 8.0),\n",
    " (336, 24.9999999996875, 8.0),\n",
    " (348, 24.9999999996875, 8.0),\n",
    " (381, 24.9999999996875, 8.0),\n",
    " (392, 24.9999999996875, 8.0),\n",
    " (395, 24.9999999996875, 8.0),\n",
    " (420, 24.9999999996875, 8.0),\n",
    " (443, 24.9999999996875, 8.0),\n",
    " (539, 24.9999999996875, 8.0),\n",
    " (543, 24.9999999996875, 8.0),\n",
    " (546, 24.9999999996875, 8.0),\n",
    " (600, 24.9999999996875, 8.0),\n",
    " (636, 24.9999999996875, 8.0),\n",
    " (641, 24.9999999996875, 8.0),\n",
    " (668, 24.9999999996875, 8.0),\n",
    " (688, 24.9999999996875, 8.0),\n",
    " (690, 24.9999999996875, 8.0),\n",
    " (715, 24.9999999996875, 8.0),\n",
    " (749, 24.9999999996875, 8.0),\n",
    " (770, 24.9999999996875, 8.0),\n",
    " (798, 24.9999999996875, 8.0),\n",
    " (835, 24.9999999996875, 8.0),\n",
    " (842, 24.9999999996875, 8.0),\n",
    " (891, 24.9999999996875, 8.0),\n",
    " (906, 24.9999999996875, 8.0),\n",
    " (932, 24.9999999996875, 8.0),\n",
    " (959, 24.9999999996875, 8.0),\n",
    " (997, 24.9999999996875, 8.0),\n",
    " (461, 24.999999999375, 4.0),\n",
    " (479, 24.999999999375, 4.0),\n",
    " (708, 24.999999999375, 4.0),\n",
    " (744, 24.999999999375, 4.0),\n",
    " (377, 23.07692307674556, 13.0),\n",
    " (457, 23.07692307674556, 13.0),\n",
    " (564, 23.07692307674556, 13.0),\n",
    " (648, 23.07692307674556, 13.0),\n",
    " (805, 23.07692307674556, 13.0),\n",
    " (829, 23.07692307674556, 13.0),\n",
    " (840, 23.07692307674556, 13.0),\n",
    " (9, 22.222222221975308, 9.0),\n",
    " (19, 22.222222221975308, 9.0),\n",
    " (117, 22.222222221975308, 9.0),\n",
    " (132, 22.222222221975308, 9.0),\n",
    " (141, 22.222222221975308, 9.0),\n",
    " (145, 22.222222221975308, 9.0),\n",
    " (194, 22.222222221975308, 9.0),\n",
    " (222, 22.222222221975308, 9.0),\n",
    " (226, 22.222222221975308, 9.0),\n",
    " (227, 22.222222221975308, 9.0),\n",
    " (265, 22.222222221975308, 9.0),\n",
    " (279, 22.222222221975308, 9.0),\n",
    " (318, 22.222222221975308, 9.0),\n",
    " (366, 22.222222221975308, 9.0),\n",
    " (370, 22.222222221975308, 9.0),\n",
    " (376, 22.222222221975308, 9.0),\n",
    " (466, 22.222222221975308, 9.0),\n",
    " (496, 22.222222221975308, 9.0),\n",
    " (502, 22.222222221975308, 9.0),\n",
    " (541, 22.222222221975308, 9.0),\n",
    " (555, 22.222222221975308, 9.0),\n",
    " (574, 22.222222221975308, 9.0),\n",
    " (589, 22.222222221975308, 9.0),\n",
    " (597, 22.222222221975308, 9.0),\n",
    " (607, 22.222222221975308, 9.0),\n",
    " (608, 22.222222221975308, 9.0),\n",
    " (610, 22.222222221975308, 9.0),\n",
    " (658, 22.222222221975308, 9.0),\n",
    " (685, 22.222222221975308, 9.0),\n",
    " (702, 22.222222221975308, 9.0),\n",
    " (707, 22.222222221975308, 9.0),\n",
    " (750, 22.222222221975308, 9.0),\n",
    " (751, 22.222222221975308, 9.0),\n",
    " (759, 22.222222221975308, 9.0),\n",
    " (761, 22.222222221975308, 9.0),\n",
    " (796, 22.222222221975308, 9.0),\n",
    " (816, 22.222222221975308, 9.0),\n",
    " (937, 22.222222221975308, 9.0),\n",
    " (249, 21.42857142841837, 14.0),\n",
    " (982, 21.052631578836568, 19.0),\n",
    " (15, 19.9999999998, 10.0),\n",
    " (42, 19.9999999998, 10.0),\n",
    " (124, 19.9999999998, 10.0),\n",
    " (163, 19.9999999998, 10.0),\n",
    " (184, 19.9999999998, 10.0),\n",
    " (205, 19.9999999998, 10.0),\n",
    " (211, 19.9999999998, 10.0),\n",
    " (214, 19.9999999998, 10.0),\n",
    " (238, 19.9999999998, 10.0),\n",
    " (245, 19.9999999998, 10.0),\n",
    " (283, 19.9999999998, 10.0),\n",
    " (339, 19.9999999998, 10.0),\n",
    " (359, 19.9999999998, 10.0),\n",
    " (361, 19.9999999998, 10.0),\n",
    " (411, 19.9999999998, 10.0),\n",
    " (429, 19.9999999998, 10.0),\n",
    " (560, 19.9999999998, 10.0),\n",
    " (616, 19.9999999998, 10.0),\n",
    " (781, 19.9999999998, 10.0),\n",
    " (795, 19.9999999998, 10.0),\n",
    " (887, 19.9999999998, 10.0),\n",
    " (916, 19.9999999998, 10.0),\n",
    " (990, 19.9999999998, 10.0),\n",
    " (994, 19.9999999998, 10.0),\n",
    " (66, 19.9999999996, 5.0),\n",
    " (266, 19.9999999996, 5.0),\n",
    " (341, 19.9999999996, 5.0),\n",
    " (552, 19.9999999996, 5.0),\n",
    " (569, 19.9999999996, 5.0),\n",
    " (596, 19.9999999996, 5.0),\n",
    " (686, 19.9999999996, 5.0),\n",
    " (696, 19.9999999996, 5.0),\n",
    " (950, 19.9999999996, 5.0),\n",
    " (34, 18.181818181652893, 11.0),\n",
    " (58, 18.181818181652893, 11.0),\n",
    " (199, 18.181818181652893, 11.0),\n",
    " (246, 18.181818181652893, 11.0),\n",
    " (258, 18.181818181652893, 11.0),\n",
    " (262, 18.181818181652893, 11.0),\n",
    " (301, 18.181818181652893, 11.0),\n",
    " (481, 18.181818181652893, 11.0),\n",
    " (517, 18.181818181652893, 11.0),\n",
    " (561, 18.181818181652893, 11.0),\n",
    " (674, 18.181818181652893, 11.0),\n",
    " (784, 18.181818181652893, 11.0),\n",
    " (789, 18.181818181652893, 11.0),\n",
    " (843, 18.181818181652893, 11.0),\n",
    " (850, 18.181818181652893, 11.0),\n",
    " (979, 18.181818181652893, 11.0),\n",
    " (995, 18.181818181652893, 11.0),\n",
    " (274, 16.666666666527778, 12.0),\n",
    " (413, 16.666666666527778, 12.0),\n",
    " (424, 16.666666666527778, 12.0),\n",
    " (577, 16.666666666527778, 12.0),\n",
    " (679, 16.666666666527778, 12.0),\n",
    " (738, 16.666666666527778, 12.0),\n",
    " (769, 16.666666666527778, 12.0),\n",
    " (846, 16.666666666527778, 12.0),\n",
    " (881, 16.666666666527778, 12.0),\n",
    " (152, 16.66666666638889, 6.0),\n",
    " (176, 16.66666666638889, 6.0),\n",
    " (313, 16.66666666638889, 6.0),\n",
    " (482, 16.66666666638889, 6.0),\n",
    " (550, 16.66666666638889, 6.0),\n",
    " (699, 16.66666666638889, 6.0),\n",
    " (705, 16.66666666638889, 6.0),\n",
    " (725, 16.66666666638889, 6.0),\n",
    " (773, 16.66666666638889, 6.0),\n",
    " (907, 16.66666666638889, 6.0),\n",
    " (924, 16.66666666638889, 6.0),\n",
    " (964, 16.66666666638889, 6.0),\n",
    " (151, 15.38461538449704, 13.0),\n",
    " (568, 15.38461538449704, 13.0),\n",
    " (578, 15.38461538449704, 13.0),\n",
    " (703, 15.38461538449704, 13.0),\n",
    " (793, 15.38461538449704, 13.0),\n",
    " (510, 14.285714285612245, 14.0),\n",
    " (532, 14.285714285612245, 14.0),\n",
    " (32, 14.285714285510204, 7.0),\n",
    " (59, 14.285714285510204, 7.0),\n",
    " (73, 14.285714285510204, 7.0),\n",
    " (78, 14.285714285510204, 7.0),\n",
    " (165, 14.285714285510204, 7.0),\n",
    " (166, 14.285714285510204, 7.0),\n",
    " (255, 14.285714285510204, 7.0),\n",
    " (261, 14.285714285510204, 7.0),\n",
    " (267, 14.285714285510204, 7.0),\n",
    " (303, 14.285714285510204, 7.0),\n",
    " (329, 14.285714285510204, 7.0),\n",
    " (356, 14.285714285510204, 7.0),\n",
    " (358, 14.285714285510204, 7.0),\n",
    " (384, 14.285714285510204, 7.0),\n",
    " (439, 14.285714285510204, 7.0),\n",
    " (456, 14.285714285510204, 7.0),\n",
    " (464, 14.285714285510204, 7.0),\n",
    " (507, 14.285714285510204, 7.0),\n",
    " (513, 14.285714285510204, 7.0),\n",
    " (516, 14.285714285510204, 7.0),\n",
    " (520, 14.285714285510204, 7.0),\n",
    " (528, 14.285714285510204, 7.0),\n",
    " (537, 14.285714285510204, 7.0),\n",
    " (683, 14.285714285510204, 7.0),\n",
    " (689, 14.285714285510204, 7.0),\n",
    " (709, 14.285714285510204, 7.0),\n",
    " (710, 14.285714285510204, 7.0),\n",
    " (714, 14.285714285510204, 7.0),\n",
    " (727, 14.285714285510204, 7.0),\n",
    " (733, 14.285714285510204, 7.0),\n",
    " (780, 14.285714285510204, 7.0),\n",
    " (844, 14.285714285510204, 7.0),\n",
    " (845, 14.285714285510204, 7.0),\n",
    " (879, 14.285714285510204, 7.0),\n",
    " (951, 14.285714285510204, 7.0),\n",
    " (18, 12.49999999984375, 8.0),\n",
    " (29, 12.49999999984375, 8.0),\n",
    " (64, 12.49999999984375, 8.0),\n",
    " (94, 12.49999999984375, 8.0),\n",
    " (95, 12.49999999984375, 8.0),\n",
    " (98, 12.49999999984375, 8.0),\n",
    " (148, 12.49999999984375, 8.0),\n",
    " (188, 12.49999999984375, 8.0),\n",
    " (190, 12.49999999984375, 8.0),\n",
    " (200, 12.49999999984375, 8.0),\n",
    " (248, 12.49999999984375, 8.0),\n",
    " (253, 12.49999999984375, 8.0),\n",
    " (286, 12.49999999984375, 8.0),\n",
    " (291, 12.49999999984375, 8.0),\n",
    " (306, 12.49999999984375, 8.0),\n",
    " (345, 12.49999999984375, 8.0),\n",
    " (390, 12.49999999984375, 8.0),\n",
    " (405, 12.49999999984375, 8.0),\n",
    " (432, 12.49999999984375, 8.0),\n",
    " (471, 12.49999999984375, 8.0),\n",
    " (488, 12.49999999984375, 8.0),\n",
    " (554, 12.49999999984375, 8.0),\n",
    " (598, 12.49999999984375, 8.0),\n",
    " (622, 12.49999999984375, 8.0),\n",
    " (630, 12.49999999984375, 8.0),\n",
    " (677, 12.49999999984375, 8.0),\n",
    " (717, 12.49999999984375, 8.0),\n",
    " (718, 12.49999999984375, 8.0),\n",
    " (728, 12.49999999984375, 8.0),\n",
    " (767, 12.49999999984375, 8.0),\n",
    " (776, 12.49999999984375, 8.0),\n",
    " (872, 12.49999999984375, 8.0),\n",
    " (889, 12.49999999984375, 8.0),\n",
    " (926, 12.49999999984375, 8.0),\n",
    " (938, 12.49999999984375, 8.0),\n",
    " (954, 12.49999999984375, 8.0),\n",
    " (1, 11.111111110987654, 9.0),\n",
    " (20, 11.111111110987654, 9.0),\n",
    " (33, 11.111111110987654, 9.0),\n",
    " (75, 11.111111110987654, 9.0),\n",
    " (92, 11.111111110987654, 9.0),\n",
    " (105, 11.111111110987654, 9.0),\n",
    " (179, 11.111111110987654, 9.0),\n",
    " (202, 11.111111110987654, 9.0),\n",
    " (206, 11.111111110987654, 9.0),\n",
    " (208, 11.111111110987654, 9.0),\n",
    " (278, 11.111111110987654, 9.0),\n",
    " (309, 11.111111110987654, 9.0),\n",
    " (344, 11.111111110987654, 9.0),\n",
    " (346, 11.111111110987654, 9.0),\n",
    " (367, 11.111111110987654, 9.0),\n",
    " (374, 11.111111110987654, 9.0),\n",
    " (391, 11.111111110987654, 9.0),\n",
    " (422, 11.111111110987654, 9.0),\n",
    " (428, 11.111111110987654, 9.0),\n",
    " (447, 11.111111110987654, 9.0),\n",
    " (486, 11.111111110987654, 9.0),\n",
    " (505, 11.111111110987654, 9.0),\n",
    " (511, 11.111111110987654, 9.0),\n",
    " (518, 11.111111110987654, 9.0),\n",
    " (579, 11.111111110987654, 9.0),\n",
    " (592, 11.111111110987654, 9.0),\n",
    " (627, 11.111111110987654, 9.0),\n",
    " (628, 11.111111110987654, 9.0),\n",
    " (647, 11.111111110987654, 9.0),\n",
    " (672, 11.111111110987654, 9.0),\n",
    " (786, 11.111111110987654, 9.0),\n",
    " (802, 11.111111110987654, 9.0),\n",
    " (803, 11.111111110987654, 9.0),\n",
    " (819, 11.111111110987654, 9.0),\n",
    " (822, 11.111111110987654, 9.0),\n",
    " (823, 11.111111110987654, 9.0),\n",
    " (852, 11.111111110987654, 9.0),\n",
    " (868, 11.111111110987654, 9.0),\n",
    " (871, 11.111111110987654, 9.0),\n",
    " (882, 11.111111110987654, 9.0),\n",
    " (914, 11.111111110987654, 9.0),\n",
    " (948, 11.111111110987654, 9.0),\n",
    " (972, 11.111111110987654, 9.0),\n",
    " (973, 11.111111110987654, 9.0),\n",
    " (13, 9.9999999999, 10.0),\n",
    " (23, 9.9999999999, 10.0),\n",
    " (104, 9.9999999999, 10.0),\n",
    " (130, 9.9999999999, 10.0),\n",
    " (147, 9.9999999999, 10.0),\n",
    " (296, 9.9999999999, 10.0),\n",
    " (310, 9.9999999999, 10.0),\n",
    " (311, 9.9999999999, 10.0),\n",
    " (324, 9.9999999999, 10.0),\n",
    " (338, 9.9999999999, 10.0),\n",
    " (369, 9.9999999999, 10.0),\n",
    " (379, 9.9999999999, 10.0),\n",
    " (402, 9.9999999999, 10.0),\n",
    " (416, 9.9999999999, 10.0),\n",
    " (433, 9.9999999999, 10.0),\n",
    " (477, 9.9999999999, 10.0),\n",
    " (519, 9.9999999999, 10.0),\n",
    " (534, 9.9999999999, 10.0),\n",
    " (535, 9.9999999999, 10.0),\n",
    " (570, 9.9999999999, 10.0),\n",
    " (617, 9.9999999999, 10.0),\n",
    " (659, 9.9999999999, 10.0),\n",
    " (697, 9.9999999999, 10.0),\n",
    " (704, 9.9999999999, 10.0),\n",
    " (719, 9.9999999999, 10.0),\n",
    " (724, 9.9999999999, 10.0),\n",
    " (754, 9.9999999999, 10.0),\n",
    " (755, 9.9999999999, 10.0),\n",
    " (760, 9.9999999999, 10.0),\n",
    " (777, 9.9999999999, 10.0),\n",
    " (785, 9.9999999999, 10.0),\n",
    " (809, 9.9999999999, 10.0),\n",
    " (848, 9.9999999999, 10.0),\n",
    " (875, 9.9999999999, 10.0),\n",
    " (895, 9.9999999999, 10.0),\n",
    " (920, 9.9999999999, 10.0),\n",
    " (927, 9.9999999999, 10.0),\n",
    " (931, 9.9999999999, 10.0),\n",
    " (934, 9.9999999999, 10.0),\n",
    " (983, 9.9999999999, 10.0),\n",
    " (998, 9.9999999999, 10.0),\n",
    " (30, 9.090909090826447, 11.0),\n",
    " (143, 9.090909090826447, 11.0),\n",
    " (183, 9.090909090826447, 11.0),\n",
    " (297, 9.090909090826447, 11.0),\n",
    " (378, 9.090909090826447, 11.0),\n",
    " (450, 9.090909090826447, 11.0),\n",
    " (500, 9.090909090826447, 11.0),\n",
    " (548, 9.090909090826447, 11.0),\n",
    " (586, 9.090909090826447, 11.0),\n",
    " (591, 9.090909090826447, 11.0),\n",
    " (651, 9.090909090826447, 11.0),\n",
    " (675, 9.090909090826447, 11.0),\n",
    " (817, 9.090909090826447, 11.0),\n",
    " (855, 9.090909090826447, 11.0),\n",
    " (861, 9.090909090826447, 11.0),\n",
    " (862, 9.090909090826447, 11.0),\n",
    " (874, 9.090909090826447, 11.0),\n",
    " (945, 9.090909090826447, 11.0),\n",
    " (966, 9.090909090826447, 11.0),\n",
    " (971, 9.090909090826447, 11.0),\n",
    " (40, 8.333333333263889, 12.0),\n",
    " (125, 8.333333333263889, 12.0),\n",
    " (159, 8.333333333263889, 12.0),\n",
    " (233, 8.333333333263889, 12.0),\n",
    " (421, 8.333333333263889, 12.0),\n",
    " (445, 8.333333333263889, 12.0),\n",
    " (451, 8.333333333263889, 12.0),\n",
    " (492, 8.333333333263889, 12.0),\n",
    " (521, 8.333333333263889, 12.0),\n",
    " (557, 8.333333333263889, 12.0),\n",
    " (602, 8.333333333263889, 12.0),\n",
    " (605, 8.333333333263889, 12.0),\n",
    " (655, 8.333333333263889, 12.0),\n",
    " (662, 8.333333333263889, 12.0),\n",
    " (743, 8.333333333263889, 12.0),\n",
    " (799, 8.333333333263889, 12.0),\n",
    " (830, 8.333333333263889, 12.0),\n",
    " (894, 8.333333333263889, 12.0),\n",
    " (933, 8.333333333263889, 12.0),\n",
    " (187, 7.69230769224852, 13.0),\n",
    " (209, 7.69230769224852, 13.0),\n",
    " (515, 7.69230769224852, 13.0),\n",
    " (549, 7.69230769224852, 13.0),\n",
    " (962, 7.69230769224852, 13.0),\n",
    " (968, 7.69230769224852, 13.0),\n",
    " (423, 7.142857142806123, 14.0),\n",
    " (431, 7.142857142806123, 14.0),\n",
    " (526, 7.142857142806123, 14.0),\n",
    " (2, 0.0, 9.0),\n",
    " (3, 0.0, 7.0),\n",
    " (4, 0.0, 9.0),\n",
    " (5, 0.0, 7.0),\n",
    " (6, 0.0, 9.0),\n",
    " (10, 0.0, 11.0),\n",
    " (21, 0.0, 10.0),\n",
    " (22, 0.0, 8.0),\n",
    " (26, 0.0, 5.0),\n",
    " (27, 0.0, 10.0),\n",
    " (31, 0.0, 11.0),\n",
    " (53, 0.0, 7.0),\n",
    " (60, 0.0, 4.0),\n",
    " (74, 0.0, 6.0),\n",
    " (80, 0.0, 6.0),\n",
    " (101, 0.0, 7.0),\n",
    " (103, 0.0, 8.0),\n",
    " (106, 0.0, 9.0),\n",
    " (107, 0.0, 8.0),\n",
    " (111, 0.0, 10.0),\n",
    " (114, 0.0, 6.0),\n",
    " (115, 0.0, 8.0),\n",
    " (118, 0.0, 9.0),\n",
    " (122, 0.0, 8.0),\n",
    " (146, 0.0, 9.0),\n",
    " (149, 0.0, 7.0),\n",
    " (150, 0.0, 12.0),\n",
    " (153, 0.0, 10.0),\n",
    " (154, 0.0, 10.0),\n",
    " (158, 0.0, 4.0),\n",
    " (167, 0.0, 5.0),\n",
    " (168, 0.0, 10.0),\n",
    " (169, 0.0, 14.0),\n",
    " (175, 0.0, 7.0),\n",
    " (180, 0.0, 9.0),\n",
    " (207, 0.0, 8.0),\n",
    " (212, 0.0, 6.0),\n",
    " (215, 0.0, 5.0),\n",
    " (220, 0.0, 6.0),\n",
    " (223, 0.0, 9.0),\n",
    " (224, 0.0, 9.0),\n",
    " (225, 0.0, 8.0),\n",
    " (228, 0.0, 9.0),\n",
    " (231, 0.0, 5.0),\n",
    " (241, 0.0, 8.0),\n",
    " (244, 0.0, 13.0),\n",
    " (252, 0.0, 10.0),\n",
    " (256, 0.0, 8.0),\n",
    " (259, 0.0, 5.0),\n",
    " (260, 0.0, 8.0),\n",
    " (268, 0.0, 10.0),\n",
    " (304, 0.0, 12.0),\n",
    " (308, 0.0, 7.0),\n",
    " (315, 0.0, 11.0),\n",
    " (317, 0.0, 9.0),\n",
    " (326, 0.0, 8.0),\n",
    " (332, 0.0, 9.0),\n",
    " (333, 0.0, 8.0),\n",
    " (354, 0.0, 8.0),\n",
    " (357, 0.0, 6.0),\n",
    " (368, 0.0, 7.0),\n",
    " (380, 0.0, 6.0),\n",
    " (385, 0.0, 7.0),\n",
    " (397, 0.0, 9.0),\n",
    " (399, 0.0, 10.0),\n",
    " (400, 0.0, 7.0),\n",
    " (403, 0.0, 13.0),\n",
    " (404, 0.0, 10.0),\n",
    " (418, 0.0, 10.0),\n",
    " (419, 0.0, 8.0),\n",
    " (434, 0.0, 7.0),\n",
    " (435, 0.0, 13.0),\n",
    " (437, 0.0, 11.0),\n",
    " (438, 0.0, 5.0),\n",
    " (441, 0.0, 7.0),\n",
    " (442, 0.0, 8.0),\n",
    " (446, 0.0, 12.0),\n",
    " (452, 0.0, 12.0),\n",
    " (459, 0.0, 5.0),\n",
    " (460, 0.0, 16.0),\n",
    " (463, 0.0, 8.0),\n",
    " (465, 0.0, 6.0),\n",
    " (473, 0.0, 9.0),\n",
    " (475, 0.0, 9.0),\n",
    " (478, 0.0, 6.0),\n",
    " (480, 0.0, 11.0),\n",
    " (484, 0.0, 11.0),\n",
    " (485, 0.0, 6.0),\n",
    " (493, 0.0, 8.0),\n",
    " (494, 0.0, 8.0),\n",
    " (499, 0.0, 6.0),\n",
    " (501, 0.0, 5.0),\n",
    " (503, 0.0, 8.0),\n",
    " (504, 0.0, 8.0),\n",
    " (508, 0.0, 10.0),\n",
    " (512, 0.0, 8.0),\n",
    " (527, 0.0, 11.0),\n",
    " (529, 0.0, 12.0),\n",
    " (530, 0.0, 7.0),\n",
    " (540, 0.0, 8.0),\n",
    " (542, 0.0, 7.0),\n",
    " (544, 0.0, 7.0),\n",
    " (551, 0.0, 11.0),\n",
    " (553, 0.0, 8.0),\n",
    " (558, 0.0, 7.0),\n",
    " (567, 0.0, 7.0),\n",
    " (583, 0.0, 11.0),\n",
    " (584, 0.0, 8.0),\n",
    " (585, 0.0, 8.0),\n",
    " (587, 0.0, 11.0),\n",
    " (590, 0.0, 10.0),\n",
    " (593, 0.0, 8.0),\n",
    " (606, 0.0, 8.0),\n",
    " (615, 0.0, 8.0),\n",
    " (618, 0.0, 6.0),\n",
    " (619, 0.0, 6.0),\n",
    " (620, 0.0, 7.0),\n",
    " (623, 0.0, 6.0),\n",
    " (626, 0.0, 10.0),\n",
    " (629, 0.0, 6.0),\n",
    " (631, 0.0, 10.0),\n",
    " (632, 0.0, 5.0),\n",
    " (633, 0.0, 10.0),\n",
    " (638, 0.0, 7.0),\n",
    " (639, 0.0, 7.0),\n",
    " (644, 0.0, 10.0),\n",
    " (649, 0.0, 12.0),\n",
    " (650, 0.0, 5.0),\n",
    " (653, 0.0, 10.0),\n",
    " (657, 0.0, 13.0),\n",
    " (666, 0.0, 8.0),\n",
    " (667, 0.0, 9.0),\n",
    " (669, 0.0, 7.0),\n",
    " (673, 0.0, 12.0),\n",
    " (676, 0.0, 7.0),\n",
    " (678, 0.0, 5.0),\n",
    " (680, 0.0, 8.0),\n",
    " (681, 0.0, 6.0),\n",
    " (682, 0.0, 9.0),\n",
    " (684, 0.0, 6.0),\n",
    " (691, 0.0, 6.0),\n",
    " (693, 0.0, 8.0),\n",
    " (700, 0.0, 6.0),\n",
    " (701, 0.0, 8.0),\n",
    " (712, 0.0, 7.0),\n",
    " (713, 0.0, 10.0),\n",
    " (720, 0.0, 7.0),\n",
    " (722, 0.0, 6.0),\n",
    " (726, 0.0, 11.0),\n",
    " (731, 0.0, 4.0),\n",
    " (732, 0.0, 10.0),\n",
    " (736, 0.0, 9.0),\n",
    " (739, 0.0, 10.0),\n",
    " (740, 0.0, 10.0),\n",
    " (742, 0.0, 10.0),\n",
    " (745, 0.0, 8.0),\n",
    " (747, 0.0, 9.0),\n",
    " (756, 0.0, 12.0),\n",
    " (758, 0.0, 10.0),\n",
    " (764, 0.0, 6.0),\n",
    " (771, 0.0, 9.0),\n",
    " (774, 0.0, 10.0),\n",
    " (778, 0.0, 7.0),\n",
    " (782, 0.0, 7.0),\n",
    " (797, 0.0, 11.0),\n",
    " (804, 0.0, 11.0),\n",
    " (810, 0.0, 7.0),\n",
    " (811, 0.0, 4.0),\n",
    " (812, 0.0, 7.0),\n",
    " (813, 0.0, 8.0),\n",
    " (814, 0.0, 10.0),\n",
    " (818, 0.0, 8.0),\n",
    " (827, 0.0, 8.0),\n",
    " (831, 0.0, 11.0),\n",
    " (833, 0.0, 8.0),\n",
    " (834, 0.0, 7.0),\n",
    " (836, 0.0, 4.0),\n",
    " (837, 0.0, 11.0),\n",
    " (838, 0.0, 10.0),\n",
    " (841, 0.0, 6.0),\n",
    " (851, 0.0, 8.0),\n",
    " (859, 0.0, 10.0),\n",
    " (864, 0.0, 9.0),\n",
    " (867, 0.0, 11.0),\n",
    " (869, 0.0, 10.0),\n",
    " (876, 0.0, 5.0),\n",
    " (883, 0.0, 9.0),\n",
    " (885, 0.0, 1.0),\n",
    " (890, 0.0, 10.0),\n",
    " (892, 0.0, 4.0),\n",
    " (893, 0.0, 7.0),\n",
    " (896, 0.0, 10.0),\n",
    " (897, 0.0, 9.0),\n",
    " (899, 0.0, 1.0),\n",
    " (901, 0.0, 6.0),\n",
    " (902, 0.0, 8.0),\n",
    " (908, 0.0, 6.0),\n",
    " (909, 0.0, 12.0),\n",
    " (910, 0.0, 8.0),\n",
    " (913, 0.0, 10.0),\n",
    " (923, 0.0, 13.0),\n",
    " (925, 0.0, 7.0),\n",
    " (928, 0.0, 9.0),\n",
    " (929, 0.0, 11.0),\n",
    " (930, 0.0, 10.0),\n",
    " (935, 0.0, 9.0),\n",
    " (940, 0.0, 9.0),\n",
    " (941, 0.0, 7.0),\n",
    " (942, 0.0, 5.0),\n",
    " (943, 0.0, 10.0),\n",
    " (957, 0.0, 8.0),\n",
    " (960, 0.0, 6.0),\n",
    " (961, 0.0, 11.0),\n",
    " (965, 0.0, 7.0),\n",
    " (967, 0.0, 12.0),\n",
    " (969, 0.0, 6.0),\n",
    " (974, 0.0, 10.0),\n",
    " (975, 0.0, 11.0),\n",
    " (976, 0.0, 6.0),\n",
    " (977, 0.0, 10.0),\n",
    " (978, 0.0, 5.0),\n",
    " (980, 0.0, 12.0),\n",
    " (986, 0.0, 9.0),\n",
    " (989, 0.0, 9.0),\n",
    " (999, 0.0, 8.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_spt(spt1, spt2):\n",
    "  spt1 = [v[1] for v in sorted(spt1, key=lambda x: x[0])]\n",
    "  spt2 = [v[1] for v in sorted(spt2, key=lambda x: x[0])]\n",
    "  print(spt1[:10])\n",
    "  print(spt2[:10])\n",
    "  diff = [abs(v1 - v2) for v1, v2 in zip(spt1, spt2)]\n",
    "  return np.mean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_spt_top(spt1, spt2, top_k):\n",
    "  spt2 = [v[1] for v in sorted(spt2, key=lambda x: x[0])]\n",
    "  diff = []\n",
    "  for index, percent, total in spt1[:top_k]:\n",
    "    print(percent, spt2[index])\n",
    "    diff.append(abs(percent - spt2[index]))\n",
    "  return np.mean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.9999999988889 66.66666666592593\n",
      "89.9999999991 99.9999999988889\n",
      "87.49999999890625 85.71428571306123\n",
      "83.33333333263889 79.9999999992\n",
      "79.9999999984 24.999999999375\n",
      "77.77777777691358 36.36363636330579\n",
      "77.77777777691358 69.9999999993\n",
      "76.92307692248521 90.90909090826446\n",
      "74.999999999375 69.9999999993\n",
      "74.9999999990625 74.9999999990625\n",
      "74.9999999990625 66.66666666592593\n",
      "72.72727272661157 72.72727272661157\n",
      "72.72727272661157 45.45454545413223\n",
      "72.72727272661157 69.9999999993\n",
      "72.72727272661157 63.63636363578512\n",
      "72.72727272661157 72.72727272661157\n",
      "72.72727272661157 49.9999999995\n",
      "72.72727272661157 46.15384615349112\n",
      "72.72727272661157 24.9999999996875\n",
      "71.42857142755102 85.71428571306123\n",
      "70.58823529370243 85.71428571367348\n",
      "69.9999999993 88.88888888790123\n",
      "69.9999999993 37.49999999953125\n",
      "69.9999999993 79.9999999992\n",
      "69.9999999993 69.9999999993\n",
      "69.9999999993 49.9999999995\n",
      "69.9999999993 66.66666666592593\n",
      "69.9999999993 72.72727272661157\n",
      "69.9999999993 83.33333333263889\n",
      "69.23076923023669 54.54545454495868\n",
      "66.66666666611111 66.66666666611111\n",
      "66.66666666592593 74.9999999990625\n",
      "66.66666666592593 87.49999999890625\n",
      "66.66666666555555 59.9999999988\n",
      "63.63636363578512 99.999999999\n",
      "63.63636363578512 66.66666666592593\n",
      "63.63636363578512 57.14285714204082\n",
      "62.49999999921875 33.333333332962965\n",
      "62.49999999921875 74.9999999990625\n",
      "62.49999999921875 44.444444443950616\n",
      "62.49999999921875 33.333333332962965\n",
      "62.49999999921875 24.9999999996875\n",
      "61.53846153798816 49.9999999995\n",
      "61.53846153798816 72.72727272661157\n",
      "61.53846153798816 69.23076923023669\n",
      "61.53846153798816 69.9999999993\n",
      "59.9999999996 66.66666666592593\n",
      "59.9999999996 41.66666666631944\n",
      "59.9999999994 33.333333332962965\n",
      "59.9999999994 85.71428571306123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15.70668351238808"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_spt_top(spt, spt_2, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0034aa9860>]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sspt = sorted(spt, key= lambda x: x[0])\n",
    "y = [v[1] for v in sspt]\n",
    "plt.plot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_lr(label, base_lr):\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.66666666592593"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='211' class='' max='400', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      52.75% [211/400 20:52:57<18:42:19]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>targeted_validation</th>\n",
       "      <th>div_metric</th>\n",
       "      <th>entropy</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.041856</td>\n",
       "      <td>9.073653</td>\n",
       "      <td>0.526000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>777.000000</td>\n",
       "      <td>9.600423</td>\n",
       "      <td>9.073654</td>\n",
       "      <td>04:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.967439</td>\n",
       "      <td>8.108307</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>500.500000</td>\n",
       "      <td>8.229180</td>\n",
       "      <td>8.108306</td>\n",
       "      <td>04:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.953693</td>\n",
       "      <td>7.048500</td>\n",
       "      <td>0.603000</td>\n",
       "      <td>0.042000</td>\n",
       "      <td>518.250000</td>\n",
       "      <td>8.528024</td>\n",
       "      <td>7.048499</td>\n",
       "      <td>04:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.848456</td>\n",
       "      <td>6.101434</td>\n",
       "      <td>0.607000</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>457.250000</td>\n",
       "      <td>8.053362</td>\n",
       "      <td>6.101433</td>\n",
       "      <td>04:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.237603</td>\n",
       "      <td>5.322343</td>\n",
       "      <td>0.648000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>468.250000</td>\n",
       "      <td>8.143393</td>\n",
       "      <td>5.322344</td>\n",
       "      <td>04:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.933229</td>\n",
       "      <td>5.097449</td>\n",
       "      <td>0.636000</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>475.250000</td>\n",
       "      <td>8.084688</td>\n",
       "      <td>5.097450</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.695633</td>\n",
       "      <td>4.868337</td>\n",
       "      <td>0.667000</td>\n",
       "      <td>0.232000</td>\n",
       "      <td>475.500000</td>\n",
       "      <td>8.058609</td>\n",
       "      <td>4.868337</td>\n",
       "      <td>04:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.506330</td>\n",
       "      <td>4.727351</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.271000</td>\n",
       "      <td>469.250000</td>\n",
       "      <td>7.972008</td>\n",
       "      <td>4.727351</td>\n",
       "      <td>04:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.396331</td>\n",
       "      <td>4.593160</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>472.000000</td>\n",
       "      <td>7.974873</td>\n",
       "      <td>4.593160</td>\n",
       "      <td>04:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.189259</td>\n",
       "      <td>4.438077</td>\n",
       "      <td>0.677000</td>\n",
       "      <td>0.318000</td>\n",
       "      <td>471.250000</td>\n",
       "      <td>7.960370</td>\n",
       "      <td>4.438077</td>\n",
       "      <td>04:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.059405</td>\n",
       "      <td>4.378273</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.321000</td>\n",
       "      <td>471.750000</td>\n",
       "      <td>7.856012</td>\n",
       "      <td>4.378274</td>\n",
       "      <td>04:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.131234</td>\n",
       "      <td>4.201299</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.344000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>7.922497</td>\n",
       "      <td>4.201299</td>\n",
       "      <td>04:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.896073</td>\n",
       "      <td>4.222261</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>0.355000</td>\n",
       "      <td>473.500000</td>\n",
       "      <td>7.852163</td>\n",
       "      <td>4.222261</td>\n",
       "      <td>04:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.852973</td>\n",
       "      <td>4.159376</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>474.750000</td>\n",
       "      <td>7.854972</td>\n",
       "      <td>4.159376</td>\n",
       "      <td>04:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.919832</td>\n",
       "      <td>4.114820</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>7.848146</td>\n",
       "      <td>4.114820</td>\n",
       "      <td>04:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.727345</td>\n",
       "      <td>4.076174</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>459.500000</td>\n",
       "      <td>7.631903</td>\n",
       "      <td>4.076174</td>\n",
       "      <td>04:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.705741</td>\n",
       "      <td>3.986420</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>0.393000</td>\n",
       "      <td>481.500000</td>\n",
       "      <td>7.966680</td>\n",
       "      <td>3.986421</td>\n",
       "      <td>04:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.655107</td>\n",
       "      <td>4.060935</td>\n",
       "      <td>0.688000</td>\n",
       "      <td>0.368000</td>\n",
       "      <td>467.250000</td>\n",
       "      <td>7.713351</td>\n",
       "      <td>4.060935</td>\n",
       "      <td>04:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.825067</td>\n",
       "      <td>4.097906</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>7.578015</td>\n",
       "      <td>4.097906</td>\n",
       "      <td>04:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.553078</td>\n",
       "      <td>3.928444</td>\n",
       "      <td>0.701000</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>461.750000</td>\n",
       "      <td>7.610547</td>\n",
       "      <td>3.928445</td>\n",
       "      <td>04:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.476068</td>\n",
       "      <td>3.910067</td>\n",
       "      <td>0.701000</td>\n",
       "      <td>0.419000</td>\n",
       "      <td>463.000000</td>\n",
       "      <td>7.695912</td>\n",
       "      <td>3.910067</td>\n",
       "      <td>04:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.633351</td>\n",
       "      <td>3.930084</td>\n",
       "      <td>0.709000</td>\n",
       "      <td>0.428000</td>\n",
       "      <td>466.750000</td>\n",
       "      <td>7.715342</td>\n",
       "      <td>3.930084</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.700930</td>\n",
       "      <td>3.818158</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.441000</td>\n",
       "      <td>482.250000</td>\n",
       "      <td>7.884916</td>\n",
       "      <td>3.818158</td>\n",
       "      <td>04:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.349887</td>\n",
       "      <td>3.863911</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>0.429000</td>\n",
       "      <td>483.750000</td>\n",
       "      <td>7.964472</td>\n",
       "      <td>3.863911</td>\n",
       "      <td>04:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.608860</td>\n",
       "      <td>3.848274</td>\n",
       "      <td>0.714000</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>483.500000</td>\n",
       "      <td>7.928101</td>\n",
       "      <td>3.848274</td>\n",
       "      <td>04:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.559458</td>\n",
       "      <td>3.778631</td>\n",
       "      <td>0.709000</td>\n",
       "      <td>0.439000</td>\n",
       "      <td>469.250000</td>\n",
       "      <td>7.645669</td>\n",
       "      <td>3.778632</td>\n",
       "      <td>04:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.514416</td>\n",
       "      <td>3.724637</td>\n",
       "      <td>0.719000</td>\n",
       "      <td>0.454000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>7.806229</td>\n",
       "      <td>3.724637</td>\n",
       "      <td>04:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.637200</td>\n",
       "      <td>3.679529</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.445000</td>\n",
       "      <td>483.500000</td>\n",
       "      <td>7.922972</td>\n",
       "      <td>3.679529</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.375875</td>\n",
       "      <td>3.741503</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>474.250000</td>\n",
       "      <td>7.802202</td>\n",
       "      <td>3.741503</td>\n",
       "      <td>04:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.413916</td>\n",
       "      <td>3.694589</td>\n",
       "      <td>0.731000</td>\n",
       "      <td>0.452000</td>\n",
       "      <td>479.250000</td>\n",
       "      <td>7.859534</td>\n",
       "      <td>3.694588</td>\n",
       "      <td>04:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.384893</td>\n",
       "      <td>3.723060</td>\n",
       "      <td>0.722000</td>\n",
       "      <td>0.459000</td>\n",
       "      <td>480.250000</td>\n",
       "      <td>7.940742</td>\n",
       "      <td>3.723060</td>\n",
       "      <td>04:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.473017</td>\n",
       "      <td>3.654643</td>\n",
       "      <td>0.723000</td>\n",
       "      <td>0.452000</td>\n",
       "      <td>473.500000</td>\n",
       "      <td>7.708302</td>\n",
       "      <td>3.654643</td>\n",
       "      <td>04:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.413868</td>\n",
       "      <td>3.701987</td>\n",
       "      <td>0.698000</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>463.500000</td>\n",
       "      <td>7.644757</td>\n",
       "      <td>3.701987</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.522996</td>\n",
       "      <td>3.693511</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.466000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>7.680330</td>\n",
       "      <td>3.693511</td>\n",
       "      <td>04:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.364244</td>\n",
       "      <td>3.663207</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.446000</td>\n",
       "      <td>490.500000</td>\n",
       "      <td>7.973759</td>\n",
       "      <td>3.663207</td>\n",
       "      <td>04:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.350085</td>\n",
       "      <td>3.719038</td>\n",
       "      <td>0.719000</td>\n",
       "      <td>0.462000</td>\n",
       "      <td>476.750000</td>\n",
       "      <td>7.854843</td>\n",
       "      <td>3.719038</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.439801</td>\n",
       "      <td>3.676779</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.465000</td>\n",
       "      <td>472.000000</td>\n",
       "      <td>7.726980</td>\n",
       "      <td>3.676779</td>\n",
       "      <td>04:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.375531</td>\n",
       "      <td>3.677859</td>\n",
       "      <td>0.722000</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>7.823643</td>\n",
       "      <td>3.677859</td>\n",
       "      <td>04:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.530799</td>\n",
       "      <td>3.628341</td>\n",
       "      <td>0.714000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>465.000000</td>\n",
       "      <td>7.525450</td>\n",
       "      <td>3.628341</td>\n",
       "      <td>04:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.432528</td>\n",
       "      <td>3.589526</td>\n",
       "      <td>0.718000</td>\n",
       "      <td>0.469000</td>\n",
       "      <td>490.750000</td>\n",
       "      <td>7.805811</td>\n",
       "      <td>3.589526</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.257742</td>\n",
       "      <td>3.618106</td>\n",
       "      <td>0.721000</td>\n",
       "      <td>0.466000</td>\n",
       "      <td>472.750000</td>\n",
       "      <td>7.666063</td>\n",
       "      <td>3.618106</td>\n",
       "      <td>04:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3.154529</td>\n",
       "      <td>3.592824</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.477000</td>\n",
       "      <td>456.000000</td>\n",
       "      <td>7.532890</td>\n",
       "      <td>3.592823</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>3.447265</td>\n",
       "      <td>3.608342</td>\n",
       "      <td>0.727000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>489.000000</td>\n",
       "      <td>7.799234</td>\n",
       "      <td>3.608343</td>\n",
       "      <td>04:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.240739</td>\n",
       "      <td>3.627439</td>\n",
       "      <td>0.723000</td>\n",
       "      <td>0.461000</td>\n",
       "      <td>484.500000</td>\n",
       "      <td>7.809368</td>\n",
       "      <td>3.627439</td>\n",
       "      <td>04:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.406253</td>\n",
       "      <td>3.591088</td>\n",
       "      <td>0.727000</td>\n",
       "      <td>0.479000</td>\n",
       "      <td>477.250000</td>\n",
       "      <td>7.589556</td>\n",
       "      <td>3.591088</td>\n",
       "      <td>04:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.278950</td>\n",
       "      <td>3.687400</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.469000</td>\n",
       "      <td>493.000000</td>\n",
       "      <td>7.991326</td>\n",
       "      <td>3.687400</td>\n",
       "      <td>04:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.393364</td>\n",
       "      <td>3.627238</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.472000</td>\n",
       "      <td>477.250000</td>\n",
       "      <td>7.730202</td>\n",
       "      <td>3.627238</td>\n",
       "      <td>04:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>3.284411</td>\n",
       "      <td>3.480690</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>7.713962</td>\n",
       "      <td>3.480690</td>\n",
       "      <td>04:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.151089</td>\n",
       "      <td>3.485301</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.483000</td>\n",
       "      <td>491.000000</td>\n",
       "      <td>8.005316</td>\n",
       "      <td>3.485301</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>3.236033</td>\n",
       "      <td>3.572376</td>\n",
       "      <td>0.724000</td>\n",
       "      <td>0.479000</td>\n",
       "      <td>502.000000</td>\n",
       "      <td>8.017759</td>\n",
       "      <td>3.572376</td>\n",
       "      <td>04:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.163230</td>\n",
       "      <td>3.502223</td>\n",
       "      <td>0.736000</td>\n",
       "      <td>0.484000</td>\n",
       "      <td>491.500000</td>\n",
       "      <td>7.894549</td>\n",
       "      <td>3.502222</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>3.124440</td>\n",
       "      <td>3.542067</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>502.750000</td>\n",
       "      <td>7.892050</td>\n",
       "      <td>3.542067</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3.435367</td>\n",
       "      <td>3.512011</td>\n",
       "      <td>0.723000</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>516.500000</td>\n",
       "      <td>8.031845</td>\n",
       "      <td>3.512011</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>3.318549</td>\n",
       "      <td>3.540148</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.469000</td>\n",
       "      <td>465.750000</td>\n",
       "      <td>7.404348</td>\n",
       "      <td>3.540148</td>\n",
       "      <td>04:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>3.427091</td>\n",
       "      <td>3.587088</td>\n",
       "      <td>0.734000</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>472.250000</td>\n",
       "      <td>7.692530</td>\n",
       "      <td>3.587088</td>\n",
       "      <td>04:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>3.186672</td>\n",
       "      <td>3.547775</td>\n",
       "      <td>0.721000</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>457.750000</td>\n",
       "      <td>7.511502</td>\n",
       "      <td>3.547775</td>\n",
       "      <td>04:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>3.174196</td>\n",
       "      <td>3.424173</td>\n",
       "      <td>0.737000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>7.890957</td>\n",
       "      <td>3.424173</td>\n",
       "      <td>04:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>3.317558</td>\n",
       "      <td>3.501664</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.483000</td>\n",
       "      <td>493.500000</td>\n",
       "      <td>7.738044</td>\n",
       "      <td>3.501663</td>\n",
       "      <td>04:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>3.157042</td>\n",
       "      <td>3.423472</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.502000</td>\n",
       "      <td>494.000000</td>\n",
       "      <td>7.889318</td>\n",
       "      <td>3.423472</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>3.325852</td>\n",
       "      <td>3.455642</td>\n",
       "      <td>0.741000</td>\n",
       "      <td>0.498000</td>\n",
       "      <td>487.750000</td>\n",
       "      <td>7.822472</td>\n",
       "      <td>3.455642</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.193030</td>\n",
       "      <td>3.498677</td>\n",
       "      <td>0.736000</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>477.750000</td>\n",
       "      <td>7.615256</td>\n",
       "      <td>3.498677</td>\n",
       "      <td>04:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>3.125951</td>\n",
       "      <td>3.443200</td>\n",
       "      <td>0.727000</td>\n",
       "      <td>0.484000</td>\n",
       "      <td>469.500000</td>\n",
       "      <td>7.555614</td>\n",
       "      <td>3.443201</td>\n",
       "      <td>04:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>3.014874</td>\n",
       "      <td>3.469418</td>\n",
       "      <td>0.734000</td>\n",
       "      <td>0.501000</td>\n",
       "      <td>496.500000</td>\n",
       "      <td>7.911593</td>\n",
       "      <td>3.469418</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>3.198950</td>\n",
       "      <td>3.473170</td>\n",
       "      <td>0.727000</td>\n",
       "      <td>0.494000</td>\n",
       "      <td>492.750000</td>\n",
       "      <td>7.859969</td>\n",
       "      <td>3.473169</td>\n",
       "      <td>04:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>3.147288</td>\n",
       "      <td>3.438348</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>501.500000</td>\n",
       "      <td>7.990469</td>\n",
       "      <td>3.438348</td>\n",
       "      <td>04:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>3.187888</td>\n",
       "      <td>3.406022</td>\n",
       "      <td>0.719000</td>\n",
       "      <td>0.503000</td>\n",
       "      <td>496.250000</td>\n",
       "      <td>8.025135</td>\n",
       "      <td>3.406022</td>\n",
       "      <td>06:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>3.207170</td>\n",
       "      <td>3.503335</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.506000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>8.099679</td>\n",
       "      <td>3.503335</td>\n",
       "      <td>06:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>3.115665</td>\n",
       "      <td>3.402387</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>0.508000</td>\n",
       "      <td>514.000000</td>\n",
       "      <td>8.100739</td>\n",
       "      <td>3.402386</td>\n",
       "      <td>06:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>3.109057</td>\n",
       "      <td>3.433690</td>\n",
       "      <td>0.726000</td>\n",
       "      <td>0.504000</td>\n",
       "      <td>481.500000</td>\n",
       "      <td>7.662258</td>\n",
       "      <td>3.433689</td>\n",
       "      <td>06:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>3.126799</td>\n",
       "      <td>3.415263</td>\n",
       "      <td>0.731000</td>\n",
       "      <td>0.499000</td>\n",
       "      <td>516.250000</td>\n",
       "      <td>8.044289</td>\n",
       "      <td>3.415262</td>\n",
       "      <td>06:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.067165</td>\n",
       "      <td>3.356048</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>0.504000</td>\n",
       "      <td>502.000000</td>\n",
       "      <td>7.961692</td>\n",
       "      <td>3.356048</td>\n",
       "      <td>06:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>3.041322</td>\n",
       "      <td>3.433603</td>\n",
       "      <td>0.736000</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>513.500000</td>\n",
       "      <td>8.013025</td>\n",
       "      <td>3.433603</td>\n",
       "      <td>06:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>3.139658</td>\n",
       "      <td>3.457303</td>\n",
       "      <td>0.721000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>503.500000</td>\n",
       "      <td>8.023423</td>\n",
       "      <td>3.457303</td>\n",
       "      <td>06:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>3.049047</td>\n",
       "      <td>3.444355</td>\n",
       "      <td>0.737000</td>\n",
       "      <td>0.487000</td>\n",
       "      <td>519.000000</td>\n",
       "      <td>8.260119</td>\n",
       "      <td>3.444355</td>\n",
       "      <td>06:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>3.178849</td>\n",
       "      <td>3.385988</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.489000</td>\n",
       "      <td>474.500000</td>\n",
       "      <td>7.578268</td>\n",
       "      <td>3.385988</td>\n",
       "      <td>06:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>3.063142</td>\n",
       "      <td>3.388636</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.506000</td>\n",
       "      <td>505.500000</td>\n",
       "      <td>7.945964</td>\n",
       "      <td>3.388636</td>\n",
       "      <td>06:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>3.089992</td>\n",
       "      <td>3.402703</td>\n",
       "      <td>0.741000</td>\n",
       "      <td>0.497000</td>\n",
       "      <td>530.250000</td>\n",
       "      <td>8.336566</td>\n",
       "      <td>3.402703</td>\n",
       "      <td>06:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>3.085750</td>\n",
       "      <td>3.381289</td>\n",
       "      <td>0.735000</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>502.250000</td>\n",
       "      <td>7.730593</td>\n",
       "      <td>3.381289</td>\n",
       "      <td>06:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>3.037519</td>\n",
       "      <td>3.362539</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>502.250000</td>\n",
       "      <td>7.779176</td>\n",
       "      <td>3.362539</td>\n",
       "      <td>06:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>3.200064</td>\n",
       "      <td>3.392810</td>\n",
       "      <td>0.729000</td>\n",
       "      <td>0.524000</td>\n",
       "      <td>508.500000</td>\n",
       "      <td>7.990377</td>\n",
       "      <td>3.392809</td>\n",
       "      <td>06:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.060713</td>\n",
       "      <td>3.421140</td>\n",
       "      <td>0.735000</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>494.500000</td>\n",
       "      <td>7.817740</td>\n",
       "      <td>3.421141</td>\n",
       "      <td>06:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>3.165314</td>\n",
       "      <td>3.353786</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.521000</td>\n",
       "      <td>516.250000</td>\n",
       "      <td>8.059892</td>\n",
       "      <td>3.353787</td>\n",
       "      <td>06:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>2.915819</td>\n",
       "      <td>3.293640</td>\n",
       "      <td>0.754000</td>\n",
       "      <td>0.529000</td>\n",
       "      <td>487.750000</td>\n",
       "      <td>7.806782</td>\n",
       "      <td>3.293641</td>\n",
       "      <td>06:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>3.120461</td>\n",
       "      <td>3.370465</td>\n",
       "      <td>0.729000</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>514.250000</td>\n",
       "      <td>7.984150</td>\n",
       "      <td>3.370465</td>\n",
       "      <td>06:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>3.068478</td>\n",
       "      <td>3.347983</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>538.750000</td>\n",
       "      <td>8.174892</td>\n",
       "      <td>3.347983</td>\n",
       "      <td>06:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2.994323</td>\n",
       "      <td>3.402264</td>\n",
       "      <td>0.724000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>542.500000</td>\n",
       "      <td>8.242147</td>\n",
       "      <td>3.402264</td>\n",
       "      <td>06:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>3.091102</td>\n",
       "      <td>3.335884</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.512000</td>\n",
       "      <td>496.500000</td>\n",
       "      <td>7.747768</td>\n",
       "      <td>3.335884</td>\n",
       "      <td>06:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>2.965253</td>\n",
       "      <td>3.293920</td>\n",
       "      <td>0.741000</td>\n",
       "      <td>0.516000</td>\n",
       "      <td>526.000000</td>\n",
       "      <td>8.116374</td>\n",
       "      <td>3.293920</td>\n",
       "      <td>06:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>3.163139</td>\n",
       "      <td>3.318126</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.524000</td>\n",
       "      <td>500.250000</td>\n",
       "      <td>7.869115</td>\n",
       "      <td>3.318125</td>\n",
       "      <td>06:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>3.003105</td>\n",
       "      <td>3.339115</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>8.075973</td>\n",
       "      <td>3.339115</td>\n",
       "      <td>06:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.902839</td>\n",
       "      <td>3.347920</td>\n",
       "      <td>0.741000</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>520.750000</td>\n",
       "      <td>8.051830</td>\n",
       "      <td>3.347920</td>\n",
       "      <td>06:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>3.084941</td>\n",
       "      <td>3.329950</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>509.250000</td>\n",
       "      <td>7.961010</td>\n",
       "      <td>3.329950</td>\n",
       "      <td>06:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>3.102738</td>\n",
       "      <td>3.303539</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.529000</td>\n",
       "      <td>510.250000</td>\n",
       "      <td>7.804418</td>\n",
       "      <td>3.303539</td>\n",
       "      <td>06:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>3.021207</td>\n",
       "      <td>3.362485</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>521.750000</td>\n",
       "      <td>7.989090</td>\n",
       "      <td>3.362485</td>\n",
       "      <td>06:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>2.956272</td>\n",
       "      <td>3.316469</td>\n",
       "      <td>0.747000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>513.250000</td>\n",
       "      <td>7.975831</td>\n",
       "      <td>3.316468</td>\n",
       "      <td>06:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>3.084657</td>\n",
       "      <td>3.273593</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.523000</td>\n",
       "      <td>492.500000</td>\n",
       "      <td>7.716519</td>\n",
       "      <td>3.273593</td>\n",
       "      <td>06:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>2.967744</td>\n",
       "      <td>3.328925</td>\n",
       "      <td>0.747000</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>503.000000</td>\n",
       "      <td>7.745691</td>\n",
       "      <td>3.328925</td>\n",
       "      <td>06:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>3.128413</td>\n",
       "      <td>3.375835</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.501000</td>\n",
       "      <td>540.750000</td>\n",
       "      <td>8.227154</td>\n",
       "      <td>3.375834</td>\n",
       "      <td>06:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>2.991794</td>\n",
       "      <td>3.308301</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.518000</td>\n",
       "      <td>524.750000</td>\n",
       "      <td>8.110838</td>\n",
       "      <td>3.308301</td>\n",
       "      <td>06:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>3.189612</td>\n",
       "      <td>3.312389</td>\n",
       "      <td>0.736000</td>\n",
       "      <td>0.506000</td>\n",
       "      <td>504.250000</td>\n",
       "      <td>7.922750</td>\n",
       "      <td>3.312389</td>\n",
       "      <td>06:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.996400</td>\n",
       "      <td>3.365741</td>\n",
       "      <td>0.739000</td>\n",
       "      <td>0.512000</td>\n",
       "      <td>495.750000</td>\n",
       "      <td>7.809695</td>\n",
       "      <td>3.365741</td>\n",
       "      <td>06:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>3.045056</td>\n",
       "      <td>3.354118</td>\n",
       "      <td>0.741000</td>\n",
       "      <td>0.522000</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>8.091427</td>\n",
       "      <td>3.354117</td>\n",
       "      <td>06:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>3.051414</td>\n",
       "      <td>3.249255</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.517000</td>\n",
       "      <td>519.750000</td>\n",
       "      <td>7.979069</td>\n",
       "      <td>3.249255</td>\n",
       "      <td>06:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>3.055193</td>\n",
       "      <td>3.391605</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.521000</td>\n",
       "      <td>516.750000</td>\n",
       "      <td>7.985795</td>\n",
       "      <td>3.391605</td>\n",
       "      <td>06:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>2.999092</td>\n",
       "      <td>3.267555</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>0.518000</td>\n",
       "      <td>484.250000</td>\n",
       "      <td>7.581544</td>\n",
       "      <td>3.267555</td>\n",
       "      <td>06:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>3.143322</td>\n",
       "      <td>3.320220</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>500.500000</td>\n",
       "      <td>7.729393</td>\n",
       "      <td>3.320219</td>\n",
       "      <td>06:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>3.088897</td>\n",
       "      <td>3.276073</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.524000</td>\n",
       "      <td>498.750000</td>\n",
       "      <td>7.791917</td>\n",
       "      <td>3.276073</td>\n",
       "      <td>06:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>3.042199</td>\n",
       "      <td>3.374778</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>506.250000</td>\n",
       "      <td>7.862089</td>\n",
       "      <td>3.374778</td>\n",
       "      <td>06:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>2.810912</td>\n",
       "      <td>3.252181</td>\n",
       "      <td>0.754000</td>\n",
       "      <td>0.521000</td>\n",
       "      <td>539.000000</td>\n",
       "      <td>8.215221</td>\n",
       "      <td>3.252181</td>\n",
       "      <td>06:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>3.030150</td>\n",
       "      <td>3.323517</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>521.000000</td>\n",
       "      <td>7.946476</td>\n",
       "      <td>3.323517</td>\n",
       "      <td>06:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.824596</td>\n",
       "      <td>3.223818</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>518.000000</td>\n",
       "      <td>8.023970</td>\n",
       "      <td>3.223818</td>\n",
       "      <td>06:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>2.979031</td>\n",
       "      <td>3.276685</td>\n",
       "      <td>0.743000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>498.000000</td>\n",
       "      <td>7.835775</td>\n",
       "      <td>3.276685</td>\n",
       "      <td>06:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>3.029001</td>\n",
       "      <td>3.281802</td>\n",
       "      <td>0.731000</td>\n",
       "      <td>0.529000</td>\n",
       "      <td>530.500000</td>\n",
       "      <td>8.123650</td>\n",
       "      <td>3.281802</td>\n",
       "      <td>06:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>2.982648</td>\n",
       "      <td>3.212106</td>\n",
       "      <td>0.762000</td>\n",
       "      <td>0.527000</td>\n",
       "      <td>511.750000</td>\n",
       "      <td>7.939227</td>\n",
       "      <td>3.212106</td>\n",
       "      <td>06:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>2.858255</td>\n",
       "      <td>3.297054</td>\n",
       "      <td>0.759000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>519.250000</td>\n",
       "      <td>7.914923</td>\n",
       "      <td>3.297054</td>\n",
       "      <td>06:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>2.957905</td>\n",
       "      <td>3.202150</td>\n",
       "      <td>0.737000</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>537.750000</td>\n",
       "      <td>8.077608</td>\n",
       "      <td>3.202150</td>\n",
       "      <td>06:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>3.011545</td>\n",
       "      <td>3.258858</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.519000</td>\n",
       "      <td>518.500000</td>\n",
       "      <td>7.901309</td>\n",
       "      <td>3.258858</td>\n",
       "      <td>06:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>2.829384</td>\n",
       "      <td>3.314767</td>\n",
       "      <td>0.734000</td>\n",
       "      <td>0.512000</td>\n",
       "      <td>528.750000</td>\n",
       "      <td>8.092052</td>\n",
       "      <td>3.314767</td>\n",
       "      <td>06:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>2.973778</td>\n",
       "      <td>3.272564</td>\n",
       "      <td>0.741000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>7.927088</td>\n",
       "      <td>3.272564</td>\n",
       "      <td>06:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>2.913557</td>\n",
       "      <td>3.264492</td>\n",
       "      <td>0.743000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>541.250000</td>\n",
       "      <td>8.301728</td>\n",
       "      <td>3.264492</td>\n",
       "      <td>06:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.962298</td>\n",
       "      <td>3.248720</td>\n",
       "      <td>0.741000</td>\n",
       "      <td>0.523000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>8.235424</td>\n",
       "      <td>3.248720</td>\n",
       "      <td>06:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>2.989217</td>\n",
       "      <td>3.176898</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.517000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>7.881534</td>\n",
       "      <td>3.176898</td>\n",
       "      <td>06:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>2.869630</td>\n",
       "      <td>3.268636</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>538.000000</td>\n",
       "      <td>8.066156</td>\n",
       "      <td>3.268636</td>\n",
       "      <td>06:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>2.919321</td>\n",
       "      <td>3.221422</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.533000</td>\n",
       "      <td>544.000000</td>\n",
       "      <td>8.082623</td>\n",
       "      <td>3.221422</td>\n",
       "      <td>06:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>3.034945</td>\n",
       "      <td>3.264639</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>503.250000</td>\n",
       "      <td>7.753582</td>\n",
       "      <td>3.264638</td>\n",
       "      <td>06:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>3.025765</td>\n",
       "      <td>3.225274</td>\n",
       "      <td>0.753000</td>\n",
       "      <td>0.519000</td>\n",
       "      <td>541.750000</td>\n",
       "      <td>8.260445</td>\n",
       "      <td>3.225274</td>\n",
       "      <td>06:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>2.946060</td>\n",
       "      <td>3.252657</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>513.000000</td>\n",
       "      <td>7.984250</td>\n",
       "      <td>3.252657</td>\n",
       "      <td>06:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>2.896769</td>\n",
       "      <td>3.252649</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.528000</td>\n",
       "      <td>533.250000</td>\n",
       "      <td>8.134182</td>\n",
       "      <td>3.252649</td>\n",
       "      <td>06:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>2.916669</td>\n",
       "      <td>3.223551</td>\n",
       "      <td>0.765000</td>\n",
       "      <td>0.549000</td>\n",
       "      <td>525.000000</td>\n",
       "      <td>8.111290</td>\n",
       "      <td>3.223551</td>\n",
       "      <td>06:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>2.863913</td>\n",
       "      <td>3.208422</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.519000</td>\n",
       "      <td>513.500000</td>\n",
       "      <td>7.960355</td>\n",
       "      <td>3.208421</td>\n",
       "      <td>06:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>3.019584</td>\n",
       "      <td>3.150723</td>\n",
       "      <td>0.771000</td>\n",
       "      <td>0.543000</td>\n",
       "      <td>524.250000</td>\n",
       "      <td>7.950286</td>\n",
       "      <td>3.150723</td>\n",
       "      <td>06:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>2.844954</td>\n",
       "      <td>3.289292</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>529.250000</td>\n",
       "      <td>8.102122</td>\n",
       "      <td>3.289292</td>\n",
       "      <td>06:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>2.925913</td>\n",
       "      <td>3.242612</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>516.750000</td>\n",
       "      <td>7.979115</td>\n",
       "      <td>3.242611</td>\n",
       "      <td>06:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>2.868477</td>\n",
       "      <td>3.279125</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>0.528000</td>\n",
       "      <td>526.000000</td>\n",
       "      <td>7.948410</td>\n",
       "      <td>3.279125</td>\n",
       "      <td>06:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>2.920678</td>\n",
       "      <td>3.237771</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.517000</td>\n",
       "      <td>521.750000</td>\n",
       "      <td>8.019085</td>\n",
       "      <td>3.237771</td>\n",
       "      <td>06:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>2.936611</td>\n",
       "      <td>3.192010</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>538.750000</td>\n",
       "      <td>7.993061</td>\n",
       "      <td>3.192011</td>\n",
       "      <td>06:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>2.887995</td>\n",
       "      <td>3.310966</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>518.750000</td>\n",
       "      <td>7.959620</td>\n",
       "      <td>3.310965</td>\n",
       "      <td>06:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>2.777689</td>\n",
       "      <td>3.262861</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.521000</td>\n",
       "      <td>524.500000</td>\n",
       "      <td>7.930230</td>\n",
       "      <td>3.262861</td>\n",
       "      <td>06:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>2.765917</td>\n",
       "      <td>3.283037</td>\n",
       "      <td>0.737000</td>\n",
       "      <td>0.522000</td>\n",
       "      <td>548.000000</td>\n",
       "      <td>8.162829</td>\n",
       "      <td>3.283038</td>\n",
       "      <td>06:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>2.792117</td>\n",
       "      <td>3.178421</td>\n",
       "      <td>0.759000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>544.750000</td>\n",
       "      <td>8.241898</td>\n",
       "      <td>3.178421</td>\n",
       "      <td>06:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.995252</td>\n",
       "      <td>3.247464</td>\n",
       "      <td>0.739000</td>\n",
       "      <td>0.529000</td>\n",
       "      <td>517.500000</td>\n",
       "      <td>7.861552</td>\n",
       "      <td>3.247464</td>\n",
       "      <td>06:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>2.990281</td>\n",
       "      <td>3.245579</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0.532000</td>\n",
       "      <td>544.000000</td>\n",
       "      <td>8.192449</td>\n",
       "      <td>3.245579</td>\n",
       "      <td>06:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>2.880368</td>\n",
       "      <td>3.212159</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>526.500000</td>\n",
       "      <td>7.953773</td>\n",
       "      <td>3.212159</td>\n",
       "      <td>06:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>2.975058</td>\n",
       "      <td>3.223986</td>\n",
       "      <td>0.736000</td>\n",
       "      <td>0.517000</td>\n",
       "      <td>519.500000</td>\n",
       "      <td>7.924760</td>\n",
       "      <td>3.223986</td>\n",
       "      <td>06:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>2.901524</td>\n",
       "      <td>3.214622</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>539.000000</td>\n",
       "      <td>8.065006</td>\n",
       "      <td>3.214622</td>\n",
       "      <td>06:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>2.948950</td>\n",
       "      <td>3.207706</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>557.000000</td>\n",
       "      <td>8.353781</td>\n",
       "      <td>3.207706</td>\n",
       "      <td>06:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>2.794885</td>\n",
       "      <td>3.267120</td>\n",
       "      <td>0.734000</td>\n",
       "      <td>0.519000</td>\n",
       "      <td>520.750000</td>\n",
       "      <td>8.058009</td>\n",
       "      <td>3.267120</td>\n",
       "      <td>06:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>2.994737</td>\n",
       "      <td>3.187457</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>7.950069</td>\n",
       "      <td>3.187457</td>\n",
       "      <td>06:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>2.890558</td>\n",
       "      <td>3.188383</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>517.750000</td>\n",
       "      <td>7.815935</td>\n",
       "      <td>3.188383</td>\n",
       "      <td>06:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>2.928802</td>\n",
       "      <td>3.186696</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.537000</td>\n",
       "      <td>576.500000</td>\n",
       "      <td>8.372744</td>\n",
       "      <td>3.186696</td>\n",
       "      <td>06:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.839649</td>\n",
       "      <td>3.142365</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.556000</td>\n",
       "      <td>542.750000</td>\n",
       "      <td>8.282669</td>\n",
       "      <td>3.142365</td>\n",
       "      <td>06:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>2.797400</td>\n",
       "      <td>3.184709</td>\n",
       "      <td>0.758000</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>532.500000</td>\n",
       "      <td>7.962376</td>\n",
       "      <td>3.184709</td>\n",
       "      <td>06:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>2.987713</td>\n",
       "      <td>3.244279</td>\n",
       "      <td>0.747000</td>\n",
       "      <td>0.524000</td>\n",
       "      <td>540.750000</td>\n",
       "      <td>8.169011</td>\n",
       "      <td>3.244278</td>\n",
       "      <td>06:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>2.893075</td>\n",
       "      <td>3.225942</td>\n",
       "      <td>0.754000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>511.250000</td>\n",
       "      <td>7.896085</td>\n",
       "      <td>3.225942</td>\n",
       "      <td>06:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>2.979929</td>\n",
       "      <td>3.180910</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.526000</td>\n",
       "      <td>536.750000</td>\n",
       "      <td>8.074684</td>\n",
       "      <td>3.180910</td>\n",
       "      <td>06:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>2.912668</td>\n",
       "      <td>3.137906</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.549000</td>\n",
       "      <td>533.500000</td>\n",
       "      <td>8.010656</td>\n",
       "      <td>3.137906</td>\n",
       "      <td>06:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>2.845348</td>\n",
       "      <td>3.181665</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>560.750000</td>\n",
       "      <td>8.201937</td>\n",
       "      <td>3.181665</td>\n",
       "      <td>06:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>2.835205</td>\n",
       "      <td>3.172124</td>\n",
       "      <td>0.759000</td>\n",
       "      <td>0.545000</td>\n",
       "      <td>574.750000</td>\n",
       "      <td>8.284008</td>\n",
       "      <td>3.172124</td>\n",
       "      <td>06:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>2.781361</td>\n",
       "      <td>3.207673</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.532000</td>\n",
       "      <td>545.500000</td>\n",
       "      <td>8.090307</td>\n",
       "      <td>3.207673</td>\n",
       "      <td>06:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>2.839679</td>\n",
       "      <td>3.206109</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.534000</td>\n",
       "      <td>526.750000</td>\n",
       "      <td>7.964663</td>\n",
       "      <td>3.206110</td>\n",
       "      <td>06:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.878113</td>\n",
       "      <td>3.110707</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>542.000000</td>\n",
       "      <td>8.070807</td>\n",
       "      <td>3.110706</td>\n",
       "      <td>06:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>3.048666</td>\n",
       "      <td>3.216237</td>\n",
       "      <td>0.764000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>530.500000</td>\n",
       "      <td>8.003363</td>\n",
       "      <td>3.216237</td>\n",
       "      <td>06:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>2.877666</td>\n",
       "      <td>3.271843</td>\n",
       "      <td>0.735000</td>\n",
       "      <td>0.527000</td>\n",
       "      <td>516.750000</td>\n",
       "      <td>7.797584</td>\n",
       "      <td>3.271843</td>\n",
       "      <td>06:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>2.782120</td>\n",
       "      <td>3.122734</td>\n",
       "      <td>0.759000</td>\n",
       "      <td>0.529000</td>\n",
       "      <td>540.750000</td>\n",
       "      <td>7.963701</td>\n",
       "      <td>3.122734</td>\n",
       "      <td>06:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>2.840529</td>\n",
       "      <td>3.205187</td>\n",
       "      <td>0.741000</td>\n",
       "      <td>0.543000</td>\n",
       "      <td>512.750000</td>\n",
       "      <td>7.906914</td>\n",
       "      <td>3.205186</td>\n",
       "      <td>06:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>2.907837</td>\n",
       "      <td>3.217613</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.531000</td>\n",
       "      <td>540.250000</td>\n",
       "      <td>8.075951</td>\n",
       "      <td>3.217613</td>\n",
       "      <td>06:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>2.788478</td>\n",
       "      <td>3.246834</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.526000</td>\n",
       "      <td>572.000000</td>\n",
       "      <td>8.056746</td>\n",
       "      <td>3.246835</td>\n",
       "      <td>06:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>2.917799</td>\n",
       "      <td>3.220932</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.531000</td>\n",
       "      <td>530.500000</td>\n",
       "      <td>7.967813</td>\n",
       "      <td>3.220933</td>\n",
       "      <td>06:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>2.901695</td>\n",
       "      <td>3.190083</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>546.000000</td>\n",
       "      <td>8.106994</td>\n",
       "      <td>3.190084</td>\n",
       "      <td>06:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>3.060876</td>\n",
       "      <td>3.266494</td>\n",
       "      <td>0.747000</td>\n",
       "      <td>0.527000</td>\n",
       "      <td>522.500000</td>\n",
       "      <td>7.952044</td>\n",
       "      <td>3.266494</td>\n",
       "      <td>06:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.877402</td>\n",
       "      <td>3.200881</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.531000</td>\n",
       "      <td>520.250000</td>\n",
       "      <td>8.039659</td>\n",
       "      <td>3.200881</td>\n",
       "      <td>06:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>2.997886</td>\n",
       "      <td>3.231050</td>\n",
       "      <td>0.759000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>541.250000</td>\n",
       "      <td>8.174556</td>\n",
       "      <td>3.231050</td>\n",
       "      <td>06:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>2.820630</td>\n",
       "      <td>3.256612</td>\n",
       "      <td>0.754000</td>\n",
       "      <td>0.529000</td>\n",
       "      <td>560.500000</td>\n",
       "      <td>8.236988</td>\n",
       "      <td>3.256612</td>\n",
       "      <td>06:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>2.790000</td>\n",
       "      <td>3.190542</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.529000</td>\n",
       "      <td>581.000000</td>\n",
       "      <td>8.211960</td>\n",
       "      <td>3.190543</td>\n",
       "      <td>06:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>2.849445</td>\n",
       "      <td>3.215853</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>551.250000</td>\n",
       "      <td>8.092755</td>\n",
       "      <td>3.215853</td>\n",
       "      <td>06:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>2.926581</td>\n",
       "      <td>3.216785</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0.533000</td>\n",
       "      <td>548.750000</td>\n",
       "      <td>8.052494</td>\n",
       "      <td>3.216785</td>\n",
       "      <td>06:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>3.087620</td>\n",
       "      <td>3.153273</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>540.000000</td>\n",
       "      <td>8.019381</td>\n",
       "      <td>3.153273</td>\n",
       "      <td>06:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>2.967889</td>\n",
       "      <td>3.148417</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.542000</td>\n",
       "      <td>539.500000</td>\n",
       "      <td>8.091598</td>\n",
       "      <td>3.148417</td>\n",
       "      <td>06:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>2.811631</td>\n",
       "      <td>3.147274</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.544000</td>\n",
       "      <td>572.250000</td>\n",
       "      <td>8.341072</td>\n",
       "      <td>3.147274</td>\n",
       "      <td>06:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>2.904289</td>\n",
       "      <td>3.129251</td>\n",
       "      <td>0.763000</td>\n",
       "      <td>0.557000</td>\n",
       "      <td>563.750000</td>\n",
       "      <td>8.059053</td>\n",
       "      <td>3.129251</td>\n",
       "      <td>06:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.900692</td>\n",
       "      <td>3.189312</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.527000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>8.254706</td>\n",
       "      <td>3.189312</td>\n",
       "      <td>06:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>2.878636</td>\n",
       "      <td>3.079697</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>549.500000</td>\n",
       "      <td>7.981597</td>\n",
       "      <td>3.079697</td>\n",
       "      <td>06:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>2.868994</td>\n",
       "      <td>3.130705</td>\n",
       "      <td>0.764000</td>\n",
       "      <td>0.552000</td>\n",
       "      <td>549.750000</td>\n",
       "      <td>8.133592</td>\n",
       "      <td>3.130705</td>\n",
       "      <td>06:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>2.834140</td>\n",
       "      <td>3.236531</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>528.500000</td>\n",
       "      <td>8.030102</td>\n",
       "      <td>3.236531</td>\n",
       "      <td>06:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>2.973341</td>\n",
       "      <td>3.155306</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.545000</td>\n",
       "      <td>528.250000</td>\n",
       "      <td>7.825991</td>\n",
       "      <td>3.155306</td>\n",
       "      <td>06:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>2.769341</td>\n",
       "      <td>3.161247</td>\n",
       "      <td>0.753000</td>\n",
       "      <td>0.533000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>7.888350</td>\n",
       "      <td>3.161247</td>\n",
       "      <td>06:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>2.813797</td>\n",
       "      <td>3.143985</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.537000</td>\n",
       "      <td>543.500000</td>\n",
       "      <td>8.024164</td>\n",
       "      <td>3.143985</td>\n",
       "      <td>06:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>2.842404</td>\n",
       "      <td>3.171029</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>0.532000</td>\n",
       "      <td>536.250000</td>\n",
       "      <td>8.243732</td>\n",
       "      <td>3.171029</td>\n",
       "      <td>06:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>2.928466</td>\n",
       "      <td>3.187730</td>\n",
       "      <td>0.754000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>562.250000</td>\n",
       "      <td>8.085867</td>\n",
       "      <td>3.187731</td>\n",
       "      <td>06:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>3.003611</td>\n",
       "      <td>3.187059</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0.531000</td>\n",
       "      <td>554.750000</td>\n",
       "      <td>8.072202</td>\n",
       "      <td>3.187059</td>\n",
       "      <td>06:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.785483</td>\n",
       "      <td>3.231780</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>0.533000</td>\n",
       "      <td>584.000000</td>\n",
       "      <td>8.306078</td>\n",
       "      <td>3.231780</td>\n",
       "      <td>06:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>2.880726</td>\n",
       "      <td>3.174783</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.549000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>8.163633</td>\n",
       "      <td>3.174783</td>\n",
       "      <td>06:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>2.893814</td>\n",
       "      <td>3.196933</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.532000</td>\n",
       "      <td>575.500000</td>\n",
       "      <td>8.500375</td>\n",
       "      <td>3.196932</td>\n",
       "      <td>06:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>2.928175</td>\n",
       "      <td>3.140961</td>\n",
       "      <td>0.763000</td>\n",
       "      <td>0.549000</td>\n",
       "      <td>579.500000</td>\n",
       "      <td>8.136200</td>\n",
       "      <td>3.140962</td>\n",
       "      <td>06:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>2.866073</td>\n",
       "      <td>3.202233</td>\n",
       "      <td>0.743000</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>536.250000</td>\n",
       "      <td>7.985208</td>\n",
       "      <td>3.202233</td>\n",
       "      <td>07:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>2.852487</td>\n",
       "      <td>3.150641</td>\n",
       "      <td>0.761000</td>\n",
       "      <td>0.542000</td>\n",
       "      <td>570.500000</td>\n",
       "      <td>8.208401</td>\n",
       "      <td>3.150642</td>\n",
       "      <td>06:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>2.862917</td>\n",
       "      <td>3.191160</td>\n",
       "      <td>0.747000</td>\n",
       "      <td>0.529000</td>\n",
       "      <td>547.500000</td>\n",
       "      <td>8.085890</td>\n",
       "      <td>3.191161</td>\n",
       "      <td>06:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>2.770938</td>\n",
       "      <td>3.184614</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>565.000000</td>\n",
       "      <td>8.189565</td>\n",
       "      <td>3.184615</td>\n",
       "      <td>07:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>2.859939</td>\n",
       "      <td>3.148699</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>575.250000</td>\n",
       "      <td>8.419850</td>\n",
       "      <td>3.148699</td>\n",
       "      <td>06:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>2.815625</td>\n",
       "      <td>3.139852</td>\n",
       "      <td>0.776000</td>\n",
       "      <td>0.558000</td>\n",
       "      <td>591.500000</td>\n",
       "      <td>8.349929</td>\n",
       "      <td>3.139853</td>\n",
       "      <td>06:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.941204</td>\n",
       "      <td>3.200491</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>558.750000</td>\n",
       "      <td>8.207750</td>\n",
       "      <td>3.200490</td>\n",
       "      <td>06:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>2.870680</td>\n",
       "      <td>3.109205</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>537.750000</td>\n",
       "      <td>7.913811</td>\n",
       "      <td>3.109205</td>\n",
       "      <td>06:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>2.760283</td>\n",
       "      <td>3.155819</td>\n",
       "      <td>0.765000</td>\n",
       "      <td>0.551000</td>\n",
       "      <td>555.000000</td>\n",
       "      <td>8.188857</td>\n",
       "      <td>3.155820</td>\n",
       "      <td>06:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>2.874754</td>\n",
       "      <td>3.217246</td>\n",
       "      <td>0.747000</td>\n",
       "      <td>0.529000</td>\n",
       "      <td>589.500000</td>\n",
       "      <td>8.217803</td>\n",
       "      <td>3.217246</td>\n",
       "      <td>06:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>2.862238</td>\n",
       "      <td>3.149818</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>0.546000</td>\n",
       "      <td>589.750000</td>\n",
       "      <td>8.166286</td>\n",
       "      <td>3.149818</td>\n",
       "      <td>05:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>2.837286</td>\n",
       "      <td>3.136303</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.551000</td>\n",
       "      <td>586.000000</td>\n",
       "      <td>8.372017</td>\n",
       "      <td>3.136302</td>\n",
       "      <td>06:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>2.977118</td>\n",
       "      <td>3.118525</td>\n",
       "      <td>0.769000</td>\n",
       "      <td>0.546000</td>\n",
       "      <td>581.250000</td>\n",
       "      <td>8.328970</td>\n",
       "      <td>3.118526</td>\n",
       "      <td>06:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>2.769108</td>\n",
       "      <td>3.172385</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>0.549000</td>\n",
       "      <td>577.750000</td>\n",
       "      <td>7.977912</td>\n",
       "      <td>3.172385</td>\n",
       "      <td>06:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>2.801168</td>\n",
       "      <td>3.111047</td>\n",
       "      <td>0.764000</td>\n",
       "      <td>0.551000</td>\n",
       "      <td>535.250000</td>\n",
       "      <td>7.928844</td>\n",
       "      <td>3.111047</td>\n",
       "      <td>06:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>2.840382</td>\n",
       "      <td>3.153379</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>0.548000</td>\n",
       "      <td>563.000000</td>\n",
       "      <td>7.986000</td>\n",
       "      <td>3.153379</td>\n",
       "      <td>06:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.793506</td>\n",
       "      <td>3.180278</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>509.000000</td>\n",
       "      <td>7.613387</td>\n",
       "      <td>3.180279</td>\n",
       "      <td>06:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='562', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[5.6856e-06],\n",
      "        [1.9181e-04],\n",
      "        [2.4252e-04],\n",
      "        [9.9029e-05],\n",
      "        [3.3708e-05],\n",
      "        [8.0436e-04],\n",
      "        [2.7772e-04],\n",
      "        [2.2970e-05],\n",
      "        [3.5408e-05],\n",
      "        [1.3953e-03],\n",
      "        [4.0851e-05],\n",
      "        [8.5288e-04],\n",
      "        [5.5088e-05],\n",
      "        [5.9643e-04],\n",
      "        [1.4670e-04],\n",
      "        [1.0158e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.982696533203125: \n",
      "target probs tensor([[6.6124e-04],\n",
      "        [3.7076e-04],\n",
      "        [1.2855e-04],\n",
      "        [1.2186e-04],\n",
      "        [1.0856e-04],\n",
      "        [5.0189e-04],\n",
      "        [1.1911e-03],\n",
      "        [2.2865e-05],\n",
      "        [5.4431e-05],\n",
      "        [5.5135e-06],\n",
      "        [2.7301e-03],\n",
      "        [6.8788e-05],\n",
      "        [3.2972e-05],\n",
      "        [2.9403e-05],\n",
      "        [2.8128e-04],\n",
      "        [8.7740e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.082886695861816: \n",
      "target probs tensor([[8.6046e-05],\n",
      "        [1.8101e-03],\n",
      "        [3.2466e-05],\n",
      "        [1.0306e-05],\n",
      "        [1.8002e-04],\n",
      "        [7.8074e-06],\n",
      "        [1.4715e-04],\n",
      "        [2.8951e-03],\n",
      "        [7.9996e-03],\n",
      "        [3.9260e-04],\n",
      "        [7.4644e-04],\n",
      "        [3.1285e-05],\n",
      "        [3.2360e-04],\n",
      "        [1.5832e-04],\n",
      "        [1.0014e-04],\n",
      "        [7.3149e-05]], device='cuda:0'), loss: 8.644204139709473: \n",
      "Better model found at epoch 0 with validation value: 0.5260000228881836.\n",
      "target probs tensor([[8.1760e-06],\n",
      "        [1.9905e-03],\n",
      "        [3.8887e-06],\n",
      "        [1.2718e-04],\n",
      "        [3.8647e-04],\n",
      "        [4.3494e-05],\n",
      "        [4.4208e-06],\n",
      "        [1.0966e-04],\n",
      "        [2.3338e-04],\n",
      "        [4.2979e-05],\n",
      "        [1.9143e-04],\n",
      "        [4.8160e-03],\n",
      "        [1.5870e-04],\n",
      "        [1.6961e-04],\n",
      "        [1.1031e-04],\n",
      "        [7.1191e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.194807052612305: \n",
      "target probs tensor([[2.0174e-04],\n",
      "        [9.3361e-05],\n",
      "        [4.8090e-05],\n",
      "        [1.5307e-04],\n",
      "        [1.2903e-03],\n",
      "        [2.2794e-05],\n",
      "        [1.2729e-03],\n",
      "        [1.7537e-03],\n",
      "        [6.1425e-03],\n",
      "        [1.3225e-04],\n",
      "        [6.0478e-05],\n",
      "        [1.8636e-04],\n",
      "        [2.4278e-05],\n",
      "        [1.7924e-04],\n",
      "        [8.0854e-04],\n",
      "        [2.4927e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.078639030456543: \n",
      "target probs tensor([[7.7488e-05],\n",
      "        [2.7845e-02],\n",
      "        [8.0902e-04],\n",
      "        [2.2781e-03],\n",
      "        [2.8574e-03],\n",
      "        [1.1360e-04],\n",
      "        [2.8937e-04],\n",
      "        [2.9536e-03],\n",
      "        [1.5390e-06],\n",
      "        [1.1677e-01],\n",
      "        [8.0546e-05],\n",
      "        [1.8146e-04],\n",
      "        [1.2074e-03],\n",
      "        [1.8063e-01],\n",
      "        [1.1744e-05],\n",
      "        [6.3146e-04]], device='cuda:0'), loss: 7.242943286895752: \n",
      "Better model found at epoch 1 with validation value: 0.546999990940094.\n",
      "target probs tensor([[2.5525e-04],\n",
      "        [2.5055e-03],\n",
      "        [1.1357e-04],\n",
      "        [1.3414e-01],\n",
      "        [7.9049e-04],\n",
      "        [1.5697e-04],\n",
      "        [3.1956e-02],\n",
      "        [1.4333e-04],\n",
      "        [1.2510e-02],\n",
      "        [4.0011e-04],\n",
      "        [1.5774e-02],\n",
      "        [1.2303e-02],\n",
      "        [2.3214e-05],\n",
      "        [1.3482e-04],\n",
      "        [2.9055e-05],\n",
      "        [3.6763e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.871091365814209: \n",
      "target probs tensor([[8.5679e-04],\n",
      "        [6.7511e-03],\n",
      "        [2.7194e-04],\n",
      "        [4.9828e-05],\n",
      "        [8.3972e-05],\n",
      "        [1.1899e-02],\n",
      "        [1.2855e-05],\n",
      "        [1.2483e-04],\n",
      "        [5.7637e-03],\n",
      "        [5.8296e-03],\n",
      "        [1.1590e-04],\n",
      "        [1.3422e-04],\n",
      "        [3.1923e-03],\n",
      "        [6.7340e-03],\n",
      "        [1.1344e-04],\n",
      "        [1.2004e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.29862642288208: \n",
      "target probs tensor([[1.4798e-03],\n",
      "        [1.2722e-04],\n",
      "        [2.4580e-03],\n",
      "        [5.8955e-04],\n",
      "        [4.1787e-02],\n",
      "        [6.9751e-03],\n",
      "        [7.6749e-04],\n",
      "        [2.5895e-04],\n",
      "        [3.5639e-03],\n",
      "        [3.1988e-05],\n",
      "        [6.0927e-03],\n",
      "        [1.5539e-03],\n",
      "        [2.4311e-04],\n",
      "        [2.3994e-03],\n",
      "        [2.4462e-04],\n",
      "        [8.7356e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.004521369934082: \n",
      "Better model found at epoch 2 with validation value: 0.6029999852180481.\n",
      "target probs tensor([[5.9425e-04],\n",
      "        [1.8909e-03],\n",
      "        [4.5678e-03],\n",
      "        [3.1831e-03],\n",
      "        [1.5847e-05],\n",
      "        [1.5417e-05],\n",
      "        [2.7132e-04],\n",
      "        [3.3487e-03],\n",
      "        [4.1570e-03],\n",
      "        [3.4420e-05],\n",
      "        [5.5013e-03],\n",
      "        [3.5669e-03],\n",
      "        [1.4180e-01],\n",
      "        [6.2020e-01],\n",
      "        [2.4615e-01],\n",
      "        [2.3864e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.372256278991699: \n",
      "target probs tensor([[3.0063e-04],\n",
      "        [3.7470e-05],\n",
      "        [2.7434e-03],\n",
      "        [7.5495e-03],\n",
      "        [5.9021e-03],\n",
      "        [9.1862e-07],\n",
      "        [1.2353e-01],\n",
      "        [1.1920e-03],\n",
      "        [1.1137e-01],\n",
      "        [3.4679e-02],\n",
      "        [2.2883e-03],\n",
      "        [2.2787e-02],\n",
      "        [6.1770e-05],\n",
      "        [3.5515e-04],\n",
      "        [3.6740e-04],\n",
      "        [7.9282e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.421371936798096: \n",
      "target probs tensor([[1.4062e-02],\n",
      "        [1.4590e-04],\n",
      "        [1.3747e-01],\n",
      "        [3.2133e-03],\n",
      "        [2.3644e-04],\n",
      "        [2.1785e-01],\n",
      "        [9.2093e-06],\n",
      "        [3.4525e-02],\n",
      "        [7.1327e-04],\n",
      "        [2.3113e-02],\n",
      "        [2.2501e-03],\n",
      "        [6.9016e-03],\n",
      "        [9.5437e-02],\n",
      "        [1.5454e-04],\n",
      "        [1.5774e-02],\n",
      "        [1.4604e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 5.596588134765625: \n",
      "Better model found at epoch 3 with validation value: 0.6069999933242798.\n",
      "target probs tensor([[2.0674e-04],\n",
      "        [2.0567e-05],\n",
      "        [1.1323e-02],\n",
      "        [4.6855e-06],\n",
      "        [1.7372e-01],\n",
      "        [2.6929e-01],\n",
      "        [7.6669e-04],\n",
      "        [5.9100e-03],\n",
      "        [3.9343e-04],\n",
      "        [6.1296e-01],\n",
      "        [7.3725e-04],\n",
      "        [3.4795e-02],\n",
      "        [1.1876e-03],\n",
      "        [1.8983e-01],\n",
      "        [3.4977e-01],\n",
      "        [1.1687e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 5.405961036682129: \n",
      "target probs tensor([[5.3950e-03],\n",
      "        [1.5398e-03],\n",
      "        [6.4772e-02],\n",
      "        [1.4857e-02],\n",
      "        [2.0024e-02],\n",
      "        [1.1543e-01],\n",
      "        [1.3206e-01],\n",
      "        [4.1850e-05],\n",
      "        [4.2417e-03],\n",
      "        [1.2308e-04],\n",
      "        [6.9297e-04],\n",
      "        [2.9221e-01],\n",
      "        [2.0372e-02],\n",
      "        [2.2452e-04],\n",
      "        [5.0452e-07],\n",
      "        [9.4133e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 5.991004943847656: \n",
      "target probs tensor([[4.8100e-01],\n",
      "        [1.0292e-02],\n",
      "        [5.5241e-04],\n",
      "        [4.0825e-04],\n",
      "        [6.8842e-01],\n",
      "        [1.5296e-04],\n",
      "        [1.3059e-05],\n",
      "        [4.4676e-04],\n",
      "        [2.6164e-01],\n",
      "        [2.7046e-04],\n",
      "        [3.4853e-03],\n",
      "        [1.8265e-01],\n",
      "        [3.0495e-03],\n",
      "        [4.1292e-04],\n",
      "        [1.3899e-03],\n",
      "        [8.9759e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 5.657729148864746: \n",
      "Better model found at epoch 4 with validation value: 0.6480000019073486.\n",
      "target probs tensor([[1.8118e-02],\n",
      "        [7.2274e-01],\n",
      "        [4.6456e-01],\n",
      "        [5.2100e-04],\n",
      "        [1.2342e-01],\n",
      "        [2.4447e-01],\n",
      "        [1.4320e-02],\n",
      "        [3.5264e-02],\n",
      "        [2.9810e-02],\n",
      "        [3.6427e-04],\n",
      "        [3.4176e-02],\n",
      "        [2.6338e-02],\n",
      "        [2.8525e-01],\n",
      "        [1.6269e-01],\n",
      "        [1.9694e-01],\n",
      "        [1.0253e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.648771047592163: \n",
      "target probs tensor([[4.3990e-03],\n",
      "        [1.2828e-04],\n",
      "        [5.4800e-01],\n",
      "        [3.2204e-04],\n",
      "        [4.5680e-03],\n",
      "        [2.8036e-02],\n",
      "        [3.3309e-06],\n",
      "        [7.5137e-05],\n",
      "        [6.4833e-01],\n",
      "        [9.0786e-04],\n",
      "        [3.8954e-04],\n",
      "        [1.8281e-04],\n",
      "        [1.2971e-02],\n",
      "        [1.0535e-01],\n",
      "        [2.0158e-02],\n",
      "        [1.4179e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 5.940939903259277: \n",
      "target probs tensor([[4.9007e-01],\n",
      "        [3.6262e-01],\n",
      "        [1.8639e-04],\n",
      "        [2.2035e-02],\n",
      "        [7.0135e-01],\n",
      "        [1.4876e-03],\n",
      "        [7.2450e-04],\n",
      "        [5.2271e-01],\n",
      "        [3.7315e-03],\n",
      "        [1.5977e-02],\n",
      "        [1.2815e-01],\n",
      "        [4.0673e-02],\n",
      "        [9.2318e-02],\n",
      "        [2.2628e-03],\n",
      "        [1.7704e-02],\n",
      "        [7.6729e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.9711928367614746: \n",
      "target probs tensor([[4.7200e-03],\n",
      "        [1.2654e-05],\n",
      "        [4.4169e-01],\n",
      "        [9.3500e-05],\n",
      "        [3.1233e-03],\n",
      "        [6.3890e-03],\n",
      "        [2.3004e-02],\n",
      "        [3.2754e-01],\n",
      "        [6.6938e-03],\n",
      "        [1.1382e-02],\n",
      "        [1.3280e-03],\n",
      "        [3.7810e-02],\n",
      "        [6.5954e-01],\n",
      "        [7.4068e-01],\n",
      "        [1.3542e-02],\n",
      "        [1.8694e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.713942050933838: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[9.6953e-02],\n",
      "        [1.4153e-02],\n",
      "        [1.8943e-04],\n",
      "        [3.1195e-06],\n",
      "        [1.0811e-04],\n",
      "        [1.2395e-03],\n",
      "        [1.3506e-01],\n",
      "        [4.4689e-03],\n",
      "        [2.6389e-04],\n",
      "        [1.5691e-05],\n",
      "        [1.0093e-04],\n",
      "        [3.4234e-03],\n",
      "        [2.2288e-03],\n",
      "        [4.0787e-03],\n",
      "        [1.2177e-01],\n",
      "        [1.2841e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.6019287109375: \n",
      "target probs tensor([[1.0349e-05],\n",
      "        [7.0680e-01],\n",
      "        [1.0205e-03],\n",
      "        [6.6276e-03],\n",
      "        [7.7021e-04],\n",
      "        [2.5208e-03],\n",
      "        [2.1400e-04],\n",
      "        [8.9124e-02],\n",
      "        [1.2983e-05],\n",
      "        [6.7472e-02],\n",
      "        [2.1363e-04],\n",
      "        [1.6230e-01],\n",
      "        [7.8692e-01],\n",
      "        [5.9596e-02],\n",
      "        [4.2163e-02],\n",
      "        [9.9164e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 5.175366401672363: \n",
      "Better model found at epoch 6 with validation value: 0.6669999957084656.\n",
      "target probs tensor([[9.5383e-01],\n",
      "        [7.9834e-02],\n",
      "        [5.7650e-01],\n",
      "        [1.5482e-01],\n",
      "        [6.1982e-02],\n",
      "        [2.5635e-01],\n",
      "        [5.8621e-04],\n",
      "        [2.5253e-03],\n",
      "        [1.0229e-03],\n",
      "        [5.2596e-02],\n",
      "        [4.2551e-03],\n",
      "        [4.8577e-04],\n",
      "        [8.2204e-03],\n",
      "        [1.2725e-01],\n",
      "        [1.3339e-03],\n",
      "        [8.8532e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.692535877227783: \n",
      "target probs tensor([[1.7602e-04],\n",
      "        [3.6459e-05],\n",
      "        [9.4711e-04],\n",
      "        [4.5820e-04],\n",
      "        [2.7723e-04],\n",
      "        [4.6420e-01],\n",
      "        [8.0049e-02],\n",
      "        [7.0589e-01],\n",
      "        [1.7370e-04],\n",
      "        [2.8616e-03],\n",
      "        [2.3817e-01],\n",
      "        [2.5041e-02],\n",
      "        [7.3926e-01],\n",
      "        [4.4298e-05],\n",
      "        [3.5484e-03],\n",
      "        [5.9045e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 5.0923380851745605: \n",
      "target probs tensor([[1.9580e-03],\n",
      "        [1.1974e-01],\n",
      "        [1.9936e-03],\n",
      "        [1.3829e-03],\n",
      "        [9.3219e-03],\n",
      "        [1.6528e-01],\n",
      "        [1.1416e-02],\n",
      "        [8.7367e-05],\n",
      "        [5.6564e-01],\n",
      "        [4.5537e-05],\n",
      "        [9.3791e-01],\n",
      "        [1.2670e-02],\n",
      "        [5.5333e-03],\n",
      "        [4.6973e-03],\n",
      "        [4.2217e-01],\n",
      "        [9.2254e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.247089385986328: \n",
      "target probs tensor([[0.0023],\n",
      "        [0.6567],\n",
      "        [0.5346],\n",
      "        [0.0011],\n",
      "        [0.0054],\n",
      "        [0.0075],\n",
      "        [0.5030],\n",
      "        [0.2592]], device='cuda:0'), loss: 3.2569899559020996: \n",
      "Better model found at epoch 7 with validation value: 0.6700000166893005.\n",
      "target probs tensor([[2.2674e-01],\n",
      "        [2.2130e-01],\n",
      "        [9.0579e-01],\n",
      "        [2.4152e-01],\n",
      "        [7.7196e-01],\n",
      "        [1.0454e-01],\n",
      "        [1.8717e-01],\n",
      "        [7.1427e-01],\n",
      "        [5.0986e-01],\n",
      "        [7.8854e-05],\n",
      "        [1.7851e-03],\n",
      "        [9.7768e-01],\n",
      "        [1.9585e-03],\n",
      "        [4.7675e-04],\n",
      "        [1.4385e-02],\n",
      "        [7.5442e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.0328803062438965: \n",
      "target probs tensor([[9.7597e-01],\n",
      "        [2.3721e-02],\n",
      "        [8.9051e-06],\n",
      "        [2.3714e-05],\n",
      "        [1.9788e-03],\n",
      "        [2.3428e-02],\n",
      "        [2.6516e-01],\n",
      "        [5.1115e-05],\n",
      "        [3.8786e-04],\n",
      "        [5.0679e-04],\n",
      "        [6.4113e-01],\n",
      "        [1.4838e-01],\n",
      "        [8.9218e-01],\n",
      "        [2.0008e-06],\n",
      "        [2.6375e-05],\n",
      "        [6.0956e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 5.8691253662109375: \n",
      "target probs tensor([[4.0668e-04],\n",
      "        [3.9116e-03],\n",
      "        [4.7378e-05],\n",
      "        [7.4018e-01],\n",
      "        [1.8302e-04],\n",
      "        [5.2281e-04],\n",
      "        [9.0279e-01],\n",
      "        [5.2962e-03],\n",
      "        [2.5379e-01],\n",
      "        [2.4553e-01],\n",
      "        [4.1617e-03],\n",
      "        [6.5184e-04],\n",
      "        [7.1277e-01],\n",
      "        [5.0226e-03],\n",
      "        [5.8410e-02],\n",
      "        [6.8979e-03]], device='cuda:0'), loss: 4.6348185539245605: \n",
      "Better model found at epoch 8 with validation value: 0.6840000152587891.\n",
      "target probs tensor([[2.6132e-02],\n",
      "        [9.9481e-01],\n",
      "        [1.7553e-01],\n",
      "        [1.0754e-03],\n",
      "        [9.0385e-01],\n",
      "        [2.5478e-01],\n",
      "        [6.8250e-05],\n",
      "        [4.8518e-02],\n",
      "        [3.4560e-01],\n",
      "        [8.4482e-01],\n",
      "        [3.0864e-01],\n",
      "        [7.0246e-01],\n",
      "        [2.0027e-05],\n",
      "        [3.5673e-01],\n",
      "        [1.1551e-01],\n",
      "        [2.1652e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.507612705230713: \n",
      "target probs tensor([[1.2684e-03],\n",
      "        [3.0202e-01],\n",
      "        [9.2147e-03],\n",
      "        [2.1808e-04],\n",
      "        [3.7745e-01],\n",
      "        [5.7481e-03],\n",
      "        [2.7529e-03],\n",
      "        [4.8292e-01],\n",
      "        [7.6392e-03],\n",
      "        [9.0441e-04],\n",
      "        [4.4339e-02],\n",
      "        [5.2716e-02],\n",
      "        [2.4297e-03],\n",
      "        [2.7078e-01],\n",
      "        [9.0446e-01],\n",
      "        [6.7106e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.1510009765625: \n",
      "target probs tensor([[3.0888e-01],\n",
      "        [9.6909e-02],\n",
      "        [1.9151e-02],\n",
      "        [6.0332e-04],\n",
      "        [8.3293e-02],\n",
      "        [1.4304e-01],\n",
      "        [4.3324e-02],\n",
      "        [3.3673e-02],\n",
      "        [4.6629e-06],\n",
      "        [4.2051e-01],\n",
      "        [6.8348e-04],\n",
      "        [1.0290e-03],\n",
      "        [6.0810e-02],\n",
      "        [7.9538e-02],\n",
      "        [4.4550e-04],\n",
      "        [7.1508e-01]], device='cuda:0'), loss: 4.158137321472168: \n",
      "target probs tensor([[5.1509e-01],\n",
      "        [9.5520e-06],\n",
      "        [5.7644e-04],\n",
      "        [1.7278e-02],\n",
      "        [1.2196e-02],\n",
      "        [1.8887e-03],\n",
      "        [5.0378e-02],\n",
      "        [5.8423e-01],\n",
      "        [7.0441e-01],\n",
      "        [2.3251e-03],\n",
      "        [2.9124e-01],\n",
      "        [7.4927e-01],\n",
      "        [9.8634e-01],\n",
      "        [7.5169e-01],\n",
      "        [2.4495e-01],\n",
      "        [7.8301e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.2772276401519775: \n",
      "target probs tensor([[0.5100],\n",
      "        [0.9404],\n",
      "        [0.6185],\n",
      "        [0.0119],\n",
      "        [0.0029],\n",
      "        [0.7078],\n",
      "        [0.9832],\n",
      "        [0.0209],\n",
      "        [0.0058],\n",
      "        [0.0085],\n",
      "        [0.2544],\n",
      "        [0.5710],\n",
      "        [0.0030],\n",
      "        [0.2901],\n",
      "        [0.0013],\n",
      "        [0.0051]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.908818244934082: \n",
      "target probs tensor([[1.4876e-01],\n",
      "        [1.0738e-03],\n",
      "        [4.6940e-04],\n",
      "        [9.5913e-01],\n",
      "        [3.1650e-02],\n",
      "        [1.2384e-02],\n",
      "        [3.7516e-02],\n",
      "        [6.0614e-01],\n",
      "        [3.1399e-01],\n",
      "        [5.8286e-04],\n",
      "        [2.0836e-04],\n",
      "        [1.3177e-01],\n",
      "        [2.8048e-01],\n",
      "        [3.4028e-01],\n",
      "        [3.6250e-02],\n",
      "        [4.5968e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.3517770767211914: \n",
      "Better model found at epoch 10 with validation value: 0.6890000104904175.\n",
      "target probs tensor([[2.4710e-01],\n",
      "        [1.4231e-01],\n",
      "        [4.2951e-01],\n",
      "        [4.7561e-05],\n",
      "        [8.4173e-01],\n",
      "        [6.3195e-01],\n",
      "        [7.3858e-01],\n",
      "        [7.3990e-01],\n",
      "        [2.0016e-04],\n",
      "        [5.2903e-05],\n",
      "        [4.3396e-04],\n",
      "        [8.0795e-01],\n",
      "        [7.1263e-02],\n",
      "        [1.4440e-03],\n",
      "        [6.4251e-01],\n",
      "        [6.8070e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.231865167617798: \n",
      "target probs tensor([[4.8990e-02],\n",
      "        [1.3408e-04],\n",
      "        [4.3718e-01],\n",
      "        [9.4042e-01],\n",
      "        [3.8034e-01],\n",
      "        [3.4264e-04],\n",
      "        [4.0574e-01],\n",
      "        [5.6973e-01],\n",
      "        [7.9926e-01],\n",
      "        [9.0309e-04],\n",
      "        [6.7688e-06],\n",
      "        [2.4671e-01],\n",
      "        [7.7441e-01],\n",
      "        [2.1174e-01],\n",
      "        [4.8484e-02],\n",
      "        [1.8995e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.141515016555786: \n",
      "target probs tensor([[7.0915e-06],\n",
      "        [8.7919e-04],\n",
      "        [4.9719e-01],\n",
      "        [5.3612e-01],\n",
      "        [8.6443e-04],\n",
      "        [5.9263e-02],\n",
      "        [3.5441e-05],\n",
      "        [6.5452e-05],\n",
      "        [1.2317e-02],\n",
      "        [1.4758e-02],\n",
      "        [4.3916e-03],\n",
      "        [8.9899e-01],\n",
      "        [4.5614e-06],\n",
      "        [7.7173e-04],\n",
      "        [2.9421e-03],\n",
      "        [8.7403e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 5.596978187561035: \n",
      "Better model found at epoch 11 with validation value: 0.6909999847412109.\n",
      "target probs tensor([[5.8461e-01],\n",
      "        [3.1451e-04],\n",
      "        [2.9398e-05],\n",
      "        [5.7791e-01],\n",
      "        [1.6474e-04],\n",
      "        [8.2341e-01],\n",
      "        [5.9171e-03],\n",
      "        [8.7369e-02],\n",
      "        [4.9347e-02],\n",
      "        [5.2075e-02],\n",
      "        [5.9430e-05],\n",
      "        [3.1629e-03],\n",
      "        [8.9803e-03],\n",
      "        [8.6436e-01],\n",
      "        [7.8234e-01],\n",
      "        [2.8614e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.991459846496582: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[2.1121e-03],\n",
      "        [3.6834e-01],\n",
      "        [8.8952e-01],\n",
      "        [5.3616e-05],\n",
      "        [1.9429e-01],\n",
      "        [9.9539e-01],\n",
      "        [2.6956e-04],\n",
      "        [1.1904e-01],\n",
      "        [9.2791e-01],\n",
      "        [1.9389e-02],\n",
      "        [2.3823e-05],\n",
      "        [8.1792e-03],\n",
      "        [1.1272e-03],\n",
      "        [1.0282e-02],\n",
      "        [4.1869e-04],\n",
      "        [4.6859e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.279394149780273: \n",
      "target probs tensor([[1.8842e-03],\n",
      "        [5.8792e-01],\n",
      "        [3.6426e-04],\n",
      "        [4.8395e-03],\n",
      "        [1.8307e-04],\n",
      "        [3.7775e-03],\n",
      "        [2.6874e-02],\n",
      "        [3.8637e-01],\n",
      "        [4.5881e-01],\n",
      "        [2.3317e-01],\n",
      "        [5.3797e-02],\n",
      "        [5.7915e-01],\n",
      "        [4.9109e-04],\n",
      "        [9.8574e-05],\n",
      "        [5.1371e-06],\n",
      "        [9.6052e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.598292350769043: \n",
      "target probs tensor([[2.9243e-01],\n",
      "        [9.8794e-01],\n",
      "        [9.0855e-01],\n",
      "        [1.7299e-01],\n",
      "        [1.8950e-01],\n",
      "        [2.4343e-05],\n",
      "        [9.5135e-01],\n",
      "        [7.8183e-01],\n",
      "        [1.0525e-04],\n",
      "        [6.3757e-01],\n",
      "        [8.1607e-01],\n",
      "        [2.2580e-06],\n",
      "        [2.6850e-02],\n",
      "        [1.2668e-02],\n",
      "        [4.1693e-01],\n",
      "        [2.8717e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.0373106002807617: \n",
      "target probs tensor([[4.7226e-01],\n",
      "        [8.9594e-03],\n",
      "        [6.3919e-02],\n",
      "        [1.4547e-03],\n",
      "        [7.8118e-06],\n",
      "        [6.1992e-01],\n",
      "        [8.0288e-01],\n",
      "        [4.6933e-05],\n",
      "        [6.8647e-01],\n",
      "        [1.3694e-03],\n",
      "        [7.4262e-01],\n",
      "        [1.1080e-03],\n",
      "        [2.3468e-02],\n",
      "        [1.9789e-01],\n",
      "        [6.1306e-02],\n",
      "        [1.8639e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.8180575370788574: \n",
      "target probs tensor([[3.6385e-03],\n",
      "        [2.7596e-02],\n",
      "        [6.3369e-01],\n",
      "        [7.0765e-04],\n",
      "        [3.8434e-02],\n",
      "        [1.6970e-01],\n",
      "        [1.3896e-04],\n",
      "        [5.9559e-04],\n",
      "        [3.6602e-03],\n",
      "        [7.5271e-01],\n",
      "        [7.0179e-01],\n",
      "        [3.0350e-01],\n",
      "        [5.1862e-01],\n",
      "        [1.4016e-02],\n",
      "        [2.1737e-04],\n",
      "        [9.7356e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.6925928592681885: \n",
      "Better model found at epoch 13 with validation value: 0.6919999718666077.\n",
      "target probs tensor([[6.7813e-02],\n",
      "        [5.8977e-02],\n",
      "        [3.0436e-01],\n",
      "        [8.9806e-01],\n",
      "        [3.2530e-06],\n",
      "        [1.7757e-02],\n",
      "        [2.2653e-03],\n",
      "        [8.4895e-01],\n",
      "        [2.1080e-05],\n",
      "        [2.4940e-04],\n",
      "        [8.5586e-02],\n",
      "        [9.9568e-01],\n",
      "        [8.5086e-01],\n",
      "        [3.0674e-04],\n",
      "        [3.3806e-02],\n",
      "        [2.6397e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.014725685119629: \n",
      "target probs tensor([[1.6803e-02],\n",
      "        [2.7910e-02],\n",
      "        [1.6341e-07],\n",
      "        [6.7520e-02],\n",
      "        [1.5851e-01],\n",
      "        [6.8338e-01],\n",
      "        [7.2226e-02],\n",
      "        [2.4212e-01],\n",
      "        [2.0679e-06],\n",
      "        [9.4177e-01],\n",
      "        [2.2456e-05],\n",
      "        [7.9055e-01],\n",
      "        [2.3763e-03],\n",
      "        [7.1893e-01],\n",
      "        [2.2019e-02],\n",
      "        [2.0780e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.2564191818237305: \n",
      "target probs tensor([[3.0028e-03],\n",
      "        [9.3255e-04],\n",
      "        [2.6786e-03],\n",
      "        [9.0460e-05],\n",
      "        [6.8663e-01],\n",
      "        [1.0526e-05],\n",
      "        [5.8490e-05],\n",
      "        [4.6269e-01],\n",
      "        [1.3960e-03],\n",
      "        [8.6826e-01],\n",
      "        [3.2624e-01],\n",
      "        [5.7843e-01],\n",
      "        [5.9130e-02],\n",
      "        [1.1216e-02],\n",
      "        [3.8179e-01],\n",
      "        [1.6416e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.878462791442871: \n",
      "target probs tensor([[2.2422e-04],\n",
      "        [2.9330e-06],\n",
      "        [4.4828e-01],\n",
      "        [1.3729e-04],\n",
      "        [5.3287e-05],\n",
      "        [2.3924e-05],\n",
      "        [8.9491e-02],\n",
      "        [3.1846e-04],\n",
      "        [8.9615e-01],\n",
      "        [3.6833e-04],\n",
      "        [5.3183e-01],\n",
      "        [1.2857e-02],\n",
      "        [9.3135e-01],\n",
      "        [8.9005e-01],\n",
      "        [5.5949e-02],\n",
      "        [9.2380e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 5.302724838256836: \n",
      "target probs tensor([[3.1251e-01],\n",
      "        [9.9778e-01],\n",
      "        [2.0085e-02],\n",
      "        [6.5512e-02],\n",
      "        [1.5311e-03],\n",
      "        [8.1894e-05],\n",
      "        [9.5468e-01],\n",
      "        [7.5500e-01],\n",
      "        [8.0150e-04],\n",
      "        [8.8409e-06],\n",
      "        [8.6505e-01],\n",
      "        [3.1252e-03],\n",
      "        [4.6225e-03],\n",
      "        [6.5106e-04],\n",
      "        [2.1268e-03],\n",
      "        [6.3436e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.251140594482422: \n",
      "target probs tensor([[7.0645e-01],\n",
      "        [5.4907e-01],\n",
      "        [5.4562e-01],\n",
      "        [7.8299e-07],\n",
      "        [7.3951e-01],\n",
      "        [1.3688e-01],\n",
      "        [2.0922e-04],\n",
      "        [9.5659e-04],\n",
      "        [1.1392e-02],\n",
      "        [1.8892e-04],\n",
      "        [1.9146e-01],\n",
      "        [7.8213e-01],\n",
      "        [2.8151e-01],\n",
      "        [5.0569e-01],\n",
      "        [9.8077e-01],\n",
      "        [6.2120e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.1700191497802734: \n",
      "target probs tensor([[7.0812e-02],\n",
      "        [8.3625e-01],\n",
      "        [9.9009e-01],\n",
      "        [9.0913e-03],\n",
      "        [2.7766e-02],\n",
      "        [6.0478e-04],\n",
      "        [4.5447e-01],\n",
      "        [2.4635e-01]], device='cuda:0'), loss: 2.5901412963867188: \n",
      "Better model found at epoch 15 with validation value: 0.7039999961853027.\n",
      "target probs tensor([[5.7916e-01],\n",
      "        [3.5390e-04],\n",
      "        [3.4689e-01],\n",
      "        [3.4241e-04],\n",
      "        [8.8407e-01],\n",
      "        [3.2931e-02],\n",
      "        [1.3118e-04],\n",
      "        [1.2053e-02],\n",
      "        [5.1364e-01],\n",
      "        [5.0814e-01],\n",
      "        [8.6768e-03],\n",
      "        [3.5842e-02],\n",
      "        [1.1113e-01],\n",
      "        [1.0762e-05],\n",
      "        [9.3099e-01],\n",
      "        [1.5262e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.290122032165527: \n",
      "target probs tensor([[3.0996e-02],\n",
      "        [3.5334e-01],\n",
      "        [5.0976e-01],\n",
      "        [3.7969e-04],\n",
      "        [8.7379e-01],\n",
      "        [9.8770e-01],\n",
      "        [5.0550e-01],\n",
      "        [8.6309e-01],\n",
      "        [4.7809e-01],\n",
      "        [1.3773e-02],\n",
      "        [7.9186e-01],\n",
      "        [3.4613e-08],\n",
      "        [3.5170e-03],\n",
      "        [2.3555e-01],\n",
      "        [8.6082e-02],\n",
      "        [2.3563e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.966709613800049: \n",
      "target probs tensor([[1.5005e-01],\n",
      "        [4.9292e-03],\n",
      "        [1.8086e-05],\n",
      "        [8.8302e-01],\n",
      "        [2.2290e-03],\n",
      "        [1.6420e-05],\n",
      "        [9.1678e-01],\n",
      "        [1.4826e-02],\n",
      "        [4.6641e-01],\n",
      "        [4.0562e-01],\n",
      "        [1.3560e-01],\n",
      "        [9.1139e-03],\n",
      "        [8.6495e-01],\n",
      "        [1.8302e-02],\n",
      "        [2.5564e-01],\n",
      "        [1.5780e-01]], device='cuda:0'), loss: 3.46205997467041: \n",
      "target probs tensor([[7.9298e-04],\n",
      "        [8.6314e-01],\n",
      "        [1.2512e-01],\n",
      "        [9.6085e-01],\n",
      "        [6.4247e-01],\n",
      "        [6.6640e-07],\n",
      "        [1.6034e-01],\n",
      "        [8.3964e-01],\n",
      "        [9.7348e-01],\n",
      "        [9.2206e-05],\n",
      "        [3.4753e-01],\n",
      "        [5.9543e-04],\n",
      "        [5.9680e-06],\n",
      "        [8.8590e-03],\n",
      "        [1.1664e-02],\n",
      "        [4.0836e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.123620986938477: \n",
      "target probs tensor([[1.9326e-01],\n",
      "        [4.4544e-04],\n",
      "        [3.3344e-03],\n",
      "        [1.0190e-01],\n",
      "        [9.4550e-01],\n",
      "        [5.3656e-02],\n",
      "        [7.6220e-05],\n",
      "        [8.6922e-05],\n",
      "        [1.5178e-03],\n",
      "        [6.3671e-01],\n",
      "        [2.0415e-02],\n",
      "        [8.2609e-01],\n",
      "        [1.1274e-03],\n",
      "        [8.6421e-01],\n",
      "        [9.5349e-01],\n",
      "        [9.0123e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.155080795288086: \n",
      "target probs tensor([[4.2985e-01],\n",
      "        [7.6980e-02],\n",
      "        [1.9214e-03],\n",
      "        [4.3541e-03],\n",
      "        [2.1293e-02],\n",
      "        [4.7896e-02],\n",
      "        [2.3060e-03],\n",
      "        [5.0099e-03],\n",
      "        [3.1530e-06],\n",
      "        [6.7421e-01],\n",
      "        [2.5410e-03],\n",
      "        [1.0152e-03],\n",
      "        [2.0444e-01],\n",
      "        [7.6501e-01],\n",
      "        [1.6084e-04],\n",
      "        [7.6688e-01]], device='cuda:0'), loss: 4.383854866027832: \n",
      "target probs tensor([[6.0414e-03],\n",
      "        [7.6937e-03],\n",
      "        [9.3039e-01],\n",
      "        [1.1399e-01],\n",
      "        [9.9314e-01],\n",
      "        [1.1409e-01],\n",
      "        [9.9946e-01],\n",
      "        [9.9060e-01],\n",
      "        [2.2314e-03],\n",
      "        [7.8771e-03],\n",
      "        [1.2151e-04],\n",
      "        [9.8019e-01],\n",
      "        [2.2605e-05],\n",
      "        [6.7673e-01],\n",
      "        [2.5860e-01],\n",
      "        [1.0223e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.5013084411621094: \n",
      "target probs tensor([[9.5277e-01],\n",
      "        [2.7198e-01],\n",
      "        [9.3962e-03],\n",
      "        [8.2570e-01],\n",
      "        [1.5534e-02],\n",
      "        [2.6937e-01],\n",
      "        [9.9091e-02],\n",
      "        [8.6891e-01],\n",
      "        [4.8415e-01],\n",
      "        [6.6189e-03],\n",
      "        [7.6124e-05],\n",
      "        [1.1396e-04],\n",
      "        [4.7420e-05],\n",
      "        [3.7484e-01],\n",
      "        [4.7069e-02],\n",
      "        [4.5860e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.7578189373016357: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[5.3094e-05],\n",
      "        [8.8648e-04],\n",
      "        [7.3561e-01],\n",
      "        [2.7547e-04],\n",
      "        [2.0418e-04],\n",
      "        [5.5559e-02],\n",
      "        [2.0185e-02],\n",
      "        [9.4625e-01],\n",
      "        [1.9441e-02],\n",
      "        [7.6897e-01],\n",
      "        [9.9187e-01],\n",
      "        [2.2961e-02],\n",
      "        [1.2558e-01],\n",
      "        [1.9334e-01],\n",
      "        [9.4805e-02],\n",
      "        [3.2754e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.9252400398254395: \n",
      "target probs tensor([[6.3852e-01],\n",
      "        [3.1235e-01],\n",
      "        [9.5488e-01],\n",
      "        [1.0824e-04],\n",
      "        [5.3435e-05],\n",
      "        [8.2662e-01],\n",
      "        [1.8306e-05],\n",
      "        [3.3325e-05],\n",
      "        [3.5248e-05],\n",
      "        [6.9264e-01],\n",
      "        [7.9765e-01],\n",
      "        [8.7524e-01],\n",
      "        [3.4730e-03],\n",
      "        [1.0185e-03],\n",
      "        [8.1283e-01],\n",
      "        [2.1307e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.351395606994629: \n",
      "target probs tensor([[4.2736e-01],\n",
      "        [2.9688e-02],\n",
      "        [8.5263e-03],\n",
      "        [1.8105e-01],\n",
      "        [5.6477e-04],\n",
      "        [9.6874e-01],\n",
      "        [1.1245e-02],\n",
      "        [4.9278e-05],\n",
      "        [2.6312e-01],\n",
      "        [9.8467e-01],\n",
      "        [2.0578e-01],\n",
      "        [8.7777e-01],\n",
      "        [6.5620e-02],\n",
      "        [8.6071e-01],\n",
      "        [4.9401e-01],\n",
      "        [1.0852e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.032938003540039: \n",
      "target probs tensor([[9.3350e-02],\n",
      "        [1.7742e-01],\n",
      "        [1.3386e-01],\n",
      "        [5.3198e-03],\n",
      "        [4.0750e-01],\n",
      "        [8.8670e-01],\n",
      "        [9.9490e-03],\n",
      "        [3.0280e-01],\n",
      "        [2.6775e-01],\n",
      "        [9.5447e-01],\n",
      "        [3.5928e-01],\n",
      "        [1.5916e-01],\n",
      "        [5.4316e-01],\n",
      "        [7.6132e-02],\n",
      "        [7.0071e-03],\n",
      "        [3.3485e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.4090628623962402: \n",
      "target probs tensor([[5.7896e-01],\n",
      "        [9.5893e-01],\n",
      "        [1.3998e-01],\n",
      "        [1.5461e-01],\n",
      "        [2.5822e-01],\n",
      "        [7.9097e-03],\n",
      "        [1.7545e-02],\n",
      "        [8.9427e-02],\n",
      "        [7.5291e-01],\n",
      "        [1.2946e-06],\n",
      "        [1.1248e-05],\n",
      "        [2.2997e-01],\n",
      "        [9.0764e-01],\n",
      "        [7.5259e-03],\n",
      "        [6.6917e-04],\n",
      "        [1.7510e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.613553524017334: \n",
      "target probs tensor([[1.0552e-06],\n",
      "        [7.3077e-01],\n",
      "        [3.7766e-02],\n",
      "        [1.5800e-02],\n",
      "        [2.5552e-04],\n",
      "        [1.6149e-02],\n",
      "        [9.2970e-01],\n",
      "        [7.9513e-03],\n",
      "        [8.3616e-02],\n",
      "        [3.8274e-05],\n",
      "        [5.7807e-01],\n",
      "        [9.1980e-01],\n",
      "        [6.0373e-03],\n",
      "        [4.0089e-04],\n",
      "        [1.6209e-03],\n",
      "        [3.6270e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.6726274490356445: \n",
      "target probs tensor([[6.5856e-02],\n",
      "        [6.4861e-01],\n",
      "        [6.5081e-01],\n",
      "        [4.6910e-01],\n",
      "        [1.4612e-02],\n",
      "        [5.6512e-06],\n",
      "        [1.1014e-01],\n",
      "        [2.7950e-01],\n",
      "        [5.5578e-03],\n",
      "        [1.9690e-04],\n",
      "        [3.0440e-02],\n",
      "        [1.5828e-02],\n",
      "        [6.7446e-05],\n",
      "        [4.7174e-05],\n",
      "        [1.3910e-01],\n",
      "        [9.4697e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.192885875701904: \n",
      "target probs tensor([[2.9510e-05],\n",
      "        [5.6489e-01],\n",
      "        [1.3946e-04],\n",
      "        [5.2527e-02],\n",
      "        [4.5186e-01],\n",
      "        [3.8786e-03],\n",
      "        [1.7755e-02],\n",
      "        [2.2985e-02],\n",
      "        [5.9588e-01],\n",
      "        [6.7949e-01],\n",
      "        [9.6166e-01],\n",
      "        [9.7381e-01],\n",
      "        [4.3337e-01],\n",
      "        [1.6718e-05],\n",
      "        [5.3854e-02],\n",
      "        [4.1619e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.4926486015319824: \n",
      "target probs tensor([[6.0833e-03],\n",
      "        [6.6605e-01],\n",
      "        [1.2825e-02],\n",
      "        [6.3463e-02],\n",
      "        [1.0390e-03],\n",
      "        [1.1301e-03],\n",
      "        [3.6343e-04],\n",
      "        [1.3707e-02],\n",
      "        [5.6415e-01],\n",
      "        [4.6117e-04],\n",
      "        [4.2663e-01],\n",
      "        [5.2243e-02],\n",
      "        [3.7237e-01],\n",
      "        [6.6619e-01],\n",
      "        [1.1194e-02],\n",
      "        [1.4721e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.934514045715332: \n",
      "target probs tensor([[4.8870e-04],\n",
      "        [2.3382e-06],\n",
      "        [7.3167e-04],\n",
      "        [1.7659e-02],\n",
      "        [5.0092e-02],\n",
      "        [1.4804e-04],\n",
      "        [2.8946e-01],\n",
      "        [9.4378e-03],\n",
      "        [4.1503e-01],\n",
      "        [1.7532e-01],\n",
      "        [9.1272e-01],\n",
      "        [8.9735e-01],\n",
      "        [5.4860e-01],\n",
      "        [5.2323e-04],\n",
      "        [6.7238e-04],\n",
      "        [4.2271e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.293935298919678: \n",
      "Better model found at epoch 21 with validation value: 0.7089999914169312.\n",
      "target probs tensor([[4.9683e-01],\n",
      "        [7.3720e-01],\n",
      "        [5.8899e-01],\n",
      "        [3.4013e-05],\n",
      "        [3.7836e-04],\n",
      "        [1.9132e-04],\n",
      "        [4.5573e-01],\n",
      "        [5.3990e-03],\n",
      "        [5.5779e-03],\n",
      "        [9.4022e-01],\n",
      "        [9.8280e-01],\n",
      "        [9.8116e-01],\n",
      "        [6.0091e-03],\n",
      "        [9.6373e-01],\n",
      "        [3.0906e-03],\n",
      "        [1.9696e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.8327503204345703: \n",
      "target probs tensor([[1.2668e-01],\n",
      "        [2.9954e-04],\n",
      "        [8.4695e-01],\n",
      "        [2.8870e-02],\n",
      "        [2.1632e-01],\n",
      "        [2.2809e-03],\n",
      "        [5.9855e-01],\n",
      "        [8.7144e-04],\n",
      "        [2.7911e-03],\n",
      "        [9.8095e-01],\n",
      "        [2.1423e-01],\n",
      "        [1.6252e-05],\n",
      "        [9.2091e-01],\n",
      "        [2.3730e-02],\n",
      "        [9.8208e-04],\n",
      "        [8.4661e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.6529717445373535: \n",
      "target probs tensor([[0.9294],\n",
      "        [0.6412],\n",
      "        [0.9750],\n",
      "        [0.2501],\n",
      "        [0.3712],\n",
      "        [0.5533],\n",
      "        [0.5476],\n",
      "        [0.9551],\n",
      "        [0.0022],\n",
      "        [0.6486],\n",
      "        [0.0452],\n",
      "        [0.7648],\n",
      "        [0.9852],\n",
      "        [0.1384],\n",
      "        [0.7837],\n",
      "        [0.0087]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.317251443862915: \n",
      "Better model found at epoch 22 with validation value: 0.7099999785423279.\n",
      "target probs tensor([[2.4008e-02],\n",
      "        [5.2499e-01],\n",
      "        [9.5802e-02],\n",
      "        [1.3644e-01],\n",
      "        [8.4113e-02],\n",
      "        [7.4073e-06],\n",
      "        [1.3810e-01],\n",
      "        [4.9886e-04],\n",
      "        [2.5431e-01],\n",
      "        [6.6006e-02],\n",
      "        [2.6838e-02],\n",
      "        [9.0877e-01],\n",
      "        [1.7127e-01],\n",
      "        [9.7572e-04],\n",
      "        [8.2378e-01],\n",
      "        [2.6874e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.449589252471924: \n",
      "target probs tensor([[8.4873e-01],\n",
      "        [1.0267e-01],\n",
      "        [1.7455e-02],\n",
      "        [1.6282e-03],\n",
      "        [9.9587e-01],\n",
      "        [1.2821e-02],\n",
      "        [1.4455e-01],\n",
      "        [7.8752e-04],\n",
      "        [3.8311e-01],\n",
      "        [2.3603e-01],\n",
      "        [4.0141e-02],\n",
      "        [6.8628e-01],\n",
      "        [8.8046e-01],\n",
      "        [7.5916e-01],\n",
      "        [7.2252e-05],\n",
      "        [7.0799e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.2399497032165527: \n",
      "target probs tensor([[0.0035],\n",
      "        [0.0336],\n",
      "        [0.0298],\n",
      "        [0.9393],\n",
      "        [0.9131],\n",
      "        [0.7837],\n",
      "        [0.1784],\n",
      "        [0.1643],\n",
      "        [0.0089],\n",
      "        [0.8399],\n",
      "        [0.9209],\n",
      "        [0.8832],\n",
      "        [0.0030],\n",
      "        [0.1493],\n",
      "        [0.9992],\n",
      "        [0.8245]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.8427711725234985: \n",
      "target probs tensor([[1.3044e-02],\n",
      "        [8.9998e-01],\n",
      "        [9.7347e-01],\n",
      "        [2.5092e-04],\n",
      "        [9.4637e-02],\n",
      "        [6.0418e-04],\n",
      "        [2.0494e-01],\n",
      "        [3.6083e-01]], device='cuda:0'), loss: 3.1419732570648193: \n",
      "Better model found at epoch 23 with validation value: 0.7149999737739563.\n",
      "target probs tensor([[5.9599e-02],\n",
      "        [8.0718e-01],\n",
      "        [9.7053e-01],\n",
      "        [3.1184e-01],\n",
      "        [7.7266e-01],\n",
      "        [2.2481e-03],\n",
      "        [9.4844e-01],\n",
      "        [9.2036e-06],\n",
      "        [3.2626e-02],\n",
      "        [2.4623e-04],\n",
      "        [9.7667e-01],\n",
      "        [2.1467e-01],\n",
      "        [3.3719e-01],\n",
      "        [6.6646e-02],\n",
      "        [1.8942e-01],\n",
      "        [7.2044e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.013939619064331: \n",
      "target probs tensor([[1.0540e-02],\n",
      "        [5.4870e-04],\n",
      "        [9.9082e-03],\n",
      "        [1.1231e-03],\n",
      "        [7.6114e-01],\n",
      "        [9.3717e-01],\n",
      "        [1.4258e-01],\n",
      "        [6.9216e-01],\n",
      "        [9.9775e-01],\n",
      "        [6.2004e-03],\n",
      "        [5.2008e-01],\n",
      "        [1.8696e-02],\n",
      "        [4.8500e-04],\n",
      "        [4.3512e-02],\n",
      "        [8.7303e-01],\n",
      "        [5.0986e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.9633984565734863: \n",
      "target probs tensor([[5.3522e-02],\n",
      "        [9.2922e-03],\n",
      "        [1.4828e-05],\n",
      "        [7.3621e-01],\n",
      "        [1.4249e-04],\n",
      "        [1.4175e-04],\n",
      "        [9.7672e-01],\n",
      "        [1.0055e-02],\n",
      "        [8.7339e-01],\n",
      "        [9.6097e-01],\n",
      "        [1.4566e-01],\n",
      "        [1.7047e-02],\n",
      "        [9.9800e-01],\n",
      "        [4.5614e-01],\n",
      "        [6.1348e-02],\n",
      "        [6.7760e-02]], device='cuda:0'), loss: 3.3634891510009766: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[4.8837e-02],\n",
      "        [7.9016e-01],\n",
      "        [9.6116e-01],\n",
      "        [1.4525e-01],\n",
      "        [9.2384e-02],\n",
      "        [1.8665e-01],\n",
      "        [2.6097e-01],\n",
      "        [7.2068e-01],\n",
      "        [1.4730e-01],\n",
      "        [4.3645e-03],\n",
      "        [3.2964e-04],\n",
      "        [4.9273e-04],\n",
      "        [4.6765e-02],\n",
      "        [4.4810e-03],\n",
      "        [7.7735e-03],\n",
      "        [3.6939e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.1602249145507812: \n",
      "target probs tensor([[8.6363e-01],\n",
      "        [1.0862e-02],\n",
      "        [7.2978e-01],\n",
      "        [1.5089e-01],\n",
      "        [9.7461e-05],\n",
      "        [1.7000e-05],\n",
      "        [9.2583e-06],\n",
      "        [1.1569e-05],\n",
      "        [2.6630e-01],\n",
      "        [9.0905e-01],\n",
      "        [9.7817e-05],\n",
      "        [2.1138e-01],\n",
      "        [9.9224e-01],\n",
      "        [9.9745e-01],\n",
      "        [6.4352e-01],\n",
      "        [2.9553e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.283195495605469: \n",
      "target probs tensor([[6.7184e-01],\n",
      "        [4.0395e-02],\n",
      "        [6.3885e-04],\n",
      "        [1.7361e-01],\n",
      "        [2.7659e-02],\n",
      "        [4.7245e-01],\n",
      "        [9.2977e-01],\n",
      "        [5.8225e-03],\n",
      "        [3.0378e-06],\n",
      "        [9.4227e-01],\n",
      "        [1.9156e-03],\n",
      "        [4.3459e-03],\n",
      "        [5.5790e-02],\n",
      "        [7.8880e-01],\n",
      "        [1.2628e-04],\n",
      "        [9.7019e-01]], device='cuda:0'), loss: 3.6787970066070557: \n",
      "target probs tensor([[7.0109e-04],\n",
      "        [4.3820e-05],\n",
      "        [4.4918e-03],\n",
      "        [2.1062e-03],\n",
      "        [9.5396e-01],\n",
      "        [9.5320e-02],\n",
      "        [9.3585e-01],\n",
      "        [2.6467e-01],\n",
      "        [2.8613e-06],\n",
      "        [8.2751e-01],\n",
      "        [9.7816e-01],\n",
      "        [5.3459e-01],\n",
      "        [7.2303e-01],\n",
      "        [6.3656e-01],\n",
      "        [9.7711e-01],\n",
      "        [8.5513e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.9510891437530518: \n",
      "target probs tensor([[7.1691e-02],\n",
      "        [1.8606e-04],\n",
      "        [4.8561e-01],\n",
      "        [1.4252e-01],\n",
      "        [6.4151e-07],\n",
      "        [6.3139e-01],\n",
      "        [4.6329e-06],\n",
      "        [8.3405e-01],\n",
      "        [1.1281e-01],\n",
      "        [1.8587e-03],\n",
      "        [5.6348e-02],\n",
      "        [9.8794e-01],\n",
      "        [2.0885e-01],\n",
      "        [9.4662e-01],\n",
      "        [9.3585e-01],\n",
      "        [6.4414e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.6980605125427246: \n",
      "target probs tensor([[4.8561e-04],\n",
      "        [1.9367e-01],\n",
      "        [9.9724e-01],\n",
      "        [9.3510e-01],\n",
      "        [1.3858e-01],\n",
      "        [4.0382e-03],\n",
      "        [9.6995e-01],\n",
      "        [7.1782e-01],\n",
      "        [9.1266e-01],\n",
      "        [6.5732e-04],\n",
      "        [4.0394e-01],\n",
      "        [7.5330e-08],\n",
      "        [9.2771e-01],\n",
      "        [4.6826e-03],\n",
      "        [8.3997e-01],\n",
      "        [3.8702e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.02998685836792: \n",
      "Better model found at epoch 26 with validation value: 0.718999981880188.\n",
      "target probs tensor([[6.7725e-01],\n",
      "        [2.3074e-03],\n",
      "        [4.8571e-05],\n",
      "        [5.7233e-03],\n",
      "        [9.1425e-01],\n",
      "        [9.9175e-01],\n",
      "        [1.5369e-05],\n",
      "        [9.1665e-01],\n",
      "        [3.4120e-01],\n",
      "        [9.3239e-03],\n",
      "        [5.3481e-02],\n",
      "        [1.8292e-02],\n",
      "        [9.8845e-01],\n",
      "        [3.6069e-03],\n",
      "        [7.5381e-01],\n",
      "        [6.8199e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.2379496097564697: \n",
      "target probs tensor([[1.6248e-01],\n",
      "        [7.7170e-05],\n",
      "        [9.3717e-01],\n",
      "        [6.4125e-02],\n",
      "        [3.1199e-06],\n",
      "        [3.9355e-01],\n",
      "        [1.6023e-04],\n",
      "        [1.0527e-02],\n",
      "        [3.5478e-02],\n",
      "        [2.3959e-04],\n",
      "        [1.4708e-02],\n",
      "        [9.9232e-01],\n",
      "        [8.8265e-01],\n",
      "        [5.3390e-04],\n",
      "        [1.3685e-03],\n",
      "        [5.3138e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.486900329589844: \n",
      "target probs tensor([[3.9990e-03],\n",
      "        [4.3346e-01],\n",
      "        [9.2661e-01],\n",
      "        [8.8236e-01],\n",
      "        [9.8532e-01],\n",
      "        [7.6087e-05],\n",
      "        [6.0875e-02],\n",
      "        [6.3433e-01],\n",
      "        [4.6689e-04],\n",
      "        [1.1879e-02],\n",
      "        [7.4396e-01],\n",
      "        [9.9856e-01],\n",
      "        [9.0764e-05],\n",
      "        [3.7409e-01],\n",
      "        [1.3868e-02],\n",
      "        [2.1928e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.1312389373779297: \n",
      "Better model found at epoch 27 with validation value: 0.7250000238418579.\n",
      "target probs tensor([[1.4182e-01],\n",
      "        [4.8659e-03],\n",
      "        [5.0609e-05],\n",
      "        [7.6629e-01],\n",
      "        [3.4567e-02],\n",
      "        [9.8940e-01],\n",
      "        [5.6625e-04],\n",
      "        [1.6501e-01],\n",
      "        [5.8035e-06],\n",
      "        [2.0137e-01],\n",
      "        [2.3592e-03],\n",
      "        [4.8339e-01],\n",
      "        [1.1356e-06],\n",
      "        [9.4601e-01],\n",
      "        [1.6881e-04],\n",
      "        [7.3339e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.579160690307617: \n",
      "target probs tensor([[6.5455e-01],\n",
      "        [4.3792e-01],\n",
      "        [4.2171e-01],\n",
      "        [9.2541e-01],\n",
      "        [7.0699e-01],\n",
      "        [7.7227e-02],\n",
      "        [9.6654e-01],\n",
      "        [1.2611e-03],\n",
      "        [3.2212e-01],\n",
      "        [3.3769e-01],\n",
      "        [9.5470e-01],\n",
      "        [1.8806e-01],\n",
      "        [2.5102e-02],\n",
      "        [1.2618e-03],\n",
      "        [1.1862e-01],\n",
      "        [1.3826e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.3201303482055664: \n",
      "target probs tensor([[3.8585e-01],\n",
      "        [6.9083e-04],\n",
      "        [1.6987e-01],\n",
      "        [5.1521e-01],\n",
      "        [9.2220e-01],\n",
      "        [9.8174e-05],\n",
      "        [2.5042e-01],\n",
      "        [7.8881e-01],\n",
      "        [8.4626e-01],\n",
      "        [6.6742e-01],\n",
      "        [8.3873e-01],\n",
      "        [7.7227e-01],\n",
      "        [8.7250e-05],\n",
      "        [6.6926e-01],\n",
      "        [1.4682e-03],\n",
      "        [2.7819e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.9413859844207764: \n",
      "target probs tensor([[7.1501e-01],\n",
      "        [8.4845e-01],\n",
      "        [1.2867e-02],\n",
      "        [8.3397e-01],\n",
      "        [9.6452e-01],\n",
      "        [7.5043e-01],\n",
      "        [1.9895e-01],\n",
      "        [1.0978e-01],\n",
      "        [2.3522e-03],\n",
      "        [4.0540e-05],\n",
      "        [1.5817e-02],\n",
      "        [4.3736e-04],\n",
      "        [2.4644e-01],\n",
      "        [1.5786e-03],\n",
      "        [9.4721e-01],\n",
      "        [9.5664e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.823698043823242: \n",
      "target probs tensor([[3.4783e-02],\n",
      "        [9.1649e-01],\n",
      "        [6.1648e-01],\n",
      "        [9.9394e-01],\n",
      "        [3.0996e-01],\n",
      "        [8.8519e-01],\n",
      "        [9.4890e-01],\n",
      "        [1.3745e-01],\n",
      "        [5.9684e-01],\n",
      "        [2.5383e-03],\n",
      "        [1.4022e-02],\n",
      "        [9.5865e-04],\n",
      "        [2.0809e-01],\n",
      "        [8.9876e-02],\n",
      "        [9.8176e-01],\n",
      "        [2.3516e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.333005428314209: \n",
      "target probs tensor([[4.1122e-04],\n",
      "        [2.2916e-04],\n",
      "        [2.7421e-01],\n",
      "        [5.5116e-04],\n",
      "        [8.9469e-01],\n",
      "        [9.3270e-01],\n",
      "        [4.3211e-05],\n",
      "        [5.9498e-02],\n",
      "        [1.3208e-02],\n",
      "        [9.3896e-01],\n",
      "        [8.7436e-01],\n",
      "        [2.2115e-01],\n",
      "        [1.4573e-06],\n",
      "        [5.1321e-02],\n",
      "        [9.4488e-01],\n",
      "        [9.6975e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.7847495079040527: \n",
      "Better model found at epoch 29 with validation value: 0.7310000061988831.\n",
      "target probs tensor([[8.4324e-01],\n",
      "        [8.8757e-01],\n",
      "        [5.9551e-03],\n",
      "        [6.4679e-01],\n",
      "        [2.1621e-04],\n",
      "        [2.8150e-02],\n",
      "        [2.3973e-04],\n",
      "        [4.4258e-01],\n",
      "        [5.0080e-04],\n",
      "        [3.9356e-03],\n",
      "        [9.7910e-01],\n",
      "        [6.1696e-01],\n",
      "        [9.9200e-01],\n",
      "        [6.4616e-01],\n",
      "        [1.9781e-01],\n",
      "        [1.1378e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.0934085845947266: \n",
      "target probs tensor([[4.3413e-01],\n",
      "        [1.1785e-04],\n",
      "        [9.9617e-04],\n",
      "        [2.5880e-04],\n",
      "        [2.4429e-03],\n",
      "        [3.4057e-03],\n",
      "        [1.1590e-01],\n",
      "        [1.6538e-01],\n",
      "        [1.6070e-02],\n",
      "        [5.5670e-04],\n",
      "        [2.1015e-04],\n",
      "        [2.7233e-02],\n",
      "        [1.4593e-02],\n",
      "        [5.1885e-01],\n",
      "        [1.1904e-04],\n",
      "        [9.1018e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.900732040405273: \n",
      "target probs tensor([[4.8856e-01],\n",
      "        [3.0765e-05],\n",
      "        [9.9699e-01],\n",
      "        [9.6429e-01],\n",
      "        [1.5845e-02],\n",
      "        [8.1358e-03],\n",
      "        [5.1133e-03],\n",
      "        [3.6457e-04],\n",
      "        [3.9110e-04],\n",
      "        [2.3896e-01],\n",
      "        [9.6929e-01],\n",
      "        [8.2348e-01],\n",
      "        [3.0548e-01],\n",
      "        [8.1502e-04],\n",
      "        [1.3578e-02],\n",
      "        [1.2979e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.589785099029541: \n",
      "target probs tensor([[3.9778e-01],\n",
      "        [2.0180e-05],\n",
      "        [2.1824e-04],\n",
      "        [3.3091e-03],\n",
      "        [6.7466e-03],\n",
      "        [3.5005e-04],\n",
      "        [7.0531e-01],\n",
      "        [3.8937e-01],\n",
      "        [2.8606e-01],\n",
      "        [2.0625e-01],\n",
      "        [5.3107e-04],\n",
      "        [8.0587e-01],\n",
      "        [8.3703e-02],\n",
      "        [6.5036e-01],\n",
      "        [1.7739e-03],\n",
      "        [8.8577e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.186450958251953: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[5.0859e-03],\n",
      "        [1.3834e-01],\n",
      "        [1.2418e-02],\n",
      "        [3.2280e-05],\n",
      "        [2.8823e-01],\n",
      "        [1.6836e-04],\n",
      "        [2.2427e-05],\n",
      "        [3.7645e-03],\n",
      "        [2.3295e-01],\n",
      "        [7.8913e-02],\n",
      "        [5.3249e-03],\n",
      "        [2.0257e-01],\n",
      "        [9.4409e-01],\n",
      "        [8.1816e-02],\n",
      "        [1.3818e-01],\n",
      "        [6.4523e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.4327545166015625: \n",
      "target probs tensor([[1.2874e-02],\n",
      "        [1.5459e-02],\n",
      "        [3.5099e-02],\n",
      "        [9.0437e-05],\n",
      "        [2.0161e-01],\n",
      "        [8.3597e-01],\n",
      "        [8.0963e-01],\n",
      "        [6.4103e-01],\n",
      "        [2.6063e-05],\n",
      "        [9.9921e-01],\n",
      "        [9.4090e-01],\n",
      "        [9.1911e-01],\n",
      "        [3.7700e-01],\n",
      "        [9.7918e-01],\n",
      "        [6.4249e-07],\n",
      "        [4.5591e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.435309886932373: \n",
      "target probs tensor([[0.0403],\n",
      "        [0.8451],\n",
      "        [0.8528],\n",
      "        [0.0012],\n",
      "        [0.4821],\n",
      "        [0.0048],\n",
      "        [0.7286],\n",
      "        [0.7251]], device='cuda:0'), loss: 2.125166416168213: \n",
      "target probs tensor([[8.7908e-03],\n",
      "        [4.7249e-01],\n",
      "        [1.8797e-04],\n",
      "        [9.3344e-02],\n",
      "        [4.4871e-01],\n",
      "        [6.5893e-01],\n",
      "        [1.6364e-03],\n",
      "        [1.3936e-01],\n",
      "        [2.7374e-01],\n",
      "        [2.2748e-01],\n",
      "        [4.6629e-01],\n",
      "        [1.0363e-06],\n",
      "        [8.3478e-01],\n",
      "        [1.6692e-06],\n",
      "        [5.6552e-03],\n",
      "        [4.2456e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.073503017425537: \n",
      "target probs tensor([[5.6885e-01],\n",
      "        [7.5772e-01],\n",
      "        [9.2194e-01],\n",
      "        [3.2446e-03],\n",
      "        [9.6666e-02],\n",
      "        [1.8133e-04],\n",
      "        [9.3369e-01],\n",
      "        [8.5794e-03],\n",
      "        [7.8986e-01],\n",
      "        [4.0343e-01],\n",
      "        [3.9248e-03],\n",
      "        [2.0833e-01],\n",
      "        [4.7138e-01],\n",
      "        [1.6420e-04],\n",
      "        [2.2332e-05],\n",
      "        [3.4114e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.82167387008667: \n",
      "target probs tensor([[5.6322e-02],\n",
      "        [1.4822e-02],\n",
      "        [4.0624e-04],\n",
      "        [9.3437e-01],\n",
      "        [1.9553e-02],\n",
      "        [2.8100e-04],\n",
      "        [9.9646e-01],\n",
      "        [4.0964e-03],\n",
      "        [8.6870e-01],\n",
      "        [8.3094e-01],\n",
      "        [8.3064e-02],\n",
      "        [2.0190e-02],\n",
      "        [9.9865e-01],\n",
      "        [2.8291e-02],\n",
      "        [3.2287e-01],\n",
      "        [1.0086e-01]], device='cuda:0'), loss: 2.892836093902588: \n",
      "target probs tensor([[6.7835e-05],\n",
      "        [3.0216e-05],\n",
      "        [2.8377e-01],\n",
      "        [6.0729e-04],\n",
      "        [9.3938e-01],\n",
      "        [8.4900e-02],\n",
      "        [4.2762e-01],\n",
      "        [2.2624e-01],\n",
      "        [9.1461e-01],\n",
      "        [4.0293e-02],\n",
      "        [4.1391e-05],\n",
      "        [8.8713e-01],\n",
      "        [1.2217e-01],\n",
      "        [1.0194e-02],\n",
      "        [3.2978e-04],\n",
      "        [9.8180e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.8608126640319824: \n",
      "target probs tensor([[1.8008e-01],\n",
      "        [8.4260e-05],\n",
      "        [9.0163e-01],\n",
      "        [7.2106e-01],\n",
      "        [1.2617e-01],\n",
      "        [1.5098e-03],\n",
      "        [8.5729e-01],\n",
      "        [1.3224e-03],\n",
      "        [1.2538e-03],\n",
      "        [4.8366e-01],\n",
      "        [7.3768e-01],\n",
      "        [1.4113e-01],\n",
      "        [3.8239e-02],\n",
      "        [7.5003e-01],\n",
      "        [9.4482e-01],\n",
      "        [2.7428e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.590430498123169: \n",
      "target probs tensor([[5.2451e-01],\n",
      "        [3.6937e-02],\n",
      "        [1.8941e-03],\n",
      "        [1.1517e-03],\n",
      "        [1.5192e-03],\n",
      "        [9.1422e-03],\n",
      "        [2.6422e-03],\n",
      "        [2.7334e-01],\n",
      "        [6.4746e-07],\n",
      "        [8.2919e-01],\n",
      "        [2.9451e-03],\n",
      "        [1.9344e-03],\n",
      "        [2.1357e-02],\n",
      "        [8.7151e-01],\n",
      "        [2.1024e-04],\n",
      "        [9.9638e-01]], device='cuda:0'), loss: 4.647789001464844: \n",
      "target probs tensor([[8.2847e-01],\n",
      "        [5.1285e-01],\n",
      "        [1.3486e-01],\n",
      "        [9.6762e-01],\n",
      "        [1.9813e-01],\n",
      "        [7.8824e-01],\n",
      "        [5.7234e-04],\n",
      "        [4.5018e-04],\n",
      "        [8.1816e-04],\n",
      "        [1.8588e-03],\n",
      "        [1.8523e-05],\n",
      "        [1.7463e-03],\n",
      "        [4.5594e-03],\n",
      "        [5.6966e-01],\n",
      "        [9.9176e-01],\n",
      "        [1.1920e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.9535930156707764: \n",
      "target probs tensor([[9.8394e-01],\n",
      "        [9.9303e-01],\n",
      "        [9.9380e-01],\n",
      "        [6.1265e-01],\n",
      "        [9.9795e-01],\n",
      "        [1.4671e-04],\n",
      "        [8.8915e-01],\n",
      "        [2.7177e-03],\n",
      "        [6.5409e-03],\n",
      "        [8.5001e-01],\n",
      "        [9.9965e-01],\n",
      "        [8.4463e-01],\n",
      "        [2.9488e-01],\n",
      "        [1.4213e-01],\n",
      "        [4.8211e-03],\n",
      "        [1.4362e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.524569511413574: \n",
      "target probs tensor([[6.9838e-02],\n",
      "        [7.7722e-01],\n",
      "        [2.2567e-01],\n",
      "        [2.0232e-02],\n",
      "        [1.5645e-01],\n",
      "        [4.8531e-05],\n",
      "        [2.0434e-04],\n",
      "        [3.5535e-02],\n",
      "        [3.2026e-02],\n",
      "        [1.1725e-01],\n",
      "        [3.5575e-01],\n",
      "        [7.9876e-01],\n",
      "        [1.9599e-03],\n",
      "        [5.4900e-05],\n",
      "        [8.8801e-01],\n",
      "        [2.5789e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.9496004581451416: \n",
      "target probs tensor([[7.0847e-03],\n",
      "        [3.4370e-03],\n",
      "        [1.2195e-03],\n",
      "        [1.5926e-03],\n",
      "        [9.1449e-01],\n",
      "        [4.4677e-01],\n",
      "        [2.3478e-02],\n",
      "        [8.0550e-02],\n",
      "        [4.1331e-01],\n",
      "        [2.2050e-01],\n",
      "        [3.9093e-04],\n",
      "        [1.0954e-06],\n",
      "        [3.9258e-03],\n",
      "        [7.8794e-01],\n",
      "        [9.0214e-01],\n",
      "        [9.1106e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.8051066398620605: \n",
      "target probs tensor([[7.3830e-01],\n",
      "        [9.8944e-01],\n",
      "        [8.4328e-01],\n",
      "        [9.6236e-01],\n",
      "        [2.1130e-03],\n",
      "        [9.3838e-01],\n",
      "        [5.7072e-03],\n",
      "        [3.4303e-01],\n",
      "        [3.2092e-04],\n",
      "        [5.1987e-05],\n",
      "        [9.9174e-01],\n",
      "        [3.5382e-05],\n",
      "        [4.7168e-01],\n",
      "        [6.2892e-04],\n",
      "        [6.4375e-01],\n",
      "        [9.5446e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.1099040508270264: \n",
      "target probs tensor([[1.4048e-01],\n",
      "        [7.3957e-01],\n",
      "        [3.4073e-02],\n",
      "        [9.1444e-01],\n",
      "        [3.2957e-06],\n",
      "        [4.7067e-04],\n",
      "        [3.5140e-04],\n",
      "        [6.8450e-03],\n",
      "        [5.7679e-01],\n",
      "        [5.5189e-04],\n",
      "        [7.6115e-03],\n",
      "        [9.1290e-04],\n",
      "        [4.2895e-03],\n",
      "        [6.9907e-01],\n",
      "        [7.8260e-01],\n",
      "        [9.2815e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.0633649826049805: \n",
      "target probs tensor([[9.2734e-01],\n",
      "        [9.7841e-01],\n",
      "        [8.2549e-01],\n",
      "        [8.5053e-01],\n",
      "        [5.6217e-01],\n",
      "        [2.8334e-03],\n",
      "        [2.7219e-05],\n",
      "        [1.8147e-03],\n",
      "        [3.5251e-03],\n",
      "        [1.4640e-01],\n",
      "        [4.4532e-01],\n",
      "        [9.9734e-01],\n",
      "        [5.9401e-01],\n",
      "        [1.2502e-01],\n",
      "        [1.7245e-01],\n",
      "        [4.6526e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.4701898097991943: \n",
      "target probs tensor([[7.9384e-01],\n",
      "        [5.1779e-02],\n",
      "        [8.9579e-01],\n",
      "        [4.9329e-04],\n",
      "        [3.2525e-03],\n",
      "        [1.3267e-04],\n",
      "        [1.6018e-05],\n",
      "        [7.5239e-02],\n",
      "        [9.8337e-01],\n",
      "        [2.6750e-04],\n",
      "        [1.6951e-03],\n",
      "        [9.3859e-01],\n",
      "        [9.7511e-01],\n",
      "        [4.2335e-03],\n",
      "        [1.0868e-04],\n",
      "        [9.9554e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.281807899475098: \n",
      "target probs tensor([[3.2230e-01],\n",
      "        [9.5167e-01],\n",
      "        [7.6748e-01],\n",
      "        [9.9613e-01],\n",
      "        [5.3675e-05],\n",
      "        [6.9557e-01],\n",
      "        [4.5871e-01],\n",
      "        [2.5821e-01],\n",
      "        [1.1050e-03],\n",
      "        [1.0990e-06],\n",
      "        [5.0554e-04],\n",
      "        [2.9633e-01],\n",
      "        [1.1385e-03],\n",
      "        [7.8070e-02],\n",
      "        [2.8037e-03],\n",
      "        [3.8714e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.848177909851074: \n",
      "target probs tensor([[3.4012e-01],\n",
      "        [4.3268e-02],\n",
      "        [8.1732e-03],\n",
      "        [2.5862e-02],\n",
      "        [8.5779e-01],\n",
      "        [4.9644e-04],\n",
      "        [7.1720e-03],\n",
      "        [6.4744e-05],\n",
      "        [6.6803e-02],\n",
      "        [9.9769e-01],\n",
      "        [8.6649e-01],\n",
      "        [9.7744e-01],\n",
      "        [3.6827e-01],\n",
      "        [1.2078e-01],\n",
      "        [4.1585e-01],\n",
      "        [5.0312e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.236661911010742: \n",
      "target probs tensor([[2.6987e-02],\n",
      "        [1.8959e-02],\n",
      "        [1.0293e-04],\n",
      "        [1.6951e-05],\n",
      "        [9.5158e-01],\n",
      "        [8.1966e-01],\n",
      "        [6.7414e-01],\n",
      "        [8.7151e-01],\n",
      "        [7.5072e-04],\n",
      "        [2.9991e-03],\n",
      "        [9.8805e-01],\n",
      "        [1.8152e-03],\n",
      "        [7.8384e-02],\n",
      "        [5.4446e-01],\n",
      "        [8.3924e-01],\n",
      "        [9.9812e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.1989729404449463: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[9.6527e-01],\n",
      "        [7.9895e-01],\n",
      "        [6.4671e-01],\n",
      "        [9.8931e-01],\n",
      "        [2.3619e-01],\n",
      "        [7.1444e-01],\n",
      "        [9.8014e-01],\n",
      "        [7.1020e-01],\n",
      "        [6.0482e-04],\n",
      "        [6.8874e-01],\n",
      "        [6.1687e-02],\n",
      "        [3.0138e-02],\n",
      "        [6.3069e-01],\n",
      "        [1.7070e-02],\n",
      "        [8.1321e-01],\n",
      "        [4.5417e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.4029130935668945: \n",
      "target probs tensor([[2.1028e-01],\n",
      "        [2.2963e-03],\n",
      "        [3.4924e-01],\n",
      "        [9.9757e-01],\n",
      "        [1.3453e-03],\n",
      "        [1.5218e-02],\n",
      "        [5.6963e-01],\n",
      "        [6.3479e-02],\n",
      "        [1.1235e-04],\n",
      "        [5.8021e-01],\n",
      "        [4.1973e-05],\n",
      "        [2.1623e-02],\n",
      "        [1.3717e-02],\n",
      "        [2.9500e-01],\n",
      "        [1.6664e-01],\n",
      "        [9.8180e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.3548355102539062: \n",
      "target probs tensor([[3.2008e-03],\n",
      "        [6.8171e-04],\n",
      "        [6.4680e-01],\n",
      "        [1.4126e-01],\n",
      "        [6.7044e-01],\n",
      "        [1.8920e-03],\n",
      "        [8.3480e-02],\n",
      "        [1.2480e-03],\n",
      "        [1.8285e-01],\n",
      "        [4.9048e-04],\n",
      "        [1.0995e-01],\n",
      "        [2.2173e-06],\n",
      "        [5.0952e-07],\n",
      "        [6.9460e-04],\n",
      "        [8.6911e-01],\n",
      "        [7.8358e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.872470855712891: \n",
      "target probs tensor([[2.2038e-05],\n",
      "        [4.2756e-01],\n",
      "        [8.0553e-01],\n",
      "        [1.4102e-04],\n",
      "        [3.0894e-01],\n",
      "        [2.3861e-04],\n",
      "        [1.2395e-01],\n",
      "        [7.5893e-01],\n",
      "        [1.8394e-04],\n",
      "        [2.9326e-04],\n",
      "        [1.1445e-02],\n",
      "        [6.9364e-01],\n",
      "        [7.3303e-01],\n",
      "        [6.2444e-04],\n",
      "        [1.0091e-04],\n",
      "        [6.5801e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.751237869262695: \n",
      "target probs tensor([[9.4340e-01],\n",
      "        [4.0724e-06],\n",
      "        [2.6471e-01],\n",
      "        [8.8349e-01],\n",
      "        [8.5002e-01],\n",
      "        [2.1146e-04],\n",
      "        [7.9968e-01],\n",
      "        [6.4428e-04],\n",
      "        [1.0781e-04],\n",
      "        [3.4091e-01],\n",
      "        [4.1180e-01],\n",
      "        [1.1809e-03],\n",
      "        [6.7836e-01],\n",
      "        [7.4453e-01],\n",
      "        [3.7665e-05],\n",
      "        [9.6549e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.9667186737060547: \n",
      "target probs tensor([[9.0864e-01],\n",
      "        [2.1975e-05],\n",
      "        [9.8694e-01],\n",
      "        [4.3692e-02],\n",
      "        [6.9173e-06],\n",
      "        [2.2017e-04],\n",
      "        [9.2224e-01],\n",
      "        [1.8757e-03],\n",
      "        [6.4989e-02],\n",
      "        [4.1622e-02],\n",
      "        [9.0224e-01],\n",
      "        [9.8191e-05],\n",
      "        [1.9668e-02],\n",
      "        [6.8677e-01],\n",
      "        [6.1371e-01],\n",
      "        [7.5418e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.8091559410095215: \n",
      "target probs tensor([[8.3365e-01],\n",
      "        [1.1567e-05],\n",
      "        [9.5366e-01],\n",
      "        [2.2860e-01],\n",
      "        [3.1351e-04],\n",
      "        [3.2193e-02],\n",
      "        [9.7890e-05],\n",
      "        [2.9693e-05],\n",
      "        [9.6364e-01],\n",
      "        [9.8588e-01],\n",
      "        [8.8609e-01],\n",
      "        [5.4497e-01],\n",
      "        [1.1454e-04],\n",
      "        [3.5786e-04],\n",
      "        [2.4005e-03],\n",
      "        [7.4005e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.272172927856445: \n",
      "target probs tensor([[0.8598],\n",
      "        [0.8751],\n",
      "        [0.9894],\n",
      "        [0.0030],\n",
      "        [0.1451],\n",
      "        [0.0031],\n",
      "        [0.0859],\n",
      "        [0.5654]], device='cuda:0'), loss: 2.103487014770508: \n",
      "target probs tensor([[8.6238e-01],\n",
      "        [3.9285e-01],\n",
      "        [9.2761e-03],\n",
      "        [9.6914e-01],\n",
      "        [9.7428e-01],\n",
      "        [1.9093e-04],\n",
      "        [1.3322e-01],\n",
      "        [4.0101e-01],\n",
      "        [9.9453e-01],\n",
      "        [4.2803e-04],\n",
      "        [2.2128e-02],\n",
      "        [9.9297e-01],\n",
      "        [1.8733e-01],\n",
      "        [1.4098e-04],\n",
      "        [1.5782e-01],\n",
      "        [4.0596e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.636414051055908: \n",
      "target probs tensor([[3.9787e-04],\n",
      "        [2.8279e-03],\n",
      "        [2.7687e-02],\n",
      "        [3.8969e-01],\n",
      "        [2.7075e-01],\n",
      "        [6.8749e-02],\n",
      "        [7.7560e-01],\n",
      "        [7.1517e-02],\n",
      "        [5.3460e-04],\n",
      "        [2.3981e-02],\n",
      "        [3.4335e-01],\n",
      "        [2.8113e-01],\n",
      "        [1.3136e-03],\n",
      "        [9.8848e-01],\n",
      "        [9.9266e-01],\n",
      "        [1.3177e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.2494289875030518: \n",
      "target probs tensor([[1.5416e-01],\n",
      "        [1.2775e-02],\n",
      "        [2.2137e-05],\n",
      "        [5.8577e-01],\n",
      "        [2.0213e-04],\n",
      "        [1.9016e-03],\n",
      "        [9.9823e-01],\n",
      "        [4.1525e-03],\n",
      "        [6.3013e-01],\n",
      "        [7.9864e-01],\n",
      "        [2.0178e-02],\n",
      "        [5.4570e-04],\n",
      "        [9.9951e-01],\n",
      "        [8.3846e-01],\n",
      "        [9.1880e-01],\n",
      "        [7.1839e-02]], device='cuda:0'), loss: 3.2961583137512207: \n",
      "target probs tensor([[3.4544e-03],\n",
      "        [6.0167e-01],\n",
      "        [9.8049e-01],\n",
      "        [4.7153e-04],\n",
      "        [7.5981e-01],\n",
      "        [1.0609e-04],\n",
      "        [4.8066e-05],\n",
      "        [4.0356e-02],\n",
      "        [9.7825e-01],\n",
      "        [1.3789e-05],\n",
      "        [9.8015e-01],\n",
      "        [9.4729e-01],\n",
      "        [5.3396e-04],\n",
      "        [1.6464e-01],\n",
      "        [1.2263e-03],\n",
      "        [4.1339e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.472254276275635: \n",
      "target probs tensor([[9.8117e-01],\n",
      "        [1.5592e-03],\n",
      "        [9.8218e-01],\n",
      "        [9.9687e-02],\n",
      "        [9.7901e-01],\n",
      "        [2.9059e-01],\n",
      "        [4.9658e-01],\n",
      "        [4.0400e-03],\n",
      "        [4.3612e-01],\n",
      "        [5.5090e-04],\n",
      "        [4.8958e-02],\n",
      "        [4.9385e-06],\n",
      "        [2.6767e-02],\n",
      "        [9.6976e-01],\n",
      "        [1.1497e-01],\n",
      "        [8.1362e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.866532802581787: \n",
      "target probs tensor([[8.2725e-01],\n",
      "        [3.6331e-01],\n",
      "        [2.0490e-03],\n",
      "        [5.2085e-01],\n",
      "        [1.4977e-02],\n",
      "        [7.4303e-02],\n",
      "        [6.3262e-03],\n",
      "        [3.5042e-03],\n",
      "        [1.7979e-06],\n",
      "        [8.5848e-01],\n",
      "        [3.8302e-03],\n",
      "        [1.8801e-01],\n",
      "        [1.9259e-01],\n",
      "        [9.9231e-01],\n",
      "        [2.9392e-04],\n",
      "        [8.9921e-01]], device='cuda:0'), loss: 3.504587173461914: \n",
      "target probs tensor([[9.6937e-01],\n",
      "        [1.1316e-04],\n",
      "        [1.6217e-02],\n",
      "        [2.1058e-02],\n",
      "        [6.9838e-01],\n",
      "        [9.8609e-01],\n",
      "        [1.7624e-03],\n",
      "        [4.4387e-02],\n",
      "        [1.4510e-05],\n",
      "        [8.8587e-01],\n",
      "        [4.4069e-04],\n",
      "        [3.4473e-01],\n",
      "        [3.6019e-03],\n",
      "        [1.8373e-01],\n",
      "        [4.0999e-01],\n",
      "        [6.3907e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.765500068664551: \n",
      "target probs tensor([[2.8208e-04],\n",
      "        [4.8429e-01],\n",
      "        [8.2372e-06],\n",
      "        [1.2312e-01],\n",
      "        [6.5648e-05],\n",
      "        [9.7429e-01],\n",
      "        [1.4981e-01],\n",
      "        [1.5950e-01],\n",
      "        [7.5884e-01],\n",
      "        [9.2102e-01],\n",
      "        [2.3361e-03],\n",
      "        [3.2238e-01],\n",
      "        [4.9805e-02],\n",
      "        [1.1044e-02],\n",
      "        [9.8106e-01],\n",
      "        [1.2816e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.3262410163879395: \n",
      "target probs tensor([[2.8853e-03],\n",
      "        [9.5489e-01],\n",
      "        [2.7214e-01],\n",
      "        [1.0616e-02],\n",
      "        [3.3287e-01],\n",
      "        [1.9986e-02],\n",
      "        [2.3702e-04],\n",
      "        [4.3016e-01],\n",
      "        [3.5853e-03],\n",
      "        [3.5655e-01],\n",
      "        [9.6406e-01],\n",
      "        [3.5814e-05],\n",
      "        [2.7003e-05],\n",
      "        [1.0445e-01],\n",
      "        [8.6274e-01],\n",
      "        [3.6117e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.6954987049102783: \n",
      "target probs tensor([[0.2515],\n",
      "        [0.9958],\n",
      "        [0.7325],\n",
      "        [0.3833],\n",
      "        [0.5166],\n",
      "        [0.7706],\n",
      "        [0.0067],\n",
      "        [0.1739],\n",
      "        [0.0030],\n",
      "        [0.6632],\n",
      "        [0.0451],\n",
      "        [0.0287],\n",
      "        [0.0021],\n",
      "        [0.6794],\n",
      "        [0.9521],\n",
      "        [0.9537]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.8671687841415405: \n",
      "target probs tensor([[8.3238e-01],\n",
      "        [9.6244e-01],\n",
      "        [9.6535e-01],\n",
      "        [7.2242e-01],\n",
      "        [9.4711e-01],\n",
      "        [1.2471e-01],\n",
      "        [1.8826e-06],\n",
      "        [3.9407e-06],\n",
      "        [7.3859e-01],\n",
      "        [9.6350e-01],\n",
      "        [2.5075e-01],\n",
      "        [8.2791e-01],\n",
      "        [9.9866e-01],\n",
      "        [9.5480e-01],\n",
      "        [9.7463e-01],\n",
      "        [5.9697e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.9279191493988037: \n",
      "target probs tensor([[1.4296e-04],\n",
      "        [9.7786e-01],\n",
      "        [9.9590e-01],\n",
      "        [6.1549e-02],\n",
      "        [1.3405e-02],\n",
      "        [4.0384e-01],\n",
      "        [8.0017e-01],\n",
      "        [3.6998e-04],\n",
      "        [4.6414e-04],\n",
      "        [2.0375e-01],\n",
      "        [2.6417e-05],\n",
      "        [3.6258e-01],\n",
      "        [9.7610e-01],\n",
      "        [3.3301e-01],\n",
      "        [2.9953e-05],\n",
      "        [6.7464e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.6104142665863037: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[9.4500e-01],\n",
      "        [4.4365e-01],\n",
      "        [2.7982e-01],\n",
      "        [6.4573e-03],\n",
      "        [2.8641e-04],\n",
      "        [8.2109e-01],\n",
      "        [5.8477e-04],\n",
      "        [2.7995e-01],\n",
      "        [2.6124e-02],\n",
      "        [5.3593e-01],\n",
      "        [1.4834e-02],\n",
      "        [7.3444e-05],\n",
      "        [3.1761e-02],\n",
      "        [1.9023e-01],\n",
      "        [3.0132e-04],\n",
      "        [3.9193e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.5255987644195557: \n",
      "target probs tensor([[8.4066e-01],\n",
      "        [9.7589e-01],\n",
      "        [9.9081e-01],\n",
      "        [2.1680e-03],\n",
      "        [4.0697e-01],\n",
      "        [6.3400e-02],\n",
      "        [3.1456e-01],\n",
      "        [1.4764e-04],\n",
      "        [3.5853e-03],\n",
      "        [7.3855e-02],\n",
      "        [6.1043e-05],\n",
      "        [1.2926e-04],\n",
      "        [3.1101e-02],\n",
      "        [3.4024e-05],\n",
      "        [9.1381e-01],\n",
      "        [1.2638e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.9242196083068848: \n",
      "target probs tensor([[5.1207e-01],\n",
      "        [1.6571e-01],\n",
      "        [1.0493e-02],\n",
      "        [1.4465e-04],\n",
      "        [1.7865e-01],\n",
      "        [9.0757e-01],\n",
      "        [7.5666e-01],\n",
      "        [3.4346e-01],\n",
      "        [6.7802e-02],\n",
      "        [9.7200e-03],\n",
      "        [5.9470e-01],\n",
      "        [1.7200e-04],\n",
      "        [1.9322e-03],\n",
      "        [6.9189e-01],\n",
      "        [2.2687e-01],\n",
      "        [1.9160e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.831096649169922: \n",
      "target probs tensor([[8.4959e-01],\n",
      "        [1.4999e-02],\n",
      "        [8.4131e-04],\n",
      "        [1.8615e-03],\n",
      "        [9.6363e-01],\n",
      "        [3.2076e-05],\n",
      "        [2.6871e-04],\n",
      "        [9.9682e-01],\n",
      "        [1.2540e-02],\n",
      "        [7.4396e-01],\n",
      "        [8.8845e-01],\n",
      "        [2.9251e-03],\n",
      "        [2.9486e-03],\n",
      "        [3.5443e-03],\n",
      "        [3.0207e-04],\n",
      "        [5.9074e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.4794816970825195: \n",
      "target probs tensor([[5.0753e-01],\n",
      "        [7.6982e-06],\n",
      "        [7.9650e-01],\n",
      "        [3.7910e-06],\n",
      "        [8.1519e-01],\n",
      "        [8.7738e-07],\n",
      "        [9.7633e-01],\n",
      "        [4.4836e-01],\n",
      "        [2.7343e-01],\n",
      "        [2.8292e-07],\n",
      "        [9.6613e-01],\n",
      "        [9.9783e-01],\n",
      "        [2.7214e-01],\n",
      "        [9.6654e-01],\n",
      "        [9.9377e-01],\n",
      "        [2.1323e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.714869737625122: \n",
      "target probs tensor([[8.4865e-01],\n",
      "        [7.1905e-02],\n",
      "        [6.4228e-02],\n",
      "        [9.5187e-03],\n",
      "        [8.5875e-01],\n",
      "        [9.5552e-01],\n",
      "        [2.7617e-04],\n",
      "        [6.6306e-04],\n",
      "        [4.3203e-04],\n",
      "        [2.4134e-04],\n",
      "        [9.9010e-01],\n",
      "        [5.3431e-01],\n",
      "        [9.1983e-01],\n",
      "        [1.7037e-03],\n",
      "        [2.7103e-01],\n",
      "        [2.6901e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.5189061164855957: \n",
      "target probs tensor([[8.6077e-05],\n",
      "        [7.0310e-01],\n",
      "        [9.3007e-01],\n",
      "        [9.6619e-01],\n",
      "        [3.9211e-01],\n",
      "        [4.9025e-01],\n",
      "        [4.4571e-02],\n",
      "        [7.4900e-01],\n",
      "        [1.4095e-03],\n",
      "        [8.6503e-01],\n",
      "        [1.2756e-04],\n",
      "        [9.5339e-01],\n",
      "        [1.0190e-01],\n",
      "        [9.6879e-01],\n",
      "        [8.7889e-03],\n",
      "        [7.7085e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.5128047466278076: \n",
      "target probs tensor([[2.9252e-02],\n",
      "        [6.8862e-01],\n",
      "        [2.7295e-01],\n",
      "        [2.2237e-02],\n",
      "        [6.3940e-04],\n",
      "        [2.5163e-03],\n",
      "        [4.6664e-05],\n",
      "        [6.6011e-06],\n",
      "        [2.2447e-02],\n",
      "        [5.1924e-02],\n",
      "        [2.5401e-07],\n",
      "        [1.8816e-02],\n",
      "        [9.7200e-01],\n",
      "        [1.1457e-02],\n",
      "        [5.2118e-01],\n",
      "        [2.1400e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 5.235118865966797: \n",
      "target probs tensor([[9.5681e-01],\n",
      "        [6.2491e-03],\n",
      "        [8.2675e-01],\n",
      "        [6.4420e-01],\n",
      "        [5.4803e-04],\n",
      "        [5.6166e-01],\n",
      "        [1.3213e-01],\n",
      "        [8.2545e-04],\n",
      "        [6.8645e-01],\n",
      "        [4.5837e-05],\n",
      "        [4.0016e-01],\n",
      "        [8.7573e-01],\n",
      "        [4.4048e-04],\n",
      "        [5.9103e-01],\n",
      "        [1.3758e-05],\n",
      "        [2.7620e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.7320919036865234: \n",
      "target probs tensor([[6.5837e-01],\n",
      "        [9.0439e-01],\n",
      "        [4.9895e-01],\n",
      "        [6.4177e-01],\n",
      "        [1.1365e-04],\n",
      "        [9.4363e-01],\n",
      "        [2.0264e-01],\n",
      "        [7.5537e-01],\n",
      "        [7.6791e-05],\n",
      "        [9.4561e-01],\n",
      "        [7.7278e-02],\n",
      "        [1.1608e-01],\n",
      "        [9.6668e-01],\n",
      "        [7.9996e-01],\n",
      "        [5.2847e-01],\n",
      "        [9.7728e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.7397847175598145: \n",
      "target probs tensor([[9.8605e-01],\n",
      "        [1.9364e-01],\n",
      "        [2.8133e-01],\n",
      "        [9.4785e-01],\n",
      "        [1.1873e-03],\n",
      "        [4.3706e-01],\n",
      "        [2.8599e-02],\n",
      "        [8.7934e-01],\n",
      "        [4.1521e-03],\n",
      "        [4.0486e-01],\n",
      "        [9.8294e-01],\n",
      "        [7.5784e-01],\n",
      "        [9.3991e-01],\n",
      "        [6.7315e-01],\n",
      "        [2.3513e-05],\n",
      "        [3.9619e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.059298038482666: \n",
      "target probs tensor([[2.4682e-04],\n",
      "        [5.6400e-04],\n",
      "        [9.2714e-01],\n",
      "        [8.7044e-01],\n",
      "        [2.6612e-02],\n",
      "        [3.8304e-01],\n",
      "        [8.7832e-01],\n",
      "        [1.0215e-05],\n",
      "        [5.4628e-02],\n",
      "        [3.8289e-05],\n",
      "        [7.1969e-02],\n",
      "        [9.4185e-01],\n",
      "        [5.2346e-01],\n",
      "        [8.2774e-06],\n",
      "        [2.4039e-05],\n",
      "        [8.3758e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.446276664733887: \n",
      "target probs tensor([[1.4998e-03],\n",
      "        [8.9423e-01],\n",
      "        [9.9586e-01],\n",
      "        [4.4721e-01],\n",
      "        [6.3482e-02],\n",
      "        [4.9974e-04],\n",
      "        [5.1648e-01],\n",
      "        [8.6570e-01]], device='cuda:0'), loss: 2.3233046531677246: \n",
      "Better model found at epoch 47 with validation value: 0.7319999933242798.\n",
      "target probs tensor([[3.5469e-02],\n",
      "        [2.8987e-04],\n",
      "        [8.5917e-01],\n",
      "        [1.8099e-04],\n",
      "        [8.8798e-03],\n",
      "        [8.4631e-01],\n",
      "        [8.6248e-02],\n",
      "        [2.1706e-01],\n",
      "        [9.8098e-01],\n",
      "        [6.5024e-01],\n",
      "        [1.1774e-04],\n",
      "        [9.0923e-01],\n",
      "        [5.0540e-04],\n",
      "        [2.0336e-03],\n",
      "        [3.2865e-03],\n",
      "        [3.0059e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.713927984237671: \n",
      "target probs tensor([[9.1271e-01],\n",
      "        [7.9640e-01],\n",
      "        [8.8812e-04],\n",
      "        [9.3665e-01],\n",
      "        [4.0421e-03],\n",
      "        [1.3639e-02],\n",
      "        [1.6710e-02],\n",
      "        [2.1374e-01],\n",
      "        [1.6053e-02],\n",
      "        [3.2636e-01],\n",
      "        [9.3774e-01],\n",
      "        [5.7536e-01],\n",
      "        [4.9025e-01],\n",
      "        [8.1098e-01],\n",
      "        [9.5079e-01],\n",
      "        [2.1266e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.952561616897583: \n",
      "target probs tensor([[4.8483e-01],\n",
      "        [1.0639e-02],\n",
      "        [1.7905e-04],\n",
      "        [9.4024e-01],\n",
      "        [2.5948e-04],\n",
      "        [1.0436e-05],\n",
      "        [9.9671e-01],\n",
      "        [3.3577e-03],\n",
      "        [3.5464e-01],\n",
      "        [9.8202e-01],\n",
      "        [3.8660e-01],\n",
      "        [7.4460e-04],\n",
      "        [9.9533e-01],\n",
      "        [7.1910e-01],\n",
      "        [9.2863e-01],\n",
      "        [3.8341e-01]], device='cuda:0'), loss: 3.122404098510742: \n",
      "target probs tensor([[8.2381e-01],\n",
      "        [1.7796e-04],\n",
      "        [9.1203e-01],\n",
      "        [3.3511e-02],\n",
      "        [9.4719e-01],\n",
      "        [6.2454e-05],\n",
      "        [7.3914e-02],\n",
      "        [1.2225e-01],\n",
      "        [2.0215e-01],\n",
      "        [8.8107e-01],\n",
      "        [9.7270e-01],\n",
      "        [7.8010e-01],\n",
      "        [4.9334e-05],\n",
      "        [3.6766e-02],\n",
      "        [8.7389e-01],\n",
      "        [8.8186e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.6399736404418945: \n",
      "target probs tensor([[2.3218e-04],\n",
      "        [9.9980e-01],\n",
      "        [5.9642e-01],\n",
      "        [7.2639e-01],\n",
      "        [2.3127e-01],\n",
      "        [2.3601e-01],\n",
      "        [2.0799e-04],\n",
      "        [1.0859e-01],\n",
      "        [8.3352e-01],\n",
      "        [2.8324e-04],\n",
      "        [4.3859e-04],\n",
      "        [8.4047e-01],\n",
      "        [8.3227e-01],\n",
      "        [6.1619e-01],\n",
      "        [3.0157e-05],\n",
      "        [9.6175e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.136491298675537: \n",
      "target probs tensor([[9.7914e-01],\n",
      "        [9.9737e-02],\n",
      "        [4.2551e-01],\n",
      "        [1.9493e-03],\n",
      "        [5.8985e-02],\n",
      "        [8.1696e-02],\n",
      "        [3.9944e-03],\n",
      "        [4.8510e-03],\n",
      "        [2.4028e-06],\n",
      "        [9.7017e-01],\n",
      "        [6.8977e-03],\n",
      "        [2.9491e-02],\n",
      "        [2.1609e-01],\n",
      "        [6.5148e-01],\n",
      "        [1.3378e-04],\n",
      "        [8.8655e-01]], device='cuda:0'), loss: 3.6298346519470215: \n",
      "target probs tensor([[4.9985e-03],\n",
      "        [1.2480e-01],\n",
      "        [1.1020e-04],\n",
      "        [9.3455e-01],\n",
      "        [1.1620e-02],\n",
      "        [1.1190e-01],\n",
      "        [4.6921e-01],\n",
      "        [1.6602e-04],\n",
      "        [1.8303e-01],\n",
      "        [3.3603e-01],\n",
      "        [1.1000e-04],\n",
      "        [8.4045e-01],\n",
      "        [5.3896e-05],\n",
      "        [9.8776e-01],\n",
      "        [9.9237e-01],\n",
      "        [1.1331e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.6920056343078613: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[9.0627e-03],\n",
      "        [3.6092e-04],\n",
      "        [4.9797e-02],\n",
      "        [2.4382e-03],\n",
      "        [4.2509e-06],\n",
      "        [2.5341e-06],\n",
      "        [3.4840e-04],\n",
      "        [6.1373e-01],\n",
      "        [9.2842e-01],\n",
      "        [7.6758e-05],\n",
      "        [9.7857e-01],\n",
      "        [3.2631e-02],\n",
      "        [4.3681e-03],\n",
      "        [4.0013e-05],\n",
      "        [3.2402e-02],\n",
      "        [3.6095e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.241596221923828: \n",
      "target probs tensor([[8.5899e-01],\n",
      "        [7.5682e-05],\n",
      "        [9.4563e-01],\n",
      "        [3.8433e-01],\n",
      "        [5.6382e-01],\n",
      "        [9.8365e-01],\n",
      "        [5.2020e-01],\n",
      "        [4.8536e-05],\n",
      "        [9.8135e-01],\n",
      "        [3.9347e-02],\n",
      "        [8.6097e-01],\n",
      "        [9.1557e-01],\n",
      "        [8.8149e-01],\n",
      "        [9.7368e-01],\n",
      "        [8.8662e-01],\n",
      "        [1.4463e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.008330821990967: \n",
      "Better model found at epoch 50 with validation value: 0.7360000014305115.\n",
      "target probs tensor([[8.7968e-01],\n",
      "        [7.2251e-01],\n",
      "        [9.0273e-02],\n",
      "        [9.9629e-01],\n",
      "        [5.6176e-06],\n",
      "        [1.1195e-01],\n",
      "        [1.9485e-01],\n",
      "        [3.1060e-04],\n",
      "        [9.9782e-01],\n",
      "        [8.3684e-03],\n",
      "        [4.0161e-02],\n",
      "        [9.5152e-01],\n",
      "        [7.2023e-01],\n",
      "        [9.9958e-01],\n",
      "        [7.9311e-01],\n",
      "        [6.3277e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.8207573890686035: \n",
      "target probs tensor([[7.2541e-02],\n",
      "        [9.9898e-01],\n",
      "        [9.0717e-01],\n",
      "        [4.5598e-03],\n",
      "        [1.6848e-03],\n",
      "        [8.3113e-01],\n",
      "        [6.4253e-04],\n",
      "        [1.0121e-02],\n",
      "        [9.6004e-01],\n",
      "        [3.0353e-05],\n",
      "        [3.9588e-02],\n",
      "        [2.4198e-01],\n",
      "        [1.1031e-02],\n",
      "        [3.3906e-04],\n",
      "        [1.8131e-03],\n",
      "        [9.9806e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.783083438873291: \n",
      "target probs tensor([[0.6982],\n",
      "        [0.9600],\n",
      "        [0.1929],\n",
      "        [0.0036],\n",
      "        [0.0272],\n",
      "        [0.9516],\n",
      "        [0.8412],\n",
      "        [0.0077],\n",
      "        [0.0027],\n",
      "        [0.3020],\n",
      "        [0.8664],\n",
      "        [0.9579],\n",
      "        [0.9344],\n",
      "        [0.0297],\n",
      "        [0.0345],\n",
      "        [0.0012]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.335689067840576: \n",
      "target probs tensor([[1.3643e-02],\n",
      "        [8.9081e-01],\n",
      "        [2.2431e-03],\n",
      "        [9.8796e-01],\n",
      "        [3.1592e-01],\n",
      "        [3.3827e-05],\n",
      "        [5.6648e-01],\n",
      "        [5.2600e-01],\n",
      "        [1.7264e-01],\n",
      "        [4.9728e-01],\n",
      "        [9.8892e-02],\n",
      "        [8.3622e-01],\n",
      "        [9.8848e-01],\n",
      "        [7.4276e-01],\n",
      "        [5.8657e-01],\n",
      "        [9.3503e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.8147997856140137: \n",
      "target probs tensor([[4.3535e-03],\n",
      "        [4.6899e-04],\n",
      "        [2.3944e-02],\n",
      "        [9.8988e-01],\n",
      "        [3.0475e-02],\n",
      "        [4.0063e-01],\n",
      "        [1.7180e-04],\n",
      "        [2.5896e-04],\n",
      "        [3.2873e-03],\n",
      "        [8.4533e-01],\n",
      "        [8.8121e-02],\n",
      "        [9.9806e-01],\n",
      "        [1.0563e-04],\n",
      "        [1.7457e-04],\n",
      "        [1.3819e-02],\n",
      "        [1.5269e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.979640483856201: \n",
      "target probs tensor([[1.2355e-02],\n",
      "        [2.3473e-05],\n",
      "        [9.5446e-01],\n",
      "        [9.8379e-01],\n",
      "        [1.5854e-04],\n",
      "        [7.3303e-02],\n",
      "        [4.9411e-03],\n",
      "        [9.7113e-01],\n",
      "        [9.4956e-02],\n",
      "        [4.9110e-01],\n",
      "        [9.6336e-01],\n",
      "        [8.5522e-01],\n",
      "        [6.2001e-01],\n",
      "        [7.9291e-01],\n",
      "        [9.5240e-01],\n",
      "        [6.1032e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.990192174911499: \n",
      "target probs tensor([[7.8233e-04],\n",
      "        [6.5049e-01],\n",
      "        [9.1895e-01],\n",
      "        [8.3916e-01],\n",
      "        [5.5786e-01],\n",
      "        [8.2884e-04],\n",
      "        [2.7505e-01],\n",
      "        [1.7638e-04],\n",
      "        [9.5212e-01],\n",
      "        [7.9620e-01],\n",
      "        [5.7568e-01],\n",
      "        [7.8065e-01],\n",
      "        [3.8085e-04],\n",
      "        [7.9524e-01],\n",
      "        [9.8913e-01],\n",
      "        [9.7739e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.1667962074279785: \n",
      "target probs tensor([[0.9383],\n",
      "        [0.9840],\n",
      "        [0.9880],\n",
      "        [0.9942],\n",
      "        [0.8341],\n",
      "        [0.0200],\n",
      "        [0.0474],\n",
      "        [0.9998],\n",
      "        [0.0186],\n",
      "        [0.0044],\n",
      "        [0.9970],\n",
      "        [0.9911],\n",
      "        [0.4038],\n",
      "        [0.8370],\n",
      "        [0.0166],\n",
      "        [0.9933]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.3659275770187378: \n",
      "target probs tensor([[3.0839e-04],\n",
      "        [8.1461e-01],\n",
      "        [9.2569e-01],\n",
      "        [2.0956e-03],\n",
      "        [5.1489e-02],\n",
      "        [2.3312e-01],\n",
      "        [9.8269e-01],\n",
      "        [1.1397e-03],\n",
      "        [9.9971e-01],\n",
      "        [9.7569e-01],\n",
      "        [1.9466e-02],\n",
      "        [5.6965e-01],\n",
      "        [2.2821e-03],\n",
      "        [9.8562e-01],\n",
      "        [9.7920e-01],\n",
      "        [9.3154e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.1426568031311035: \n",
      "target probs tensor([[8.8212e-01],\n",
      "        [2.1647e-01],\n",
      "        [9.8947e-01],\n",
      "        [2.4295e-01],\n",
      "        [2.7969e-03],\n",
      "        [3.9626e-04],\n",
      "        [6.3595e-01],\n",
      "        [2.9853e-04],\n",
      "        [8.3850e-04],\n",
      "        [9.8124e-01],\n",
      "        [2.1767e-04],\n",
      "        [3.3300e-01],\n",
      "        [7.8742e-01],\n",
      "        [1.0774e-03],\n",
      "        [2.6399e-02],\n",
      "        [4.9719e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.3377199172973633: \n",
      "target probs tensor([[4.1196e-04],\n",
      "        [1.8327e-02],\n",
      "        [4.4107e-01],\n",
      "        [3.6710e-05],\n",
      "        [1.1582e-01],\n",
      "        [7.8637e-04],\n",
      "        [7.7122e-02],\n",
      "        [9.5238e-01],\n",
      "        [2.2140e-03],\n",
      "        [8.9126e-01],\n",
      "        [1.1662e-04],\n",
      "        [2.9306e-03],\n",
      "        [5.8354e-01],\n",
      "        [3.0208e-03],\n",
      "        [3.2692e-01],\n",
      "        [2.0553e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.3439621925354: \n",
      "target probs tensor([[6.5874e-01],\n",
      "        [9.1118e-01],\n",
      "        [4.9325e-06],\n",
      "        [4.4570e-02],\n",
      "        [9.2447e-03],\n",
      "        [7.3600e-01],\n",
      "        [6.9394e-01],\n",
      "        [4.0171e-05],\n",
      "        [7.9274e-01],\n",
      "        [8.7120e-01],\n",
      "        [9.5367e-01],\n",
      "        [3.1323e-04],\n",
      "        [9.9817e-01],\n",
      "        [9.8549e-01],\n",
      "        [8.4475e-01],\n",
      "        [2.7644e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.723649024963379: \n",
      "target probs tensor([[2.1182e-03],\n",
      "        [9.6982e-01],\n",
      "        [3.0804e-01],\n",
      "        [1.8927e-01],\n",
      "        [1.3576e-01],\n",
      "        [9.8653e-01],\n",
      "        [8.8312e-06],\n",
      "        [7.6671e-05],\n",
      "        [9.3794e-01],\n",
      "        [2.3571e-02],\n",
      "        [1.1258e-04],\n",
      "        [8.8586e-01],\n",
      "        [7.8542e-01],\n",
      "        [9.5765e-01],\n",
      "        [9.7421e-01],\n",
      "        [6.2890e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.8720686435699463: \n",
      "target probs tensor([[8.4364e-02],\n",
      "        [9.9950e-01],\n",
      "        [1.1255e-02],\n",
      "        [4.0902e-01],\n",
      "        [5.5620e-01],\n",
      "        [9.8538e-03],\n",
      "        [8.3320e-01],\n",
      "        [7.2884e-01],\n",
      "        [7.9275e-04],\n",
      "        [5.9843e-01],\n",
      "        [3.2561e-07],\n",
      "        [2.8716e-01],\n",
      "        [7.9922e-01],\n",
      "        [4.9042e-03],\n",
      "        [9.8542e-03],\n",
      "        [3.5287e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.037569046020508: \n",
      "target probs tensor([[1.5557e-01],\n",
      "        [1.3156e-03],\n",
      "        [5.4083e-03],\n",
      "        [1.1533e-02],\n",
      "        [1.7085e-02],\n",
      "        [4.1604e-02],\n",
      "        [9.7547e-01],\n",
      "        [9.3782e-01],\n",
      "        [1.4159e-03],\n",
      "        [9.2002e-01],\n",
      "        [4.0561e-04],\n",
      "        [5.1575e-01],\n",
      "        [9.3671e-01],\n",
      "        [8.3459e-01],\n",
      "        [9.5716e-01],\n",
      "        [1.1334e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.981415271759033: \n",
      "target probs tensor([[8.6677e-04],\n",
      "        [8.2607e-01],\n",
      "        [9.6862e-01],\n",
      "        [7.1490e-01],\n",
      "        [2.3070e-02],\n",
      "        [9.3710e-04],\n",
      "        [7.2582e-01],\n",
      "        [3.8676e-01]], device='cuda:0'), loss: 2.452702760696411: \n",
      "target probs tensor([[9.4392e-01],\n",
      "        [6.6212e-01],\n",
      "        [1.0988e-04],\n",
      "        [8.7047e-01],\n",
      "        [3.9518e-01],\n",
      "        [3.8737e-03],\n",
      "        [3.9152e-01],\n",
      "        [5.9243e-01],\n",
      "        [9.5885e-01],\n",
      "        [5.0226e-02],\n",
      "        [1.6246e-01],\n",
      "        [5.3011e-02],\n",
      "        [6.0515e-02],\n",
      "        [4.3895e-03],\n",
      "        [7.5885e-01],\n",
      "        [2.4735e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.7857859134674072: \n",
      "target probs tensor([[5.1935e-02],\n",
      "        [1.7124e-02],\n",
      "        [9.5617e-01],\n",
      "        [4.4181e-04],\n",
      "        [1.5232e-04],\n",
      "        [2.0916e-01],\n",
      "        [6.1571e-04],\n",
      "        [2.5331e-01],\n",
      "        [6.6074e-04],\n",
      "        [8.9471e-01],\n",
      "        [9.8745e-01],\n",
      "        [9.9065e-01],\n",
      "        [8.1797e-05],\n",
      "        [9.6534e-01],\n",
      "        [9.3858e-01],\n",
      "        [3.1453e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.3961968421936035: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[2.1115e-01],\n",
      "        [5.5272e-02],\n",
      "        [1.1643e-04],\n",
      "        [7.0043e-01],\n",
      "        [1.1714e-02],\n",
      "        [1.6167e-06],\n",
      "        [9.9630e-01],\n",
      "        [2.0422e-03],\n",
      "        [4.4528e-01],\n",
      "        [9.8221e-01],\n",
      "        [2.5179e-02],\n",
      "        [7.6214e-05],\n",
      "        [9.3704e-01],\n",
      "        [3.5734e-02],\n",
      "        [7.5331e-01],\n",
      "        [4.4997e-01]], device='cuda:0'), loss: 3.5196077823638916: \n",
      "Better model found at epoch 56 with validation value: 0.7369999885559082.\n",
      "target probs tensor([[2.8150e-03],\n",
      "        [7.6457e-04],\n",
      "        [6.9379e-07],\n",
      "        [7.1369e-01],\n",
      "        [5.3027e-01],\n",
      "        [9.8120e-01],\n",
      "        [1.2469e-02],\n",
      "        [3.8619e-01],\n",
      "        [4.7909e-02],\n",
      "        [4.5760e-05],\n",
      "        [7.7753e-01],\n",
      "        [1.6254e-05],\n",
      "        [1.6751e-04],\n",
      "        [8.5228e-01],\n",
      "        [9.8685e-01],\n",
      "        [1.6068e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.429015159606934: \n",
      "target probs tensor([[0.2123],\n",
      "        [0.2487],\n",
      "        [0.9458],\n",
      "        [0.9938],\n",
      "        [0.9737],\n",
      "        [0.9559],\n",
      "        [0.1529],\n",
      "        [0.8923],\n",
      "        [0.4681],\n",
      "        [0.0035],\n",
      "        [0.9661],\n",
      "        [0.2042],\n",
      "        [0.9983],\n",
      "        [0.9821],\n",
      "        [0.0072],\n",
      "        [0.8508]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.1389927864074707: \n",
      "target probs tensor([[3.9170e-01],\n",
      "        [5.5671e-01],\n",
      "        [1.5458e-03],\n",
      "        [2.5536e-02],\n",
      "        [1.5651e-02],\n",
      "        [8.3695e-03],\n",
      "        [5.7786e-03],\n",
      "        [1.3620e-01],\n",
      "        [6.4046e-07],\n",
      "        [9.7897e-01],\n",
      "        [3.3995e-01],\n",
      "        [4.6686e-02],\n",
      "        [7.4364e-01],\n",
      "        [6.3950e-01],\n",
      "        [1.2045e-04],\n",
      "        [9.8719e-01]], device='cuda:0'), loss: 3.497270345687866: \n",
      "target probs tensor([[1.8113e-03],\n",
      "        [3.3054e-03],\n",
      "        [9.0127e-01],\n",
      "        [2.5134e-01],\n",
      "        [9.3927e-01],\n",
      "        [9.0915e-01],\n",
      "        [9.8871e-01],\n",
      "        [7.4225e-01],\n",
      "        [8.6407e-01],\n",
      "        [9.3739e-01],\n",
      "        [6.6860e-06],\n",
      "        [9.9318e-01],\n",
      "        [2.3602e-01],\n",
      "        [1.1353e-03],\n",
      "        [7.7693e-01],\n",
      "        [4.2299e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.6472795009613037: \n",
      "target probs tensor([[1.0933e-04],\n",
      "        [1.4682e-04],\n",
      "        [6.5182e-04],\n",
      "        [3.5602e-02],\n",
      "        [9.2428e-01],\n",
      "        [7.6975e-04],\n",
      "        [6.6219e-01],\n",
      "        [1.8706e-01],\n",
      "        [8.0523e-01],\n",
      "        [4.4737e-03],\n",
      "        [9.4797e-01],\n",
      "        [9.2059e-01],\n",
      "        [1.0795e-03],\n",
      "        [9.6296e-01],\n",
      "        [7.4280e-01],\n",
      "        [2.1566e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.2761268615722656: \n",
      "target probs tensor([[5.3558e-01],\n",
      "        [6.8847e-01],\n",
      "        [8.1800e-01],\n",
      "        [9.7334e-01],\n",
      "        [6.7941e-01],\n",
      "        [1.5513e-04],\n",
      "        [2.4694e-04],\n",
      "        [1.0069e-03],\n",
      "        [2.1152e-04],\n",
      "        [9.3969e-01],\n",
      "        [8.3878e-01],\n",
      "        [4.5925e-01],\n",
      "        [2.3638e-03],\n",
      "        [4.3990e-05],\n",
      "        [9.6878e-01],\n",
      "        [9.2286e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.2036895751953125: \n",
      "Better model found at epoch 58 with validation value: 0.7379999756813049.\n",
      "target probs tensor([[7.8094e-01],\n",
      "        [1.1070e-02],\n",
      "        [9.8088e-01],\n",
      "        [3.9042e-05],\n",
      "        [6.8775e-03],\n",
      "        [9.9772e-01],\n",
      "        [5.2659e-01],\n",
      "        [3.0194e-02],\n",
      "        [6.1199e-01],\n",
      "        [8.9336e-03],\n",
      "        [4.5460e-01],\n",
      "        [4.5866e-05],\n",
      "        [9.9899e-01],\n",
      "        [7.6234e-02],\n",
      "        [1.6770e-01],\n",
      "        [4.2301e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.8282594680786133: \n",
      "target probs tensor([[8.9491e-01],\n",
      "        [1.6839e-01],\n",
      "        [9.9818e-01],\n",
      "        [9.7537e-01],\n",
      "        [4.8696e-01],\n",
      "        [6.5101e-01],\n",
      "        [6.7773e-01],\n",
      "        [9.9232e-01],\n",
      "        [9.7068e-01],\n",
      "        [7.7249e-01],\n",
      "        [7.7662e-03],\n",
      "        [6.5136e-01],\n",
      "        [1.9823e-04],\n",
      "        [3.2771e-03],\n",
      "        [9.9758e-01],\n",
      "        [9.8002e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.4568030834197998: \n",
      "target probs tensor([[9.9671e-01],\n",
      "        [8.9984e-01],\n",
      "        [8.8416e-01],\n",
      "        [6.9072e-04],\n",
      "        [8.4897e-01],\n",
      "        [4.4399e-05],\n",
      "        [5.1895e-04],\n",
      "        [9.0001e-04],\n",
      "        [2.8367e-05],\n",
      "        [6.1652e-02],\n",
      "        [7.9392e-01],\n",
      "        [9.5091e-01],\n",
      "        [2.4971e-02],\n",
      "        [9.7139e-01],\n",
      "        [2.1143e-02],\n",
      "        [4.6026e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.3851046562194824: \n",
      "Better model found at epoch 59 with validation value: 0.7409999966621399.\n",
      "target probs tensor([[3.4348e-01],\n",
      "        [4.8578e-03],\n",
      "        [1.9430e-03],\n",
      "        [9.8789e-01],\n",
      "        [6.0084e-01],\n",
      "        [3.2616e-03],\n",
      "        [8.5310e-01],\n",
      "        [3.8206e-02],\n",
      "        [5.3308e-02],\n",
      "        [8.1304e-03],\n",
      "        [1.2066e-04],\n",
      "        [3.0641e-04],\n",
      "        [3.9628e-04],\n",
      "        [3.7902e-01],\n",
      "        [9.9177e-01],\n",
      "        [9.7270e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.5004096031188965: \n",
      "target probs tensor([[6.6525e-01],\n",
      "        [2.9971e-03],\n",
      "        [5.9796e-01],\n",
      "        [9.2128e-01],\n",
      "        [9.1911e-01],\n",
      "        [2.3449e-04],\n",
      "        [2.3449e-01],\n",
      "        [5.5669e-01],\n",
      "        [9.2693e-03],\n",
      "        [1.4998e-02],\n",
      "        [6.4030e-03],\n",
      "        [7.8756e-03],\n",
      "        [9.5057e-01],\n",
      "        [9.4174e-01],\n",
      "        [1.4276e-01],\n",
      "        [6.2050e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.8444178104400635: \n",
      "target probs tensor([[1.9437e-02],\n",
      "        [9.7027e-01],\n",
      "        [1.6793e-05],\n",
      "        [5.1934e-01],\n",
      "        [9.1498e-01],\n",
      "        [8.3956e-01],\n",
      "        [8.0608e-01],\n",
      "        [6.0288e-03],\n",
      "        [3.5623e-02],\n",
      "        [9.5219e-01],\n",
      "        [9.1949e-01],\n",
      "        [5.5256e-04],\n",
      "        [3.1871e-01],\n",
      "        [5.1536e-06],\n",
      "        [1.6125e-04],\n",
      "        [9.9074e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.3900465965270996: \n",
      "target probs tensor([[4.3267e-03],\n",
      "        [9.4266e-01],\n",
      "        [2.5286e-02],\n",
      "        [1.2455e-04],\n",
      "        [1.1340e-03],\n",
      "        [2.1957e-02],\n",
      "        [9.7429e-01],\n",
      "        [9.5202e-01],\n",
      "        [2.4090e-03],\n",
      "        [8.9887e-01],\n",
      "        [7.1523e-01],\n",
      "        [9.4675e-01],\n",
      "        [1.9553e-01],\n",
      "        [6.8237e-02],\n",
      "        [5.6623e-02],\n",
      "        [9.7516e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.6615357398986816: \n",
      "target probs tensor([[1.1543e-06],\n",
      "        [9.9223e-01],\n",
      "        [4.8314e-04],\n",
      "        [1.3590e-02],\n",
      "        [1.6919e-04],\n",
      "        [7.7537e-02],\n",
      "        [9.9531e-01],\n",
      "        [3.0593e-03],\n",
      "        [9.0298e-01],\n",
      "        [1.1364e-02],\n",
      "        [3.7690e-01],\n",
      "        [4.2892e-01],\n",
      "        [2.6782e-05],\n",
      "        [9.5037e-01],\n",
      "        [4.7776e-01],\n",
      "        [4.7282e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.10764741897583: \n",
      "target probs tensor([[9.0890e-01],\n",
      "        [7.1046e-01],\n",
      "        [1.6152e-05],\n",
      "        [4.1215e-01],\n",
      "        [5.6792e-01],\n",
      "        [7.0454e-05],\n",
      "        [2.8805e-04],\n",
      "        [6.2425e-01],\n",
      "        [8.8645e-01],\n",
      "        [4.2932e-05],\n",
      "        [8.9489e-01],\n",
      "        [9.6135e-01],\n",
      "        [8.7826e-01],\n",
      "        [9.1492e-01],\n",
      "        [2.4661e-01],\n",
      "        [7.9832e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.7048721313476562: \n",
      "target probs tensor([[2.9977e-01],\n",
      "        [9.5614e-01],\n",
      "        [5.4957e-01],\n",
      "        [2.9276e-05],\n",
      "        [2.2107e-02],\n",
      "        [2.0518e-03],\n",
      "        [8.6094e-01],\n",
      "        [7.2353e-03],\n",
      "        [1.1073e-02],\n",
      "        [9.8042e-02],\n",
      "        [6.7323e-01],\n",
      "        [7.4047e-02],\n",
      "        [4.4897e-05],\n",
      "        [3.0681e-01],\n",
      "        [9.9256e-01],\n",
      "        [8.7014e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.0331223011016846: \n",
      "target probs tensor([[5.9866e-01],\n",
      "        [9.7398e-01],\n",
      "        [4.3340e-02],\n",
      "        [4.7393e-02],\n",
      "        [1.3896e-04],\n",
      "        [2.8217e-01],\n",
      "        [4.3705e-05],\n",
      "        [3.8990e-02],\n",
      "        [3.9117e-01],\n",
      "        [8.2026e-01],\n",
      "        [3.3339e-03],\n",
      "        [4.2846e-01],\n",
      "        [1.5666e-02],\n",
      "        [1.9315e-03],\n",
      "        [9.9611e-01],\n",
      "        [5.7901e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.0500216484069824: \n",
      "target probs tensor([[9.7594e-01],\n",
      "        [3.7324e-02],\n",
      "        [5.1749e-02],\n",
      "        [8.6389e-05],\n",
      "        [9.1496e-01],\n",
      "        [9.6212e-01],\n",
      "        [5.5464e-05],\n",
      "        [9.9505e-01],\n",
      "        [1.5278e-01],\n",
      "        [1.8775e-02],\n",
      "        [9.5579e-01],\n",
      "        [9.8952e-01],\n",
      "        [4.0690e-02],\n",
      "        [6.1627e-03],\n",
      "        [5.5161e-01],\n",
      "        [8.2284e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.534583568572998: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[1.8540e-03],\n",
      "        [2.2594e-04],\n",
      "        [8.0250e-02],\n",
      "        [8.1893e-01],\n",
      "        [3.9430e-01],\n",
      "        [1.9719e-04],\n",
      "        [1.1412e-02],\n",
      "        [6.4513e-02],\n",
      "        [9.2010e-01],\n",
      "        [3.9517e-02],\n",
      "        [1.2511e-02],\n",
      "        [9.9677e-01],\n",
      "        [1.5357e-04],\n",
      "        [6.2973e-01],\n",
      "        [5.1232e-04],\n",
      "        [8.0952e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.9637198448181152: \n",
      "target probs tensor([[9.5385e-01],\n",
      "        [9.0150e-03],\n",
      "        [8.3953e-03],\n",
      "        [1.3944e-01],\n",
      "        [1.0316e-04],\n",
      "        [9.5618e-01],\n",
      "        [8.8683e-01],\n",
      "        [8.8057e-01],\n",
      "        [2.8615e-02],\n",
      "        [9.5705e-01],\n",
      "        [9.6749e-01],\n",
      "        [1.2295e-02],\n",
      "        [2.4522e-02],\n",
      "        [2.3304e-03],\n",
      "        [1.2341e-02],\n",
      "        [8.5099e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.708315849304199: \n",
      "target probs tensor([[1.3790e-01],\n",
      "        [5.6483e-05],\n",
      "        [4.5919e-04],\n",
      "        [2.0090e-01],\n",
      "        [5.1607e-05],\n",
      "        [5.2835e-01],\n",
      "        [1.4178e-01],\n",
      "        [5.4280e-02],\n",
      "        [9.7243e-01],\n",
      "        [3.8668e-07],\n",
      "        [6.1435e-01],\n",
      "        [5.5044e-01],\n",
      "        [6.9905e-01],\n",
      "        [2.5473e-01],\n",
      "        [9.8237e-01],\n",
      "        [8.8615e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.961430788040161: \n",
      "target probs tensor([[0.0095],\n",
      "        [0.9940],\n",
      "        [0.9690],\n",
      "        [0.0109],\n",
      "        [0.0273],\n",
      "        [0.0019],\n",
      "        [0.5926],\n",
      "        [0.8568]], device='cuda:0'), loss: 2.4692535400390625: \n",
      "target probs tensor([[1.0876e-03],\n",
      "        [3.6888e-02],\n",
      "        [1.8631e-01],\n",
      "        [2.2945e-04],\n",
      "        [1.4244e-01],\n",
      "        [7.7412e-05],\n",
      "        [9.1981e-01],\n",
      "        [4.8826e-01],\n",
      "        [2.0368e-01],\n",
      "        [3.0642e-04],\n",
      "        [7.8724e-03],\n",
      "        [6.2681e-05],\n",
      "        [9.4772e-01],\n",
      "        [3.1067e-01],\n",
      "        [3.5535e-01],\n",
      "        [1.9346e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.9253625869750977: \n",
      "target probs tensor([[9.1928e-01],\n",
      "        [1.1600e-01],\n",
      "        [9.1330e-01],\n",
      "        [1.6507e-04],\n",
      "        [7.1834e-01],\n",
      "        [1.1588e-04],\n",
      "        [8.6945e-01],\n",
      "        [9.9743e-01],\n",
      "        [5.2493e-04],\n",
      "        [8.2073e-01],\n",
      "        [1.4159e-01],\n",
      "        [1.1444e-02],\n",
      "        [1.6432e-01],\n",
      "        [9.8709e-01],\n",
      "        [7.3455e-01],\n",
      "        [7.3871e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.323726177215576: \n",
      "target probs tensor([[2.0651e-01],\n",
      "        [5.4551e-02],\n",
      "        [7.0687e-05],\n",
      "        [6.4654e-01],\n",
      "        [3.9320e-03],\n",
      "        [1.8759e-06],\n",
      "        [9.7664e-01],\n",
      "        [1.9036e-02],\n",
      "        [8.9799e-01],\n",
      "        [9.8521e-01],\n",
      "        [8.6065e-01],\n",
      "        [7.3659e-03],\n",
      "        [9.8819e-01],\n",
      "        [3.8653e-03],\n",
      "        [2.6371e-01],\n",
      "        [9.1055e-01]], device='cuda:0'), loss: 3.0854403972625732: \n",
      "Better model found at epoch 64 with validation value: 0.7440000176429749.\n",
      "target probs tensor([[8.5913e-04],\n",
      "        [9.1792e-01],\n",
      "        [2.9434e-02],\n",
      "        [9.9769e-01],\n",
      "        [8.1212e-01],\n",
      "        [4.7556e-08],\n",
      "        [7.3978e-04],\n",
      "        [9.8701e-01],\n",
      "        [5.2920e-04],\n",
      "        [7.8937e-01],\n",
      "        [3.4422e-01],\n",
      "        [2.8239e-01],\n",
      "        [3.2336e-02],\n",
      "        [2.4357e-05],\n",
      "        [9.7087e-01],\n",
      "        [9.9748e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.6975412368774414: \n",
      "target probs tensor([[3.7284e-04],\n",
      "        [2.0327e-04],\n",
      "        [2.9360e-01],\n",
      "        [1.2621e-02],\n",
      "        [1.0224e-01],\n",
      "        [3.2347e-04],\n",
      "        [8.0450e-05],\n",
      "        [9.7632e-03],\n",
      "        [5.9314e-05],\n",
      "        [9.8516e-01],\n",
      "        [4.2162e-01],\n",
      "        [1.8263e-01],\n",
      "        [7.1026e-05],\n",
      "        [1.9970e-03],\n",
      "        [3.3850e-03],\n",
      "        [9.6117e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 5.01094913482666: \n",
      "target probs tensor([[8.2347e-01],\n",
      "        [4.2948e-01],\n",
      "        [3.9295e-03],\n",
      "        [2.5536e-03],\n",
      "        [1.9290e-02],\n",
      "        [1.1547e-02],\n",
      "        [4.7369e-01],\n",
      "        [1.2941e-01],\n",
      "        [2.1211e-05],\n",
      "        [9.0841e-01],\n",
      "        [2.3135e-01],\n",
      "        [6.5880e-03],\n",
      "        [1.2329e-01],\n",
      "        [9.6942e-01],\n",
      "        [2.2804e-04],\n",
      "        [9.8725e-01]], device='cuda:0'), loss: 3.226052761077881: \n",
      "target probs tensor([[0.0018],\n",
      "        [0.6499],\n",
      "        [0.1297],\n",
      "        [0.3568],\n",
      "        [0.8836],\n",
      "        [0.6607],\n",
      "        [0.0016],\n",
      "        [0.0155],\n",
      "        [0.7905],\n",
      "        [0.9943],\n",
      "        [0.7691],\n",
      "        [0.0577],\n",
      "        [0.4861],\n",
      "        [0.9668],\n",
      "        [0.0740],\n",
      "        [0.9993]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.7298800945281982: \n",
      "target probs tensor([[6.8509e-04],\n",
      "        [4.8181e-01],\n",
      "        [9.7238e-01],\n",
      "        [1.9152e-05],\n",
      "        [5.0511e-05],\n",
      "        [4.6562e-05],\n",
      "        [8.3925e-01],\n",
      "        [2.7739e-05],\n",
      "        [6.6514e-01],\n",
      "        [9.6444e-01],\n",
      "        [9.2570e-01],\n",
      "        [2.1179e-02],\n",
      "        [1.4664e-01],\n",
      "        [9.3652e-01],\n",
      "        [3.2044e-02],\n",
      "        [6.2887e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.019636154174805: \n",
      "target probs tensor([[9.8831e-01],\n",
      "        [9.8328e-02],\n",
      "        [3.0511e-04],\n",
      "        [4.4071e-04],\n",
      "        [9.9922e-01],\n",
      "        [9.7719e-01],\n",
      "        [9.1461e-01],\n",
      "        [8.5509e-02],\n",
      "        [2.7298e-03],\n",
      "        [6.0673e-01],\n",
      "        [1.1642e-01],\n",
      "        [9.9727e-01],\n",
      "        [9.2280e-01],\n",
      "        [9.6282e-01],\n",
      "        [7.8059e-01],\n",
      "        [9.4001e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.8568592071533203: \n",
      "target probs tensor([[6.7183e-01],\n",
      "        [4.9849e-02],\n",
      "        [9.5112e-01],\n",
      "        [9.7603e-01],\n",
      "        [4.8629e-02],\n",
      "        [1.1729e-05],\n",
      "        [9.8971e-01],\n",
      "        [9.8010e-01],\n",
      "        [8.3677e-01],\n",
      "        [1.1795e-04],\n",
      "        [8.6596e-01],\n",
      "        [1.1824e-03],\n",
      "        [1.7953e-04],\n",
      "        [9.9214e-01],\n",
      "        [1.2303e-02],\n",
      "        [8.1177e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.9515905380249023: \n",
      "target probs tensor([[8.2675e-01],\n",
      "        [9.4886e-01],\n",
      "        [2.4852e-01],\n",
      "        [9.5456e-01],\n",
      "        [9.9083e-01],\n",
      "        [9.8370e-01],\n",
      "        [1.1497e-05],\n",
      "        [8.9125e-01],\n",
      "        [7.8285e-02],\n",
      "        [9.9371e-01],\n",
      "        [3.6439e-01],\n",
      "        [9.0523e-01],\n",
      "        [4.5675e-01],\n",
      "        [9.7677e-01],\n",
      "        [9.9768e-01],\n",
      "        [5.6811e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.1395870447158813: \n",
      "target probs tensor([[9.8267e-01],\n",
      "        [1.7212e-04],\n",
      "        [6.4255e-03],\n",
      "        [2.7802e-04],\n",
      "        [9.8748e-01],\n",
      "        [9.2818e-01],\n",
      "        [2.0557e-01],\n",
      "        [5.3379e-02],\n",
      "        [9.9054e-01],\n",
      "        [8.8102e-01],\n",
      "        [7.8961e-01],\n",
      "        [4.5159e-04],\n",
      "        [3.4269e-06],\n",
      "        [1.2821e-04],\n",
      "        [5.7430e-04],\n",
      "        [8.0560e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.132589817047119: \n",
      "target probs tensor([[1.7306e-03],\n",
      "        [7.6014e-01],\n",
      "        [8.8323e-05],\n",
      "        [9.0375e-01],\n",
      "        [9.9809e-01],\n",
      "        [2.6150e-05],\n",
      "        [2.0471e-04],\n",
      "        [2.6552e-04],\n",
      "        [4.7261e-01],\n",
      "        [1.3174e-01],\n",
      "        [3.3092e-02],\n",
      "        [4.4435e-03],\n",
      "        [8.1188e-03],\n",
      "        [4.0245e-01],\n",
      "        [9.9141e-01],\n",
      "        [1.0426e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.077972888946533: \n",
      "target probs tensor([[1.0275e-01],\n",
      "        [5.3620e-04],\n",
      "        [1.1711e-03],\n",
      "        [9.5286e-01],\n",
      "        [1.6044e-01],\n",
      "        [6.5620e-01],\n",
      "        [8.7661e-01],\n",
      "        [4.4454e-01],\n",
      "        [2.8174e-01],\n",
      "        [7.9006e-01],\n",
      "        [9.9701e-01],\n",
      "        [6.1154e-04],\n",
      "        [7.7844e-01],\n",
      "        [6.0196e-01],\n",
      "        [9.7014e-01],\n",
      "        [9.2434e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.279862403869629: \n",
      "target probs tensor([[2.7772e-04],\n",
      "        [6.5110e-01],\n",
      "        [1.2322e-04],\n",
      "        [1.5664e-03],\n",
      "        [9.9387e-01],\n",
      "        [8.6596e-01],\n",
      "        [9.9581e-01],\n",
      "        [9.0259e-01],\n",
      "        [4.9868e-01],\n",
      "        [6.7553e-01],\n",
      "        [5.9267e-01],\n",
      "        [5.9030e-01],\n",
      "        [8.4132e-01],\n",
      "        [9.4448e-01],\n",
      "        [4.5822e-01],\n",
      "        [2.2131e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.8120040893554688: \n",
      "target probs tensor([[3.3197e-01],\n",
      "        [7.5880e-01],\n",
      "        [2.6271e-01],\n",
      "        [2.9441e-03],\n",
      "        [5.0892e-05],\n",
      "        [8.4221e-01],\n",
      "        [9.6318e-01],\n",
      "        [9.0147e-06],\n",
      "        [9.9628e-03],\n",
      "        [8.0687e-02],\n",
      "        [9.4923e-01],\n",
      "        [1.3395e-03],\n",
      "        [7.1013e-02],\n",
      "        [3.0056e-06],\n",
      "        [3.7464e-03],\n",
      "        [9.1629e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.211597442626953: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[3.6385e-01],\n",
      "        [2.0199e-05],\n",
      "        [5.0953e-04],\n",
      "        [8.6667e-02],\n",
      "        [3.8102e-01],\n",
      "        [1.1401e-02],\n",
      "        [2.0541e-06],\n",
      "        [2.8549e-02],\n",
      "        [9.3687e-01],\n",
      "        [8.7549e-07],\n",
      "        [5.9355e-01],\n",
      "        [3.2251e-01],\n",
      "        [9.6559e-01],\n",
      "        [9.6602e-01],\n",
      "        [1.6443e-02],\n",
      "        [7.3773e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.293309211730957: \n",
      "target probs tensor([[2.0404e-02],\n",
      "        [4.3236e-01],\n",
      "        [7.9766e-04],\n",
      "        [2.0596e-05],\n",
      "        [9.2833e-01],\n",
      "        [8.3704e-01],\n",
      "        [2.8429e-02],\n",
      "        [1.2102e-02],\n",
      "        [9.9198e-01],\n",
      "        [6.4312e-01],\n",
      "        [1.0069e-04],\n",
      "        [9.8888e-01],\n",
      "        [1.1408e-02],\n",
      "        [5.1371e-02],\n",
      "        [9.7857e-01],\n",
      "        [7.7987e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.0161399841308594: \n",
      "target probs tensor([[3.1151e-05],\n",
      "        [8.9266e-01],\n",
      "        [6.5986e-01],\n",
      "        [1.2972e-06],\n",
      "        [9.8250e-01],\n",
      "        [7.8396e-01],\n",
      "        [8.8473e-01],\n",
      "        [3.4487e-01],\n",
      "        [1.2688e-04],\n",
      "        [8.4125e-02],\n",
      "        [1.1244e-02],\n",
      "        [9.6031e-03],\n",
      "        [1.0885e-05],\n",
      "        [8.5645e-05],\n",
      "        [2.1345e-05],\n",
      "        [5.1395e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.919018745422363: \n",
      "target probs tensor([[4.9525e-04],\n",
      "        [4.5608e-04],\n",
      "        [9.4733e-01],\n",
      "        [8.4151e-01],\n",
      "        [8.2844e-02],\n",
      "        [9.3393e-01],\n",
      "        [3.9198e-04],\n",
      "        [2.2136e-02],\n",
      "        [1.2535e-03],\n",
      "        [3.2954e-02],\n",
      "        [9.8978e-01],\n",
      "        [2.0103e-03],\n",
      "        [1.6028e-02],\n",
      "        [9.8290e-01],\n",
      "        [1.7026e-03],\n",
      "        [1.7626e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.220664978027344: \n",
      "target probs tensor([[9.9257e-01],\n",
      "        [9.7914e-01],\n",
      "        [2.6972e-01],\n",
      "        [9.1760e-01],\n",
      "        [4.2677e-04],\n",
      "        [2.6263e-03],\n",
      "        [8.5462e-01],\n",
      "        [1.9667e-03],\n",
      "        [8.6732e-01],\n",
      "        [9.4460e-02],\n",
      "        [5.4665e-01],\n",
      "        [1.1271e-01],\n",
      "        [4.8374e-06],\n",
      "        [2.1187e-01],\n",
      "        [5.4874e-01],\n",
      "        [6.2909e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.6036362648010254: \n",
      "target probs tensor([[5.3955e-05],\n",
      "        [1.1475e-04],\n",
      "        [1.0914e-02],\n",
      "        [7.7141e-02],\n",
      "        [5.3408e-01],\n",
      "        [9.8781e-01],\n",
      "        [5.8076e-05],\n",
      "        [4.3133e-04],\n",
      "        [9.5428e-01],\n",
      "        [9.9170e-01],\n",
      "        [5.0019e-05],\n",
      "        [6.5145e-01],\n",
      "        [4.3817e-01],\n",
      "        [8.9314e-03],\n",
      "        [7.1192e-01],\n",
      "        [1.4477e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.470918655395508: \n",
      "target probs tensor([[2.1703e-02],\n",
      "        [9.8977e-02],\n",
      "        [2.8149e-01],\n",
      "        [9.8836e-05],\n",
      "        [3.5475e-05],\n",
      "        [8.6427e-01],\n",
      "        [2.3650e-01],\n",
      "        [4.7590e-02],\n",
      "        [7.0324e-05],\n",
      "        [9.9468e-01],\n",
      "        [2.4260e-06],\n",
      "        [7.0980e-01],\n",
      "        [4.3744e-03],\n",
      "        [9.5956e-01],\n",
      "        [1.6968e-05],\n",
      "        [6.6674e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.88266134262085: \n",
      "target probs tensor([[4.3261e-06],\n",
      "        [4.7349e-03],\n",
      "        [8.1045e-01],\n",
      "        [9.3278e-06],\n",
      "        [3.8011e-04],\n",
      "        [1.1413e-01],\n",
      "        [4.0474e-01],\n",
      "        [8.1724e-01],\n",
      "        [2.8469e-02],\n",
      "        [2.6012e-01],\n",
      "        [8.1723e-05],\n",
      "        [9.2225e-01],\n",
      "        [4.6131e-01],\n",
      "        [1.5530e-04],\n",
      "        [1.2392e-01],\n",
      "        [5.2801e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.207334995269775: \n",
      "target probs tensor([[8.5508e-03],\n",
      "        [9.9969e-01],\n",
      "        [9.9948e-01],\n",
      "        [1.0237e-04],\n",
      "        [9.0814e-03],\n",
      "        [1.8980e-03],\n",
      "        [5.7341e-01],\n",
      "        [8.7410e-01]], device='cuda:0'), loss: 3.201080322265625: \n",
      "target probs tensor([[8.8595e-01],\n",
      "        [5.9257e-01],\n",
      "        [1.2630e-04],\n",
      "        [8.5735e-01],\n",
      "        [1.6732e-01],\n",
      "        [9.9744e-01],\n",
      "        [8.8609e-04],\n",
      "        [5.6564e-01],\n",
      "        [7.0638e-01],\n",
      "        [8.4339e-01],\n",
      "        [3.7078e-01],\n",
      "        [8.6782e-01],\n",
      "        [9.6007e-01],\n",
      "        [9.7832e-01],\n",
      "        [3.6042e-02],\n",
      "        [9.7638e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.5140960216522217: \n",
      "target probs tensor([[6.8071e-01],\n",
      "        [9.7618e-01],\n",
      "        [3.7202e-01],\n",
      "        [1.5248e-01],\n",
      "        [6.1475e-03],\n",
      "        [9.0088e-01],\n",
      "        [3.3181e-02],\n",
      "        [7.6255e-01],\n",
      "        [9.9314e-01],\n",
      "        [4.6495e-03],\n",
      "        [6.1956e-02],\n",
      "        [9.7770e-01],\n",
      "        [3.0392e-01],\n",
      "        [9.7892e-01],\n",
      "        [2.1825e-04],\n",
      "        [4.1399e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.9285619258880615: \n",
      "target probs tensor([[5.9910e-03],\n",
      "        [1.2451e-02],\n",
      "        [7.2875e-04],\n",
      "        [8.9696e-01],\n",
      "        [5.4679e-03],\n",
      "        [2.4113e-06],\n",
      "        [9.9929e-01],\n",
      "        [1.3113e-02],\n",
      "        [8.5916e-01],\n",
      "        [9.6326e-01],\n",
      "        [9.1929e-02],\n",
      "        [5.4648e-05],\n",
      "        [9.5205e-01],\n",
      "        [6.7010e-02],\n",
      "        [5.2576e-01],\n",
      "        [5.9457e-01]], device='cuda:0'), loss: 3.476302146911621: \n",
      "target probs tensor([[1.9587e-02],\n",
      "        [1.7704e-02],\n",
      "        [9.3296e-01],\n",
      "        [4.9815e-01],\n",
      "        [3.0726e-01],\n",
      "        [2.3044e-01],\n",
      "        [1.8585e-04],\n",
      "        [2.4769e-05],\n",
      "        [1.1059e-04],\n",
      "        [6.2135e-02],\n",
      "        [1.1898e-04],\n",
      "        [9.9149e-01],\n",
      "        [9.2517e-01],\n",
      "        [2.1653e-02],\n",
      "        [2.2466e-01],\n",
      "        [2.4593e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.076537132263184: \n",
      "target probs tensor([[0.3066],\n",
      "        [0.8607],\n",
      "        [0.0021],\n",
      "        [0.1861],\n",
      "        [0.8100],\n",
      "        [0.9918],\n",
      "        [0.0016],\n",
      "        [0.7360],\n",
      "        [0.9984],\n",
      "        [0.9735],\n",
      "        [0.2888],\n",
      "        [0.0395],\n",
      "        [0.2168],\n",
      "        [0.0095],\n",
      "        [0.6585],\n",
      "        [0.9946]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.7045220136642456: \n",
      "target probs tensor([[6.6228e-01],\n",
      "        [9.9951e-01],\n",
      "        [1.9473e-02],\n",
      "        [9.8195e-01],\n",
      "        [5.7175e-01],\n",
      "        [9.9839e-01],\n",
      "        [9.9360e-01],\n",
      "        [9.8217e-01],\n",
      "        [9.9980e-01],\n",
      "        [6.5665e-02],\n",
      "        [8.2979e-01],\n",
      "        [9.4663e-01],\n",
      "        [2.7738e-04],\n",
      "        [3.8695e-03],\n",
      "        [7.9751e-03],\n",
      "        [9.3304e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.6603068113327026: \n",
      "target probs tensor([[8.5697e-01],\n",
      "        [1.3865e-01],\n",
      "        [1.2708e-03],\n",
      "        [3.0692e-03],\n",
      "        [4.7105e-03],\n",
      "        [7.3240e-01],\n",
      "        [7.1446e-03],\n",
      "        [3.9563e-02],\n",
      "        [1.2965e-06],\n",
      "        [9.7956e-01],\n",
      "        [3.1085e-03],\n",
      "        [2.2150e-02],\n",
      "        [9.5006e-02],\n",
      "        [9.3024e-01],\n",
      "        [2.8863e-04],\n",
      "        [9.8485e-01]], device='cuda:0'), loss: 3.886058807373047: \n",
      "Better model found at epoch 81 with validation value: 0.7480000257492065.\n",
      "target probs tensor([[9.9521e-01],\n",
      "        [9.5475e-01],\n",
      "        [2.6167e-05],\n",
      "        [6.2006e-01],\n",
      "        [9.9736e-01],\n",
      "        [9.6272e-02],\n",
      "        [2.7603e-02],\n",
      "        [4.0148e-02],\n",
      "        [5.8353e-01],\n",
      "        [2.0993e-01],\n",
      "        [2.0553e-01],\n",
      "        [9.9323e-01],\n",
      "        [8.6253e-01],\n",
      "        [3.2328e-04],\n",
      "        [9.9503e-01],\n",
      "        [3.0703e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.2243828773498535: \n",
      "target probs tensor([[5.5670e-01],\n",
      "        [2.9098e-02],\n",
      "        [9.9486e-01],\n",
      "        [2.2025e-05],\n",
      "        [7.4479e-01],\n",
      "        [4.3935e-01],\n",
      "        [9.1132e-01],\n",
      "        [8.2644e-03],\n",
      "        [5.4117e-05],\n",
      "        [4.6228e-01],\n",
      "        [7.6786e-01],\n",
      "        [6.7562e-03],\n",
      "        [9.5504e-01],\n",
      "        [9.4800e-01],\n",
      "        [9.9925e-01],\n",
      "        [8.9285e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.5958216190338135: \n",
      "target probs tensor([[3.3466e-01],\n",
      "        [2.0506e-04],\n",
      "        [3.9591e-02],\n",
      "        [3.2294e-02],\n",
      "        [9.7491e-01],\n",
      "        [9.9198e-01],\n",
      "        [7.3289e-01],\n",
      "        [1.3433e-03],\n",
      "        [2.2971e-03],\n",
      "        [3.3561e-01],\n",
      "        [8.4703e-01],\n",
      "        [7.4563e-01],\n",
      "        [9.4130e-01],\n",
      "        [6.6415e-04],\n",
      "        [1.4488e-05],\n",
      "        [2.1919e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.467240571975708: \n",
      "Better model found at epoch 82 with validation value: 0.7540000081062317.\n",
      "target probs tensor([[8.6129e-03],\n",
      "        [3.5238e-02],\n",
      "        [5.4796e-01],\n",
      "        [4.6210e-01],\n",
      "        [1.0100e-02],\n",
      "        [2.6453e-01],\n",
      "        [8.2021e-01],\n",
      "        [1.7644e-04],\n",
      "        [1.4477e-02],\n",
      "        [9.3982e-01],\n",
      "        [9.4652e-01],\n",
      "        [9.2015e-03],\n",
      "        [3.2998e-01],\n",
      "        [4.9060e-01],\n",
      "        [6.8430e-02],\n",
      "        [7.1781e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.382145404815674: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[9.8133e-01],\n",
      "        [2.0625e-06],\n",
      "        [5.4769e-03],\n",
      "        [9.5303e-01],\n",
      "        [6.0471e-01],\n",
      "        [2.6341e-04],\n",
      "        [3.8540e-01],\n",
      "        [7.3675e-01],\n",
      "        [9.9703e-01],\n",
      "        [9.9425e-01],\n",
      "        [5.0084e-02],\n",
      "        [8.7617e-01],\n",
      "        [2.0454e-04],\n",
      "        [9.4967e-01],\n",
      "        [4.4974e-04],\n",
      "        [9.4688e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.9882688522338867: \n",
      "target probs tensor([[9.5442e-01],\n",
      "        [7.9322e-01],\n",
      "        [2.5672e-04],\n",
      "        [9.2725e-01],\n",
      "        [8.8584e-02],\n",
      "        [8.2362e-03],\n",
      "        [9.0408e-01],\n",
      "        [1.2235e-03],\n",
      "        [9.0907e-04],\n",
      "        [5.9005e-04],\n",
      "        [1.4464e-03],\n",
      "        [2.4656e-01],\n",
      "        [9.4513e-01],\n",
      "        [5.4321e-03],\n",
      "        [4.6201e-04],\n",
      "        [3.7894e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.684414863586426: \n",
      "target probs tensor([[4.8371e-05],\n",
      "        [5.8434e-03],\n",
      "        [4.8793e-03],\n",
      "        [1.3589e-01],\n",
      "        [9.4408e-01],\n",
      "        [1.3215e-02],\n",
      "        [8.1188e-02],\n",
      "        [9.1447e-01],\n",
      "        [4.8489e-04],\n",
      "        [2.3924e-02],\n",
      "        [6.7182e-03],\n",
      "        [2.9378e-02],\n",
      "        [6.0223e-01],\n",
      "        [9.5527e-01],\n",
      "        [5.3800e-02],\n",
      "        [1.1476e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.5762393474578857: \n",
      "target probs tensor([[2.9330e-03],\n",
      "        [3.3738e-05],\n",
      "        [9.6209e-01],\n",
      "        [9.0934e-04],\n",
      "        [6.8199e-01],\n",
      "        [2.4513e-02],\n",
      "        [4.8200e-06],\n",
      "        [6.7079e-02],\n",
      "        [5.9568e-01],\n",
      "        [1.2336e-02],\n",
      "        [1.3891e-02],\n",
      "        [1.9086e-03],\n",
      "        [3.5334e-03],\n",
      "        [8.0477e-04],\n",
      "        [8.4325e-01],\n",
      "        [2.5068e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.786682605743408: \n",
      "target probs tensor([[9.8637e-01],\n",
      "        [4.4335e-04],\n",
      "        [7.2604e-01],\n",
      "        [1.1842e-03],\n",
      "        [1.0611e-04],\n",
      "        [1.3864e-04],\n",
      "        [7.2927e-01],\n",
      "        [3.1918e-02],\n",
      "        [4.7017e-02],\n",
      "        [8.9358e-01],\n",
      "        [6.9339e-01],\n",
      "        [1.1578e-01],\n",
      "        [9.6756e-01],\n",
      "        [1.2980e-03],\n",
      "        [8.8949e-01],\n",
      "        [6.5314e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.5257136821746826: \n",
      "target probs tensor([[3.0729e-01],\n",
      "        [9.8412e-01],\n",
      "        [1.3063e-01],\n",
      "        [8.3047e-01],\n",
      "        [9.9668e-01],\n",
      "        [9.1528e-01],\n",
      "        [4.0593e-02],\n",
      "        [9.9504e-01],\n",
      "        [8.2213e-04],\n",
      "        [7.7347e-01],\n",
      "        [9.6185e-01],\n",
      "        [9.9920e-01],\n",
      "        [6.5546e-01],\n",
      "        [1.7542e-01],\n",
      "        [4.2414e-04],\n",
      "        [2.9025e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.5802348852157593: \n",
      "target probs tensor([[4.2014e-01],\n",
      "        [3.4653e-02],\n",
      "        [5.9521e-01],\n",
      "        [8.4706e-06],\n",
      "        [8.8239e-03],\n",
      "        [8.5602e-01],\n",
      "        [9.5681e-03],\n",
      "        [2.4215e-02],\n",
      "        [7.0491e-01],\n",
      "        [8.4815e-01],\n",
      "        [8.0183e-01],\n",
      "        [2.7475e-03],\n",
      "        [9.9869e-01],\n",
      "        [3.0906e-01],\n",
      "        [4.0071e-05],\n",
      "        [8.0914e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.1331369876861572: \n",
      "target probs tensor([[9.9960e-01],\n",
      "        [1.3468e-03],\n",
      "        [1.8319e-01],\n",
      "        [1.0450e-01],\n",
      "        [9.9585e-01],\n",
      "        [6.6221e-03],\n",
      "        [4.8396e-01],\n",
      "        [3.6329e-01],\n",
      "        [3.2036e-01],\n",
      "        [7.2243e-05],\n",
      "        [5.8887e-06],\n",
      "        [9.9951e-01],\n",
      "        [5.2291e-03],\n",
      "        [3.2889e-01],\n",
      "        [1.1221e-03],\n",
      "        [1.5180e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.442878484725952: \n",
      "target probs tensor([[8.9548e-01],\n",
      "        [2.8983e-04],\n",
      "        [5.6926e-02],\n",
      "        [9.0641e-06],\n",
      "        [9.7847e-01],\n",
      "        [1.6120e-03],\n",
      "        [2.7718e-04],\n",
      "        [9.0146e-01],\n",
      "        [9.9505e-01],\n",
      "        [3.5729e-02],\n",
      "        [8.5657e-06],\n",
      "        [1.2571e-03],\n",
      "        [3.5246e-02],\n",
      "        [9.4078e-01],\n",
      "        [3.8582e-02],\n",
      "        [7.9967e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.7036967277526855: \n",
      "target probs tensor([[7.0986e-06],\n",
      "        [9.5671e-01],\n",
      "        [7.0649e-01],\n",
      "        [6.4925e-01],\n",
      "        [9.4436e-04],\n",
      "        [6.6946e-01],\n",
      "        [1.4126e-01],\n",
      "        [9.6006e-01],\n",
      "        [1.1617e-01],\n",
      "        [8.3024e-01],\n",
      "        [7.0293e-01],\n",
      "        [2.8913e-04],\n",
      "        [4.1325e-06],\n",
      "        [9.2871e-01],\n",
      "        [7.3976e-01],\n",
      "        [1.7026e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.108022689819336: \n",
      "target probs tensor([[1.5099e-03],\n",
      "        [2.1478e-03],\n",
      "        [2.8720e-02],\n",
      "        [1.5494e-02],\n",
      "        [3.5681e-01],\n",
      "        [1.4110e-04],\n",
      "        [5.4295e-05],\n",
      "        [2.6501e-02],\n",
      "        [3.4381e-02],\n",
      "        [4.2445e-01],\n",
      "        [3.6050e-01],\n",
      "        [9.6178e-01],\n",
      "        [1.4942e-01],\n",
      "        [3.9678e-01],\n",
      "        [3.3205e-05],\n",
      "        [9.6076e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.8855881690979004: \n",
      "target probs tensor([[9.9911e-01],\n",
      "        [4.6908e-04],\n",
      "        [4.2246e-01],\n",
      "        [8.8067e-01],\n",
      "        [1.7404e-02],\n",
      "        [1.5273e-05],\n",
      "        [1.1826e-02],\n",
      "        [9.7449e-02],\n",
      "        [3.3044e-02],\n",
      "        [9.3955e-05],\n",
      "        [5.9138e-01],\n",
      "        [3.6778e-02],\n",
      "        [1.3388e-02],\n",
      "        [1.7678e-01],\n",
      "        [5.7830e-01],\n",
      "        [1.6739e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.4658002853393555: \n",
      "target probs tensor([[9.2836e-01],\n",
      "        [7.6564e-01],\n",
      "        [9.6085e-01],\n",
      "        [1.0446e-01],\n",
      "        [7.0108e-02],\n",
      "        [7.4447e-03],\n",
      "        [2.6666e-01],\n",
      "        [9.7595e-01],\n",
      "        [7.6488e-06],\n",
      "        [1.8137e-01],\n",
      "        [2.7410e-03],\n",
      "        [9.9832e-01],\n",
      "        [9.3003e-01],\n",
      "        [9.5696e-01],\n",
      "        [1.1653e-03],\n",
      "        [2.4902e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.7375221252441406: \n",
      "target probs tensor([[9.1182e-01],\n",
      "        [8.2659e-01],\n",
      "        [1.3748e-01],\n",
      "        [7.6846e-01],\n",
      "        [8.7088e-01],\n",
      "        [9.5366e-02],\n",
      "        [9.6256e-01],\n",
      "        [1.4970e-04],\n",
      "        [7.9593e-04],\n",
      "        [8.2940e-01],\n",
      "        [9.9748e-01],\n",
      "        [8.3876e-01],\n",
      "        [2.7929e-05],\n",
      "        [1.9105e-02],\n",
      "        [2.7601e-03],\n",
      "        [8.3090e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.617908000946045: \n",
      "target probs tensor([[1.0913e-02],\n",
      "        [9.9983e-01],\n",
      "        [9.8082e-01],\n",
      "        [2.9986e-04],\n",
      "        [2.5980e-02],\n",
      "        [3.1039e-02],\n",
      "        [5.7538e-01],\n",
      "        [8.7816e-01]], device='cuda:0'), loss: 2.5568881034851074: \n",
      "target probs tensor([[1.7545e-02],\n",
      "        [9.2784e-02],\n",
      "        [2.5008e-01],\n",
      "        [6.2520e-01],\n",
      "        [7.4203e-01],\n",
      "        [3.3045e-01],\n",
      "        [9.4329e-02],\n",
      "        [3.4792e-01],\n",
      "        [5.4939e-05],\n",
      "        [1.5479e-01],\n",
      "        [1.7113e-03],\n",
      "        [9.9248e-01],\n",
      "        [2.9248e-01],\n",
      "        [8.4236e-01],\n",
      "        [4.7259e-04],\n",
      "        [6.5462e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.5395889282226562: \n",
      "target probs tensor([[1.2458e-01],\n",
      "        [9.5247e-01],\n",
      "        [8.3691e-01],\n",
      "        [1.7151e-03],\n",
      "        [1.9846e-05],\n",
      "        [9.5878e-01],\n",
      "        [7.1234e-01],\n",
      "        [2.7503e-01],\n",
      "        [1.3171e-01],\n",
      "        [5.7841e-03],\n",
      "        [1.0019e-03],\n",
      "        [7.4599e-01],\n",
      "        [6.4964e-04],\n",
      "        [9.8239e-01],\n",
      "        [9.9098e-01],\n",
      "        [4.9630e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.014244556427002: \n",
      "target probs tensor([[8.8331e-01],\n",
      "        [1.6163e-02],\n",
      "        [1.6993e-03],\n",
      "        [8.6572e-01],\n",
      "        [3.7192e-04],\n",
      "        [4.0277e-04],\n",
      "        [9.8089e-01],\n",
      "        [6.0286e-03],\n",
      "        [9.4081e-01],\n",
      "        [9.6979e-01],\n",
      "        [1.1375e-01],\n",
      "        [1.9235e-03],\n",
      "        [9.9239e-01],\n",
      "        [3.7957e-01],\n",
      "        [9.9077e-01],\n",
      "        [6.2994e-01]], device='cuda:0'), loss: 2.5988850593566895: \n",
      "target probs tensor([[6.9638e-01],\n",
      "        [3.7559e-01],\n",
      "        [9.3628e-06],\n",
      "        [2.8451e-06],\n",
      "        [1.7659e-02],\n",
      "        [5.4429e-01],\n",
      "        [2.0516e-04],\n",
      "        [3.7138e-04],\n",
      "        [4.3055e-03],\n",
      "        [6.1109e-01],\n",
      "        [7.2781e-01],\n",
      "        [9.3523e-01],\n",
      "        [9.8350e-07],\n",
      "        [2.2753e-06],\n",
      "        [8.8839e-01],\n",
      "        [4.2560e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 5.052973747253418: \n",
      "target probs tensor([[9.0840e-01],\n",
      "        [9.1484e-01],\n",
      "        [8.2506e-01],\n",
      "        [7.9640e-01],\n",
      "        [9.8606e-01],\n",
      "        [6.1768e-04],\n",
      "        [9.4354e-01],\n",
      "        [1.7329e-03],\n",
      "        [9.9992e-01],\n",
      "        [3.2824e-06],\n",
      "        [9.9549e-01],\n",
      "        [2.1780e-04],\n",
      "        [3.5524e-02],\n",
      "        [5.9441e-03],\n",
      "        [7.2694e-02],\n",
      "        [1.7544e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.16347074508667: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[8.4415e-01],\n",
      "        [6.4052e-01],\n",
      "        [1.9035e-04],\n",
      "        [1.2648e-02],\n",
      "        [1.3865e-01],\n",
      "        [1.4037e-02],\n",
      "        [3.5111e-01],\n",
      "        [4.6901e-02],\n",
      "        [1.9804e-06],\n",
      "        [8.6200e-01],\n",
      "        [3.3427e-03],\n",
      "        [2.9376e-03],\n",
      "        [9.2464e-01],\n",
      "        [8.9921e-01],\n",
      "        [2.3789e-04],\n",
      "        [9.9318e-01]], device='cuda:0'), loss: 3.5779354572296143: \n",
      "target probs tensor([[2.8303e-03],\n",
      "        [2.6518e-01],\n",
      "        [9.0354e-01],\n",
      "        [7.7573e-06],\n",
      "        [3.9031e-03],\n",
      "        [7.4888e-04],\n",
      "        [9.6362e-05],\n",
      "        [2.2191e-01],\n",
      "        [2.8454e-03],\n",
      "        [1.0729e-03],\n",
      "        [2.2124e-04],\n",
      "        [3.0005e-02],\n",
      "        [9.4659e-01],\n",
      "        [2.6534e-01],\n",
      "        [2.1875e-01],\n",
      "        [9.5288e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.670988082885742: \n",
      "target probs tensor([[9.8906e-01],\n",
      "        [6.8556e-03],\n",
      "        [5.3601e-02],\n",
      "        [4.5756e-01],\n",
      "        [7.7978e-01],\n",
      "        [2.0044e-04],\n",
      "        [1.0466e-04],\n",
      "        [6.4982e-01],\n",
      "        [9.4641e-01],\n",
      "        [9.5030e-01],\n",
      "        [2.3276e-02],\n",
      "        [7.1414e-01],\n",
      "        [1.0506e-01],\n",
      "        [3.2092e-03],\n",
      "        [1.1546e-03],\n",
      "        [6.2554e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.049691915512085: \n",
      "target probs tensor([[7.9441e-05],\n",
      "        [4.0314e-01],\n",
      "        [7.1587e-03],\n",
      "        [2.0834e-02],\n",
      "        [8.0603e-01],\n",
      "        [9.9486e-01],\n",
      "        [9.6196e-01],\n",
      "        [1.7303e-02],\n",
      "        [2.7907e-01],\n",
      "        [7.4054e-01],\n",
      "        [8.5729e-01],\n",
      "        [8.7602e-01],\n",
      "        [2.4756e-03],\n",
      "        [1.2207e-04],\n",
      "        [1.1248e-01],\n",
      "        [2.6139e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.7423691749572754: \n",
      "target probs tensor([[9.0719e-01],\n",
      "        [6.8587e-04],\n",
      "        [5.2359e-03],\n",
      "        [8.7326e-01],\n",
      "        [1.3560e-01],\n",
      "        [9.9215e-01],\n",
      "        [9.8553e-01],\n",
      "        [9.5557e-01],\n",
      "        [6.5944e-01],\n",
      "        [7.4365e-01],\n",
      "        [4.1319e-04],\n",
      "        [7.2240e-05],\n",
      "        [2.2351e-04],\n",
      "        [4.8036e-01],\n",
      "        [4.4020e-01],\n",
      "        [8.2425e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.6892926692962646: \n",
      "target probs tensor([[3.5353e-01],\n",
      "        [9.9767e-01],\n",
      "        [9.9680e-01],\n",
      "        [2.7376e-02],\n",
      "        [7.7092e-01],\n",
      "        [5.8953e-02],\n",
      "        [5.2121e-01],\n",
      "        [3.3124e-02],\n",
      "        [7.7225e-01],\n",
      "        [9.8820e-01],\n",
      "        [5.6364e-01],\n",
      "        [2.7259e-03],\n",
      "        [9.9428e-01],\n",
      "        [4.6079e-06],\n",
      "        [9.5672e-01],\n",
      "        [2.3187e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.16525936126709: \n",
      "target probs tensor([[4.6181e-02],\n",
      "        [5.8406e-04],\n",
      "        [4.9310e-01],\n",
      "        [5.7466e-01],\n",
      "        [5.1293e-03],\n",
      "        [7.6879e-01],\n",
      "        [8.1884e-01],\n",
      "        [3.7247e-01],\n",
      "        [9.8059e-01],\n",
      "        [1.0123e-01],\n",
      "        [9.7376e-01],\n",
      "        [9.2570e-01],\n",
      "        [8.2741e-01],\n",
      "        [4.3536e-01],\n",
      "        [9.5501e-03],\n",
      "        [9.2620e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.6667214632034302: \n",
      "target probs tensor([[3.4929e-03],\n",
      "        [9.9498e-01],\n",
      "        [2.9698e-05],\n",
      "        [1.7693e-01],\n",
      "        [2.3716e-03],\n",
      "        [6.5039e-01],\n",
      "        [4.4634e-02],\n",
      "        [2.0090e-02],\n",
      "        [4.5063e-01],\n",
      "        [5.2165e-04],\n",
      "        [1.9910e-04],\n",
      "        [9.2825e-01],\n",
      "        [9.6194e-01],\n",
      "        [8.8816e-01],\n",
      "        [6.9742e-01],\n",
      "        [1.4897e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.1677005290985107: \n",
      "target probs tensor([[8.1011e-01],\n",
      "        [9.9901e-01],\n",
      "        [9.0013e-01],\n",
      "        [6.8419e-01],\n",
      "        [3.2154e-01],\n",
      "        [8.9880e-01],\n",
      "        [8.7446e-01],\n",
      "        [8.0954e-01],\n",
      "        [8.9542e-01],\n",
      "        [9.2811e-01],\n",
      "        [2.0889e-04],\n",
      "        [8.5358e-01],\n",
      "        [5.3982e-01],\n",
      "        [9.5987e-01],\n",
      "        [7.7291e-07],\n",
      "        [9.9970e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.614434003829956: \n",
      "target probs tensor([[9.8714e-01],\n",
      "        [1.7113e-01],\n",
      "        [5.9705e-05],\n",
      "        [9.8474e-01],\n",
      "        [1.6933e-01],\n",
      "        [9.6036e-01],\n",
      "        [5.2594e-02],\n",
      "        [9.7687e-01],\n",
      "        [3.7054e-01],\n",
      "        [9.5466e-05],\n",
      "        [1.5973e-01],\n",
      "        [1.6132e-02],\n",
      "        [1.3664e-05],\n",
      "        [3.3068e-02],\n",
      "        [9.9663e-01],\n",
      "        [4.0608e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.0018677711486816: \n",
      "target probs tensor([[5.7329e-05],\n",
      "        [6.6641e-01],\n",
      "        [9.6254e-02],\n",
      "        [1.4802e-01],\n",
      "        [4.6806e-01],\n",
      "        [5.4070e-02],\n",
      "        [2.2580e-02],\n",
      "        [1.0490e-04],\n",
      "        [3.4135e-03],\n",
      "        [9.9492e-01],\n",
      "        [9.7432e-01],\n",
      "        [2.2422e-01],\n",
      "        [2.9793e-02],\n",
      "        [9.0417e-01],\n",
      "        [6.1444e-03],\n",
      "        [9.9101e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.935955762863159: \n",
      "target probs tensor([[5.6156e-01],\n",
      "        [2.8260e-01],\n",
      "        [9.6182e-01],\n",
      "        [2.7160e-03],\n",
      "        [9.4720e-01],\n",
      "        [6.3687e-01],\n",
      "        [1.1947e-04],\n",
      "        [1.5839e-04],\n",
      "        [9.8978e-01],\n",
      "        [3.7407e-02],\n",
      "        [9.5862e-01],\n",
      "        [6.9023e-01],\n",
      "        [9.9714e-01],\n",
      "        [7.7792e-01],\n",
      "        [9.9139e-01],\n",
      "        [5.5686e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.634169578552246: \n",
      "target probs tensor([[9.2998e-01],\n",
      "        [7.8953e-05],\n",
      "        [6.8894e-01],\n",
      "        [3.2305e-05],\n",
      "        [3.2750e-04],\n",
      "        [4.6968e-03],\n",
      "        [9.5141e-01],\n",
      "        [9.5016e-01],\n",
      "        [6.0383e-01],\n",
      "        [3.9755e-04],\n",
      "        [9.4650e-01],\n",
      "        [7.2380e-01],\n",
      "        [2.9021e-02],\n",
      "        [8.9836e-01],\n",
      "        [3.8444e-02],\n",
      "        [6.8902e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.2507119178771973: \n",
      "target probs tensor([[0.3086],\n",
      "        [0.9857],\n",
      "        [0.2123],\n",
      "        [0.7776],\n",
      "        [0.8636],\n",
      "        [0.9929],\n",
      "        [0.8230],\n",
      "        [0.0031],\n",
      "        [0.0035],\n",
      "        [0.9093],\n",
      "        [0.1265],\n",
      "        [0.9130],\n",
      "        [0.0718],\n",
      "        [0.0064],\n",
      "        [0.9935],\n",
      "        [0.1543]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.661230444908142: \n",
      "target probs tensor([[0.7103],\n",
      "        [0.9004],\n",
      "        [0.9569],\n",
      "        [0.9451],\n",
      "        [0.9429],\n",
      "        [0.8477],\n",
      "        [0.4977],\n",
      "        [0.0033],\n",
      "        [0.0288],\n",
      "        [0.0874],\n",
      "        [0.9984],\n",
      "        [0.9965],\n",
      "        [0.0088],\n",
      "        [0.0097],\n",
      "        [0.9714],\n",
      "        [0.0266]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.6368632316589355: \n",
      "target probs tensor([[2.2408e-03],\n",
      "        [1.1662e-01],\n",
      "        [5.7160e-02],\n",
      "        [6.0528e-01],\n",
      "        [2.2724e-04],\n",
      "        [6.2831e-05],\n",
      "        [8.0718e-01],\n",
      "        [3.7049e-04],\n",
      "        [1.3186e-02],\n",
      "        [2.4000e-02],\n",
      "        [5.0554e-02],\n",
      "        [7.0924e-01],\n",
      "        [9.6885e-01],\n",
      "        [1.4994e-04],\n",
      "        [6.5543e-01],\n",
      "        [6.1869e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.6824567317962646: \n",
      "target probs tensor([[9.9472e-01],\n",
      "        [8.8188e-01],\n",
      "        [5.3635e-03],\n",
      "        [8.4592e-01],\n",
      "        [2.8436e-02],\n",
      "        [8.2628e-01],\n",
      "        [4.5889e-01],\n",
      "        [9.9150e-01],\n",
      "        [3.0113e-01],\n",
      "        [6.0164e-01],\n",
      "        [5.3381e-05],\n",
      "        [6.3493e-03],\n",
      "        [1.2679e-02],\n",
      "        [1.2627e-01],\n",
      "        [2.0596e-02],\n",
      "        [5.1404e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.6413145065307617: \n",
      "target probs tensor([[8.5975e-04],\n",
      "        [9.7292e-01],\n",
      "        [8.9793e-01],\n",
      "        [7.6505e-01],\n",
      "        [2.9361e-05],\n",
      "        [1.0379e-01],\n",
      "        [1.2294e-04],\n",
      "        [2.3645e-01],\n",
      "        [1.6980e-01],\n",
      "        [8.7566e-05],\n",
      "        [9.2366e-01],\n",
      "        [4.9714e-06],\n",
      "        [8.5069e-01],\n",
      "        [8.7379e-01],\n",
      "        [9.2699e-02],\n",
      "        [1.0061e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.2623820304870605: \n",
      "target probs tensor([[8.9847e-01],\n",
      "        [8.7174e-04],\n",
      "        [9.4638e-01],\n",
      "        [9.9568e-01],\n",
      "        [6.1357e-01],\n",
      "        [1.1336e-01],\n",
      "        [9.9942e-01],\n",
      "        [2.6196e-04],\n",
      "        [1.1214e-02],\n",
      "        [1.2553e-02],\n",
      "        [9.8395e-01],\n",
      "        [2.6318e-04],\n",
      "        [9.9355e-01],\n",
      "        [4.3767e-05],\n",
      "        [3.2903e-04],\n",
      "        [2.4999e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.9944710731506348: \n",
      "target probs tensor([[1.8093e-02],\n",
      "        [9.6746e-01],\n",
      "        [3.9885e-03],\n",
      "        [7.8770e-01],\n",
      "        [8.8076e-06],\n",
      "        [1.6296e-03],\n",
      "        [3.2185e-05],\n",
      "        [3.6619e-02],\n",
      "        [8.3391e-01],\n",
      "        [1.5677e-02],\n",
      "        [3.4043e-01],\n",
      "        [6.9161e-01],\n",
      "        [9.6019e-01],\n",
      "        [7.2985e-01],\n",
      "        [7.1275e-01],\n",
      "        [1.1063e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.1373770236968994: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[8.1837e-01],\n",
      "        [1.4760e-03],\n",
      "        [9.9640e-01],\n",
      "        [4.0361e-02],\n",
      "        [6.8550e-01],\n",
      "        [2.7827e-01],\n",
      "        [8.9345e-01],\n",
      "        [9.8923e-01],\n",
      "        [7.2090e-01],\n",
      "        [6.9953e-03],\n",
      "        [9.4116e-01],\n",
      "        [1.8266e-04],\n",
      "        [8.6767e-01],\n",
      "        [9.9491e-01],\n",
      "        [1.8023e-02],\n",
      "        [8.9892e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.1591105461120605: \n",
      "target probs tensor([[7.2659e-04],\n",
      "        [9.3696e-01],\n",
      "        [2.1025e-01],\n",
      "        [7.8259e-03],\n",
      "        [7.3092e-01],\n",
      "        [3.6620e-03],\n",
      "        [8.7291e-01],\n",
      "        [8.7185e-01],\n",
      "        [8.3833e-01],\n",
      "        [5.7761e-03],\n",
      "        [3.2255e-01],\n",
      "        [9.9544e-01],\n",
      "        [8.3731e-02],\n",
      "        [9.9566e-01],\n",
      "        [3.2628e-05],\n",
      "        [9.0843e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.454726219177246: \n",
      "target probs tensor([[9.8329e-01],\n",
      "        [9.8798e-01],\n",
      "        [8.2676e-07],\n",
      "        [1.8602e-03],\n",
      "        [6.8822e-01],\n",
      "        [9.9112e-01],\n",
      "        [7.3822e-01],\n",
      "        [9.8382e-01],\n",
      "        [3.3964e-03],\n",
      "        [6.1760e-04],\n",
      "        [9.5137e-01],\n",
      "        [5.1806e-03],\n",
      "        [2.5461e-04],\n",
      "        [1.0792e-03],\n",
      "        [9.7471e-04],\n",
      "        [7.9653e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.8549764156341553: \n",
      "target probs tensor([[9.5306e-01],\n",
      "        [6.3988e-04],\n",
      "        [9.5957e-01],\n",
      "        [8.9304e-01],\n",
      "        [3.0663e-04],\n",
      "        [9.6046e-01],\n",
      "        [3.1236e-05],\n",
      "        [9.9931e-01],\n",
      "        [9.7901e-01],\n",
      "        [1.3964e-01],\n",
      "        [2.6818e-01],\n",
      "        [7.4355e-04],\n",
      "        [7.4389e-03],\n",
      "        [1.3257e-04],\n",
      "        [4.9843e-01],\n",
      "        [9.5220e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.1966423988342285: \n",
      "target probs tensor([[5.6095e-01],\n",
      "        [9.8750e-01],\n",
      "        [9.9712e-01],\n",
      "        [4.2370e-04],\n",
      "        [1.0848e-01],\n",
      "        [2.4717e-03],\n",
      "        [8.6176e-01],\n",
      "        [9.1163e-01]], device='cuda:0'), loss: 2.103181838989258: \n",
      "Better model found at epoch 199 with validation value: 0.7760000228881836.\n",
      "target probs tensor([[9.9395e-01],\n",
      "        [3.0996e-03],\n",
      "        [4.4853e-01],\n",
      "        [8.9312e-01],\n",
      "        [9.8785e-01],\n",
      "        [9.0890e-01],\n",
      "        [5.5071e-01],\n",
      "        [3.5124e-01],\n",
      "        [9.7147e-01],\n",
      "        [9.7925e-01],\n",
      "        [1.2197e-01],\n",
      "        [8.7347e-01],\n",
      "        [6.1613e-01],\n",
      "        [1.5637e-04],\n",
      "        [1.1359e-03],\n",
      "        [2.5782e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.0453572273254395: \n",
      "target probs tensor([[3.3911e-02],\n",
      "        [2.3572e-02],\n",
      "        [5.5080e-01],\n",
      "        [7.1135e-03],\n",
      "        [9.9879e-01],\n",
      "        [1.7869e-02],\n",
      "        [7.4405e-01],\n",
      "        [1.0330e-02],\n",
      "        [1.2263e-01],\n",
      "        [9.9907e-01],\n",
      "        [1.6808e-02],\n",
      "        [5.9491e-01],\n",
      "        [4.9844e-02],\n",
      "        [9.5201e-01],\n",
      "        [5.9700e-04],\n",
      "        [8.7116e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.430152416229248: \n",
      "target probs tensor([[4.5054e-01],\n",
      "        [1.7092e-02],\n",
      "        [2.5003e-04],\n",
      "        [8.0692e-01],\n",
      "        [3.8911e-03],\n",
      "        [9.4695e-04],\n",
      "        [9.9877e-01],\n",
      "        [3.4719e-03],\n",
      "        [9.5298e-01],\n",
      "        [9.8417e-01],\n",
      "        [9.0608e-03],\n",
      "        [2.4120e-02],\n",
      "        [9.9887e-01],\n",
      "        [9.5403e-01],\n",
      "        [9.4810e-01],\n",
      "        [8.8855e-02]], device='cuda:0'), loss: 2.6603336334228516: \n",
      "target probs tensor([[1.9201e-03],\n",
      "        [4.5397e-02],\n",
      "        [9.4909e-01],\n",
      "        [9.9861e-01],\n",
      "        [6.4871e-01],\n",
      "        [2.1588e-02],\n",
      "        [7.0453e-01],\n",
      "        [6.9200e-01],\n",
      "        [9.9616e-01],\n",
      "        [2.4823e-01],\n",
      "        [9.2800e-01],\n",
      "        [6.8884e-01],\n",
      "        [3.3496e-04],\n",
      "        [9.9992e-01],\n",
      "        [9.0727e-05],\n",
      "        [7.8474e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.111527681350708: \n",
      "target probs tensor([[2.8537e-04],\n",
      "        [8.4783e-02],\n",
      "        [1.6785e-03],\n",
      "        [7.8015e-01],\n",
      "        [6.3267e-01],\n",
      "        [1.4776e-05],\n",
      "        [1.3279e-01],\n",
      "        [1.3404e-03],\n",
      "        [8.5036e-01],\n",
      "        [4.0875e-01],\n",
      "        [9.8827e-01],\n",
      "        [5.8634e-02],\n",
      "        [2.6295e-02],\n",
      "        [1.9941e-01],\n",
      "        [4.7318e-05],\n",
      "        [6.1423e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.9994406700134277: \n",
      "target probs tensor([[9.1817e-01],\n",
      "        [3.8069e-01],\n",
      "        [4.2206e-01],\n",
      "        [2.3151e-02],\n",
      "        [2.0435e-02],\n",
      "        [6.9654e-01],\n",
      "        [8.6372e-01],\n",
      "        [1.0971e-01],\n",
      "        [1.2277e-06],\n",
      "        [9.5828e-01],\n",
      "        [2.0854e-03],\n",
      "        [3.6214e-01],\n",
      "        [3.5017e-01],\n",
      "        [9.0632e-01],\n",
      "        [4.5582e-05],\n",
      "        [9.9734e-01]], device='cuda:0'), loss: 2.7672412395477295: \n",
      "target probs tensor([[4.2341e-04],\n",
      "        [1.6817e-01],\n",
      "        [2.1298e-02],\n",
      "        [2.0657e-02],\n",
      "        [7.1030e-04],\n",
      "        [9.5485e-01],\n",
      "        [6.8319e-01],\n",
      "        [9.8745e-01],\n",
      "        [9.3104e-01],\n",
      "        [7.1119e-01],\n",
      "        [6.5903e-04],\n",
      "        [9.9816e-01],\n",
      "        [6.2639e-01],\n",
      "        [9.5404e-01],\n",
      "        [9.9888e-01],\n",
      "        [9.9528e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.0767481327056885: \n",
      "target probs tensor([[8.9502e-06],\n",
      "        [9.9498e-01],\n",
      "        [9.9290e-01],\n",
      "        [7.5648e-01],\n",
      "        [8.7316e-01],\n",
      "        [1.3779e-01],\n",
      "        [7.0135e-03],\n",
      "        [3.7587e-02],\n",
      "        [6.5320e-03],\n",
      "        [4.7338e-05],\n",
      "        [1.2262e-02],\n",
      "        [8.3601e-01],\n",
      "        [4.4375e-01],\n",
      "        [6.6974e-03],\n",
      "        [2.3692e-01],\n",
      "        [8.9328e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.0759215354919434: \n",
      "target probs tensor([[2.0159e-02],\n",
      "        [1.5819e-06],\n",
      "        [3.9611e-01],\n",
      "        [4.6493e-01],\n",
      "        [1.1962e-01],\n",
      "        [8.4267e-03],\n",
      "        [3.4246e-04],\n",
      "        [3.3588e-04],\n",
      "        [9.3361e-06],\n",
      "        [7.7590e-01],\n",
      "        [9.9015e-01],\n",
      "        [9.5652e-01],\n",
      "        [3.3924e-01],\n",
      "        [4.6687e-03],\n",
      "        [4.7793e-03],\n",
      "        [7.6085e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.255481719970703: \n",
      "target probs tensor([[8.2958e-01],\n",
      "        [9.0918e-01],\n",
      "        [3.7060e-02],\n",
      "        [3.0364e-02],\n",
      "        [2.1188e-01],\n",
      "        [2.5348e-07],\n",
      "        [5.1609e-04],\n",
      "        [4.9393e-03],\n",
      "        [6.1659e-03],\n",
      "        [8.8107e-02],\n",
      "        [9.0462e-01],\n",
      "        [2.1482e-04],\n",
      "        [9.4839e-01],\n",
      "        [2.8382e-04],\n",
      "        [4.1097e-02],\n",
      "        [5.2288e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.050943374633789: \n",
      "target probs tensor([[7.2460e-02],\n",
      "        [9.9545e-01],\n",
      "        [8.1995e-01],\n",
      "        [4.7108e-01],\n",
      "        [2.8178e-02],\n",
      "        [7.3288e-01],\n",
      "        [9.9980e-01],\n",
      "        [3.0448e-05],\n",
      "        [9.2279e-07],\n",
      "        [9.9690e-01],\n",
      "        [5.3814e-02],\n",
      "        [9.1183e-01],\n",
      "        [4.7319e-04],\n",
      "        [6.4408e-01],\n",
      "        [5.7909e-01],\n",
      "        [5.3744e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.184030294418335: \n",
      "target probs tensor([[4.2026e-02],\n",
      "        [8.8572e-01],\n",
      "        [9.8720e-01],\n",
      "        [9.2649e-01],\n",
      "        [2.6139e-02],\n",
      "        [9.3236e-03],\n",
      "        [6.3305e-01],\n",
      "        [7.6575e-04],\n",
      "        [1.6785e-02],\n",
      "        [8.5758e-05],\n",
      "        [9.9903e-01],\n",
      "        [9.9357e-01],\n",
      "        [4.5853e-01],\n",
      "        [1.2743e-02],\n",
      "        [1.1596e-01],\n",
      "        [9.9481e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.5057716369628906: \n",
      "target probs tensor([[1.2166e-02],\n",
      "        [4.8166e-01],\n",
      "        [9.9287e-01],\n",
      "        [5.1427e-01],\n",
      "        [6.1088e-01],\n",
      "        [9.2215e-01],\n",
      "        [3.8561e-01],\n",
      "        [4.6460e-01],\n",
      "        [1.8320e-04],\n",
      "        [1.3609e-04],\n",
      "        [3.8057e-01],\n",
      "        [5.3485e-02],\n",
      "        [9.4686e-02],\n",
      "        [2.0067e-04],\n",
      "        [4.0706e-04],\n",
      "        [1.9326e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.6899003982543945: \n",
      "target probs tensor([[0.1789],\n",
      "        [0.9335],\n",
      "        [0.9835],\n",
      "        [0.9788],\n",
      "        [0.9405],\n",
      "        [0.9457],\n",
      "        [1.0000],\n",
      "        [0.9978],\n",
      "        [0.7076],\n",
      "        [0.0013],\n",
      "        [0.9726],\n",
      "        [0.0488],\n",
      "        [0.9952],\n",
      "        [0.0889],\n",
      "        [0.1023],\n",
      "        [0.0056]], device='cuda:0', grad_fn=<GatherBackward>), loss: 1.36532461643219: \n",
      "target probs tensor([[8.5570e-01],\n",
      "        [2.1268e-02],\n",
      "        [9.3298e-01],\n",
      "        [7.9135e-06],\n",
      "        [1.5960e-03],\n",
      "        [2.9071e-02],\n",
      "        [9.3802e-01],\n",
      "        [6.9499e-01],\n",
      "        [7.0273e-01],\n",
      "        [7.3883e-01],\n",
      "        [7.4127e-03],\n",
      "        [3.4423e-02],\n",
      "        [2.7438e-05],\n",
      "        [1.2250e-04],\n",
      "        [8.2619e-01],\n",
      "        [4.3106e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.2008867263793945: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[1.5884e-03],\n",
      "        [7.6597e-02],\n",
      "        [2.4517e-04],\n",
      "        [4.8977e-04],\n",
      "        [2.2674e-04],\n",
      "        [4.8691e-05],\n",
      "        [7.5895e-06],\n",
      "        [2.1238e-01],\n",
      "        [5.2603e-01],\n",
      "        [9.8833e-01],\n",
      "        [5.1645e-02],\n",
      "        [9.7113e-01],\n",
      "        [3.4842e-02],\n",
      "        [9.9282e-01],\n",
      "        [9.9581e-01],\n",
      "        [6.2906e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 4.293317794799805: \n",
      "target probs tensor([[7.6388e-05],\n",
      "        [3.3538e-01],\n",
      "        [8.6166e-01],\n",
      "        [1.1158e-03],\n",
      "        [6.8616e-03],\n",
      "        [5.2213e-04],\n",
      "        [5.4549e-04],\n",
      "        [9.9961e-01],\n",
      "        [4.7873e-03],\n",
      "        [9.8673e-01],\n",
      "        [9.7084e-01],\n",
      "        [5.3989e-01],\n",
      "        [9.7200e-01],\n",
      "        [9.9235e-01],\n",
      "        [7.4637e-03],\n",
      "        [9.9244e-01]], device='cuda:0', grad_fn=<GatherBackward>), loss: 3.0322086811065674: \n",
      "target probs tensor([[0.0047],\n",
      "        [0.9340],\n",
      "        [0.0022],\n",
      "        [0.0017],\n",
      "        [0.1535],\n",
      "        [0.0521],\n",
      "        [0.4117],\n",
      "        [0.0380],\n",
      "        [0.8552],\n",
      "        [0.0294],\n",
      "        [0.9369],\n",
      "        [0.0292],\n",
      "        [0.9910],\n",
      "        [0.6500],\n",
      "        [0.9807],\n",
      "        [0.5307]], device='cuda:0', grad_fn=<GatherBackward>), loss: 2.203815221786499: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-d125a34c3353>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# callbacks.append(file_ctrl)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# callbacks.append(cyclical_sched)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# with Hooks(gen, append_stats_normal) as hooks:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m def get_preds(model:nn.Module, dl:DataLoader, pbar:Optional[PBar]=None, cb_handler:Optional[CallbackHandler]=None,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# RUN SITE\n",
    "if 'x' in env.save_filename and mode != 'sanity_check':\n",
    "  raise ValueError('save_filename contains x')\n",
    "\n",
    "\n",
    "saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=env.save_filename + \"-best\")\n",
    "saver_every_epoch = SaveModelCallback(learn, every='epoch', name=env.save_filename)\n",
    "# fooling_weight_scheduler = FoolingWeightScheduler(learn)\n",
    "# lr_anneal = LRAnneal(learn, 1e-4)\n",
    "# file_ctrl = FileControl(learn, '/root/Derakhshani/adversarial/ctrl', learn.model)\n",
    "# cyclical_sched = CyclicalLRScheduler(learn, 3e-2, 6e-4, 4)\n",
    "\n",
    "callbacks = [saver_best, saver_every_epoch]\n",
    "# callbacks.append(lr_anneal)\n",
    "# callbacks.append(fooling_weight_scheduler)\n",
    "# callbacks.append(file_ctrl)\n",
    "# callbacks.append(cyclical_sched)\n",
    "learn.fit(400, lr=1e-2, wd = 0., callbacks=callbacks)\n",
    "\n",
    "# with Hooks(gen, append_stats_normal) as hooks:\n",
    "#   learn.fit(1, lr=5e-03, wd = 0., callbacks=[saver_best, saver_every_epoch])\n",
    "\n",
    "# for i in range(10):\n",
    "#   learn.fit_one_cycle(7, wd = 0.,max_lr=1., div_factor = 1000.) \n",
    "\n",
    "shutil.copyfile(env.temp_csv_path + '/' + env.save_filename + \".csv\", env.get_csv_path() + '.csv')\n",
    "shutil.copytree(env.data_path/env.get_learner_models_dir(), env.get_models_path())\n",
    "shutil.rmtree(env.data_path/env.get_learner_models_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models_directory returned is:  models/529\n",
      "models_directory returned is:  models/529\n"
     ]
    }
   ],
   "source": [
    "shutil.copyfile(env.temp_csv_path + '/' + env.save_filename + \".csv\", env.get_csv_path() + '.csv')\n",
    "shutil.copytree(env.data_path/env.get_learner_models_dir(), env.get_models_path())\n",
    "shutil.rmtree(env.data_path/env.get_learner_models_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x1440 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanup import cleanup_models_folder\n",
    "cleanup_models_folder('/root/Derakhshani/adversarial/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "percent_total: \n",
      "[(7, 99.999999999, 10.0), (61, 99.999999999, 10.0), (90, 99.999999999, 10.0), (163, 99.999999999, 10.0), (274, 99.999999999, 10.0), (376, 99.999999999, 10.0), (398, 99.999999999, 10.0), (411, 99.999999999, 10.0), (464, 99.999999999, 10.0), (474, 99.999999999, 10.0), (483, 99.999999999, 10.0), (533, 99.999999999, 10.0), (566, 99.999999999, 10.0), (580, 99.999999999, 10.0), (625, 99.999999999, 10.0), (637, 99.999999999, 10.0), (661, 99.999999999, 10.0), (738, 99.999999999, 10.0), (783, 99.999999999, 10.0), (787, 99.999999999, 10.0), (791, 99.999999999, 10.0), (816, 99.999999999, 10.0), (937, 99.999999999, 10.0), (987, 99.999999999, 10.0), (292, 89.99999999955001, 20.0), (489, 89.99999999955001, 20.0), (37, 89.9999999991, 10.0), (45, 89.9999999991, 10.0), (66, 89.9999999991, 10.0), (96, 89.9999999991, 10.0), (102, 89.9999999991, 10.0), (242, 89.9999999991, 10.0), (291, 89.9999999991, 10.0), (331, 89.9999999991, 10.0), (498, 89.9999999991, 10.0), (506, 89.9999999991, 10.0), (581, 89.9999999991, 10.0), (612, 89.9999999991, 10.0), (694, 89.9999999991, 10.0), (709, 89.9999999991, 10.0), (33, 79.9999999992, 10.0), (189, 79.9999999992, 10.0), (235, 79.9999999992, 10.0), (360, 79.9999999992, 10.0), (375, 79.9999999992, 10.0), (401, 79.9999999992, 10.0), (409, 79.9999999992, 10.0), (417, 79.9999999992, 10.0), (443, 79.9999999992, 10.0), (468, 79.9999999992, 10.0), (507, 79.9999999992, 10.0), (570, 79.9999999992, 10.0), (579, 79.9999999992, 10.0), (746, 79.9999999992, 10.0), (822, 79.9999999992, 10.0), (870, 79.9999999992, 10.0), (955, 79.9999999992, 10.0), (984, 79.9999999992, 10.0), (25, 74.999999999625, 20.0), (538, 69.99999999965, 20.0), (872, 69.99999999965, 20.0), (52, 69.9999999993, 10.0), (86, 69.9999999993, 10.0), (92, 69.9999999993, 10.0), (160, 69.9999999993, 10.0), (164, 69.9999999993, 10.0), (198, 69.9999999993, 10.0), (260, 69.9999999993, 10.0), (321, 69.9999999993, 10.0), (347, 69.9999999993, 10.0), (349, 69.9999999993, 10.0), (423, 69.9999999993, 10.0), (425, 69.9999999993, 10.0), (528, 69.9999999993, 10.0), (555, 69.9999999993, 10.0), (582, 69.9999999993, 10.0), (586, 69.9999999993, 10.0), (629, 69.9999999993, 10.0), (668, 69.9999999993, 10.0), (716, 69.9999999993, 10.0), (762, 69.9999999993, 10.0), (776, 69.9999999993, 10.0), (781, 69.9999999993, 10.0), (803, 69.9999999993, 10.0), (939, 69.9999999993, 10.0), (992, 69.9999999993, 10.0), (72, 64.99999999967501, 20.0), (828, 64.99999999967501, 20.0), (230, 59.99999999970001, 20.0), (858, 59.99999999970001, 20.0), (15, 59.9999999994, 10.0), (21, 59.9999999994, 10.0), (42, 59.9999999994, 10.0), (120, 59.9999999994, 10.0), (252, 59.9999999994, 10.0), (300, 59.9999999994, 10.0), (307, 59.9999999994, 10.0), (341, 59.9999999994, 10.0), (369, 59.9999999994, 10.0), (430, 59.9999999994, 10.0), (518, 59.9999999994, 10.0), (568, 59.9999999994, 10.0), (576, 59.9999999994, 10.0), (585, 59.9999999994, 10.0), (609, 59.9999999994, 10.0), (658, 59.9999999994, 10.0), (684, 59.9999999994, 10.0), (698, 59.9999999994, 10.0), (712, 59.9999999994, 10.0), (729, 59.9999999994, 10.0), (779, 59.9999999994, 10.0), (789, 59.9999999994, 10.0), (806, 59.9999999994, 10.0), (847, 59.9999999994, 10.0), (864, 59.9999999994, 10.0), (873, 59.9999999994, 10.0), (900, 59.9999999994, 10.0), (906, 59.9999999994, 10.0), (920, 59.9999999994, 10.0), (944, 59.9999999994, 10.0), (57, 49.99999999975, 20.0), (91, 49.99999999975, 20.0), (454, 49.99999999975, 20.0), (953, 49.99999999975, 20.0), (8, 49.9999999995, 10.0), (24, 49.9999999995, 10.0), (28, 49.9999999995, 10.0), (83, 49.9999999995, 10.0), (87, 49.9999999995, 10.0), (115, 49.9999999995, 10.0), (151, 49.9999999995, 10.0), (183, 49.9999999995, 10.0), (186, 49.9999999995, 10.0), (188, 49.9999999995, 10.0), (197, 49.9999999995, 10.0), (232, 49.9999999995, 10.0), (286, 49.9999999995, 10.0), (289, 49.9999999995, 10.0), (293, 49.9999999995, 10.0), (303, 49.9999999995, 10.0), (306, 49.9999999995, 10.0), (313, 49.9999999995, 10.0), (314, 49.9999999995, 10.0), (316, 49.9999999995, 10.0), (378, 49.9999999995, 10.0), (393, 49.9999999995, 10.0), (399, 49.9999999995, 10.0), (407, 49.9999999995, 10.0), (429, 49.9999999995, 10.0), (496, 49.9999999995, 10.0), (509, 49.9999999995, 10.0), (511, 49.9999999995, 10.0), (534, 49.9999999995, 10.0), (567, 49.9999999995, 10.0), (572, 49.9999999995, 10.0), (619, 49.9999999995, 10.0), (621, 49.9999999995, 10.0), (646, 49.9999999995, 10.0), (734, 49.9999999995, 10.0), (751, 49.9999999995, 10.0), (753, 49.9999999995, 10.0), (765, 49.9999999995, 10.0), (905, 49.9999999995, 10.0), (919, 49.9999999995, 10.0), (128, 44.99999999977501, 20.0), (281, 44.99999999977501, 20.0), (334, 44.99999999977501, 20.0), (721, 44.99999999977501, 20.0), (75, 39.99999999986667, 30.0), (491, 39.9999999998, 20.0), (10, 39.9999999996, 10.0), (31, 39.9999999996, 10.0), (79, 39.9999999996, 10.0), (141, 39.9999999996, 10.0), (145, 39.9999999996, 10.0), (153, 39.9999999996, 10.0), (192, 39.9999999996, 10.0), (196, 39.9999999996, 10.0), (228, 39.9999999996, 10.0), (276, 39.9999999996, 10.0), (342, 39.9999999996, 10.0), (465, 39.9999999996, 10.0), (495, 39.9999999996, 10.0), (584, 39.9999999996, 10.0), (593, 39.9999999996, 10.0), (614, 39.9999999996, 10.0), (620, 39.9999999996, 10.0), (727, 39.9999999996, 10.0), (755, 39.9999999996, 10.0), (802, 39.9999999996, 10.0), (826, 39.9999999996, 10.0), (981, 39.9999999996, 10.0), (985, 39.9999999996, 10.0), (134, 36.666666666544444, 30.0), (55, 34.999999999825, 20.0), (84, 34.999999999825, 20.0), (343, 34.999999999825, 20.0), (565, 34.999999999825, 20.0), (777, 29.999999999850004, 20.0), (831, 29.999999999850004, 20.0), (47, 29.9999999997, 10.0), (99, 29.9999999997, 10.0), (109, 29.9999999997, 10.0), (116, 29.9999999997, 10.0), (144, 29.9999999997, 10.0), (218, 29.9999999997, 10.0), (319, 29.9999999997, 10.0), (381, 29.9999999997, 10.0), (392, 29.9999999997, 10.0), (424, 29.9999999997, 10.0), (442, 29.9999999997, 10.0), (502, 29.9999999997, 10.0), (651, 29.9999999997, 10.0), (706, 29.9999999997, 10.0), (890, 29.9999999997, 10.0), (926, 29.9999999997, 10.0), (517, 26.66666666657778, 30.0), (575, 26.66666666657778, 30.0), (68, 24.999999999875, 20.0), (205, 24.999999999875, 20.0), (437, 24.999999999875, 20.0), (547, 24.999999999875, 20.0), (558, 24.999999999875, 20.0), (607, 24.999999999875, 20.0), (722, 24.999999999875, 20.0), (918, 24.999999999875, 20.0), (238, 23.333333333255556, 30.0), (344, 19.9999999999, 20.0), (416, 19.9999999999, 20.0), (110, 19.9999999998, 10.0), (158, 19.9999999998, 10.0), (176, 19.9999999998, 10.0), (202, 19.9999999998, 10.0), (233, 19.9999999998, 10.0), (290, 19.9999999998, 10.0), (315, 19.9999999998, 10.0), (388, 19.9999999998, 10.0), (402, 19.9999999998, 10.0), (452, 19.9999999998, 10.0), (477, 19.9999999998, 10.0), (520, 19.9999999998, 10.0), (522, 19.9999999998, 10.0), (574, 19.9999999998, 10.0), (624, 19.9999999998, 10.0), (645, 19.9999999998, 10.0), (819, 19.9999999998, 10.0), (825, 19.9999999998, 10.0), (839, 19.9999999998, 10.0), (854, 19.9999999998, 10.0), (904, 19.9999999998, 10.0), (917, 19.9999999998, 10.0), (463, 16.66666666661111, 30.0), (472, 16.66666666661111, 30.0), (652, 14.9999999999625, 40.0), (38, 14.999999999925002, 20.0), (384, 14.999999999925002, 20.0), (405, 14.999999999925002, 20.0), (492, 14.999999999925002, 20.0), (848, 14.999999999925002, 20.0), (850, 14.999999999925002, 20.0), (893, 14.999999999925002, 20.0), (554, 13.33333333328889, 30.0), (921, 9.999999999966667, 30.0), (595, 9.99999999995, 20.0), (656, 9.99999999995, 20.0), (869, 9.99999999995, 20.0), (909, 9.99999999995, 20.0), (1, 9.9999999999, 10.0), (18, 9.9999999999, 10.0), (59, 9.9999999999, 10.0), (60, 9.9999999999, 10.0), (98, 9.9999999999, 10.0), (123, 9.9999999999, 10.0), (139, 9.9999999999, 10.0), (150, 9.9999999999, 10.0), (182, 9.9999999999, 10.0), (193, 9.9999999999, 10.0), (204, 9.9999999999, 10.0), (213, 9.9999999999, 10.0), (236, 9.9999999999, 10.0), (253, 9.9999999999, 10.0), (258, 9.9999999999, 10.0), (264, 9.9999999999, 10.0), (275, 9.9999999999, 10.0), (294, 9.9999999999, 10.0), (301, 9.9999999999, 10.0), (302, 9.9999999999, 10.0), (326, 9.9999999999, 10.0), (330, 9.9999999999, 10.0), (355, 9.9999999999, 10.0), (406, 9.9999999999, 10.0), (420, 9.9999999999, 10.0), (439, 9.9999999999, 10.0), (459, 9.9999999999, 10.0), (486, 9.9999999999, 10.0), (490, 9.9999999999, 10.0), (505, 9.9999999999, 10.0), (530, 9.9999999999, 10.0), (535, 9.9999999999, 10.0), (543, 9.9999999999, 10.0), (560, 9.9999999999, 10.0), (571, 9.9999999999, 10.0), (577, 9.9999999999, 10.0), (599, 9.9999999999, 10.0), (605, 9.9999999999, 10.0), (606, 9.9999999999, 10.0), (611, 9.9999999999, 10.0), (669, 9.9999999999, 10.0), (707, 9.9999999999, 10.0), (735, 9.9999999999, 10.0), (739, 9.9999999999, 10.0), (796, 9.9999999999, 10.0), (808, 9.9999999999, 10.0), (813, 9.9999999999, 10.0), (821, 9.9999999999, 10.0), (833, 9.9999999999, 10.0), (842, 9.9999999999, 10.0), (871, 9.9999999999, 10.0), (901, 9.9999999999, 10.0), (910, 9.9999999999, 10.0), (915, 9.9999999999, 10.0), (932, 9.9999999999, 10.0), (934, 9.9999999999, 10.0), (973, 9.9999999999, 10.0), (979, 9.9999999999, 10.0), (271, 4.999999999975, 20.0), (284, 4.999999999975, 20.0), (305, 4.999999999975, 20.0), (377, 4.999999999975, 20.0), (962, 4.999999999975, 20.0), (996, 4.999999999975, 20.0)]\n"
     ]
    }
   ],
   "source": [
    "total_histogram = fooled_histogram + unfooled_histogram\n",
    "percent_total = [(i, 100. * u / (total_histogram[i] + 1e-10), total_histogram[i]) for i, u in enumerate(unfooled_histogram)]\n",
    "sorted_percent_total = sorted(percent_total, key =lambda x: x[1], reverse = True)\n",
    "print('\\npercent_total: ')\n",
    "print(list(filter(lambda x: x[1] > 0.0, sorted_percent_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram\n",
    "fig, axes = plt.subplots(len(hooks),1, figsize=(30,12))\n",
    "for ax,h in zip(axes.flatten(), hooks):\n",
    "  ax.imshow(get_hist(h), origin='lower')\n",
    "  ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mean and std\n",
    "fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "for h in hooks:\n",
    "  ms, ss, _ = h.stats\n",
    "  ax0.plot(ms[:100])\n",
    "  ax1.plot(ss[:100])\n",
    "plt.legend(range(len(hooks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "for h in hooks:\n",
    "  ms, ss, _ = h.stats\n",
    "  ax0.plot(ms)\n",
    "  ax1.plot(ss)\n",
    "plt.legend(range(len(hooks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero precentage:\n",
    "fig,axes = plt.subplots(len(hooks),1, figsize=(30,30))\n",
    "for ax,h in zip(axes.flatten(), hooks):\n",
    "    ax.plot(get_min(h))\n",
    "    ax.set_ylim(0,1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MO2fZ-hSSUzJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# z1 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "# z2 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "z1 = torch.tensor([0.8, -0.5] * 5).cuda()\n",
    "z2 = torch.tensor([-1.] * 10).cuda()\n",
    "print(\"z1: \", z1)\n",
    "print(\"z2: \", z2)\n",
    "print(\"distance: \", torch.norm(z1-z2,p=2))\n",
    "model = learn.model.eval()\n",
    "\n",
    "z_s = interpolate(z1, z2, 0.1)\n",
    "print(len(z_s))\n",
    "\n",
    "for i,z in enumerate(z_s):\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n",
    "  #img.save('./pics/' + str(i) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea : have 200 noises (1 for each class), then start iterating the dataset, and for each image, randomly apply one noise and record the result\n",
    "def targeted_diversity(learn, n_perturbations = 200, percentage = 95):\n",
    "  model = learn.model.eval()\n",
    "\n",
    "  one_hot_conditions = [torch.empty(z_dim).uniform_(0,1).cuda().detach() for _ in range(n_perturbations)]\n",
    "#   for i in range(z_dim):\n",
    "#     one_hot_conditions[i][i] = 1.\n",
    "\n",
    "  perturbations = [model.forward_single_z(z) for z in one_hot_conditions]\n",
    "\n",
    "  hist = [0.] * 1000\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 : print(\"at batch_no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbations[np.random.randint(0,len(perturbations))][None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      hist[pred] += 1\n",
    "\n",
    "  pred_histogram_sum = np.sum(hist)\n",
    "  indexed_pred_histogram = [(i, hist_element) for i,hist_element in  \n",
    "                            enumerate(hist)]\n",
    "\n",
    "  indexed_pred_histogram.sort(key=lambda x: x[1], reverse = True)\n",
    "\n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = indexed_pred_histogram[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / pred_histogram_sum) * 100.\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, indexed_pred_histogram\n",
    "\n",
    "def targeted_diversity_average(learn, n_perturbations = 200, percentage = 95, average_over = 4):\n",
    "  results = []\n",
    "  for i in range(average_over):\n",
    "    n, _ = targeted_diversity(learn, n_perturbations, percentage)\n",
    "    print(f'done with the {i}th calculation: {n}')\n",
    "    results.append(n)\n",
    "  return np.mean(results)\n",
    "\n",
    "def diversity_average(learn, n_perturbations = 10, percentage = 95, average_over = 4):\n",
    "  results = []\n",
    "  for i in range(average_over):\n",
    "    n, _ = diversity(learn, n_perturbations, percentage, verbose = False)\n",
    "    print(f'done with the {i}th calculation: {n}')\n",
    "    results.append(n)\n",
    "  return np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 5\n",
      "at batch no 10\n",
      "at batch no 15\n",
      "at batch no 20\n",
      "at batch no 25\n",
      "at batch no 30\n",
      "at batch no 35\n",
      "at batch no 40\n",
      "at batch no 45\n",
      "at batch no 50\n",
      "at batch no 55\n",
      "at batch no 60\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(566,\n",
       " [(794, 50.099998474121094),\n",
       "  (599, 21.100000381469727),\n",
       "  (668, 20.200000762939453),\n",
       "  (904, 15.0),\n",
       "  (973, 13.600000381469727),\n",
       "  (490, 12.899999618530273),\n",
       "  (39, 12.699999809265137),\n",
       "  (770, 12.600000381469727),\n",
       "  (741, 11.300000190734863),\n",
       "  (828, 11.100000381469727),\n",
       "  (109, 9.199999809265137),\n",
       "  (556, 8.600000381469727),\n",
       "  (489, 8.399999618530273),\n",
       "  (955, 8.399999618530273),\n",
       "  (887, 8.300000190734863),\n",
       "  (669, 8.100000381469727),\n",
       "  (84, 7.400000095367432),\n",
       "  (855, 7.0),\n",
       "  (538, 6.800000190734863),\n",
       "  (108, 6.599999904632568),\n",
       "  (124, 6.0),\n",
       "  (397, 5.900000095367432),\n",
       "  (48, 5.800000190734863),\n",
       "  (61, 5.5),\n",
       "  (777, 4.900000095367432),\n",
       "  (721, 4.800000190734863),\n",
       "  (401, 4.5),\n",
       "  (971, 4.400000095367432),\n",
       "  (893, 4.300000190734863),\n",
       "  (857, 4.099999904632568),\n",
       "  (591, 4.0),\n",
       "  (711, 3.9000000953674316),\n",
       "  (709, 3.799999952316284),\n",
       "  (455, 3.700000047683716),\n",
       "  (55, 3.5999999046325684),\n",
       "  (414, 3.5),\n",
       "  (906, 3.5),\n",
       "  (151, 3.4000000953674316),\n",
       "  (389, 3.4000000953674316),\n",
       "  (406, 3.4000000953674316),\n",
       "  (865, 3.299999952316284),\n",
       "  (750, 3.200000047683716),\n",
       "  (581, 3.0),\n",
       "  (0, 2.9000000953674316),\n",
       "  (1, 2.9000000953674316),\n",
       "  (476, 2.9000000953674316),\n",
       "  (915, 2.9000000953674316),\n",
       "  (363, 2.799999952316284),\n",
       "  (837, 2.799999952316284),\n",
       "  (982, 2.799999952316284),\n",
       "  (110, 2.700000047683716),\n",
       "  (46, 2.5999999046325684),\n",
       "  (62, 2.5999999046325684),\n",
       "  (593, 2.5999999046325684),\n",
       "  (640, 2.5999999046325684),\n",
       "  (735, 2.5999999046325684),\n",
       "  (819, 2.5999999046325684),\n",
       "  (94, 2.5),\n",
       "  (126, 2.5),\n",
       "  (431, 2.5),\n",
       "  (611, 2.4000000953674316),\n",
       "  (864, 2.4000000953674316),\n",
       "  (868, 2.4000000953674316),\n",
       "  (222, 2.299999952316284),\n",
       "  (558, 2.299999952316284),\n",
       "  (572, 2.299999952316284),\n",
       "  (850, 2.299999952316284),\n",
       "  (115, 2.200000047683716),\n",
       "  (410, 2.200000047683716),\n",
       "  (698, 2.200000047683716),\n",
       "  (763, 2.200000047683716),\n",
       "  (772, 2.200000047683716),\n",
       "  (779, 2.200000047683716),\n",
       "  (97, 2.0999999046325684),\n",
       "  (238, 2.0999999046325684),\n",
       "  (440, 2.0999999046325684),\n",
       "  (447, 2.0999999046325684),\n",
       "  (464, 2.0999999046325684),\n",
       "  (872, 2.0999999046325684),\n",
       "  (898, 2.0999999046325684),\n",
       "  (68, 2.0),\n",
       "  (118, 2.0),\n",
       "  (182, 2.0),\n",
       "  (242, 2.0),\n",
       "  (292, 2.0),\n",
       "  (393, 2.0),\n",
       "  (520, 2.0),\n",
       "  (621, 2.0),\n",
       "  (60, 1.899999976158142),\n",
       "  (188, 1.899999976158142),\n",
       "  (189, 1.899999976158142),\n",
       "  (192, 1.899999976158142),\n",
       "  (342, 1.899999976158142),\n",
       "  (411, 1.899999976158142),\n",
       "  (472, 1.899999976158142),\n",
       "  (570, 1.899999976158142),\n",
       "  (619, 1.899999976158142),\n",
       "  (620, 1.899999976158142),\n",
       "  (724, 1.899999976158142),\n",
       "  (791, 1.899999976158142),\n",
       "  (800, 1.899999976158142),\n",
       "  (199, 1.7999999523162842),\n",
       "  (290, 1.7999999523162842),\n",
       "  (348, 1.7999999523162842),\n",
       "  (423, 1.7999999523162842),\n",
       "  (761, 1.7999999523162842),\n",
       "  (762, 1.7999999523162842),\n",
       "  (870, 1.7999999523162842),\n",
       "  (920, 1.7999999523162842),\n",
       "  (72, 1.7000000476837158),\n",
       "  (128, 1.7000000476837158),\n",
       "  (155, 1.7000000476837158),\n",
       "  (195, 1.7000000476837158),\n",
       "  (334, 1.7000000476837158),\n",
       "  (526, 1.7000000476837158),\n",
       "  (547, 1.7000000476837158),\n",
       "  (671, 1.7000000476837158),\n",
       "  (725, 1.7000000476837158),\n",
       "  (775, 1.7000000476837158),\n",
       "  (783, 1.7000000476837158),\n",
       "  (824, 1.7000000476837158),\n",
       "  (871, 1.7000000476837158),\n",
       "  (123, 1.600000023841858),\n",
       "  (375, 1.600000023841858),\n",
       "  (457, 1.600000023841858),\n",
       "  (468, 1.600000023841858),\n",
       "  (492, 1.600000023841858),\n",
       "  (508, 1.600000023841858),\n",
       "  (550, 1.600000023841858),\n",
       "  (562, 1.600000023841858),\n",
       "  (803, 1.600000023841858),\n",
       "  (817, 1.600000023841858),\n",
       "  (953, 1.600000023841858),\n",
       "  (963, 1.600000023841858),\n",
       "  (107, 1.5),\n",
       "  (119, 1.5),\n",
       "  (202, 1.5),\n",
       "  (230, 1.5),\n",
       "  (420, 1.5),\n",
       "  (477, 1.5),\n",
       "  (564, 1.5),\n",
       "  (748, 1.5),\n",
       "  (815, 1.5),\n",
       "  (907, 1.5),\n",
       "  (76, 1.399999976158142),\n",
       "  (83, 1.399999976158142),\n",
       "  (193, 1.399999976158142),\n",
       "  (231, 1.399999976158142),\n",
       "  (274, 1.399999976158142),\n",
       "  (293, 1.399999976158142),\n",
       "  (305, 1.399999976158142),\n",
       "  (314, 1.399999976158142),\n",
       "  (336, 1.399999976158142),\n",
       "  (552, 1.399999976158142),\n",
       "  (565, 1.399999976158142),\n",
       "  (579, 1.399999976158142),\n",
       "  (597, 1.399999976158142),\n",
       "  (624, 1.399999976158142),\n",
       "  (679, 1.399999976158142),\n",
       "  (784, 1.399999976158142),\n",
       "  (786, 1.399999976158142),\n",
       "  (801, 1.399999976158142),\n",
       "  (891, 1.399999976158142),\n",
       "  (902, 1.399999976158142),\n",
       "  (33, 1.2999999523162842),\n",
       "  (57, 1.2999999523162842),\n",
       "  (96, 1.2999999523162842),\n",
       "  (120, 1.2999999523162842),\n",
       "  (247, 1.2999999523162842),\n",
       "  (275, 1.2999999523162842),\n",
       "  (328, 1.2999999523162842),\n",
       "  (355, 1.2999999523162842),\n",
       "  (409, 1.2999999523162842),\n",
       "  (441, 1.2999999523162842),\n",
       "  (505, 1.2999999523162842),\n",
       "  (586, 1.2999999523162842),\n",
       "  (588, 1.2999999523162842),\n",
       "  (633, 1.2999999523162842),\n",
       "  (638, 1.2999999523162842),\n",
       "  (821, 1.2999999523162842),\n",
       "  (842, 1.2999999523162842),\n",
       "  (892, 1.2999999523162842),\n",
       "  (58, 1.2000000476837158),\n",
       "  (65, 1.2000000476837158),\n",
       "  (171, 1.2000000476837158),\n",
       "  (204, 1.2000000476837158),\n",
       "  (205, 1.2000000476837158),\n",
       "  (219, 1.2000000476837158),\n",
       "  (307, 1.2000000476837158),\n",
       "  (308, 1.2000000476837158),\n",
       "  (327, 1.2000000476837158),\n",
       "  (331, 1.2000000476837158),\n",
       "  (353, 1.2000000476837158),\n",
       "  (366, 1.2000000476837158),\n",
       "  (443, 1.2000000476837158),\n",
       "  (454, 1.2000000476837158),\n",
       "  (474, 1.2000000476837158),\n",
       "  (495, 1.2000000476837158),\n",
       "  (563, 1.2000000476837158),\n",
       "  (574, 1.2000000476837158),\n",
       "  (602, 1.2000000476837158),\n",
       "  (641, 1.2000000476837158),\n",
       "  (654, 1.2000000476837158),\n",
       "  (658, 1.2000000476837158),\n",
       "  (781, 1.2000000476837158),\n",
       "  (823, 1.2000000476837158),\n",
       "  (848, 1.2000000476837158),\n",
       "  (854, 1.2000000476837158),\n",
       "  (858, 1.2000000476837158),\n",
       "  (883, 1.2000000476837158),\n",
       "  (24, 1.100000023841858),\n",
       "  (41, 1.100000023841858),\n",
       "  (51, 1.100000023841858),\n",
       "  (113, 1.100000023841858),\n",
       "  (116, 1.100000023841858),\n",
       "  (236, 1.100000023841858),\n",
       "  (249, 1.100000023841858),\n",
       "  (253, 1.100000023841858),\n",
       "  (271, 1.100000023841858),\n",
       "  (281, 1.100000023841858),\n",
       "  (300, 1.100000023841858),\n",
       "  (310, 1.100000023841858),\n",
       "  (317, 1.100000023841858),\n",
       "  (318, 1.100000023841858),\n",
       "  (319, 1.100000023841858),\n",
       "  (350, 1.100000023841858),\n",
       "  (381, 1.100000023841858),\n",
       "  (395, 1.100000023841858),\n",
       "  (396, 1.100000023841858),\n",
       "  (491, 1.100000023841858),\n",
       "  (496, 1.100000023841858),\n",
       "  (497, 1.100000023841858),\n",
       "  (506, 1.100000023841858),\n",
       "  (507, 1.100000023841858),\n",
       "  (527, 1.100000023841858),\n",
       "  (544, 1.100000023841858),\n",
       "  (575, 1.100000023841858),\n",
       "  (609, 1.100000023841858),\n",
       "  (626, 1.100000023841858),\n",
       "  (759, 1.100000023841858),\n",
       "  (787, 1.100000023841858),\n",
       "  (796, 1.100000023841858),\n",
       "  (806, 1.100000023841858),\n",
       "  (820, 1.100000023841858),\n",
       "  (834, 1.100000023841858),\n",
       "  (863, 1.100000023841858),\n",
       "  (882, 1.100000023841858),\n",
       "  (894, 1.100000023841858),\n",
       "  (918, 1.100000023841858),\n",
       "  (981, 1.100000023841858),\n",
       "  (996, 1.100000023841858),\n",
       "  (7, 1.0),\n",
       "  (8, 1.0),\n",
       "  (10, 1.0),\n",
       "  (15, 1.0),\n",
       "  (17, 1.0),\n",
       "  (19, 1.0),\n",
       "  (25, 1.0),\n",
       "  (28, 1.0),\n",
       "  (37, 1.0),\n",
       "  (42, 1.0),\n",
       "  (45, 1.0),\n",
       "  (49, 1.0),\n",
       "  (52, 1.0),\n",
       "  (53, 1.0),\n",
       "  (63, 1.0),\n",
       "  (70, 1.0),\n",
       "  (75, 1.0),\n",
       "  (79, 1.0),\n",
       "  (86, 1.0),\n",
       "  (87, 1.0),\n",
       "  (90, 1.0),\n",
       "  (91, 1.0),\n",
       "  (92, 1.0),\n",
       "  (98, 1.0),\n",
       "  (102, 1.0),\n",
       "  (105, 1.0),\n",
       "  (117, 1.0),\n",
       "  (134, 1.0),\n",
       "  (139, 1.0),\n",
       "  (140, 1.0),\n",
       "  (141, 1.0),\n",
       "  (144, 1.0),\n",
       "  (158, 1.0),\n",
       "  (161, 1.0),\n",
       "  (162, 1.0),\n",
       "  (163, 1.0),\n",
       "  (164, 1.0),\n",
       "  (173, 1.0),\n",
       "  (183, 1.0),\n",
       "  (186, 1.0),\n",
       "  (196, 1.0),\n",
       "  (197, 1.0),\n",
       "  (198, 1.0),\n",
       "  (206, 1.0),\n",
       "  (213, 1.0),\n",
       "  (218, 1.0),\n",
       "  (228, 1.0),\n",
       "  (235, 1.0),\n",
       "  (260, 1.0),\n",
       "  (273, 1.0),\n",
       "  (284, 1.0),\n",
       "  (289, 1.0),\n",
       "  (291, 1.0),\n",
       "  (301, 1.0),\n",
       "  (304, 1.0),\n",
       "  (306, 1.0),\n",
       "  (312, 1.0),\n",
       "  (313, 1.0),\n",
       "  (316, 1.0),\n",
       "  (321, 1.0),\n",
       "  (323, 1.0),\n",
       "  (337, 1.0),\n",
       "  (347, 1.0),\n",
       "  (360, 1.0),\n",
       "  (376, 1.0),\n",
       "  (378, 1.0),\n",
       "  (387, 1.0),\n",
       "  (392, 1.0),\n",
       "  (398, 1.0),\n",
       "  (417, 1.0),\n",
       "  (425, 1.0),\n",
       "  (428, 1.0),\n",
       "  (429, 1.0),\n",
       "  (433, 1.0),\n",
       "  (445, 1.0),\n",
       "  (451, 1.0),\n",
       "  (483, 1.0),\n",
       "  (488, 1.0),\n",
       "  (498, 1.0),\n",
       "  (518, 1.0),\n",
       "  (528, 1.0),\n",
       "  (530, 1.0),\n",
       "  (531, 1.0),\n",
       "  (533, 1.0),\n",
       "  (566, 1.0),\n",
       "  (580, 1.0),\n",
       "  (608, 1.0),\n",
       "  (612, 1.0),\n",
       "  (616, 1.0),\n",
       "  (625, 1.0),\n",
       "  (629, 1.0),\n",
       "  (637, 1.0),\n",
       "  (645, 1.0),\n",
       "  (646, 1.0),\n",
       "  (651, 1.0),\n",
       "  (655, 1.0),\n",
       "  (661, 1.0),\n",
       "  (684, 1.0),\n",
       "  (687, 1.0),\n",
       "  (691, 1.0),\n",
       "  (692, 1.0),\n",
       "  (694, 1.0),\n",
       "  (716, 1.0),\n",
       "  (719, 1.0),\n",
       "  (734, 1.0),\n",
       "  (738, 1.0),\n",
       "  (746, 1.0),\n",
       "  (753, 1.0),\n",
       "  (768, 1.0),\n",
       "  (793, 1.0),\n",
       "  (802, 1.0),\n",
       "  (816, 1.0),\n",
       "  (826, 1.0),\n",
       "  (830, 1.0),\n",
       "  (831, 1.0),\n",
       "  (847, 1.0),\n",
       "  (873, 1.0),\n",
       "  (884, 1.0),\n",
       "  (905, 1.0),\n",
       "  (923, 1.0),\n",
       "  (932, 1.0),\n",
       "  (934, 1.0),\n",
       "  (937, 1.0),\n",
       "  (939, 1.0),\n",
       "  (944, 1.0),\n",
       "  (946, 1.0),\n",
       "  (957, 1.0),\n",
       "  (959, 1.0),\n",
       "  (984, 1.0),\n",
       "  (985, 1.0),\n",
       "  (987, 1.0),\n",
       "  (989, 1.0),\n",
       "  (992, 1.0),\n",
       "  (9, 0.8999999761581421),\n",
       "  (18, 0.8999999761581421),\n",
       "  (21, 0.8999999761581421),\n",
       "  (36, 0.8999999761581421),\n",
       "  (47, 0.8999999761581421),\n",
       "  (85, 0.8999999761581421),\n",
       "  (135, 0.8999999761581421),\n",
       "  (142, 0.8999999761581421),\n",
       "  (145, 0.8999999761581421),\n",
       "  (176, 0.8999999761581421),\n",
       "  (187, 0.8999999761581421),\n",
       "  (263, 0.8999999761581421),\n",
       "  (266, 0.8999999761581421),\n",
       "  (303, 0.8999999761581421),\n",
       "  (315, 0.8999999761581421),\n",
       "  (326, 0.8999999761581421),\n",
       "  (365, 0.8999999761581421),\n",
       "  (384, 0.8999999761581421),\n",
       "  (390, 0.8999999761581421),\n",
       "  (391, 0.8999999761581421),\n",
       "  (408, 0.8999999761581421),\n",
       "  (459, 0.8999999761581421),\n",
       "  (463, 0.8999999761581421),\n",
       "  (482, 0.8999999761581421),\n",
       "  (503, 0.8999999761581421),\n",
       "  (534, 0.8999999761581421),\n",
       "  (535, 0.8999999761581421),\n",
       "  (555, 0.8999999761581421),\n",
       "  (577, 0.8999999761581421),\n",
       "  (635, 0.8999999761581421),\n",
       "  (663, 0.8999999761581421),\n",
       "  (674, 0.8999999761581421),\n",
       "  (702, 0.8999999761581421),\n",
       "  (703, 0.8999999761581421),\n",
       "  (712, 0.8999999761581421),\n",
       "  (743, 0.8999999761581421),\n",
       "  (757, 0.8999999761581421),\n",
       "  (764, 0.8999999761581421),\n",
       "  (776, 0.8999999761581421),\n",
       "  (788, 0.8999999761581421),\n",
       "  (808, 0.8999999761581421),\n",
       "  (832, 0.8999999761581421),\n",
       "  (833, 0.8999999761581421),\n",
       "  (900, 0.8999999761581421),\n",
       "  (968, 0.8999999761581421),\n",
       "  (988, 0.8999999761581421),\n",
       "  (997, 0.8999999761581421),\n",
       "  (38, 0.800000011920929),\n",
       "  (77, 0.800000011920929),\n",
       "  (93, 0.800000011920929),\n",
       "  (100, 0.800000011920929),\n",
       "  (160, 0.800000011920929),\n",
       "  (246, 0.800000011920929),\n",
       "  (254, 0.800000011920929),\n",
       "  (280, 0.800000011920929),\n",
       "  (294, 0.800000011920929),\n",
       "  (344, 0.800000011920929),\n",
       "  (372, 0.800000011920929),\n",
       "  (377, 0.800000011920929),\n",
       "  (399, 0.800000011920929),\n",
       "  (407, 0.800000011920929),\n",
       "  (415, 0.800000011920929),\n",
       "  (430, 0.800000011920929),\n",
       "  (432, 0.800000011920929),\n",
       "  (439, 0.800000011920929),\n",
       "  (458, 0.800000011920929),\n",
       "  (514, 0.800000011920929),\n",
       "  (545, 0.800000011920929),\n",
       "  (546, 0.800000011920929),\n",
       "  (595, 0.800000011920929),\n",
       "  (603, 0.800000011920929),\n",
       "  (643, 0.800000011920929),\n",
       "  (644, 0.800000011920929),\n",
       "  (672, 0.800000011920929),\n",
       "  (696, 0.800000011920929),\n",
       "  (729, 0.800000011920929),\n",
       "  (732, 0.800000011920929),\n",
       "  (809, 0.800000011920929),\n",
       "  (822, 0.800000011920929),\n",
       "  (829, 0.800000011920929),\n",
       "  (838, 0.800000011920929),\n",
       "  (843, 0.800000011920929),\n",
       "  (852, 0.800000011920929),\n",
       "  (885, 0.800000011920929),\n",
       "  (889, 0.800000011920929),\n",
       "  (901, 0.800000011920929),\n",
       "  (956, 0.800000011920929),\n",
       "  (34, 0.699999988079071),\n",
       "  (50, 0.699999988079071),\n",
       "  (99, 0.699999988079071),\n",
       "  (112, 0.699999988079071),\n",
       "  (168, 0.699999988079071),\n",
       "  (184, 0.699999988079071),\n",
       "  (214, 0.699999988079071),\n",
       "  (216, 0.699999988079071),\n",
       "  (217, 0.699999988079071),\n",
       "  (232, 0.699999988079071),\n",
       "  (270, 0.699999988079071),\n",
       "  (320, 0.699999988079071),\n",
       "  (330, 0.699999988079071),\n",
       "  (335, 0.699999988079071),\n",
       "  (345, 0.699999988079071),\n",
       "  (361, 0.699999988079071),\n",
       "  (388, 0.699999988079071),\n",
       "  (412, 0.699999988079071),\n",
       "  (512, 0.699999988079071),\n",
       "  (561, 0.699999988079071),\n",
       "  (576, 0.699999988079071),\n",
       "  (632, 0.699999988079071),\n",
       "  (695, 0.699999988079071),\n",
       "  (805, 0.699999988079071),\n",
       "  (879, 0.699999988079071),\n",
       "  (886, 0.699999988079071),\n",
       "  (952, 0.699999988079071),\n",
       "  (32, 0.6000000238418579),\n",
       "  (88, 0.6000000238418579),\n",
       "  (146, 0.6000000238418579),\n",
       "  (148, 0.6000000238418579),\n",
       "  (209, 0.6000000238418579),\n",
       "  (211, 0.6000000238418579),\n",
       "  (322, 0.6000000238418579),\n",
       "  (340, 0.6000000238418579),\n",
       "  (343, 0.6000000238418579),\n",
       "  (442, 0.6000000238418579),\n",
       "  (450, 0.6000000238418579),\n",
       "  (470, 0.6000000238418579),\n",
       "  (532, 0.6000000238418579),\n",
       "  (539, 0.6000000238418579),\n",
       "  (554, 0.6000000238418579),\n",
       "  (560, 0.6000000238418579),\n",
       "  (571, 0.6000000238418579),\n",
       "  (584, 0.6000000238418579),\n",
       "  (589, 0.6000000238418579),\n",
       "  (656, 0.6000000238418579),\n",
       "  (699, 0.6000000238418579),\n",
       "  (736, 0.6000000238418579),\n",
       "  (754, 0.6000000238418579),\n",
       "  (758, 0.6000000238418579),\n",
       "  (765, 0.6000000238418579),\n",
       "  (790, 0.6000000238418579),\n",
       "  (888, 0.6000000238418579),\n",
       "  (890, 0.6000000238418579),\n",
       "  (972, 0.6000000238418579),\n",
       "  (979, 0.6000000238418579),\n",
       "  (12, 0.5),\n",
       "  (16, 0.5),\n",
       "  (22, 0.5),\n",
       "  (143, 0.5),\n",
       "  (157, 0.5),\n",
       "  (166, 0.5),\n",
       "  (203, 0.5),\n",
       "  (212, 0.5),\n",
       "  (221, 0.5),\n",
       "  (224, 0.5),\n",
       "  (229, 0.5),\n",
       "  (234, 0.5),\n",
       "  (237, 0.5),\n",
       "  (252, 0.5),\n",
       "  (276, 0.5),\n",
       "  (282, 0.5),\n",
       "  (288, 0.5),\n",
       "  (295, 0.5),\n",
       "  (296, 0.5),\n",
       "  (309, 0.5),\n",
       "  (329, 0.5),\n",
       "  (402, 0.5),\n",
       "  (413, 0.5),\n",
       "  (436, 0.5),\n",
       "  (444, 0.5),\n",
       "  (448, 0.5),\n",
       "  (471, 0.5),\n",
       "  (540, 0.5),\n",
       "  (604, 0.5),\n",
       "  (639, 0.5),\n",
       "  (647, 0.5),\n",
       "  (683, 0.5),\n",
       "  (697, 0.5),\n",
       "  (715, 0.5),\n",
       "  (752, 0.5),\n",
       "  (755, 0.5),\n",
       "  (795, 0.5),\n",
       "  (811, 0.5),\n",
       "  (839, 0.5),\n",
       "  (853, 0.5),\n",
       "  (862, 0.5),\n",
       "  (877, 0.5),\n",
       "  (897, 0.5),\n",
       "  (910, 0.5),\n",
       "  (911, 0.5),\n",
       "  (927, 0.5),\n",
       "  (977, 0.5),\n",
       "  (20, 0.4000000059604645),\n",
       "  (29, 0.4000000059604645),\n",
       "  (69, 0.4000000059604645),\n",
       "  (122, 0.4000000059604645),\n",
       "  (125, 0.4000000059604645),\n",
       "  (130, 0.4000000059604645),\n",
       "  (132, 0.4000000059604645),\n",
       "  (178, 0.4000000059604645),\n",
       "  (180, 0.4000000059604645),\n",
       "  (256, 0.4000000059604645),\n",
       "  (264, 0.4000000059604645),\n",
       "  (298, 0.4000000059604645),\n",
       "  (370, 0.4000000059604645),\n",
       "  (419, 0.4000000059604645),\n",
       "  (424, 0.4000000059604645),\n",
       "  (480, 0.4000000059604645),\n",
       "  (484, 0.4000000059604645),\n",
       "  (509, 0.4000000059604645),\n",
       "  (511, 0.4000000059604645),\n",
       "  (537, 0.4000000059604645),\n",
       "  (582, 0.4000000059604645),\n",
       "  (587, 0.4000000059604645),\n",
       "  (590, 0.4000000059604645),\n",
       "  (606, 0.4000000059604645),\n",
       "  (615, 0.4000000059604645),\n",
       "  (652, 0.4000000059604645),\n",
       "  (670, 0.4000000059604645),\n",
       "  (689, 0.4000000059604645),\n",
       "  (700, 0.4000000059604645),\n",
       "  (707, 0.4000000059604645),\n",
       "  (720, 0.4000000059604645),\n",
       "  (727, 0.4000000059604645),\n",
       "  (745, 0.4000000059604645),\n",
       "  (804, 0.4000000059604645),\n",
       "  (844, 0.4000000059604645),\n",
       "  (866, 0.4000000059604645),\n",
       "  (878, 0.4000000059604645),\n",
       "  (881, 0.4000000059604645),\n",
       "  (896, 0.4000000059604645),\n",
       "  (925, 0.4000000059604645),\n",
       "  (2, 0.30000001192092896),\n",
       "  (5, 0.30000001192092896),\n",
       "  (74, 0.30000001192092896),\n",
       "  (78, 0.30000001192092896),\n",
       "  (81, 0.30000001192092896),\n",
       "  (89, 0.30000001192092896),\n",
       "  (101, 0.30000001192092896),\n",
       "  (136, 0.30000001192092896),\n",
       "  (138, 0.30000001192092896),\n",
       "  (169, 0.30000001192092896),\n",
       "  (190, 0.30000001192092896),\n",
       "  (207, 0.30000001192092896),\n",
       "  (208, 0.30000001192092896),\n",
       "  (215, 0.30000001192092896),\n",
       "  (243, 0.30000001192092896),\n",
       "  (250, 0.30000001192092896),\n",
       "  (285, 0.30000001192092896),\n",
       "  (286, 0.30000001192092896),\n",
       "  (302, 0.30000001192092896),\n",
       "  (332, 0.30000001192092896),\n",
       "  (341, 0.30000001192092896),\n",
       "  (357, 0.30000001192092896),\n",
       "  (364, 0.30000001192092896),\n",
       "  (369, 0.30000001192092896),\n",
       "  (386, 0.30000001192092896),\n",
       "  (403, 0.30000001192092896),\n",
       "  (438, 0.30000001192092896),\n",
       "  (456, 0.30000001192092896),\n",
       "  (461, 0.30000001192092896),\n",
       "  (487, 0.30000001192092896),\n",
       "  (523, 0.30000001192092896),\n",
       "  (541, 0.30000001192092896),\n",
       "  (542, 0.30000001192092896),\n",
       "  (607, 0.30000001192092896),\n",
       "  (613, 0.30000001192092896),\n",
       "  (636, 0.30000001192092896),\n",
       "  (650, 0.30000001192092896),\n",
       "  (704, 0.30000001192092896),\n",
       "  (740, 0.30000001192092896),\n",
       "  (818, 0.30000001192092896),\n",
       "  (867, 0.30000001192092896),\n",
       "  (6, 0.20000000298023224),\n",
       "  (11, 0.20000000298023224),\n",
       "  (14, 0.20000000298023224),\n",
       "  (23, 0.20000000298023224),\n",
       "  (35, 0.20000000298023224),\n",
       "  (40, 0.20000000298023224),\n",
       "  (44, 0.20000000298023224),\n",
       "  (56, 0.20000000298023224),\n",
       "  (71, 0.20000000298023224),\n",
       "  (114, 0.20000000298023224),\n",
       "  (159, 0.20000000298023224),\n",
       "  (201, 0.20000000298023224),\n",
       "  (223, 0.20000000298023224),\n",
       "  (241, 0.20000000298023224),\n",
       "  (248, 0.20000000298023224),\n",
       "  (261, 0.20000000298023224),\n",
       "  (265, 0.20000000298023224),\n",
       "  (269, 0.20000000298023224),\n",
       "  (272, 0.20000000298023224),\n",
       "  (352, 0.20000000298023224),\n",
       "  (394, 0.20000000298023224),\n",
       "  (453, 0.20000000298023224),\n",
       "  (486, 0.20000000298023224),\n",
       "  (502, 0.20000000298023224),\n",
       "  (513, 0.20000000298023224),\n",
       "  (516, 0.20000000298023224),\n",
       "  (524, 0.20000000298023224),\n",
       "  (529, 0.20000000298023224),\n",
       "  (567, 0.20000000298023224),\n",
       "  (592, 0.20000000298023224),\n",
       "  (601, 0.20000000298023224),\n",
       "  (614, 0.20000000298023224),\n",
       "  (634, 0.20000000298023224),\n",
       "  (642, 0.20000000298023224),\n",
       "  (649, 0.20000000298023224),\n",
       "  (662, 0.20000000298023224),\n",
       "  (664, 0.20000000298023224),\n",
       "  (667, 0.20000000298023224),\n",
       "  (678, 0.20000000298023224),\n",
       "  (680, 0.20000000298023224),\n",
       "  (688, 0.20000000298023224),\n",
       "  (706, 0.20000000298023224),\n",
       "  (751, 0.20000000298023224),\n",
       "  (766, 0.20000000298023224),\n",
       "  (773, 0.20000000298023224),\n",
       "  (799, 0.20000000298023224),\n",
       "  (827, 0.20000000298023224),\n",
       "  (859, 0.20000000298023224),\n",
       "  (962, 0.20000000298023224),\n",
       "  (976, 0.20000000298023224),\n",
       "  (978, 0.20000000298023224),\n",
       "  (994, 0.20000000298023224),\n",
       "  (4, 0.10000000149011612),\n",
       "  (31, 0.10000000149011612),\n",
       "  (67, 0.10000000149011612),\n",
       "  (95, 0.10000000149011612),\n",
       "  (121, 0.10000000149011612),\n",
       "  (150, 0.10000000149011612),\n",
       "  (170, 0.10000000149011612),\n",
       "  (181, 0.10000000149011612),\n",
       "  (226, 0.10000000149011612),\n",
       "  (227, 0.10000000149011612),\n",
       "  (245, 0.10000000149011612),\n",
       "  (257, 0.10000000149011612),\n",
       "  (259, 0.10000000149011612),\n",
       "  (267, 0.10000000149011612),\n",
       "  (279, 0.10000000149011612),\n",
       "  (297, 0.10000000149011612),\n",
       "  (311, 0.10000000149011612),\n",
       "  (324, 0.10000000149011612),\n",
       "  (354, 0.10000000149011612),\n",
       "  (358, 0.10000000149011612),\n",
       "  (359, 0.10000000149011612),\n",
       "  (362, 0.10000000149011612),\n",
       "  (379, 0.10000000149011612),\n",
       "  (380, 0.10000000149011612),\n",
       "  (382, 0.10000000149011612),\n",
       "  (383, 0.10000000149011612),\n",
       "  (385, 0.10000000149011612),\n",
       "  (400, 0.10000000149011612),\n",
       "  (422, 0.10000000149011612),\n",
       "  (434, 0.10000000149011612),\n",
       "  (452, 0.10000000149011612),\n",
       "  (501, 0.10000000149011612),\n",
       "  (517, 0.10000000149011612),\n",
       "  (568, 0.10000000149011612),\n",
       "  (578, 0.10000000149011612),\n",
       "  (585, 0.10000000149011612),\n",
       "  (605, 0.10000000149011612),\n",
       "  (618, 0.10000000149011612),\n",
       "  (628, 0.10000000149011612),\n",
       "  (657, 0.10000000149011612),\n",
       "  (665, 0.10000000149011612),\n",
       "  (676, 0.10000000149011612),\n",
       "  (685, 0.10000000149011612),\n",
       "  (690, 0.10000000149011612),\n",
       "  (701, 0.10000000149011612),\n",
       "  (710, 0.10000000149011612),\n",
       "  (749, 0.10000000149011612),\n",
       "  (771, 0.10000000149011612),\n",
       "  (774, 0.10000000149011612),\n",
       "  (778, 0.10000000149011612),\n",
       "  (782, 0.10000000149011612),\n",
       "  (797, 0.10000000149011612),\n",
       "  (814, 0.10000000149011612),\n",
       "  (825, 0.10000000149011612),\n",
       "  (849, 0.10000000149011612),\n",
       "  (869, 0.10000000149011612),\n",
       "  (874, 0.10000000149011612),\n",
       "  (875, 0.10000000149011612),\n",
       "  (903, 0.10000000149011612),\n",
       "  (919, 0.10000000149011612),\n",
       "  (921, 0.10000000149011612),\n",
       "  (933, 0.10000000149011612),\n",
       "  (938, 0.10000000149011612),\n",
       "  (943, 0.10000000149011612),\n",
       "  (949, 0.10000000149011612),\n",
       "  (983, 0.10000000149011612),\n",
       "  (990, 0.10000000149011612),\n",
       "  (3, 0.0),\n",
       "  (13, 0.0),\n",
       "  (26, 0.0),\n",
       "  (27, 0.0),\n",
       "  (30, 0.0),\n",
       "  (43, 0.0),\n",
       "  (54, 0.0),\n",
       "  (59, 0.0),\n",
       "  (64, 0.0),\n",
       "  (66, 0.0),\n",
       "  (73, 0.0),\n",
       "  (80, 0.0),\n",
       "  (82, 0.0),\n",
       "  (103, 0.0),\n",
       "  (104, 0.0),\n",
       "  (106, 0.0),\n",
       "  (111, 0.0),\n",
       "  (127, 0.0),\n",
       "  (129, 0.0),\n",
       "  (131, 0.0),\n",
       "  (133, 0.0),\n",
       "  (137, 0.0),\n",
       "  (147, 0.0),\n",
       "  (149, 0.0),\n",
       "  (152, 0.0),\n",
       "  (153, 0.0),\n",
       "  (154, 0.0),\n",
       "  (156, 0.0),\n",
       "  (165, 0.0),\n",
       "  (167, 0.0),\n",
       "  (172, 0.0),\n",
       "  (174, 0.0),\n",
       "  (175, 0.0),\n",
       "  (177, 0.0),\n",
       "  (179, 0.0),\n",
       "  (185, 0.0),\n",
       "  (191, 0.0),\n",
       "  (194, 0.0),\n",
       "  (200, 0.0),\n",
       "  (210, 0.0),\n",
       "  (220, 0.0),\n",
       "  (225, 0.0),\n",
       "  (233, 0.0),\n",
       "  (239, 0.0),\n",
       "  (240, 0.0),\n",
       "  (244, 0.0),\n",
       "  (251, 0.0),\n",
       "  (255, 0.0),\n",
       "  (258, 0.0),\n",
       "  (262, 0.0),\n",
       "  (268, 0.0),\n",
       "  (277, 0.0),\n",
       "  (278, 0.0),\n",
       "  (283, 0.0),\n",
       "  (287, 0.0),\n",
       "  (299, 0.0),\n",
       "  (325, 0.0),\n",
       "  (333, 0.0),\n",
       "  (338, 0.0),\n",
       "  (339, 0.0),\n",
       "  (346, 0.0),\n",
       "  (349, 0.0),\n",
       "  (351, 0.0),\n",
       "  (356, 0.0),\n",
       "  (367, 0.0),\n",
       "  (368, 0.0),\n",
       "  (371, 0.0),\n",
       "  (373, 0.0),\n",
       "  (374, 0.0),\n",
       "  (404, 0.0),\n",
       "  (405, 0.0),\n",
       "  (416, 0.0),\n",
       "  (418, 0.0),\n",
       "  (421, 0.0),\n",
       "  (426, 0.0),\n",
       "  (427, 0.0),\n",
       "  (435, 0.0),\n",
       "  (437, 0.0),\n",
       "  (446, 0.0),\n",
       "  (449, 0.0),\n",
       "  (460, 0.0),\n",
       "  (462, 0.0),\n",
       "  (465, 0.0),\n",
       "  (466, 0.0),\n",
       "  (467, 0.0),\n",
       "  (469, 0.0),\n",
       "  (473, 0.0),\n",
       "  (475, 0.0),\n",
       "  (478, 0.0),\n",
       "  (479, 0.0),\n",
       "  (481, 0.0),\n",
       "  (485, 0.0),\n",
       "  (493, 0.0),\n",
       "  (494, 0.0),\n",
       "  (499, 0.0),\n",
       "  (500, 0.0),\n",
       "  (504, 0.0),\n",
       "  (510, 0.0),\n",
       "  (515, 0.0),\n",
       "  (519, 0.0),\n",
       "  (521, 0.0),\n",
       "  (522, 0.0),\n",
       "  (525, 0.0),\n",
       "  (536, 0.0),\n",
       "  (543, 0.0),\n",
       "  (548, 0.0),\n",
       "  (549, 0.0),\n",
       "  (551, 0.0),\n",
       "  (553, 0.0),\n",
       "  (557, 0.0),\n",
       "  (559, 0.0),\n",
       "  (569, 0.0),\n",
       "  (573, 0.0),\n",
       "  (583, 0.0),\n",
       "  (594, 0.0),\n",
       "  (596, 0.0),\n",
       "  (598, 0.0),\n",
       "  (600, 0.0),\n",
       "  (610, 0.0),\n",
       "  (617, 0.0),\n",
       "  (622, 0.0),\n",
       "  (623, 0.0),\n",
       "  (627, 0.0),\n",
       "  (630, 0.0),\n",
       "  (631, 0.0),\n",
       "  (648, 0.0),\n",
       "  (653, 0.0),\n",
       "  (659, 0.0),\n",
       "  (660, 0.0),\n",
       "  (666, 0.0),\n",
       "  (673, 0.0),\n",
       "  (675, 0.0),\n",
       "  (677, 0.0),\n",
       "  (681, 0.0),\n",
       "  (682, 0.0),\n",
       "  (686, 0.0),\n",
       "  (693, 0.0),\n",
       "  (705, 0.0),\n",
       "  (708, 0.0),\n",
       "  (713, 0.0),\n",
       "  (714, 0.0),\n",
       "  (717, 0.0),\n",
       "  (718, 0.0),\n",
       "  (722, 0.0),\n",
       "  (723, 0.0),\n",
       "  (726, 0.0),\n",
       "  (728, 0.0),\n",
       "  (730, 0.0),\n",
       "  (731, 0.0),\n",
       "  (733, 0.0),\n",
       "  (737, 0.0),\n",
       "  (739, 0.0),\n",
       "  (742, 0.0),\n",
       "  (744, 0.0),\n",
       "  (747, 0.0),\n",
       "  (756, 0.0),\n",
       "  (760, 0.0),\n",
       "  (767, 0.0),\n",
       "  (769, 0.0),\n",
       "  (780, 0.0),\n",
       "  (785, 0.0),\n",
       "  (789, 0.0),\n",
       "  (792, 0.0),\n",
       "  (798, 0.0),\n",
       "  (807, 0.0),\n",
       "  (810, 0.0),\n",
       "  (812, 0.0),\n",
       "  (813, 0.0),\n",
       "  (835, 0.0),\n",
       "  (836, 0.0),\n",
       "  (840, 0.0),\n",
       "  (841, 0.0),\n",
       "  (845, 0.0),\n",
       "  (846, 0.0),\n",
       "  (851, 0.0),\n",
       "  (856, 0.0),\n",
       "  (860, 0.0),\n",
       "  (861, 0.0),\n",
       "  (876, 0.0),\n",
       "  (880, 0.0),\n",
       "  (895, 0.0),\n",
       "  (899, 0.0),\n",
       "  (908, 0.0),\n",
       "  (909, 0.0),\n",
       "  (912, 0.0),\n",
       "  (913, 0.0),\n",
       "  (914, 0.0),\n",
       "  (916, 0.0),\n",
       "  (917, 0.0),\n",
       "  (922, 0.0),\n",
       "  (924, 0.0),\n",
       "  (926, 0.0),\n",
       "  (928, 0.0),\n",
       "  (929, 0.0),\n",
       "  (930, 0.0),\n",
       "  (931, 0.0),\n",
       "  (935, 0.0),\n",
       "  (936, 0.0),\n",
       "  (940, 0.0),\n",
       "  (941, 0.0),\n",
       "  (942, 0.0),\n",
       "  (945, 0.0),\n",
       "  (947, 0.0),\n",
       "  (948, 0.0),\n",
       "  (950, 0.0),\n",
       "  (951, 0.0),\n",
       "  (954, 0.0),\n",
       "  (958, 0.0),\n",
       "  (960, 0.0),\n",
       "  (961, 0.0),\n",
       "  (964, 0.0),\n",
       "  (965, 0.0),\n",
       "  (966, 0.0),\n",
       "  (967, 0.0),\n",
       "  (969, 0.0),\n",
       "  (970, 0.0),\n",
       "  (974, 0.0),\n",
       "  (975, 0.0),\n",
       "  (980, 0.0),\n",
       "  (986, 0.0),\n",
       "  (991, 0.0),\n",
       "  (993, 0.0),\n",
       "  (995, 0.0),\n",
       "  (998, 0.0),\n",
       "  (999, 0.0)])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch_no 0\n",
      "done with the 0th calculation: 717\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 718\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 720\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 718\n",
      "result for n_pert: 10 is 718.25\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 713\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 724\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 715\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 719\n",
      "result for n_pert: 20 is 717.75\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 712\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 722\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 720\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 717\n",
      "result for n_pert: 30 is 717.75\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 716\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 717\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 713\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 714\n",
      "result for n_pert: 40 is 715.0\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 717\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 723\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 722\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 720\n",
      "result for n_pert: 50 is 720.5\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 719\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 712\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 712\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 720\n",
      "result for n_pert: 60 is 715.75\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 723\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 721\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 722\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 711\n",
      "result for n_pert: 70 is 719.25\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 715\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 715\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 717\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 715\n",
      "result for n_pert: 80 is 715.5\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 721\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 716\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 716\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 714\n",
      "result for n_pert: 90 is 716.75\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 718\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 719\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 724\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 723\n",
      "result for n_pert: 100 is 721.0\n"
     ]
    }
   ],
   "source": [
    "results_1 = []\n",
    "for n_pert in range(10, 110, 10):\n",
    "  n = targeted_diversity_average(learn, n_pert, 95, 4)\n",
    "  print(f'result for n_pert: {n_pert} is {n}')\n",
    "  results_1.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7feac1262b70>]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(10, 110, 10))\n",
    "plt.ylim(0, 800)\n",
    "plt.plot(x, results_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 548\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 524\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 519\n",
      "result for n_pert: 10 is 530.3333333333334\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 524\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 546\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 518\n",
      "result for n_pert: 20 is 529.3333333333334\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 561\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 543\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 523\n",
      "result for n_pert: 30 is 542.3333333333334\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 571\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 548\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 548\n",
      "result for n_pert: 40 is 555.6666666666666\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 552\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 535\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 538\n",
      "result for n_pert: 50 is 541.6666666666666\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 558\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 540\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 540\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 534\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 534\n",
      "result for n_pert: 70 is 536.0\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 547\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 521\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 542\n",
      "result for n_pert: 80 is 536.6666666666666\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 539\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 564\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 530\n",
      "result for n_pert: 90 is 544.3333333333334\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 550\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 540\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 540\n",
      "result for n_pert: 100 is 543.3333333333334\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for n_pert in range(10, 110, 10):\n",
    "  n = diversity_average(learn, n_pert, 95, 3)\n",
    "  print(f'result for n_pert: {n_pert} is {n}')\n",
    "  results.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fea6868afd0>]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(10, 110, 10))\n",
    "plt.ylim(0, 600)\n",
    "plt.plot(x, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#targeted:\n",
    "targeted_div_metrics = results_1\n",
    "div_metrics = results\n",
    "\n",
    "#non-targeted:\n",
    "n_targeted_div_metrics = [244.0, 247.0, 265.3333333333333, 246.66666666666666, 241.0, 231.33333333333334, \n",
    "                          247.66666666666666, 229.0, 222.33333333333334, 236.0]\n",
    "n_div_metrics = [132, 118, 122, 135, 133, 129, 136, 132, 124, 143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, targeted_div_metrics)\n",
    "plt.plot(x, n_targeted_div_metrics)\n",
    "plt.title('targeted_div_metric')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, div_metrics)\n",
    "plt.plot(x, n_div_metrics)\n",
    "plt.title('div_metric')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 5\n",
      "at batch no 10\n",
      "at batch no 15\n",
      "at batch no 20\n",
      "at batch no 25\n",
      "at batch no 30\n",
      "at batch no 35\n",
      "at batch no 40\n",
      "at batch no 45\n",
      "at batch no 50\n",
      "at batch no 55\n",
      "at batch no 60\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(364,\n",
       " [(721, 143.20),\n",
       "  (750, 49.40),\n",
       "  (971, 48.40),\n",
       "  (794, 47.10),\n",
       "  (431, 35.10),\n",
       "  (669, 35.10),\n",
       "  (414, 30.60),\n",
       "  (588, 28.40),\n",
       "  (520, 24.90),\n",
       "  (61, 24.20),\n",
       "  (904, 18.30),\n",
       "  (411, 14.30),\n",
       "  (828, 13.50),\n",
       "  (39, 11.70),\n",
       "  (556, 11.40),\n",
       "  (581, 9.20),\n",
       "  (651, 8.30),\n",
       "  (489, 7.80),\n",
       "  (599, 6.60),\n",
       "  (84, 5.50),\n",
       "  (572, 4.80),\n",
       "  (907, 4.70),\n",
       "  (987, 4.40),\n",
       "  (401, 4.30),\n",
       "  (490, 4.30),\n",
       "  (614, 4.30),\n",
       "  (60, 4.00),\n",
       "  (955, 3.50),\n",
       "  (711, 3.30),\n",
       "  (48, 3.20),\n",
       "  (691, 3.10),\n",
       "  (709, 3.00),\n",
       "  (419, 2.90),\n",
       "  (770, 2.90),\n",
       "  (815, 2.90),\n",
       "  (864, 2.80),\n",
       "  (879, 2.80),\n",
       "  (108, 2.70),\n",
       "  (441, 2.70),\n",
       "  (632, 2.70),\n",
       "  (56, 2.50),\n",
       "  (604, 2.50),\n",
       "  (55, 2.40),\n",
       "  (464, 2.30),\n",
       "  (549, 2.30),\n",
       "  (575, 2.20),\n",
       "  (893, 2.20),\n",
       "  (973, 2.20),\n",
       "  (96, 2.10),\n",
       "  (496, 2.10),\n",
       "  (518, 2.10),\n",
       "  (412, 2.00),\n",
       "  (641, 2.00),\n",
       "  (762, 2.00),\n",
       "  (801, 2.00),\n",
       "  (872, 2.00),\n",
       "  (858, 1.90),\n",
       "  (871, 1.90),\n",
       "  (580, 1.80),\n",
       "  (621, 1.80),\n",
       "  (746, 1.80),\n",
       "  (806, 1.80),\n",
       "  (46, 1.70),\n",
       "  (151, 1.70),\n",
       "  (171, 1.70),\n",
       "  (633, 1.70),\n",
       "  (781, 1.70),\n",
       "  (790, 1.70),\n",
       "  (850, 1.70),\n",
       "  (424, 1.60),\n",
       "  (443, 1.60),\n",
       "  (453, 1.60),\n",
       "  (646, 1.60),\n",
       "  (981, 1.60),\n",
       "  (128, 1.50),\n",
       "  (360, 1.50),\n",
       "  (457, 1.50),\n",
       "  (545, 1.50),\n",
       "  (680, 1.50),\n",
       "  (722, 1.50),\n",
       "  (743, 1.50),\n",
       "  (1, 1.40),\n",
       "  (94, 1.40),\n",
       "  (230, 1.40),\n",
       "  (292, 1.40),\n",
       "  (406, 1.40),\n",
       "  (410, 1.40),\n",
       "  (440, 1.40),\n",
       "  (506, 1.40),\n",
       "  (847, 1.40),\n",
       "  (897, 1.40),\n",
       "  (67, 1.30),\n",
       "  (68, 1.30),\n",
       "  (124, 1.30),\n",
       "  (417, 1.30),\n",
       "  (538, 1.30),\n",
       "  (706, 1.30),\n",
       "  (779, 1.30),\n",
       "  (791, 1.30),\n",
       "  (826, 1.30),\n",
       "  (865, 1.30),\n",
       "  (898, 1.30),\n",
       "  (937, 1.30),\n",
       "  (953, 1.30),\n",
       "  (242, 1.20),\n",
       "  (310, 1.20),\n",
       "  (408, 1.20),\n",
       "  (582, 1.20),\n",
       "  (591, 1.20),\n",
       "  (698, 1.20),\n",
       "  (857, 1.20),\n",
       "  (896, 1.20),\n",
       "  (963, 1.20),\n",
       "  (83, 1.10),\n",
       "  (90, 1.10),\n",
       "  (92, 1.10),\n",
       "  (123, 1.10),\n",
       "  (293, 1.10),\n",
       "  (300, 1.10),\n",
       "  (316, 1.10),\n",
       "  (393, 1.10),\n",
       "  (407, 1.10),\n",
       "  (468, 1.10),\n",
       "  (612, 1.10),\n",
       "  (619, 1.10),\n",
       "  (645, 1.10),\n",
       "  (656, 1.10),\n",
       "  (738, 1.10),\n",
       "  (783, 1.10),\n",
       "  (805, 1.10),\n",
       "  (817, 1.10),\n",
       "  (906, 1.10),\n",
       "  (7, 1.00),\n",
       "  (25, 1.00),\n",
       "  (31, 1.00),\n",
       "  (33, 1.00),\n",
       "  (37, 1.00),\n",
       "  (57, 1.00),\n",
       "  (75, 1.00),\n",
       "  (88, 1.00),\n",
       "  (91, 1.00),\n",
       "  (102, 1.00),\n",
       "  (115, 1.00),\n",
       "  (120, 1.00),\n",
       "  (163, 1.00),\n",
       "  (176, 1.00),\n",
       "  (186, 1.00),\n",
       "  (195, 1.00),\n",
       "  (218, 1.00),\n",
       "  (231, 1.00),\n",
       "  (247, 1.00),\n",
       "  (260, 1.00),\n",
       "  (274, 1.00),\n",
       "  (281, 1.00),\n",
       "  (290, 1.00),\n",
       "  (306, 1.00),\n",
       "  (307, 1.00),\n",
       "  (314, 1.00),\n",
       "  (318, 1.00),\n",
       "  (334, 1.00),\n",
       "  (376, 1.00),\n",
       "  (392, 1.00),\n",
       "  (429, 1.00),\n",
       "  (474, 1.00),\n",
       "  (483, 1.00),\n",
       "  (488, 1.00),\n",
       "  (492, 1.00),\n",
       "  (498, 1.00),\n",
       "  (507, 1.00),\n",
       "  (516, 1.00),\n",
       "  (528, 1.00),\n",
       "  (530, 1.00),\n",
       "  (533, 1.00),\n",
       "  (562, 1.00),\n",
       "  (566, 1.00),\n",
       "  (602, 1.00),\n",
       "  (625, 1.00),\n",
       "  (637, 1.00),\n",
       "  (661, 1.00),\n",
       "  (684, 1.00),\n",
       "  (694, 1.00),\n",
       "  (716, 1.00),\n",
       "  (725, 1.00),\n",
       "  (734, 1.00),\n",
       "  (787, 1.00),\n",
       "  (816, 1.00),\n",
       "  (822, 1.00),\n",
       "  (824, 1.00),\n",
       "  (837, 1.00),\n",
       "  (873, 1.00),\n",
       "  (923, 1.00),\n",
       "  (934, 1.00),\n",
       "  (944, 1.00),\n",
       "  (992, 1.00),\n",
       "  (997, 1.00),\n",
       "  (40, 0.90),\n",
       "  (62, 0.90),\n",
       "  (105, 0.90),\n",
       "  (164, 0.90),\n",
       "  (301, 0.90),\n",
       "  (347, 0.90),\n",
       "  (369, 0.90),\n",
       "  (397, 0.90),\n",
       "  (425, 0.90),\n",
       "  (444, 0.90),\n",
       "  (753, 0.90),\n",
       "  (819, 0.90),\n",
       "  (868, 0.90),\n",
       "  (889, 0.90),\n",
       "  (982, 0.90),\n",
       "  (47, 0.80),\n",
       "  (72, 0.80),\n",
       "  (86, 0.80),\n",
       "  (97, 0.80),\n",
       "  (118, 0.80),\n",
       "  (254, 0.80),\n",
       "  (275, 0.80),\n",
       "  (304, 0.80),\n",
       "  (331, 0.80),\n",
       "  (463, 0.80),\n",
       "  (509, 0.80),\n",
       "  (532, 0.80),\n",
       "  (576, 0.80),\n",
       "  (586, 0.80),\n",
       "  (606, 0.80),\n",
       "  (638, 0.80),\n",
       "  (703, 0.80),\n",
       "  (732, 0.80),\n",
       "  (772, 0.80),\n",
       "  (786, 0.80),\n",
       "  (803, 0.80),\n",
       "  (829, 0.80),\n",
       "  (878, 0.80),\n",
       "  (899, 0.80),\n",
       "  (905, 0.80),\n",
       "  (915, 0.80),\n",
       "  (984, 0.80),\n",
       "  (8, 0.70),\n",
       "  (15, 0.70),\n",
       "  (38, 0.70),\n",
       "  (42, 0.70),\n",
       "  (109, 0.70),\n",
       "  (116, 0.70),\n",
       "  (144, 0.70),\n",
       "  (155, 0.70),\n",
       "  (189, 0.70),\n",
       "  (235, 0.70),\n",
       "  (313, 0.70),\n",
       "  (341, 0.70),\n",
       "  (353, 0.70),\n",
       "  (355, 0.70),\n",
       "  (387, 0.70),\n",
       "  (438, 0.70),\n",
       "  (476, 0.70),\n",
       "  (497, 0.70),\n",
       "  (508, 0.70),\n",
       "  (517, 0.70),\n",
       "  (526, 0.70),\n",
       "  (547, 0.70),\n",
       "  (552, 0.70),\n",
       "  (564, 0.70),\n",
       "  (570, 0.70),\n",
       "  (579, 0.70),\n",
       "  (620, 0.70),\n",
       "  (629, 0.70),\n",
       "  (727, 0.70),\n",
       "  (741, 0.70),\n",
       "  (752, 0.70),\n",
       "  (778, 0.70),\n",
       "  (788, 0.70),\n",
       "  (814, 0.70),\n",
       "  (925, 0.70),\n",
       "  (17, 0.60),\n",
       "  (19, 0.60),\n",
       "  (87, 0.60),\n",
       "  (289, 0.60),\n",
       "  (291, 0.60),\n",
       "  (294, 0.60),\n",
       "  (327, 0.60),\n",
       "  (375, 0.60),\n",
       "  (398, 0.60),\n",
       "  (409, 0.60),\n",
       "  (433, 0.60),\n",
       "  (445, 0.60),\n",
       "  (472, 0.60),\n",
       "  (482, 0.60),\n",
       "  (535, 0.60),\n",
       "  (544, 0.60),\n",
       "  (565, 0.60),\n",
       "  (671, 0.60),\n",
       "  (679, 0.60),\n",
       "  (696, 0.60),\n",
       "  (701, 0.60),\n",
       "  (751, 0.60),\n",
       "  (760, 0.60),\n",
       "  (796, 0.60),\n",
       "  (797, 0.60),\n",
       "  (820, 0.60),\n",
       "  (867, 0.60),\n",
       "  (892, 0.60),\n",
       "  (902, 0.60),\n",
       "  (920, 0.60),\n",
       "  (998, 0.60),\n",
       "  (0, 0.50),\n",
       "  (45, 0.50),\n",
       "  (50, 0.50),\n",
       "  (52, 0.50),\n",
       "  (107, 0.50),\n",
       "  (149, 0.50),\n",
       "  (159, 0.50),\n",
       "  (336, 0.50),\n",
       "  (342, 0.50),\n",
       "  (348, 0.50),\n",
       "  (391, 0.50),\n",
       "  (495, 0.50),\n",
       "  (515, 0.50),\n",
       "  (523, 0.50),\n",
       "  (527, 0.50),\n",
       "  (539, 0.50),\n",
       "  (555, 0.50),\n",
       "  (605, 0.50),\n",
       "  (607, 0.50),\n",
       "  (609, 0.50),\n",
       "  (626, 0.50),\n",
       "  (654, 0.50),\n",
       "  (655, 0.50),\n",
       "  (664, 0.50),\n",
       "  (674, 0.50),\n",
       "  (705, 0.50),\n",
       "  (719, 0.50),\n",
       "  (745, 0.50),\n",
       "  (754, 0.50),\n",
       "  (759, 0.50),\n",
       "  (768, 0.50),\n",
       "  (823, 0.50),\n",
       "  (853, 0.50),\n",
       "  (900, 0.50),\n",
       "  (917, 0.50),\n",
       "  (985, 0.50),\n",
       "  (9, 0.40),\n",
       "  (23, 0.40),\n",
       "  (24, 0.40),\n",
       "  (28, 0.40),\n",
       "  (63, 0.40),\n",
       "  (77, 0.40),\n",
       "  (126, 0.40),\n",
       "  (134, 0.40),\n",
       "  (140, 0.40),\n",
       "  (168, 0.40),\n",
       "  (192, 0.40),\n",
       "  (197, 0.40),\n",
       "  (205, 0.40),\n",
       "  (219, 0.40),\n",
       "  (224, 0.40),\n",
       "  (236, 0.40),\n",
       "  (249, 0.40),\n",
       "  (305, 0.40),\n",
       "  (319, 0.40),\n",
       "  (321, 0.40),\n",
       "  (332, 0.40),\n",
       "  (363, 0.40),\n",
       "  (381, 0.40),\n",
       "  (383, 0.40),\n",
       "  (388, 0.40),\n",
       "  (389, 0.40),\n",
       "  (396, 0.40),\n",
       "  (473, 0.40),\n",
       "  (481, 0.40),\n",
       "  (491, 0.40),\n",
       "  (522, 0.40),\n",
       "  (546, 0.40),\n",
       "  (574, 0.40),\n",
       "  (584, 0.40),\n",
       "  (672, 0.40),\n",
       "  (692, 0.40),\n",
       "  (697, 0.40),\n",
       "  (712, 0.40),\n",
       "  (802, 0.40),\n",
       "  (843, 0.40),\n",
       "  (854, 0.40),\n",
       "  (870, 0.40),\n",
       "  (880, 0.40),\n",
       "  (882, 0.40),\n",
       "  (890, 0.40),\n",
       "  (918, 0.40),\n",
       "  (938, 0.40),\n",
       "  (991, 0.40),\n",
       "  (41, 0.30),\n",
       "  (65, 0.30),\n",
       "  (71, 0.30),\n",
       "  (74, 0.30),\n",
       "  (113, 0.30),\n",
       "  (183, 0.30),\n",
       "  (191, 0.30),\n",
       "  (196, 0.30),\n",
       "  (202, 0.30),\n",
       "  (228, 0.30),\n",
       "  (232, 0.30),\n",
       "  (253, 0.30),\n",
       "  (303, 0.30),\n",
       "  (320, 0.30),\n",
       "  (337, 0.30),\n",
       "  (350, 0.30),\n",
       "  (358, 0.30),\n",
       "  (364, 0.30),\n",
       "  (399, 0.30),\n",
       "  (452, 0.30),\n",
       "  (454, 0.30),\n",
       "  (477, 0.30),\n",
       "  (480, 0.30),\n",
       "  (512, 0.30),\n",
       "  (537, 0.30),\n",
       "  (550, 0.30),\n",
       "  (554, 0.30),\n",
       "  (560, 0.30),\n",
       "  (593, 0.30),\n",
       "  (640, 0.30),\n",
       "  (644, 0.30),\n",
       "  (683, 0.30),\n",
       "  (707, 0.30),\n",
       "  (720, 0.30),\n",
       "  (729, 0.30),\n",
       "  (748, 0.30),\n",
       "  (757, 0.30),\n",
       "  (758, 0.30),\n",
       "  (777, 0.30),\n",
       "  (811, 0.30),\n",
       "  (831, 0.30),\n",
       "  (840, 0.30),\n",
       "  (846, 0.30),\n",
       "  (875, 0.30),\n",
       "  (883, 0.30),\n",
       "  (932, 0.30),\n",
       "  (939, 0.30),\n",
       "  (951, 0.30),\n",
       "  (957, 0.30),\n",
       "  (968, 0.30),\n",
       "  (995, 0.30),\n",
       "  (58, 0.20),\n",
       "  (82, 0.20),\n",
       "  (93, 0.20),\n",
       "  (98, 0.20),\n",
       "  (99, 0.20),\n",
       "  (100, 0.20),\n",
       "  (117, 0.20),\n",
       "  (122, 0.20),\n",
       "  (131, 0.20),\n",
       "  (138, 0.20),\n",
       "  (153, 0.20),\n",
       "  (161, 0.20),\n",
       "  (178, 0.20),\n",
       "  (188, 0.20),\n",
       "  (198, 0.20),\n",
       "  (206, 0.20),\n",
       "  (238, 0.20),\n",
       "  (283, 0.20),\n",
       "  (284, 0.20),\n",
       "  (317, 0.20),\n",
       "  (323, 0.20),\n",
       "  (326, 0.20),\n",
       "  (340, 0.20),\n",
       "  (344, 0.20),\n",
       "  (379, 0.20),\n",
       "  (395, 0.20),\n",
       "  (420, 0.20),\n",
       "  (432, 0.20),\n",
       "  (435, 0.20),\n",
       "  (436, 0.20),\n",
       "  (455, 0.20),\n",
       "  (484, 0.20),\n",
       "  (485, 0.20),\n",
       "  (487, 0.20),\n",
       "  (502, 0.20),\n",
       "  (505, 0.20),\n",
       "  (514, 0.20),\n",
       "  (534, 0.20),\n",
       "  (557, 0.20),\n",
       "  (585, 0.20),\n",
       "  (595, 0.20),\n",
       "  (618, 0.20),\n",
       "  (635, 0.20),\n",
       "  (643, 0.20),\n",
       "  (681, 0.20),\n",
       "  (700, 0.20),\n",
       "  (717, 0.20),\n",
       "  (723, 0.20),\n",
       "  (728, 0.20),\n",
       "  (736, 0.20),\n",
       "  (747, 0.20),\n",
       "  (785, 0.20),\n",
       "  (800, 0.20),\n",
       "  (808, 0.20),\n",
       "  (809, 0.20),\n",
       "  (818, 0.20),\n",
       "  (821, 0.20),\n",
       "  (825, 0.20),\n",
       "  (832, 0.20),\n",
       "  (844, 0.20),\n",
       "  (852, 0.20),\n",
       "  (876, 0.20),\n",
       "  (881, 0.20),\n",
       "  (926, 0.20),\n",
       "  (962, 0.20),\n",
       "  (966, 0.20),\n",
       "  (996, 0.20),\n",
       "  (11, 0.10),\n",
       "  (18, 0.10),\n",
       "  (21, 0.10),\n",
       "  (22, 0.10),\n",
       "  (26, 0.10),\n",
       "  (66, 0.10),\n",
       "  (70, 0.10),\n",
       "  (76, 0.10),\n",
       "  (79, 0.10),\n",
       "  (95, 0.10),\n",
       "  (111, 0.10),\n",
       "  (114, 0.10),\n",
       "  (121, 0.10),\n",
       "  (125, 0.10),\n",
       "  (129, 0.10),\n",
       "  (132, 0.10),\n",
       "  (139, 0.10),\n",
       "  (141, 0.10),\n",
       "  (142, 0.10),\n",
       "  (146, 0.10),\n",
       "  (158, 0.10),\n",
       "  (173, 0.10),\n",
       "  (179, 0.10),\n",
       "  (193, 0.10),\n",
       "  (214, 0.10),\n",
       "  (222, 0.10),\n",
       "  (229, 0.10),\n",
       "  (237, 0.10),\n",
       "  (252, 0.10),\n",
       "  (256, 0.10),\n",
       "  (268, 0.10),\n",
       "  (273, 0.10),\n",
       "  (276, 0.10),\n",
       "  (278, 0.10),\n",
       "  (282, 0.10),\n",
       "  (295, 0.10),\n",
       "  (298, 0.10),\n",
       "  (308, 0.10),\n",
       "  (309, 0.10),\n",
       "  (311, 0.10),\n",
       "  (315, 0.10),\n",
       "  (330, 0.10),\n",
       "  (343, 0.10),\n",
       "  (346, 0.10),\n",
       "  (351, 0.10),\n",
       "  (352, 0.10),\n",
       "  (356, 0.10),\n",
       "  (361, 0.10),\n",
       "  (365, 0.10),\n",
       "  (366, 0.10),\n",
       "  (377, 0.10),\n",
       "  (413, 0.10),\n",
       "  (423, 0.10),\n",
       "  (427, 0.10),\n",
       "  (428, 0.10),\n",
       "  (439, 0.10),\n",
       "  (442, 0.10),\n",
       "  (447, 0.10),\n",
       "  (448, 0.10),\n",
       "  (450, 0.10),\n",
       "  (451, 0.10),\n",
       "  (459, 0.10),\n",
       "  (475, 0.10),\n",
       "  (486, 0.10),\n",
       "  (493, 0.10),\n",
       "  (504, 0.10),\n",
       "  (521, 0.10),\n",
       "  (540, 0.10),\n",
       "  (551, 0.10),\n",
       "  (563, 0.10),\n",
       "  (569, 0.10),\n",
       "  (577, 0.10),\n",
       "  (603, 0.10),\n",
       "  (611, 0.10),\n",
       "  (613, 0.10),\n",
       "  (615, 0.10),\n",
       "  (616, 0.10),\n",
       "  (636, 0.10),\n",
       "  (639, 0.10),\n",
       "  (650, 0.10),\n",
       "  (665, 0.10),\n",
       "  (668, 0.10),\n",
       "  (673, 0.10),\n",
       "  (730, 0.10),\n",
       "  (735, 0.10),\n",
       "  (755, 0.10),\n",
       "  (756, 0.10),\n",
       "  (761, 0.10),\n",
       "  (763, 0.10),\n",
       "  (764, 0.10),\n",
       "  (765, 0.10),\n",
       "  (775, 0.10),\n",
       "  (792, 0.10),\n",
       "  (804, 0.10),\n",
       "  (813, 0.10),\n",
       "  (830, 0.10),\n",
       "  (833, 0.10),\n",
       "  (834, 0.10),\n",
       "  (836, 0.10),\n",
       "  (839, 0.10),\n",
       "  (842, 0.10),\n",
       "  (855, 0.10),\n",
       "  (856, 0.10),\n",
       "  (863, 0.10),\n",
       "  (866, 0.10),\n",
       "  (884, 0.10),\n",
       "  (885, 0.10),\n",
       "  (901, 0.10),\n",
       "  (927, 0.10),\n",
       "  (946, 0.10),\n",
       "  (950, 0.10),\n",
       "  (952, 0.10),\n",
       "  (954, 0.10),\n",
       "  (978, 0.10),\n",
       "  (983, 0.10),\n",
       "  (988, 0.10),\n",
       "  (999, 0.10),\n",
       "  (2, 0.00),\n",
       "  (3, 0.00),\n",
       "  (4, 0.00),\n",
       "  (5, 0.00),\n",
       "  (6, 0.00),\n",
       "  (10, 0.00),\n",
       "  (12, 0.00),\n",
       "  (13, 0.00),\n",
       "  (14, 0.00),\n",
       "  (16, 0.00),\n",
       "  (20, 0.00),\n",
       "  (27, 0.00),\n",
       "  (29, 0.00),\n",
       "  (30, 0.00),\n",
       "  (32, 0.00),\n",
       "  (34, 0.00),\n",
       "  (35, 0.00),\n",
       "  (36, 0.00),\n",
       "  (43, 0.00),\n",
       "  (44, 0.00),\n",
       "  (49, 0.00),\n",
       "  (51, 0.00),\n",
       "  (53, 0.00),\n",
       "  (54, 0.00),\n",
       "  (59, 0.00),\n",
       "  (64, 0.00),\n",
       "  (69, 0.00),\n",
       "  (73, 0.00),\n",
       "  (78, 0.00),\n",
       "  (80, 0.00),\n",
       "  (81, 0.00),\n",
       "  (85, 0.00),\n",
       "  (89, 0.00),\n",
       "  (101, 0.00),\n",
       "  (103, 0.00),\n",
       "  (104, 0.00),\n",
       "  (106, 0.00),\n",
       "  (110, 0.00),\n",
       "  (112, 0.00),\n",
       "  (119, 0.00),\n",
       "  (127, 0.00),\n",
       "  (130, 0.00),\n",
       "  (133, 0.00),\n",
       "  (135, 0.00),\n",
       "  (136, 0.00),\n",
       "  (137, 0.00),\n",
       "  (143, 0.00),\n",
       "  (145, 0.00),\n",
       "  (147, 0.00),\n",
       "  (148, 0.00),\n",
       "  (150, 0.00),\n",
       "  (152, 0.00),\n",
       "  (154, 0.00),\n",
       "  (156, 0.00),\n",
       "  (157, 0.00),\n",
       "  (160, 0.00),\n",
       "  (162, 0.00),\n",
       "  (165, 0.00),\n",
       "  (166, 0.00),\n",
       "  (167, 0.00),\n",
       "  (169, 0.00),\n",
       "  (170, 0.00),\n",
       "  (172, 0.00),\n",
       "  (174, 0.00),\n",
       "  (175, 0.00),\n",
       "  (177, 0.00),\n",
       "  (180, 0.00),\n",
       "  (181, 0.00),\n",
       "  (182, 0.00),\n",
       "  (184, 0.00),\n",
       "  (185, 0.00),\n",
       "  (187, 0.00),\n",
       "  (190, 0.00),\n",
       "  (194, 0.00),\n",
       "  (199, 0.00),\n",
       "  (200, 0.00),\n",
       "  (201, 0.00),\n",
       "  (203, 0.00),\n",
       "  (204, 0.00),\n",
       "  (207, 0.00),\n",
       "  (208, 0.00),\n",
       "  (209, 0.00),\n",
       "  (210, 0.00),\n",
       "  (211, 0.00),\n",
       "  (212, 0.00),\n",
       "  (213, 0.00),\n",
       "  (215, 0.00),\n",
       "  (216, 0.00),\n",
       "  (217, 0.00),\n",
       "  (220, 0.00),\n",
       "  (221, 0.00),\n",
       "  (223, 0.00),\n",
       "  (225, 0.00),\n",
       "  (226, 0.00),\n",
       "  (227, 0.00),\n",
       "  (233, 0.00),\n",
       "  (234, 0.00),\n",
       "  (239, 0.00),\n",
       "  (240, 0.00),\n",
       "  (241, 0.00),\n",
       "  (243, 0.00),\n",
       "  (244, 0.00),\n",
       "  (245, 0.00),\n",
       "  (246, 0.00),\n",
       "  (248, 0.00),\n",
       "  (250, 0.00),\n",
       "  (251, 0.00),\n",
       "  (255, 0.00),\n",
       "  (257, 0.00),\n",
       "  (258, 0.00),\n",
       "  (259, 0.00),\n",
       "  (261, 0.00),\n",
       "  (262, 0.00),\n",
       "  (263, 0.00),\n",
       "  (264, 0.00),\n",
       "  (265, 0.00),\n",
       "  (266, 0.00),\n",
       "  (267, 0.00),\n",
       "  (269, 0.00),\n",
       "  (270, 0.00),\n",
       "  (271, 0.00),\n",
       "  (272, 0.00),\n",
       "  (277, 0.00),\n",
       "  (279, 0.00),\n",
       "  (280, 0.00),\n",
       "  (285, 0.00),\n",
       "  (286, 0.00),\n",
       "  (287, 0.00),\n",
       "  (288, 0.00),\n",
       "  (296, 0.00),\n",
       "  (297, 0.00),\n",
       "  (299, 0.00),\n",
       "  (302, 0.00),\n",
       "  (312, 0.00),\n",
       "  (322, 0.00),\n",
       "  (324, 0.00),\n",
       "  (325, 0.00),\n",
       "  (328, 0.00),\n",
       "  (329, 0.00),\n",
       "  (333, 0.00),\n",
       "  (335, 0.00),\n",
       "  (338, 0.00),\n",
       "  (339, 0.00),\n",
       "  (345, 0.00),\n",
       "  (349, 0.00),\n",
       "  (354, 0.00),\n",
       "  (357, 0.00),\n",
       "  (359, 0.00),\n",
       "  (362, 0.00),\n",
       "  (367, 0.00),\n",
       "  (368, 0.00),\n",
       "  (370, 0.00),\n",
       "  (371, 0.00),\n",
       "  (372, 0.00),\n",
       "  (373, 0.00),\n",
       "  (374, 0.00),\n",
       "  (378, 0.00),\n",
       "  (380, 0.00),\n",
       "  (382, 0.00),\n",
       "  (384, 0.00),\n",
       "  (385, 0.00),\n",
       "  (386, 0.00),\n",
       "  (390, 0.00),\n",
       "  (394, 0.00),\n",
       "  (400, 0.00),\n",
       "  (402, 0.00),\n",
       "  (403, 0.00),\n",
       "  (404, 0.00),\n",
       "  (405, 0.00),\n",
       "  (415, 0.00),\n",
       "  (416, 0.00),\n",
       "  (418, 0.00),\n",
       "  (421, 0.00),\n",
       "  (422, 0.00),\n",
       "  (426, 0.00),\n",
       "  (430, 0.00),\n",
       "  (434, 0.00),\n",
       "  (437, 0.00),\n",
       "  (446, 0.00),\n",
       "  (449, 0.00),\n",
       "  (456, 0.00),\n",
       "  (458, 0.00),\n",
       "  (460, 0.00),\n",
       "  (461, 0.00),\n",
       "  (462, 0.00),\n",
       "  (465, 0.00),\n",
       "  (466, 0.00),\n",
       "  (467, 0.00),\n",
       "  (469, 0.00),\n",
       "  (470, 0.00),\n",
       "  (471, 0.00),\n",
       "  (478, 0.00),\n",
       "  (479, 0.00),\n",
       "  (494, 0.00),\n",
       "  (499, 0.00),\n",
       "  (500, 0.00),\n",
       "  (501, 0.00),\n",
       "  (503, 0.00),\n",
       "  (510, 0.00),\n",
       "  (511, 0.00),\n",
       "  (513, 0.00),\n",
       "  (519, 0.00),\n",
       "  (524, 0.00),\n",
       "  (525, 0.00),\n",
       "  (529, 0.00),\n",
       "  (531, 0.00),\n",
       "  (536, 0.00),\n",
       "  (541, 0.00),\n",
       "  (542, 0.00),\n",
       "  (543, 0.00),\n",
       "  (548, 0.00),\n",
       "  (553, 0.00),\n",
       "  (558, 0.00),\n",
       "  (559, 0.00),\n",
       "  (561, 0.00),\n",
       "  (567, 0.00),\n",
       "  (568, 0.00),\n",
       "  (571, 0.00),\n",
       "  (573, 0.00),\n",
       "  (578, 0.00),\n",
       "  (583, 0.00),\n",
       "  (587, 0.00),\n",
       "  (589, 0.00),\n",
       "  (590, 0.00),\n",
       "  (592, 0.00),\n",
       "  (594, 0.00),\n",
       "  (596, 0.00),\n",
       "  (597, 0.00),\n",
       "  (598, 0.00),\n",
       "  (600, 0.00),\n",
       "  (601, 0.00),\n",
       "  (608, 0.00),\n",
       "  (610, 0.00),\n",
       "  (617, 0.00),\n",
       "  (622, 0.00),\n",
       "  (623, 0.00),\n",
       "  (624, 0.00),\n",
       "  (627, 0.00),\n",
       "  (628, 0.00),\n",
       "  (630, 0.00),\n",
       "  (631, 0.00),\n",
       "  (634, 0.00),\n",
       "  (642, 0.00),\n",
       "  (647, 0.00),\n",
       "  (648, 0.00),\n",
       "  (649, 0.00),\n",
       "  (652, 0.00),\n",
       "  (653, 0.00),\n",
       "  (657, 0.00),\n",
       "  (658, 0.00),\n",
       "  (659, 0.00),\n",
       "  (660, 0.00),\n",
       "  (662, 0.00),\n",
       "  (663, 0.00),\n",
       "  (666, 0.00),\n",
       "  (667, 0.00),\n",
       "  (670, 0.00),\n",
       "  (675, 0.00),\n",
       "  (676, 0.00),\n",
       "  (677, 0.00),\n",
       "  (678, 0.00),\n",
       "  (682, 0.00),\n",
       "  (685, 0.00),\n",
       "  (686, 0.00),\n",
       "  (687, 0.00),\n",
       "  (688, 0.00),\n",
       "  (689, 0.00),\n",
       "  (690, 0.00),\n",
       "  (693, 0.00),\n",
       "  (695, 0.00),\n",
       "  (699, 0.00),\n",
       "  (702, 0.00),\n",
       "  (704, 0.00),\n",
       "  (708, 0.00),\n",
       "  (710, 0.00),\n",
       "  (713, 0.00),\n",
       "  (714, 0.00),\n",
       "  (715, 0.00),\n",
       "  (718, 0.00),\n",
       "  (724, 0.00),\n",
       "  (726, 0.00),\n",
       "  (731, 0.00),\n",
       "  (733, 0.00),\n",
       "  (737, 0.00),\n",
       "  (739, 0.00),\n",
       "  (740, 0.00),\n",
       "  (742, 0.00),\n",
       "  (744, 0.00),\n",
       "  (749, 0.00),\n",
       "  (766, 0.00),\n",
       "  (767, 0.00),\n",
       "  (769, 0.00),\n",
       "  (771, 0.00),\n",
       "  (773, 0.00),\n",
       "  (774, 0.00),\n",
       "  (776, 0.00),\n",
       "  (780, 0.00),\n",
       "  (782, 0.00),\n",
       "  (784, 0.00),\n",
       "  (789, 0.00),\n",
       "  (793, 0.00),\n",
       "  (795, 0.00),\n",
       "  (798, 0.00),\n",
       "  (799, 0.00),\n",
       "  (807, 0.00),\n",
       "  (810, 0.00),\n",
       "  (812, 0.00),\n",
       "  (827, 0.00),\n",
       "  (835, 0.00),\n",
       "  (838, 0.00),\n",
       "  (841, 0.00),\n",
       "  (845, 0.00),\n",
       "  (848, 0.00),\n",
       "  (849, 0.00),\n",
       "  (851, 0.00),\n",
       "  (859, 0.00),\n",
       "  (860, 0.00),\n",
       "  (861, 0.00),\n",
       "  (862, 0.00),\n",
       "  (869, 0.00),\n",
       "  (874, 0.00),\n",
       "  (877, 0.00),\n",
       "  (886, 0.00),\n",
       "  (887, 0.00),\n",
       "  (888, 0.00),\n",
       "  (891, 0.00),\n",
       "  (894, 0.00),\n",
       "  (895, 0.00),\n",
       "  (903, 0.00),\n",
       "  (908, 0.00),\n",
       "  (909, 0.00),\n",
       "  (910, 0.00),\n",
       "  (911, 0.00),\n",
       "  (912, 0.00),\n",
       "  (913, 0.00),\n",
       "  (914, 0.00),\n",
       "  (916, 0.00),\n",
       "  (919, 0.00),\n",
       "  (921, 0.00),\n",
       "  (922, 0.00),\n",
       "  (924, 0.00),\n",
       "  (928, 0.00),\n",
       "  (929, 0.00),\n",
       "  (930, 0.00),\n",
       "  (931, 0.00),\n",
       "  (933, 0.00),\n",
       "  (935, 0.00),\n",
       "  (936, 0.00),\n",
       "  (940, 0.00),\n",
       "  (941, 0.00),\n",
       "  (942, 0.00),\n",
       "  (943, 0.00),\n",
       "  (945, 0.00),\n",
       "  (947, 0.00),\n",
       "  (948, 0.00),\n",
       "  (949, 0.00),\n",
       "  (956, 0.00),\n",
       "  (958, 0.00),\n",
       "  (959, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (964, 0.00),\n",
       "  (965, 0.00),\n",
       "  (967, 0.00),\n",
       "  (969, 0.00),\n",
       "  (970, 0.00),\n",
       "  (972, 0.00),\n",
       "  (974, 0.00),\n",
       "  (975, 0.00),\n",
       "  (976, 0.00),\n",
       "  (977, 0.00),\n",
       "  (979, 0.00),\n",
       "  (980, 0.00),\n",
       "  (986, 0.00),\n",
       "  (989, 0.00),\n",
       "  (990, 0.00),\n",
       "  (993, 0.00),\n",
       "  (994, 0.00)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f59cf2afe48>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_hist = sorted(hist, key=lambda x: x[0], reverse = False)\n",
    "values = [elem[1] for elem in sorted_hist]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.6468)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch_no 0\n",
      "at batch_no 100\n",
      "at batch_no 200\n",
      "at batch_no 300\n",
      "at batch_no 400\n",
      "at batch_no 500\n",
      "at batch_no 600\n",
      "at batch_no 700\n",
      "at batch_no 800\n",
      "at batch_no 900\n",
      "at batch_no 1000\n",
      "at batch_no 1100\n",
      "at batch_no 1200\n",
      "at batch_no 1300\n",
      "at batch_no 1400\n",
      "at batch_no 1500\n",
      "at batch_no 1600\n",
      "at batch_no 1700\n",
      "at batch_no 1800\n",
      "at batch_no 1900\n",
      "at batch_no 2000\n",
      "at batch_no 2100\n",
      "at batch_no 2200\n",
      "at batch_no 2300\n",
      "at batch_no 2400\n",
      "at batch_no 2500\n",
      "at batch_no 2600\n",
      "at batch_no 2700\n",
      "at batch_no 2800\n",
      "at batch_no 2900\n",
      "at batch_no 3000\n",
      "at batch_no 3100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(885,\n",
       " [(815, 234.00),\n",
       "  (652, 177.00),\n",
       "  (646, 163.00),\n",
       "  (580, 160.00),\n",
       "  (611, 156.00),\n",
       "  (489, 145.00),\n",
       "  (591, 144.00),\n",
       "  (621, 141.00),\n",
       "  (737, 139.00),\n",
       "  (904, 130.00),\n",
       "  (94, 129.00),\n",
       "  (868, 127.00),\n",
       "  (582, 125.00),\n",
       "  (497, 123.00),\n",
       "  (893, 120.00),\n",
       "  (794, 118.00),\n",
       "  (116, 115.00),\n",
       "  (955, 115.00),\n",
       "  (979, 114.00),\n",
       "  (679, 112.00),\n",
       "  (721, 109.00),\n",
       "  (39, 108.00),\n",
       "  (565, 108.00),\n",
       "  (741, 106.00),\n",
       "  (491, 105.00),\n",
       "  (562, 103.00),\n",
       "  (839, 102.00),\n",
       "  (109, 101.00),\n",
       "  (162, 101.00),\n",
       "  (549, 99.00),\n",
       "  (46, 97.00),\n",
       "  (48, 96.00),\n",
       "  (84, 95.00),\n",
       "  (750, 95.00),\n",
       "  (82, 94.00),\n",
       "  (973, 94.00),\n",
       "  (151, 93.00),\n",
       "  (492, 93.00),\n",
       "  (695, 93.00),\n",
       "  (199, 91.00),\n",
       "  (843, 89.00),\n",
       "  (51, 88.00),\n",
       "  (971, 88.00),\n",
       "  (640, 87.00),\n",
       "  (424, 86.00),\n",
       "  (669, 86.00),\n",
       "  (692, 86.00),\n",
       "  (879, 86.00),\n",
       "  (281, 85.00),\n",
       "  (47, 84.00),\n",
       "  (783, 84.00),\n",
       "  (203, 83.00),\n",
       "  (310, 83.00),\n",
       "  (382, 83.00),\n",
       "  (411, 83.00),\n",
       "  (866, 83.00),\n",
       "  (743, 82.00),\n",
       "  (364, 81.00),\n",
       "  (577, 81.00),\n",
       "  (197, 80.00),\n",
       "  (318, 80.00),\n",
       "  (319, 80.00),\n",
       "  (406, 80.00),\n",
       "  (725, 80.00),\n",
       "  (828, 80.00),\n",
       "  (180, 79.00),\n",
       "  (189, 79.00),\n",
       "  (208, 79.00),\n",
       "  (298, 79.00),\n",
       "  (703, 79.00),\n",
       "  (754, 79.00),\n",
       "  (905, 79.00),\n",
       "  (956, 79.00),\n",
       "  (847, 78.00),\n",
       "  (76, 77.00),\n",
       "  (182, 77.00),\n",
       "  (342, 77.00),\n",
       "  (455, 77.00),\n",
       "  (572, 77.00),\n",
       "  (711, 77.00),\n",
       "  (762, 77.00),\n",
       "  (896, 77.00),\n",
       "  (963, 77.00),\n",
       "  (217, 76.00),\n",
       "  (440, 76.00),\n",
       "  (824, 76.00),\n",
       "  (830, 76.00),\n",
       "  (55, 75.00),\n",
       "  (219, 75.00),\n",
       "  (270, 75.00),\n",
       "  (700, 75.00),\n",
       "  (730, 75.00),\n",
       "  (791, 75.00),\n",
       "  (128, 74.00),\n",
       "  (849, 74.00),\n",
       "  (938, 74.00),\n",
       "  (982, 74.00),\n",
       "  (237, 73.00),\n",
       "  (343, 73.00),\n",
       "  (363, 73.00),\n",
       "  (454, 73.00),\n",
       "  (570, 73.00),\n",
       "  (645, 73.00),\n",
       "  (735, 73.00),\n",
       "  (778, 73.00),\n",
       "  (825, 73.00),\n",
       "  (222, 72.00),\n",
       "  (304, 72.00),\n",
       "  (436, 72.00),\n",
       "  (483, 72.00),\n",
       "  (800, 72.00),\n",
       "  (826, 72.00),\n",
       "  (61, 71.00),\n",
       "  (74, 71.00),\n",
       "  (471, 71.00),\n",
       "  (472, 71.00),\n",
       "  (805, 71.00),\n",
       "  (806, 71.00),\n",
       "  (916, 71.00),\n",
       "  (30, 70.00),\n",
       "  (496, 70.00),\n",
       "  (716, 70.00),\n",
       "  (23, 69.00),\n",
       "  (341, 69.00),\n",
       "  (425, 69.00),\n",
       "  (775, 69.00),\n",
       "  (808, 69.00),\n",
       "  (878, 69.00),\n",
       "  (556, 68.00),\n",
       "  (620, 68.00),\n",
       "  (845, 68.00),\n",
       "  (184, 67.00),\n",
       "  (192, 67.00),\n",
       "  (195, 67.00),\n",
       "  (300, 67.00),\n",
       "  (361, 67.00),\n",
       "  (671, 67.00),\n",
       "  (887, 67.00),\n",
       "  (912, 67.00),\n",
       "  (37, 66.00),\n",
       "  (178, 66.00),\n",
       "  (313, 66.00),\n",
       "  (458, 66.00),\n",
       "  (654, 66.00),\n",
       "  (772, 66.00),\n",
       "  (820, 66.00),\n",
       "  (863, 66.00),\n",
       "  (870, 66.00),\n",
       "  (77, 65.00),\n",
       "  (124, 65.00),\n",
       "  (423, 65.00),\n",
       "  (655, 65.00),\n",
       "  (949, 65.00),\n",
       "  (988, 65.00),\n",
       "  (6, 64.00),\n",
       "  (211, 64.00),\n",
       "  (506, 64.00),\n",
       "  (688, 64.00),\n",
       "  (926, 64.00),\n",
       "  (972, 64.00),\n",
       "  (234, 63.00),\n",
       "  (272, 63.00),\n",
       "  (293, 63.00),\n",
       "  (481, 63.00),\n",
       "  (595, 63.00),\n",
       "  (852, 63.00),\n",
       "  (871, 63.00),\n",
       "  (895, 63.00),\n",
       "  (953, 63.00),\n",
       "  (975, 63.00),\n",
       "  (992, 63.00),\n",
       "  (90, 62.00),\n",
       "  (463, 62.00),\n",
       "  (505, 62.00),\n",
       "  (561, 62.00),\n",
       "  (697, 62.00),\n",
       "  (809, 62.00),\n",
       "  (864, 62.00),\n",
       "  (944, 62.00),\n",
       "  (987, 62.00),\n",
       "  (97, 61.00),\n",
       "  (99, 61.00),\n",
       "  (118, 61.00),\n",
       "  (125, 61.00),\n",
       "  (135, 61.00),\n",
       "  (155, 61.00),\n",
       "  (238, 61.00),\n",
       "  (292, 61.00),\n",
       "  (331, 61.00),\n",
       "  (468, 61.00),\n",
       "  (474, 61.00),\n",
       "  (597, 61.00),\n",
       "  (788, 61.00),\n",
       "  (834, 61.00),\n",
       "  (913, 61.00),\n",
       "  (50, 60.00),\n",
       "  (311, 60.00),\n",
       "  (401, 60.00),\n",
       "  (404, 60.00),\n",
       "  (420, 60.00),\n",
       "  (457, 60.00),\n",
       "  (476, 60.00),\n",
       "  (515, 60.00),\n",
       "  (532, 60.00),\n",
       "  (586, 60.00),\n",
       "  (594, 60.00),\n",
       "  (635, 60.00),\n",
       "  (649, 60.00),\n",
       "  (781, 60.00),\n",
       "  (850, 60.00),\n",
       "  (880, 60.00),\n",
       "  (892, 60.00),\n",
       "  (937, 60.00),\n",
       "  (946, 60.00),\n",
       "  (33, 59.00),\n",
       "  (129, 59.00),\n",
       "  (263, 59.00),\n",
       "  (372, 59.00),\n",
       "  (519, 59.00),\n",
       "  (564, 59.00),\n",
       "  (607, 59.00),\n",
       "  (724, 59.00),\n",
       "  (766, 59.00),\n",
       "  (770, 59.00),\n",
       "  (875, 59.00),\n",
       "  (957, 59.00),\n",
       "  (85, 58.00),\n",
       "  (119, 58.00),\n",
       "  (161, 58.00),\n",
       "  (181, 58.00),\n",
       "  (249, 58.00),\n",
       "  (259, 58.00),\n",
       "  (280, 58.00),\n",
       "  (348, 58.00),\n",
       "  (383, 58.00),\n",
       "  (441, 58.00),\n",
       "  (522, 58.00),\n",
       "  (539, 58.00),\n",
       "  (552, 58.00),\n",
       "  (563, 58.00),\n",
       "  (601, 58.00),\n",
       "  (619, 58.00),\n",
       "  (696, 58.00),\n",
       "  (748, 58.00),\n",
       "  (816, 58.00),\n",
       "  (21, 57.00),\n",
       "  (69, 57.00),\n",
       "  (89, 57.00),\n",
       "  (113, 57.00),\n",
       "  (115, 57.00),\n",
       "  (218, 57.00),\n",
       "  (232, 57.00),\n",
       "  (250, 57.00),\n",
       "  (284, 57.00),\n",
       "  (316, 57.00),\n",
       "  (407, 57.00),\n",
       "  (428, 57.00),\n",
       "  (488, 57.00),\n",
       "  (581, 57.00),\n",
       "  (603, 57.00),\n",
       "  (614, 57.00),\n",
       "  (626, 57.00),\n",
       "  (698, 57.00),\n",
       "  (774, 57.00),\n",
       "  (777, 57.00),\n",
       "  (784, 57.00),\n",
       "  (790, 57.00),\n",
       "  (842, 57.00),\n",
       "  (962, 57.00),\n",
       "  (8, 56.00),\n",
       "  (31, 56.00),\n",
       "  (57, 56.00),\n",
       "  (60, 56.00),\n",
       "  (171, 56.00),\n",
       "  (225, 56.00),\n",
       "  (275, 56.00),\n",
       "  (317, 56.00),\n",
       "  (334, 56.00),\n",
       "  (391, 56.00),\n",
       "  (443, 56.00),\n",
       "  (490, 56.00),\n",
       "  (527, 56.00),\n",
       "  (569, 56.00),\n",
       "  (593, 56.00),\n",
       "  (609, 56.00),\n",
       "  (642, 56.00),\n",
       "  (792, 56.00),\n",
       "  (819, 56.00),\n",
       "  (857, 56.00),\n",
       "  (924, 56.00),\n",
       "  (952, 56.00),\n",
       "  (24, 55.00),\n",
       "  (25, 55.00),\n",
       "  (67, 55.00),\n",
       "  (229, 55.00),\n",
       "  (231, 55.00),\n",
       "  (283, 55.00),\n",
       "  (291, 55.00),\n",
       "  (308, 55.00),\n",
       "  (328, 55.00),\n",
       "  (386, 55.00),\n",
       "  (396, 55.00),\n",
       "  (410, 55.00),\n",
       "  (509, 55.00),\n",
       "  (512, 55.00),\n",
       "  (612, 55.00),\n",
       "  (661, 55.00),\n",
       "  (822, 55.00),\n",
       "  (858, 55.00),\n",
       "  (884, 55.00),\n",
       "  (950, 55.00),\n",
       "  (985, 55.00),\n",
       "  (986, 55.00),\n",
       "  (991, 55.00),\n",
       "  (88, 54.00),\n",
       "  (159, 54.00),\n",
       "  (170, 54.00),\n",
       "  (206, 54.00),\n",
       "  (228, 54.00),\n",
       "  (241, 54.00),\n",
       "  (269, 54.00),\n",
       "  (276, 54.00),\n",
       "  (285, 54.00),\n",
       "  (327, 54.00),\n",
       "  (487, 54.00),\n",
       "  (547, 54.00),\n",
       "  (657, 54.00),\n",
       "  (709, 54.00),\n",
       "  (768, 54.00),\n",
       "  (780, 54.00),\n",
       "  (801, 54.00),\n",
       "  (832, 54.00),\n",
       "  (855, 54.00),\n",
       "  (936, 54.00),\n",
       "  (990, 54.00),\n",
       "  (995, 54.00),\n",
       "  (3, 53.00),\n",
       "  (35, 53.00),\n",
       "  (58, 53.00),\n",
       "  (70, 53.00),\n",
       "  (104, 53.00),\n",
       "  (138, 53.00),\n",
       "  (177, 53.00),\n",
       "  (251, 53.00),\n",
       "  (254, 53.00),\n",
       "  (274, 53.00),\n",
       "  (307, 53.00),\n",
       "  (367, 53.00),\n",
       "  (444, 53.00),\n",
       "  (452, 53.00),\n",
       "  (477, 53.00),\n",
       "  (508, 53.00),\n",
       "  (524, 53.00),\n",
       "  (526, 53.00),\n",
       "  (528, 53.00),\n",
       "  (533, 53.00),\n",
       "  (641, 53.00),\n",
       "  (653, 53.00),\n",
       "  (665, 53.00),\n",
       "  (668, 53.00),\n",
       "  (739, 53.00),\n",
       "  (818, 53.00),\n",
       "  (835, 53.00),\n",
       "  (888, 53.00),\n",
       "  (903, 53.00),\n",
       "  (922, 53.00),\n",
       "  (1, 52.00),\n",
       "  (92, 52.00),\n",
       "  (164, 52.00),\n",
       "  (176, 52.00),\n",
       "  (216, 52.00),\n",
       "  (239, 52.00),\n",
       "  (431, 52.00),\n",
       "  (448, 52.00),\n",
       "  (478, 52.00),\n",
       "  (701, 52.00),\n",
       "  (738, 52.00),\n",
       "  (752, 52.00),\n",
       "  (779, 52.00),\n",
       "  (787, 52.00),\n",
       "  (829, 52.00),\n",
       "  (833, 52.00),\n",
       "  (840, 52.00),\n",
       "  (877, 52.00),\n",
       "  (917, 52.00),\n",
       "  (939, 52.00),\n",
       "  (943, 52.00),\n",
       "  (133, 51.00),\n",
       "  (160, 51.00),\n",
       "  (188, 51.00),\n",
       "  (196, 51.00),\n",
       "  (212, 51.00),\n",
       "  (221, 51.00),\n",
       "  (286, 51.00),\n",
       "  (362, 51.00),\n",
       "  (377, 51.00),\n",
       "  (416, 51.00),\n",
       "  (419, 51.00),\n",
       "  (514, 51.00),\n",
       "  (545, 51.00),\n",
       "  (592, 51.00),\n",
       "  (636, 51.00),\n",
       "  (637, 51.00),\n",
       "  (746, 51.00),\n",
       "  (757, 51.00),\n",
       "  (764, 51.00),\n",
       "  (776, 51.00),\n",
       "  (902, 51.00),\n",
       "  (927, 51.00),\n",
       "  (13, 50.00),\n",
       "  (36, 50.00),\n",
       "  (102, 50.00),\n",
       "  (114, 50.00),\n",
       "  (126, 50.00),\n",
       "  (261, 50.00),\n",
       "  (277, 50.00),\n",
       "  (289, 50.00),\n",
       "  (294, 50.00),\n",
       "  (295, 50.00),\n",
       "  (301, 50.00),\n",
       "  (352, 50.00),\n",
       "  (358, 50.00),\n",
       "  (373, 50.00),\n",
       "  (449, 50.00),\n",
       "  (467, 50.00),\n",
       "  (604, 50.00),\n",
       "  (608, 50.00),\n",
       "  (618, 50.00),\n",
       "  (639, 50.00),\n",
       "  (659, 50.00),\n",
       "  (685, 50.00),\n",
       "  (765, 50.00),\n",
       "  (997, 50.00),\n",
       "  (0, 49.00),\n",
       "  (9, 49.00),\n",
       "  (123, 49.00),\n",
       "  (172, 49.00),\n",
       "  (267, 49.00),\n",
       "  (325, 49.00),\n",
       "  (375, 49.00),\n",
       "  (376, 49.00),\n",
       "  (378, 49.00),\n",
       "  (387, 49.00),\n",
       "  (388, 49.00),\n",
       "  (398, 49.00),\n",
       "  (523, 49.00),\n",
       "  (535, 49.00),\n",
       "  (541, 49.00),\n",
       "  (566, 49.00),\n",
       "  (583, 49.00),\n",
       "  (602, 49.00),\n",
       "  (667, 49.00),\n",
       "  (704, 49.00),\n",
       "  (763, 49.00),\n",
       "  (771, 49.00),\n",
       "  (874, 49.00),\n",
       "  (11, 48.00),\n",
       "  (12, 48.00),\n",
       "  (14, 48.00),\n",
       "  (15, 48.00),\n",
       "  (16, 48.00),\n",
       "  (18, 48.00),\n",
       "  (41, 48.00),\n",
       "  (71, 48.00),\n",
       "  (78, 48.00),\n",
       "  (134, 48.00),\n",
       "  (137, 48.00),\n",
       "  (141, 48.00),\n",
       "  (156, 48.00),\n",
       "  (194, 48.00),\n",
       "  (209, 48.00),\n",
       "  (214, 48.00),\n",
       "  (255, 48.00),\n",
       "  (264, 48.00),\n",
       "  (279, 48.00),\n",
       "  (288, 48.00),\n",
       "  (290, 48.00),\n",
       "  (312, 48.00),\n",
       "  (336, 48.00),\n",
       "  (340, 48.00),\n",
       "  (349, 48.00),\n",
       "  (354, 48.00),\n",
       "  (413, 48.00),\n",
       "  (451, 48.00),\n",
       "  (517, 48.00),\n",
       "  (628, 48.00),\n",
       "  (683, 48.00),\n",
       "  (684, 48.00),\n",
       "  (758, 48.00),\n",
       "  (797, 48.00),\n",
       "  (865, 48.00),\n",
       "  (872, 48.00),\n",
       "  (881, 48.00),\n",
       "  (890, 48.00),\n",
       "  (891, 48.00),\n",
       "  (915, 48.00),\n",
       "  (951, 48.00),\n",
       "  (994, 48.00),\n",
       "  (40, 47.00),\n",
       "  (53, 47.00),\n",
       "  (100, 47.00),\n",
       "  (101, 47.00),\n",
       "  (110, 47.00),\n",
       "  (130, 47.00),\n",
       "  (136, 47.00),\n",
       "  (227, 47.00),\n",
       "  (243, 47.00),\n",
       "  (253, 47.00),\n",
       "  (256, 47.00),\n",
       "  (265, 47.00),\n",
       "  (321, 47.00),\n",
       "  (333, 47.00),\n",
       "  (335, 47.00),\n",
       "  (337, 47.00),\n",
       "  (350, 47.00),\n",
       "  (351, 47.00),\n",
       "  (395, 47.00),\n",
       "  (426, 47.00),\n",
       "  (503, 47.00),\n",
       "  (518, 47.00),\n",
       "  (560, 47.00),\n",
       "  (576, 47.00),\n",
       "  (606, 47.00),\n",
       "  (707, 47.00),\n",
       "  (732, 47.00),\n",
       "  (759, 47.00),\n",
       "  (769, 47.00),\n",
       "  (807, 47.00),\n",
       "  (886, 47.00),\n",
       "  (5, 46.00),\n",
       "  (22, 46.00),\n",
       "  (56, 46.00),\n",
       "  (63, 46.00),\n",
       "  (65, 46.00),\n",
       "  (72, 46.00),\n",
       "  (79, 46.00),\n",
       "  (87, 46.00),\n",
       "  (95, 46.00),\n",
       "  (108, 46.00),\n",
       "  (149, 46.00),\n",
       "  (157, 46.00),\n",
       "  (201, 46.00),\n",
       "  (215, 46.00),\n",
       "  (266, 46.00),\n",
       "  (320, 46.00),\n",
       "  (323, 46.00),\n",
       "  (324, 46.00),\n",
       "  (366, 46.00),\n",
       "  (370, 46.00),\n",
       "  (397, 46.00),\n",
       "  (422, 46.00),\n",
       "  (432, 46.00),\n",
       "  (433, 46.00),\n",
       "  (434, 46.00),\n",
       "  (530, 46.00),\n",
       "  (616, 46.00),\n",
       "  (658, 46.00),\n",
       "  (664, 46.00),\n",
       "  (751, 46.00),\n",
       "  (753, 46.00),\n",
       "  (796, 46.00),\n",
       "  (823, 46.00),\n",
       "  (848, 46.00),\n",
       "  (867, 46.00),\n",
       "  (882, 46.00),\n",
       "  (918, 46.00),\n",
       "  (983, 46.00),\n",
       "  (989, 46.00),\n",
       "  (993, 46.00),\n",
       "  (2, 45.00),\n",
       "  (28, 45.00),\n",
       "  (66, 45.00),\n",
       "  (96, 45.00),\n",
       "  (105, 45.00),\n",
       "  (121, 45.00),\n",
       "  (139, 45.00),\n",
       "  (144, 45.00),\n",
       "  (169, 45.00),\n",
       "  (191, 45.00),\n",
       "  (247, 45.00),\n",
       "  (273, 45.00),\n",
       "  (299, 45.00),\n",
       "  (326, 45.00),\n",
       "  (332, 45.00),\n",
       "  (339, 45.00),\n",
       "  (344, 45.00),\n",
       "  (355, 45.00),\n",
       "  (365, 45.00),\n",
       "  (389, 45.00),\n",
       "  (392, 45.00),\n",
       "  (445, 45.00),\n",
       "  (571, 45.00),\n",
       "  (599, 45.00),\n",
       "  (625, 45.00),\n",
       "  (674, 45.00),\n",
       "  (734, 45.00),\n",
       "  (755, 45.00),\n",
       "  (817, 45.00),\n",
       "  (853, 45.00),\n",
       "  (900, 45.00),\n",
       "  (910, 45.00),\n",
       "  (75, 44.00),\n",
       "  (131, 44.00),\n",
       "  (186, 44.00),\n",
       "  (223, 44.00),\n",
       "  (244, 44.00),\n",
       "  (252, 44.00),\n",
       "  (405, 44.00),\n",
       "  (412, 44.00),\n",
       "  (417, 44.00),\n",
       "  (473, 44.00),\n",
       "  (475, 44.00),\n",
       "  (486, 44.00),\n",
       "  (579, 44.00),\n",
       "  (613, 44.00),\n",
       "  (682, 44.00),\n",
       "  (702, 44.00),\n",
       "  (706, 44.00),\n",
       "  (727, 44.00),\n",
       "  (821, 44.00),\n",
       "  (941, 44.00),\n",
       "  (968, 44.00),\n",
       "  (984, 44.00),\n",
       "  (44, 43.00),\n",
       "  (83, 43.00),\n",
       "  (107, 43.00),\n",
       "  (142, 43.00),\n",
       "  (198, 43.00),\n",
       "  (268, 43.00),\n",
       "  (330, 43.00),\n",
       "  (353, 43.00),\n",
       "  (357, 43.00),\n",
       "  (360, 43.00),\n",
       "  (381, 43.00),\n",
       "  (384, 43.00),\n",
       "  (414, 43.00),\n",
       "  (495, 43.00),\n",
       "  (537, 43.00),\n",
       "  (542, 43.00),\n",
       "  (717, 43.00),\n",
       "  (782, 43.00),\n",
       "  (844, 43.00),\n",
       "  (873, 43.00),\n",
       "  (907, 43.00),\n",
       "  (920, 43.00),\n",
       "  (959, 43.00),\n",
       "  (998, 43.00),\n",
       "  (7, 42.00),\n",
       "  (17, 42.00),\n",
       "  (19, 42.00),\n",
       "  (42, 42.00),\n",
       "  (80, 42.00),\n",
       "  (93, 42.00),\n",
       "  (112, 42.00),\n",
       "  (132, 42.00),\n",
       "  (154, 42.00),\n",
       "  (174, 42.00),\n",
       "  (193, 42.00),\n",
       "  (245, 42.00),\n",
       "  (260, 42.00),\n",
       "  (306, 42.00),\n",
       "  (309, 42.00),\n",
       "  (421, 42.00),\n",
       "  (429, 42.00),\n",
       "  (437, 42.00),\n",
       "  (447, 42.00),\n",
       "  (464, 42.00),\n",
       "  (485, 42.00),\n",
       "  (507, 42.00),\n",
       "  (510, 42.00),\n",
       "  (538, 42.00),\n",
       "  (575, 42.00),\n",
       "  (633, 42.00),\n",
       "  (656, 42.00),\n",
       "  (666, 42.00),\n",
       "  (690, 42.00),\n",
       "  (710, 42.00),\n",
       "  (799, 42.00),\n",
       "  (889, 42.00),\n",
       "  (894, 42.00),\n",
       "  (919, 42.00),\n",
       "  (932, 42.00),\n",
       "  (933, 42.00),\n",
       "  (981, 42.00),\n",
       "  (999, 42.00),\n",
       "  (117, 41.00),\n",
       "  (127, 41.00),\n",
       "  (153, 41.00),\n",
       "  (190, 41.00),\n",
       "  (224, 41.00),\n",
       "  (258, 41.00),\n",
       "  (278, 41.00),\n",
       "  (322, 41.00),\n",
       "  (390, 41.00),\n",
       "  (393, 41.00),\n",
       "  (430, 41.00),\n",
       "  (574, 41.00),\n",
       "  (712, 41.00),\n",
       "  (723, 41.00),\n",
       "  (795, 41.00),\n",
       "  (854, 41.00),\n",
       "  (898, 41.00),\n",
       "  (929, 41.00),\n",
       "  (27, 40.00),\n",
       "  (230, 40.00),\n",
       "  (242, 40.00),\n",
       "  (257, 40.00),\n",
       "  (287, 40.00),\n",
       "  (418, 40.00),\n",
       "  (500, 40.00),\n",
       "  (546, 40.00),\n",
       "  (548, 40.00),\n",
       "  (558, 40.00),\n",
       "  (573, 40.00),\n",
       "  (584, 40.00),\n",
       "  (600, 40.00),\n",
       "  (610, 40.00),\n",
       "  (713, 40.00),\n",
       "  (736, 40.00),\n",
       "  (749, 40.00),\n",
       "  (837, 40.00),\n",
       "  (862, 40.00),\n",
       "  (921, 40.00),\n",
       "  (925, 40.00),\n",
       "  (996, 40.00),\n",
       "  (173, 39.00),\n",
       "  (183, 39.00),\n",
       "  (262, 39.00),\n",
       "  (296, 39.00),\n",
       "  (297, 39.00),\n",
       "  (302, 39.00),\n",
       "  (329, 39.00),\n",
       "  (368, 39.00),\n",
       "  (435, 39.00),\n",
       "  (470, 39.00),\n",
       "  (480, 39.00),\n",
       "  (513, 39.00),\n",
       "  (520, 39.00),\n",
       "  (529, 39.00),\n",
       "  (553, 39.00),\n",
       "  (670, 39.00),\n",
       "  (672, 39.00),\n",
       "  (677, 39.00),\n",
       "  (694, 39.00),\n",
       "  (722, 39.00),\n",
       "  (726, 39.00),\n",
       "  (756, 39.00),\n",
       "  (812, 39.00),\n",
       "  (945, 39.00),\n",
       "  (43, 38.00),\n",
       "  (91, 38.00),\n",
       "  (210, 38.00),\n",
       "  (235, 38.00),\n",
       "  (236, 38.00),\n",
       "  (305, 38.00),\n",
       "  (314, 38.00),\n",
       "  (439, 38.00),\n",
       "  (442, 38.00),\n",
       "  (456, 38.00),\n",
       "  (462, 38.00),\n",
       "  (466, 38.00),\n",
       "  (501, 38.00),\n",
       "  (540, 38.00),\n",
       "  (555, 38.00),\n",
       "  (559, 38.00),\n",
       "  (643, 38.00),\n",
       "  (687, 38.00),\n",
       "  (699, 38.00),\n",
       "  (708, 38.00),\n",
       "  (719, 38.00),\n",
       "  (38, 37.00),\n",
       "  (52, 37.00),\n",
       "  (111, 37.00),\n",
       "  (140, 37.00),\n",
       "  (175, 37.00),\n",
       "  (204, 37.00),\n",
       "  (248, 37.00),\n",
       "  (338, 37.00),\n",
       "  (346, 37.00),\n",
       "  (379, 37.00),\n",
       "  (450, 37.00),\n",
       "  (494, 37.00),\n",
       "  (554, 37.00),\n",
       "  (615, 37.00),\n",
       "  (630, 37.00),\n",
       "  (650, 37.00),\n",
       "  (678, 37.00),\n",
       "  (693, 37.00),\n",
       "  (714, 37.00),\n",
       "  (761, 37.00),\n",
       "  (814, 37.00),\n",
       "  (831, 37.00),\n",
       "  (934, 37.00),\n",
       "  (10, 36.00),\n",
       "  (26, 36.00),\n",
       "  (45, 36.00),\n",
       "  (62, 36.00),\n",
       "  (98, 36.00),\n",
       "  (120, 36.00),\n",
       "  (145, 36.00),\n",
       "  (200, 36.00),\n",
       "  (207, 36.00),\n",
       "  (408, 36.00),\n",
       "  (427, 36.00),\n",
       "  (453, 36.00),\n",
       "  (511, 36.00),\n",
       "  (588, 36.00),\n",
       "  (827, 36.00),\n",
       "  (851, 36.00),\n",
       "  (20, 35.00),\n",
       "  (143, 35.00),\n",
       "  (148, 35.00),\n",
       "  (220, 35.00),\n",
       "  (347, 35.00),\n",
       "  (374, 35.00),\n",
       "  (380, 35.00),\n",
       "  (409, 35.00),\n",
       "  (415, 35.00),\n",
       "  (543, 35.00),\n",
       "  (605, 35.00),\n",
       "  (627, 35.00),\n",
       "  (745, 35.00),\n",
       "  (760, 35.00),\n",
       "  (793, 35.00),\n",
       "  (798, 35.00),\n",
       "  (846, 35.00),\n",
       "  (958, 35.00),\n",
       "  (73, 34.00),\n",
       "  (226, 34.00),\n",
       "  (399, 34.00),\n",
       "  (465, 34.00),\n",
       "  (720, 34.00),\n",
       "  (786, 34.00),\n",
       "  (802, 34.00),\n",
       "  (861, 34.00),\n",
       "  (897, 34.00),\n",
       "  (928, 34.00),\n",
       "  (81, 33.00),\n",
       "  (152, 33.00),\n",
       "  (158, 33.00),\n",
       "  (205, 33.00),\n",
       "  (213, 33.00),\n",
       "  (385, 33.00),\n",
       "  (647, 33.00),\n",
       "  (803, 33.00),\n",
       "  (859, 33.00),\n",
       "  (964, 33.00),\n",
       "  (59, 32.00),\n",
       "  (86, 32.00),\n",
       "  (146, 32.00),\n",
       "  (356, 32.00),\n",
       "  (359, 32.00),\n",
       "  (371, 32.00),\n",
       "  (402, 32.00),\n",
       "  (629, 32.00),\n",
       "  (883, 32.00),\n",
       "  (947, 32.00),\n",
       "  (980, 32.00),\n",
       "  (106, 31.00),\n",
       "  (179, 31.00),\n",
       "  (233, 31.00),\n",
       "  (403, 31.00),\n",
       "  (459, 31.00),\n",
       "  (502, 31.00),\n",
       "  (521, 31.00),\n",
       "  (744, 31.00),\n",
       "  (914, 31.00),\n",
       "  (923, 31.00),\n",
       "  (954, 31.00),\n",
       "  (64, 30.00),\n",
       "  (166, 30.00),\n",
       "  (202, 30.00),\n",
       "  (345, 30.00),\n",
       "  (469, 30.00),\n",
       "  (484, 30.00),\n",
       "  (531, 30.00),\n",
       "  (551, 30.00),\n",
       "  (568, 30.00),\n",
       "  (578, 30.00),\n",
       "  (589, 30.00),\n",
       "  (634, 30.00),\n",
       "  (663, 30.00),\n",
       "  (705, 30.00),\n",
       "  (948, 30.00),\n",
       "  (187, 29.00),\n",
       "  (461, 29.00),\n",
       "  (498, 29.00),\n",
       "  (557, 29.00),\n",
       "  (585, 29.00),\n",
       "  (617, 29.00),\n",
       "  (860, 29.00),\n",
       "  (966, 29.00),\n",
       "  (4, 28.00),\n",
       "  (29, 28.00),\n",
       "  (122, 28.00),\n",
       "  (596, 28.00),\n",
       "  (632, 28.00),\n",
       "  (676, 28.00),\n",
       "  (691, 28.00),\n",
       "  (733, 28.00),\n",
       "  (747, 28.00),\n",
       "  (49, 27.00),\n",
       "  (54, 27.00),\n",
       "  (147, 27.00),\n",
       "  (150, 27.00),\n",
       "  (271, 27.00),\n",
       "  (303, 27.00),\n",
       "  (369, 27.00),\n",
       "  (567, 27.00),\n",
       "  (587, 27.00),\n",
       "  (624, 27.00),\n",
       "  (644, 27.00),\n",
       "  (660, 27.00),\n",
       "  (718, 27.00),\n",
       "  (767, 27.00),\n",
       "  (789, 27.00),\n",
       "  (32, 26.00),\n",
       "  (516, 26.00),\n",
       "  (544, 26.00),\n",
       "  (631, 26.00),\n",
       "  (731, 26.00),\n",
       "  (804, 26.00),\n",
       "  (869, 26.00),\n",
       "  (965, 26.00),\n",
       "  (163, 25.00),\n",
       "  (168, 25.00),\n",
       "  (394, 25.00),\n",
       "  (479, 25.00),\n",
       "  (482, 25.00),\n",
       "  (536, 25.00),\n",
       "  (590, 25.00),\n",
       "  (623, 25.00),\n",
       "  (686, 25.00),\n",
       "  (838, 25.00),\n",
       "  (908, 25.00),\n",
       "  (165, 24.00),\n",
       "  (785, 24.00),\n",
       "  (901, 24.00),\n",
       "  (931, 24.00),\n",
       "  (942, 24.00),\n",
       "  (185, 23.00),\n",
       "  (240, 23.00),\n",
       "  (499, 23.00),\n",
       "  (638, 23.00),\n",
       "  (680, 23.00),\n",
       "  (876, 23.00),\n",
       "  (974, 23.00),\n",
       "  (68, 22.00),\n",
       "  (525, 22.00),\n",
       "  (740, 22.00),\n",
       "  (773, 22.00),\n",
       "  (811, 22.00),\n",
       "  (856, 22.00),\n",
       "  (909, 22.00),\n",
       "  (911, 22.00),\n",
       "  (246, 21.00),\n",
       "  (438, 21.00),\n",
       "  (675, 21.00),\n",
       "  (885, 21.00),\n",
       "  (315, 20.00),\n",
       "  (400, 20.00),\n",
       "  (662, 20.00),\n",
       "  (715, 20.00),\n",
       "  (977, 20.00),\n",
       "  (598, 19.00),\n",
       "  (729, 19.00),\n",
       "  (622, 18.00),\n",
       "  (651, 18.00),\n",
       "  (103, 17.00),\n",
       "  (550, 17.00),\n",
       "  (648, 17.00),\n",
       "  (728, 17.00),\n",
       "  (841, 17.00),\n",
       "  (976, 17.00),\n",
       "  (460, 16.00),\n",
       "  (504, 16.00),\n",
       "  (836, 16.00),\n",
       "  (930, 16.00),\n",
       "  (970, 16.00),\n",
       "  (282, 15.00),\n",
       "  (446, 15.00),\n",
       "  (493, 15.00),\n",
       "  (534, 15.00),\n",
       "  (906, 15.00),\n",
       "  (813, 14.00),\n",
       "  (969, 13.00),\n",
       "  (742, 12.00),\n",
       "  (940, 12.00),\n",
       "  (34, 11.00),\n",
       "  (167, 11.00),\n",
       "  (689, 11.00),\n",
       "  (967, 11.00),\n",
       "  (978, 11.00),\n",
       "  (681, 9.00),\n",
       "  (899, 9.00),\n",
       "  (960, 9.00),\n",
       "  (673, 8.00),\n",
       "  (810, 7.00),\n",
       "  (935, 6.00),\n",
       "  (961, 2.00)])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = targeted_diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch_no 0\n",
      "at batch_no 100\n",
      "at batch_no 200\n",
      "at batch_no 300\n",
      "at batch_no 400\n",
      "at batch_no 500\n",
      "at batch_no 600\n",
      "at batch_no 700\n",
      "at batch_no 800\n",
      "at batch_no 900\n",
      "at batch_no 1000\n",
      "at batch_no 1100\n",
      "at batch_no 1200\n",
      "at batch_no 1300\n",
      "at batch_no 1400\n",
      "at batch_no 1500\n",
      "at batch_no 1600\n",
      "at batch_no 1700\n",
      "at batch_no 1800\n",
      "at batch_no 1900\n",
      "at batch_no 2000\n",
      "at batch_no 2100\n",
      "at batch_no 2200\n",
      "at batch_no 2300\n",
      "at batch_no 2400\n",
      "at batch_no 2500\n",
      "at batch_no 2600\n",
      "at batch_no 2700\n",
      "at batch_no 2800\n",
      "at batch_no 2900\n",
      "at batch_no 3000\n",
      "at batch_no 3100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(885,\n",
       " [(815, 239.00),\n",
       "  (652, 181.00),\n",
       "  (646, 168.00),\n",
       "  (580, 163.00),\n",
       "  (611, 155.00),\n",
       "  (591, 148.00),\n",
       "  (737, 143.00),\n",
       "  (489, 142.00),\n",
       "  (621, 139.00),\n",
       "  (904, 134.00),\n",
       "  (794, 130.00),\n",
       "  (497, 127.00),\n",
       "  (893, 127.00),\n",
       "  (582, 125.00),\n",
       "  (94, 123.00),\n",
       "  (955, 120.00),\n",
       "  (116, 115.00),\n",
       "  (868, 115.00),\n",
       "  (39, 112.00),\n",
       "  (679, 112.00),\n",
       "  (565, 110.00),\n",
       "  (162, 109.00),\n",
       "  (491, 108.00),\n",
       "  (721, 108.00),\n",
       "  (979, 108.00),\n",
       "  (741, 105.00),\n",
       "  (839, 105.00),\n",
       "  (109, 104.00),\n",
       "  (562, 104.00),\n",
       "  (84, 103.00),\n",
       "  (549, 102.00),\n",
       "  (48, 99.00),\n",
       "  (82, 99.00),\n",
       "  (492, 99.00),\n",
       "  (51, 98.00),\n",
       "  (973, 96.00),\n",
       "  (199, 95.00),\n",
       "  (750, 95.00),\n",
       "  (46, 93.00),\n",
       "  (640, 92.00),\n",
       "  (695, 92.00),\n",
       "  (151, 88.00),\n",
       "  (971, 88.00),\n",
       "  (783, 87.00),\n",
       "  (843, 87.00),\n",
       "  (203, 86.00),\n",
       "  (669, 86.00),\n",
       "  (692, 86.00),\n",
       "  (424, 84.00),\n",
       "  (956, 84.00),\n",
       "  (281, 83.00),\n",
       "  (577, 83.00),\n",
       "  (828, 82.00),\n",
       "  (866, 82.00),\n",
       "  (47, 81.00),\n",
       "  (310, 81.00),\n",
       "  (743, 81.00),\n",
       "  (847, 81.00),\n",
       "  (61, 80.00),\n",
       "  (208, 80.00),\n",
       "  (703, 80.00),\n",
       "  (180, 79.00),\n",
       "  (182, 79.00),\n",
       "  (382, 79.00),\n",
       "  (830, 79.00),\n",
       "  (189, 78.00),\n",
       "  (219, 78.00),\n",
       "  (319, 78.00),\n",
       "  (343, 78.00),\n",
       "  (411, 78.00),\n",
       "  (725, 78.00),\n",
       "  (879, 78.00),\n",
       "  (342, 77.00),\n",
       "  (406, 77.00),\n",
       "  (440, 77.00),\n",
       "  (454, 77.00),\n",
       "  (778, 77.00),\n",
       "  (128, 76.00),\n",
       "  (197, 76.00),\n",
       "  (270, 76.00),\n",
       "  (298, 76.00),\n",
       "  (570, 76.00),\n",
       "  (824, 76.00),\n",
       "  (982, 76.00),\n",
       "  (711, 75.00),\n",
       "  (754, 75.00),\n",
       "  (762, 75.00),\n",
       "  (791, 75.00),\n",
       "  (963, 75.00),\n",
       "  (55, 74.00),\n",
       "  (217, 74.00),\n",
       "  (237, 74.00),\n",
       "  (364, 74.00),\n",
       "  (455, 74.00),\n",
       "  (572, 74.00),\n",
       "  (671, 74.00),\n",
       "  (805, 74.00),\n",
       "  (905, 74.00),\n",
       "  (318, 73.00),\n",
       "  (436, 73.00),\n",
       "  (735, 73.00),\n",
       "  (30, 72.00),\n",
       "  (304, 72.00),\n",
       "  (363, 72.00),\n",
       "  (472, 72.00),\n",
       "  (645, 72.00),\n",
       "  (730, 72.00),\n",
       "  (800, 72.00),\n",
       "  (845, 72.00),\n",
       "  (76, 71.00),\n",
       "  (483, 71.00),\n",
       "  (496, 71.00),\n",
       "  (716, 71.00),\n",
       "  (878, 71.00),\n",
       "  (938, 71.00),\n",
       "  (23, 70.00),\n",
       "  (50, 70.00),\n",
       "  (313, 70.00),\n",
       "  (471, 70.00),\n",
       "  (654, 70.00),\n",
       "  (700, 70.00),\n",
       "  (806, 70.00),\n",
       "  (826, 70.00),\n",
       "  (192, 69.00),\n",
       "  (820, 69.00),\n",
       "  (825, 69.00),\n",
       "  (896, 69.00),\n",
       "  (195, 68.00),\n",
       "  (425, 68.00),\n",
       "  (849, 68.00),\n",
       "  (916, 68.00),\n",
       "  (74, 67.00),\n",
       "  (222, 67.00),\n",
       "  (341, 67.00),\n",
       "  (468, 67.00),\n",
       "  (808, 67.00),\n",
       "  (949, 67.00),\n",
       "  (37, 66.00),\n",
       "  (77, 66.00),\n",
       "  (556, 66.00),\n",
       "  (688, 66.00),\n",
       "  (850, 66.00),\n",
       "  (912, 66.00),\n",
       "  (944, 66.00),\n",
       "  (972, 66.00),\n",
       "  (655, 65.00),\n",
       "  (775, 65.00),\n",
       "  (926, 65.00),\n",
       "  (97, 64.00),\n",
       "  (124, 64.00),\n",
       "  (178, 64.00),\n",
       "  (293, 64.00),\n",
       "  (772, 64.00),\n",
       "  (863, 64.00),\n",
       "  (870, 64.00),\n",
       "  (880, 64.00),\n",
       "  (992, 64.00),\n",
       "  (125, 63.00),\n",
       "  (181, 63.00),\n",
       "  (184, 63.00),\n",
       "  (211, 63.00),\n",
       "  (238, 63.00),\n",
       "  (272, 63.00),\n",
       "  (300, 63.00),\n",
       "  (311, 63.00),\n",
       "  (331, 63.00),\n",
       "  (420, 63.00),\n",
       "  (458, 63.00),\n",
       "  (506, 63.00),\n",
       "  (515, 63.00),\n",
       "  (620, 63.00),\n",
       "  (864, 63.00),\n",
       "  (892, 63.00),\n",
       "  (953, 63.00),\n",
       "  (90, 62.00),\n",
       "  (135, 62.00),\n",
       "  (155, 62.00),\n",
       "  (161, 62.00),\n",
       "  (234, 62.00),\n",
       "  (505, 62.00),\n",
       "  (561, 62.00),\n",
       "  (619, 62.00),\n",
       "  (784, 62.00),\n",
       "  (819, 62.00),\n",
       "  (871, 62.00),\n",
       "  (975, 62.00),\n",
       "  (988, 62.00),\n",
       "  (6, 61.00),\n",
       "  (249, 61.00),\n",
       "  (292, 61.00),\n",
       "  (476, 61.00),\n",
       "  (527, 61.00),\n",
       "  (635, 61.00),\n",
       "  (724, 61.00),\n",
       "  (788, 61.00),\n",
       "  (895, 61.00),\n",
       "  (937, 61.00),\n",
       "  (946, 61.00),\n",
       "  (99, 60.00),\n",
       "  (225, 60.00),\n",
       "  (423, 60.00),\n",
       "  (463, 60.00),\n",
       "  (481, 60.00),\n",
       "  (586, 60.00),\n",
       "  (595, 60.00),\n",
       "  (626, 60.00),\n",
       "  (748, 60.00),\n",
       "  (781, 60.00),\n",
       "  (792, 60.00),\n",
       "  (875, 60.00),\n",
       "  (884, 60.00),\n",
       "  (887, 60.00),\n",
       "  (231, 59.00),\n",
       "  (316, 59.00),\n",
       "  (327, 59.00),\n",
       "  (361, 59.00),\n",
       "  (391, 59.00),\n",
       "  (401, 59.00),\n",
       "  (474, 59.00),\n",
       "  (509, 59.00),\n",
       "  (593, 59.00),\n",
       "  (597, 59.00),\n",
       "  (641, 59.00),\n",
       "  (697, 59.00),\n",
       "  (790, 59.00),\n",
       "  (952, 59.00),\n",
       "  (8, 58.00),\n",
       "  (115, 58.00),\n",
       "  (118, 58.00),\n",
       "  (348, 58.00),\n",
       "  (404, 58.00),\n",
       "  (410, 58.00),\n",
       "  (478, 58.00),\n",
       "  (532, 58.00),\n",
       "  (594, 58.00),\n",
       "  (609, 58.00),\n",
       "  (614, 58.00),\n",
       "  (770, 58.00),\n",
       "  (809, 58.00),\n",
       "  (957, 58.00),\n",
       "  (962, 58.00),\n",
       "  (991, 58.00),\n",
       "  (24, 57.00),\n",
       "  (33, 57.00),\n",
       "  (113, 57.00),\n",
       "  (129, 57.00),\n",
       "  (275, 57.00),\n",
       "  (308, 57.00),\n",
       "  (407, 57.00),\n",
       "  (441, 57.00),\n",
       "  (457, 57.00),\n",
       "  (477, 57.00),\n",
       "  (522, 57.00),\n",
       "  (539, 57.00),\n",
       "  (564, 57.00),\n",
       "  (581, 57.00),\n",
       "  (607, 57.00),\n",
       "  (698, 57.00),\n",
       "  (774, 57.00),\n",
       "  (816, 57.00),\n",
       "  (834, 57.00),\n",
       "  (842, 57.00),\n",
       "  (877, 57.00),\n",
       "  (913, 57.00),\n",
       "  (25, 56.00),\n",
       "  (57, 56.00),\n",
       "  (119, 56.00),\n",
       "  (171, 56.00),\n",
       "  (280, 56.00),\n",
       "  (284, 56.00),\n",
       "  (317, 56.00),\n",
       "  (334, 56.00),\n",
       "  (396, 56.00),\n",
       "  (428, 56.00),\n",
       "  (443, 56.00),\n",
       "  (488, 56.00),\n",
       "  (601, 56.00),\n",
       "  (642, 56.00),\n",
       "  (649, 56.00),\n",
       "  (661, 56.00),\n",
       "  (696, 56.00),\n",
       "  (852, 56.00),\n",
       "  (950, 56.00),\n",
       "  (987, 56.00),\n",
       "  (21, 55.00),\n",
       "  (60, 55.00),\n",
       "  (69, 55.00),\n",
       "  (85, 55.00),\n",
       "  (88, 55.00),\n",
       "  (104, 55.00),\n",
       "  (170, 55.00),\n",
       "  (232, 55.00),\n",
       "  (250, 55.00),\n",
       "  (254, 55.00),\n",
       "  (259, 55.00),\n",
       "  (274, 55.00),\n",
       "  (283, 55.00),\n",
       "  (291, 55.00),\n",
       "  (328, 55.00),\n",
       "  (367, 55.00),\n",
       "  (372, 55.00),\n",
       "  (375, 55.00),\n",
       "  (452, 55.00),\n",
       "  (512, 55.00),\n",
       "  (547, 55.00),\n",
       "  (563, 55.00),\n",
       "  (569, 55.00),\n",
       "  (603, 55.00),\n",
       "  (608, 55.00),\n",
       "  (618, 55.00),\n",
       "  (653, 55.00),\n",
       "  (766, 55.00),\n",
       "  (780, 55.00),\n",
       "  (797, 55.00),\n",
       "  (801, 55.00),\n",
       "  (822, 55.00),\n",
       "  (829, 55.00),\n",
       "  (840, 55.00),\n",
       "  (857, 55.00),\n",
       "  (903, 55.00),\n",
       "  (936, 55.00),\n",
       "  (206, 54.00),\n",
       "  (228, 54.00),\n",
       "  (251, 54.00),\n",
       "  (263, 54.00),\n",
       "  (285, 54.00),\n",
       "  (307, 54.00),\n",
       "  (386, 54.00),\n",
       "  (419, 54.00),\n",
       "  (448, 54.00),\n",
       "  (524, 54.00),\n",
       "  (701, 54.00),\n",
       "  (768, 54.00),\n",
       "  (777, 54.00),\n",
       "  (832, 54.00),\n",
       "  (835, 54.00),\n",
       "  (888, 54.00),\n",
       "  (902, 54.00),\n",
       "  (924, 54.00),\n",
       "  (939, 54.00),\n",
       "  (943, 54.00),\n",
       "  (985, 54.00),\n",
       "  (986, 54.00),\n",
       "  (31, 53.00),\n",
       "  (58, 53.00),\n",
       "  (159, 53.00),\n",
       "  (256, 53.00),\n",
       "  (294, 53.00),\n",
       "  (383, 53.00),\n",
       "  (487, 53.00),\n",
       "  (533, 53.00),\n",
       "  (541, 53.00),\n",
       "  (612, 53.00),\n",
       "  (657, 53.00),\n",
       "  (664, 53.00),\n",
       "  (667, 53.00),\n",
       "  (709, 53.00),\n",
       "  (858, 53.00),\n",
       "  (917, 53.00),\n",
       "  (922, 53.00),\n",
       "  (990, 53.00),\n",
       "  (995, 53.00),\n",
       "  (997, 53.00),\n",
       "  (0, 52.00),\n",
       "  (89, 52.00),\n",
       "  (164, 52.00),\n",
       "  (218, 52.00),\n",
       "  (229, 52.00),\n",
       "  (269, 52.00),\n",
       "  (276, 52.00),\n",
       "  (289, 52.00),\n",
       "  (352, 52.00),\n",
       "  (451, 52.00),\n",
       "  (490, 52.00),\n",
       "  (528, 52.00),\n",
       "  (545, 52.00),\n",
       "  (665, 52.00),\n",
       "  (752, 52.00),\n",
       "  (771, 52.00),\n",
       "  (927, 52.00),\n",
       "  (13, 51.00),\n",
       "  (36, 51.00),\n",
       "  (67, 51.00),\n",
       "  (70, 51.00),\n",
       "  (78, 51.00),\n",
       "  (126, 51.00),\n",
       "  (133, 51.00),\n",
       "  (177, 51.00),\n",
       "  (194, 51.00),\n",
       "  (196, 51.00),\n",
       "  (209, 51.00),\n",
       "  (239, 51.00),\n",
       "  (241, 51.00),\n",
       "  (261, 51.00),\n",
       "  (264, 51.00),\n",
       "  (286, 51.00),\n",
       "  (295, 51.00),\n",
       "  (362, 51.00),\n",
       "  (388, 51.00),\n",
       "  (431, 51.00),\n",
       "  (508, 51.00),\n",
       "  (526, 51.00),\n",
       "  (602, 51.00),\n",
       "  (616, 51.00),\n",
       "  (685, 51.00),\n",
       "  (738, 51.00),\n",
       "  (739, 51.00),\n",
       "  (1, 50.00),\n",
       "  (3, 50.00),\n",
       "  (92, 50.00),\n",
       "  (100, 50.00),\n",
       "  (138, 50.00),\n",
       "  (172, 50.00),\n",
       "  (216, 50.00),\n",
       "  (277, 50.00),\n",
       "  (325, 50.00),\n",
       "  (340, 50.00),\n",
       "  (358, 50.00),\n",
       "  (373, 50.00),\n",
       "  (378, 50.00),\n",
       "  (433, 50.00),\n",
       "  (519, 50.00),\n",
       "  (552, 50.00),\n",
       "  (639, 50.00),\n",
       "  (668, 50.00),\n",
       "  (683, 50.00),\n",
       "  (684, 50.00),\n",
       "  (746, 50.00),\n",
       "  (757, 50.00),\n",
       "  (779, 50.00),\n",
       "  (833, 50.00),\n",
       "  (865, 50.00),\n",
       "  (9, 49.00),\n",
       "  (14, 49.00),\n",
       "  (15, 49.00),\n",
       "  (35, 49.00),\n",
       "  (63, 49.00),\n",
       "  (66, 49.00),\n",
       "  (96, 49.00),\n",
       "  (123, 49.00),\n",
       "  (160, 49.00),\n",
       "  (176, 49.00),\n",
       "  (212, 49.00),\n",
       "  (214, 49.00),\n",
       "  (267, 49.00),\n",
       "  (326, 49.00),\n",
       "  (333, 49.00),\n",
       "  (336, 49.00),\n",
       "  (349, 49.00),\n",
       "  (376, 49.00),\n",
       "  (377, 49.00),\n",
       "  (387, 49.00),\n",
       "  (416, 49.00),\n",
       "  (444, 49.00),\n",
       "  (467, 49.00),\n",
       "  (583, 49.00),\n",
       "  (604, 49.00),\n",
       "  (606, 49.00),\n",
       "  (628, 49.00),\n",
       "  (659, 49.00),\n",
       "  (704, 49.00),\n",
       "  (732, 49.00),\n",
       "  (765, 49.00),\n",
       "  (776, 49.00),\n",
       "  (787, 49.00),\n",
       "  (855, 49.00),\n",
       "  (11, 48.00),\n",
       "  (16, 48.00),\n",
       "  (18, 48.00),\n",
       "  (22, 48.00),\n",
       "  (41, 48.00),\n",
       "  (95, 48.00),\n",
       "  (102, 48.00),\n",
       "  (114, 48.00),\n",
       "  (188, 48.00),\n",
       "  (191, 48.00),\n",
       "  (243, 48.00),\n",
       "  (253, 48.00),\n",
       "  (265, 48.00),\n",
       "  (290, 48.00),\n",
       "  (337, 48.00),\n",
       "  (344, 48.00),\n",
       "  (365, 48.00),\n",
       "  (397, 48.00),\n",
       "  (432, 48.00),\n",
       "  (449, 48.00),\n",
       "  (514, 48.00),\n",
       "  (535, 48.00),\n",
       "  (566, 48.00),\n",
       "  (576, 48.00),\n",
       "  (592, 48.00),\n",
       "  (633, 48.00),\n",
       "  (674, 48.00),\n",
       "  (706, 48.00),\n",
       "  (707, 48.00),\n",
       "  (764, 48.00),\n",
       "  (853, 48.00),\n",
       "  (874, 48.00),\n",
       "  (890, 48.00),\n",
       "  (891, 48.00),\n",
       "  (915, 48.00),\n",
       "  (5, 47.00),\n",
       "  (12, 47.00),\n",
       "  (53, 47.00),\n",
       "  (72, 47.00),\n",
       "  (79, 47.00),\n",
       "  (121, 47.00),\n",
       "  (134, 47.00),\n",
       "  (144, 47.00),\n",
       "  (156, 47.00),\n",
       "  (169, 47.00),\n",
       "  (193, 47.00),\n",
       "  (201, 47.00),\n",
       "  (255, 47.00),\n",
       "  (279, 47.00),\n",
       "  (288, 47.00),\n",
       "  (301, 47.00),\n",
       "  (321, 47.00),\n",
       "  (350, 47.00),\n",
       "  (351, 47.00),\n",
       "  (366, 47.00),\n",
       "  (413, 47.00),\n",
       "  (503, 47.00),\n",
       "  (560, 47.00),\n",
       "  (571, 47.00),\n",
       "  (753, 47.00),\n",
       "  (769, 47.00),\n",
       "  (796, 47.00),\n",
       "  (823, 47.00),\n",
       "  (867, 47.00),\n",
       "  (886, 47.00),\n",
       "  (889, 47.00),\n",
       "  (951, 47.00),\n",
       "  (989, 47.00),\n",
       "  (28, 46.00),\n",
       "  (65, 46.00),\n",
       "  (87, 46.00),\n",
       "  (101, 46.00),\n",
       "  (110, 46.00),\n",
       "  (130, 46.00),\n",
       "  (136, 46.00),\n",
       "  (139, 46.00),\n",
       "  (149, 46.00),\n",
       "  (221, 46.00),\n",
       "  (227, 46.00),\n",
       "  (273, 46.00),\n",
       "  (312, 46.00),\n",
       "  (323, 46.00),\n",
       "  (392, 46.00),\n",
       "  (395, 46.00),\n",
       "  (398, 46.00),\n",
       "  (414, 46.00),\n",
       "  (486, 46.00),\n",
       "  (523, 46.00),\n",
       "  (530, 46.00),\n",
       "  (625, 46.00),\n",
       "  (727, 46.00),\n",
       "  (751, 46.00),\n",
       "  (758, 46.00),\n",
       "  (795, 46.00),\n",
       "  (872, 46.00),\n",
       "  (881, 46.00),\n",
       "  (882, 46.00),\n",
       "  (907, 46.00),\n",
       "  (959, 46.00),\n",
       "  (983, 46.00),\n",
       "  (994, 46.00),\n",
       "  (2, 45.00),\n",
       "  (56, 45.00),\n",
       "  (71, 45.00),\n",
       "  (131, 45.00),\n",
       "  (137, 45.00),\n",
       "  (141, 45.00),\n",
       "  (157, 45.00),\n",
       "  (247, 45.00),\n",
       "  (320, 45.00),\n",
       "  (324, 45.00),\n",
       "  (335, 45.00),\n",
       "  (339, 45.00),\n",
       "  (354, 45.00),\n",
       "  (384, 45.00),\n",
       "  (417, 45.00),\n",
       "  (422, 45.00),\n",
       "  (426, 45.00),\n",
       "  (475, 45.00),\n",
       "  (517, 45.00),\n",
       "  (599, 45.00),\n",
       "  (613, 45.00),\n",
       "  (666, 45.00),\n",
       "  (702, 45.00),\n",
       "  (759, 45.00),\n",
       "  (763, 45.00),\n",
       "  (818, 45.00),\n",
       "  (821, 45.00),\n",
       "  (848, 45.00),\n",
       "  (873, 45.00),\n",
       "  (900, 45.00),\n",
       "  (918, 45.00),\n",
       "  (44, 44.00),\n",
       "  (75, 44.00),\n",
       "  (105, 44.00),\n",
       "  (107, 44.00),\n",
       "  (112, 44.00),\n",
       "  (215, 44.00),\n",
       "  (244, 44.00),\n",
       "  (252, 44.00),\n",
       "  (258, 44.00),\n",
       "  (332, 44.00),\n",
       "  (390, 44.00),\n",
       "  (445, 44.00),\n",
       "  (456, 44.00),\n",
       "  (518, 44.00),\n",
       "  (579, 44.00),\n",
       "  (658, 44.00),\n",
       "  (690, 44.00),\n",
       "  (717, 44.00),\n",
       "  (755, 44.00),\n",
       "  (807, 44.00),\n",
       "  (920, 44.00),\n",
       "  (941, 44.00),\n",
       "  (984, 44.00),\n",
       "  (993, 44.00),\n",
       "  (80, 43.00),\n",
       "  (108, 43.00),\n",
       "  (142, 43.00),\n",
       "  (174, 43.00),\n",
       "  (198, 43.00),\n",
       "  (242, 43.00),\n",
       "  (268, 43.00),\n",
       "  (287, 43.00),\n",
       "  (306, 43.00),\n",
       "  (330, 43.00),\n",
       "  (355, 43.00),\n",
       "  (357, 43.00),\n",
       "  (381, 43.00),\n",
       "  (421, 43.00),\n",
       "  (434, 43.00),\n",
       "  (473, 43.00),\n",
       "  (495, 43.00),\n",
       "  (510, 43.00),\n",
       "  (637, 43.00),\n",
       "  (672, 43.00),\n",
       "  (812, 43.00),\n",
       "  (817, 43.00),\n",
       "  (894, 43.00),\n",
       "  (929, 43.00),\n",
       "  (933, 43.00),\n",
       "  (981, 43.00),\n",
       "  (998, 43.00),\n",
       "  (7, 42.00),\n",
       "  (42, 42.00),\n",
       "  (153, 42.00),\n",
       "  (173, 42.00),\n",
       "  (257, 42.00),\n",
       "  (266, 42.00),\n",
       "  (299, 42.00),\n",
       "  (360, 42.00),\n",
       "  (370, 42.00),\n",
       "  (389, 42.00),\n",
       "  (418, 42.00),\n",
       "  (442, 42.00),\n",
       "  (520, 42.00),\n",
       "  (538, 42.00),\n",
       "  (573, 42.00),\n",
       "  (723, 42.00),\n",
       "  (734, 42.00),\n",
       "  (756, 42.00),\n",
       "  (837, 42.00),\n",
       "  (898, 42.00),\n",
       "  (910, 42.00),\n",
       "  (968, 42.00),\n",
       "  (19, 41.00),\n",
       "  (38, 41.00),\n",
       "  (40, 41.00),\n",
       "  (83, 41.00),\n",
       "  (132, 41.00),\n",
       "  (223, 41.00),\n",
       "  (260, 41.00),\n",
       "  (302, 41.00),\n",
       "  (305, 41.00),\n",
       "  (353, 41.00),\n",
       "  (405, 41.00),\n",
       "  (429, 41.00),\n",
       "  (430, 41.00),\n",
       "  (437, 41.00),\n",
       "  (537, 41.00),\n",
       "  (558, 41.00),\n",
       "  (574, 41.00),\n",
       "  (575, 41.00),\n",
       "  (636, 41.00),\n",
       "  (694, 41.00),\n",
       "  (710, 41.00),\n",
       "  (712, 41.00),\n",
       "  (851, 41.00),\n",
       "  (919, 41.00),\n",
       "  (932, 41.00),\n",
       "  (996, 41.00),\n",
       "  (93, 40.00),\n",
       "  (117, 40.00),\n",
       "  (190, 40.00),\n",
       "  (224, 40.00),\n",
       "  (236, 40.00),\n",
       "  (245, 40.00),\n",
       "  (278, 40.00),\n",
       "  (322, 40.00),\n",
       "  (380, 40.00),\n",
       "  (447, 40.00),\n",
       "  (485, 40.00),\n",
       "  (500, 40.00),\n",
       "  (507, 40.00),\n",
       "  (542, 40.00),\n",
       "  (555, 40.00),\n",
       "  (559, 40.00),\n",
       "  (584, 40.00),\n",
       "  (643, 40.00),\n",
       "  (682, 40.00),\n",
       "  (699, 40.00),\n",
       "  (713, 40.00),\n",
       "  (726, 40.00),\n",
       "  (799, 40.00),\n",
       "  (862, 40.00),\n",
       "  (925, 40.00),\n",
       "  (945, 40.00),\n",
       "  (999, 40.00),\n",
       "  (10, 39.00),\n",
       "  (17, 39.00),\n",
       "  (27, 39.00),\n",
       "  (91, 39.00),\n",
       "  (111, 39.00),\n",
       "  (127, 39.00),\n",
       "  (140, 39.00),\n",
       "  (145, 39.00),\n",
       "  (148, 39.00),\n",
       "  (186, 39.00),\n",
       "  (329, 39.00),\n",
       "  (338, 39.00),\n",
       "  (379, 39.00),\n",
       "  (393, 39.00),\n",
       "  (439, 39.00),\n",
       "  (480, 39.00),\n",
       "  (501, 39.00),\n",
       "  (546, 39.00),\n",
       "  (670, 39.00),\n",
       "  (722, 39.00),\n",
       "  (749, 39.00),\n",
       "  (761, 39.00),\n",
       "  (883, 39.00),\n",
       "  (204, 38.00),\n",
       "  (230, 38.00),\n",
       "  (297, 38.00),\n",
       "  (309, 38.00),\n",
       "  (314, 38.00),\n",
       "  (368, 38.00),\n",
       "  (412, 38.00),\n",
       "  (470, 38.00),\n",
       "  (529, 38.00),\n",
       "  (540, 38.00),\n",
       "  (548, 38.00),\n",
       "  (553, 38.00),\n",
       "  (656, 38.00),\n",
       "  (687, 38.00),\n",
       "  (708, 38.00),\n",
       "  (714, 38.00),\n",
       "  (736, 38.00),\n",
       "  (782, 38.00),\n",
       "  (831, 38.00),\n",
       "  (844, 38.00),\n",
       "  (854, 38.00),\n",
       "  (20, 37.00),\n",
       "  (52, 37.00),\n",
       "  (152, 37.00),\n",
       "  (154, 37.00),\n",
       "  (175, 37.00),\n",
       "  (183, 37.00),\n",
       "  (210, 37.00),\n",
       "  (235, 37.00),\n",
       "  (262, 37.00),\n",
       "  (296, 37.00),\n",
       "  (347, 37.00),\n",
       "  (435, 37.00),\n",
       "  (462, 37.00),\n",
       "  (466, 37.00),\n",
       "  (494, 37.00),\n",
       "  (554, 37.00),\n",
       "  (600, 37.00),\n",
       "  (610, 37.00),\n",
       "  (630, 37.00),\n",
       "  (678, 37.00),\n",
       "  (814, 37.00),\n",
       "  (921, 37.00),\n",
       "  (98, 36.00),\n",
       "  (200, 36.00),\n",
       "  (205, 36.00),\n",
       "  (207, 36.00),\n",
       "  (399, 36.00),\n",
       "  (408, 36.00),\n",
       "  (453, 36.00),\n",
       "  (511, 36.00),\n",
       "  (513, 36.00),\n",
       "  (650, 36.00),\n",
       "  (693, 36.00),\n",
       "  (719, 36.00),\n",
       "  (720, 36.00),\n",
       "  (861, 36.00),\n",
       "  (928, 36.00),\n",
       "  (934, 36.00),\n",
       "  (120, 35.00),\n",
       "  (143, 35.00),\n",
       "  (248, 35.00),\n",
       "  (346, 35.00),\n",
       "  (356, 35.00),\n",
       "  (605, 35.00),\n",
       "  (802, 35.00),\n",
       "  (43, 34.00),\n",
       "  (45, 34.00),\n",
       "  (62, 34.00),\n",
       "  (86, 34.00),\n",
       "  (220, 34.00),\n",
       "  (359, 34.00),\n",
       "  (374, 34.00),\n",
       "  (450, 34.00),\n",
       "  (464, 34.00),\n",
       "  (615, 34.00),\n",
       "  (677, 34.00),\n",
       "  (745, 34.00),\n",
       "  (793, 34.00),\n",
       "  (798, 34.00),\n",
       "  (827, 34.00),\n",
       "  (947, 34.00),\n",
       "  (958, 34.00),\n",
       "  (964, 34.00),\n",
       "  (59, 33.00),\n",
       "  (158, 33.00),\n",
       "  (213, 33.00),\n",
       "  (226, 33.00),\n",
       "  (233, 33.00),\n",
       "  (402, 33.00),\n",
       "  (403, 33.00),\n",
       "  (409, 33.00),\n",
       "  (415, 33.00),\n",
       "  (588, 33.00),\n",
       "  (691, 33.00),\n",
       "  (954, 33.00),\n",
       "  (26, 32.00),\n",
       "  (64, 32.00),\n",
       "  (73, 32.00),\n",
       "  (106, 32.00),\n",
       "  (385, 32.00),\n",
       "  (427, 32.00),\n",
       "  (465, 32.00),\n",
       "  (543, 32.00),\n",
       "  (627, 32.00),\n",
       "  (760, 32.00),\n",
       "  (786, 32.00),\n",
       "  (859, 32.00),\n",
       "  (897, 32.00),\n",
       "  (948, 32.00),\n",
       "  (980, 32.00),\n",
       "  (122, 31.00),\n",
       "  (371, 31.00),\n",
       "  (461, 31.00),\n",
       "  (469, 31.00),\n",
       "  (484, 31.00),\n",
       "  (502, 31.00),\n",
       "  (521, 31.00),\n",
       "  (531, 31.00),\n",
       "  (551, 31.00),\n",
       "  (568, 31.00),\n",
       "  (578, 31.00),\n",
       "  (644, 31.00),\n",
       "  (647, 31.00),\n",
       "  (705, 31.00),\n",
       "  (744, 31.00),\n",
       "  (789, 31.00),\n",
       "  (846, 31.00),\n",
       "  (914, 31.00),\n",
       "  (81, 30.00),\n",
       "  (146, 30.00),\n",
       "  (166, 30.00),\n",
       "  (179, 30.00),\n",
       "  (345, 30.00),\n",
       "  (459, 30.00),\n",
       "  (557, 30.00),\n",
       "  (589, 30.00),\n",
       "  (617, 30.00),\n",
       "  (629, 30.00),\n",
       "  (632, 30.00),\n",
       "  (803, 30.00),\n",
       "  (966, 30.00),\n",
       "  (303, 29.00),\n",
       "  (498, 29.00),\n",
       "  (585, 29.00),\n",
       "  (634, 29.00),\n",
       "  (4, 28.00),\n",
       "  (29, 28.00),\n",
       "  (54, 28.00),\n",
       "  (567, 28.00),\n",
       "  (587, 28.00),\n",
       "  (631, 28.00),\n",
       "  (663, 28.00),\n",
       "  (718, 28.00),\n",
       "  (731, 28.00),\n",
       "  (767, 28.00),\n",
       "  (860, 28.00),\n",
       "  (876, 28.00),\n",
       "  (909, 28.00),\n",
       "  (923, 28.00),\n",
       "  (147, 27.00),\n",
       "  (187, 27.00),\n",
       "  (202, 27.00),\n",
       "  (479, 27.00),\n",
       "  (482, 27.00),\n",
       "  (660, 27.00),\n",
       "  (733, 27.00),\n",
       "  (965, 27.00),\n",
       "  (49, 26.00),\n",
       "  (150, 26.00),\n",
       "  (271, 26.00),\n",
       "  (624, 26.00),\n",
       "  (676, 26.00),\n",
       "  (686, 26.00),\n",
       "  (785, 26.00),\n",
       "  (869, 26.00),\n",
       "  (931, 26.00),\n",
       "  (32, 25.00),\n",
       "  (163, 25.00),\n",
       "  (240, 25.00),\n",
       "  (369, 25.00),\n",
       "  (544, 25.00),\n",
       "  (804, 25.00),\n",
       "  (885, 25.00),\n",
       "  (516, 24.00),\n",
       "  (536, 24.00),\n",
       "  (590, 24.00),\n",
       "  (596, 24.00),\n",
       "  (747, 24.00),\n",
       "  (856, 24.00),\n",
       "  (901, 24.00),\n",
       "  (168, 23.00),\n",
       "  (185, 23.00),\n",
       "  (315, 23.00),\n",
       "  (394, 23.00),\n",
       "  (438, 23.00),\n",
       "  (623, 23.00),\n",
       "  (680, 23.00),\n",
       "  (811, 23.00),\n",
       "  (838, 23.00),\n",
       "  (908, 23.00),\n",
       "  (911, 23.00),\n",
       "  (974, 23.00),\n",
       "  (165, 22.00),\n",
       "  (550, 22.00),\n",
       "  (638, 22.00),\n",
       "  (675, 22.00),\n",
       "  (942, 22.00),\n",
       "  (499, 21.00),\n",
       "  (525, 21.00),\n",
       "  (773, 21.00),\n",
       "  (68, 20.00),\n",
       "  (662, 20.00),\n",
       "  (715, 20.00),\n",
       "  (729, 20.00),\n",
       "  (740, 20.00),\n",
       "  (246, 19.00),\n",
       "  (400, 19.00),\n",
       "  (598, 19.00),\n",
       "  (622, 19.00),\n",
       "  (648, 18.00),\n",
       "  (651, 18.00),\n",
       "  (728, 18.00),\n",
       "  (836, 18.00),\n",
       "  (977, 18.00),\n",
       "  (504, 17.00),\n",
       "  (813, 17.00),\n",
       "  (906, 17.00),\n",
       "  (976, 17.00),\n",
       "  (460, 16.00),\n",
       "  (534, 16.00),\n",
       "  (841, 16.00),\n",
       "  (930, 16.00),\n",
       "  (970, 15.00),\n",
       "  (103, 14.00),\n",
       "  (282, 14.00),\n",
       "  (742, 14.00),\n",
       "  (969, 14.00),\n",
       "  (34, 13.00),\n",
       "  (446, 13.00),\n",
       "  (493, 13.00),\n",
       "  (967, 12.00),\n",
       "  (167, 11.00),\n",
       "  (940, 11.00),\n",
       "  (978, 11.00),\n",
       "  (689, 10.00),\n",
       "  (899, 9.00),\n",
       "  (960, 9.00),\n",
       "  (673, 8.00),\n",
       "  (681, 8.00),\n",
       "  (810, 8.00),\n",
       "  (935, 8.00),\n",
       "  (961, 2.00)])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = targeted_diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch_no 0\n",
      "at batch_no 100\n",
      "at batch_no 200\n",
      "at batch_no 300\n",
      "at batch_no 400\n",
      "at batch_no 500\n",
      "at batch_no 600\n",
      "at batch_no 700\n",
      "at batch_no 800\n",
      "at batch_no 900\n",
      "at batch_no 1000\n",
      "at batch_no 1100\n",
      "at batch_no 1200\n",
      "at batch_no 1300\n",
      "at batch_no 1400\n",
      "at batch_no 1500\n",
      "at batch_no 1600\n",
      "at batch_no 1700\n",
      "at batch_no 1800\n",
      "at batch_no 1900\n",
      "at batch_no 2000\n",
      "at batch_no 2100\n",
      "at batch_no 2200\n",
      "at batch_no 2300\n",
      "at batch_no 2400\n",
      "at batch_no 2500\n",
      "at batch_no 2600\n",
      "at batch_no 2700\n",
      "at batch_no 2800\n",
      "at batch_no 2900\n",
      "at batch_no 3000\n",
      "at batch_no 3100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(885,\n",
       " [(815, 227.00),\n",
       "  (652, 183.00),\n",
       "  (646, 172.00),\n",
       "  (580, 155.00),\n",
       "  (591, 149.00),\n",
       "  (621, 148.00),\n",
       "  (737, 148.00),\n",
       "  (611, 147.00),\n",
       "  (489, 136.00),\n",
       "  (904, 129.00),\n",
       "  (497, 125.00),\n",
       "  (868, 125.00),\n",
       "  (979, 124.00),\n",
       "  (582, 122.00),\n",
       "  (794, 122.00),\n",
       "  (94, 118.00),\n",
       "  (955, 118.00),\n",
       "  (893, 117.00),\n",
       "  (721, 116.00),\n",
       "  (116, 114.00),\n",
       "  (491, 108.00),\n",
       "  (565, 108.00),\n",
       "  (741, 107.00),\n",
       "  (562, 106.00),\n",
       "  (679, 106.00),\n",
       "  (39, 105.00),\n",
       "  (839, 104.00),\n",
       "  (109, 102.00),\n",
       "  (162, 101.00),\n",
       "  (750, 100.00),\n",
       "  (46, 98.00),\n",
       "  (695, 98.00),\n",
       "  (549, 97.00),\n",
       "  (82, 95.00),\n",
       "  (199, 95.00),\n",
       "  (492, 95.00),\n",
       "  (84, 94.00),\n",
       "  (973, 94.00),\n",
       "  (51, 93.00),\n",
       "  (151, 91.00),\n",
       "  (48, 90.00),\n",
       "  (424, 89.00),\n",
       "  (843, 89.00),\n",
       "  (203, 87.00),\n",
       "  (692, 87.00),\n",
       "  (971, 87.00),\n",
       "  (640, 86.00),\n",
       "  (669, 86.00),\n",
       "  (197, 84.00),\n",
       "  (440, 84.00),\n",
       "  (879, 84.00),\n",
       "  (47, 83.00),\n",
       "  (180, 83.00),\n",
       "  (577, 83.00),\n",
       "  (208, 81.00),\n",
       "  (743, 81.00),\n",
       "  (783, 81.00),\n",
       "  (847, 81.00),\n",
       "  (189, 80.00),\n",
       "  (281, 80.00),\n",
       "  (382, 80.00),\n",
       "  (406, 80.00),\n",
       "  (703, 80.00),\n",
       "  (824, 80.00),\n",
       "  (905, 80.00),\n",
       "  (61, 79.00),\n",
       "  (318, 79.00),\n",
       "  (319, 79.00),\n",
       "  (411, 79.00),\n",
       "  (762, 79.00),\n",
       "  (866, 79.00),\n",
       "  (219, 78.00),\n",
       "  (310, 78.00),\n",
       "  (938, 78.00),\n",
       "  (76, 77.00),\n",
       "  (270, 77.00),\n",
       "  (364, 77.00),\n",
       "  (849, 77.00),\n",
       "  (182, 76.00),\n",
       "  (298, 76.00),\n",
       "  (791, 76.00),\n",
       "  (828, 76.00),\n",
       "  (896, 76.00),\n",
       "  (956, 76.00),\n",
       "  (963, 76.00),\n",
       "  (55, 75.00),\n",
       "  (217, 75.00),\n",
       "  (342, 75.00),\n",
       "  (343, 75.00),\n",
       "  (730, 75.00),\n",
       "  (778, 75.00),\n",
       "  (830, 75.00),\n",
       "  (455, 74.00),\n",
       "  (483, 74.00),\n",
       "  (800, 74.00),\n",
       "  (982, 74.00),\n",
       "  (304, 73.00),\n",
       "  (363, 73.00),\n",
       "  (436, 73.00),\n",
       "  (471, 73.00),\n",
       "  (472, 73.00),\n",
       "  (645, 73.00),\n",
       "  (805, 73.00),\n",
       "  (128, 72.00),\n",
       "  (725, 72.00),\n",
       "  (754, 72.00),\n",
       "  (878, 71.00),\n",
       "  (192, 70.00),\n",
       "  (341, 70.00),\n",
       "  (454, 70.00),\n",
       "  (556, 70.00),\n",
       "  (572, 70.00),\n",
       "  (735, 70.00),\n",
       "  (806, 70.00),\n",
       "  (825, 70.00),\n",
       "  (826, 70.00),\n",
       "  (178, 69.00),\n",
       "  (195, 69.00),\n",
       "  (570, 69.00),\n",
       "  (671, 69.00),\n",
       "  (716, 69.00),\n",
       "  (23, 68.00),\n",
       "  (50, 68.00),\n",
       "  (654, 68.00),\n",
       "  (700, 68.00),\n",
       "  (775, 68.00),\n",
       "  (30, 67.00),\n",
       "  (77, 67.00),\n",
       "  (425, 67.00),\n",
       "  (496, 67.00),\n",
       "  (845, 67.00),\n",
       "  (916, 67.00),\n",
       "  (949, 67.00),\n",
       "  (37, 66.00),\n",
       "  (124, 66.00),\n",
       "  (420, 66.00),\n",
       "  (586, 66.00),\n",
       "  (620, 66.00),\n",
       "  (784, 66.00),\n",
       "  (895, 66.00),\n",
       "  (926, 66.00),\n",
       "  (155, 65.00),\n",
       "  (234, 65.00),\n",
       "  (238, 65.00),\n",
       "  (293, 65.00),\n",
       "  (313, 65.00),\n",
       "  (649, 65.00),\n",
       "  (688, 65.00),\n",
       "  (820, 65.00),\n",
       "  (863, 65.00),\n",
       "  (864, 65.00),\n",
       "  (870, 65.00),\n",
       "  (871, 65.00),\n",
       "  (892, 65.00),\n",
       "  (988, 65.00),\n",
       "  (6, 64.00),\n",
       "  (74, 64.00),\n",
       "  (222, 64.00),\n",
       "  (237, 64.00),\n",
       "  (361, 64.00),\n",
       "  (463, 64.00),\n",
       "  (468, 64.00),\n",
       "  (711, 64.00),\n",
       "  (772, 64.00),\n",
       "  (887, 64.00),\n",
       "  (944, 64.00),\n",
       "  (97, 63.00),\n",
       "  (119, 63.00),\n",
       "  (331, 63.00),\n",
       "  (474, 63.00),\n",
       "  (819, 63.00),\n",
       "  (913, 63.00),\n",
       "  (972, 63.00),\n",
       "  (8, 62.00),\n",
       "  (90, 62.00),\n",
       "  (99, 62.00),\n",
       "  (125, 62.00),\n",
       "  (161, 62.00),\n",
       "  (184, 62.00),\n",
       "  (272, 62.00),\n",
       "  (300, 62.00),\n",
       "  (348, 62.00),\n",
       "  (506, 62.00),\n",
       "  (515, 62.00),\n",
       "  (593, 62.00),\n",
       "  (788, 62.00),\n",
       "  (808, 62.00),\n",
       "  (842, 62.00),\n",
       "  (875, 62.00),\n",
       "  (880, 62.00),\n",
       "  (992, 62.00),\n",
       "  (135, 61.00),\n",
       "  (225, 61.00),\n",
       "  (280, 61.00),\n",
       "  (292, 61.00),\n",
       "  (311, 61.00),\n",
       "  (316, 61.00),\n",
       "  (476, 61.00),\n",
       "  (505, 61.00),\n",
       "  (561, 61.00),\n",
       "  (609, 61.00),\n",
       "  (698, 61.00),\n",
       "  (809, 61.00),\n",
       "  (850, 61.00),\n",
       "  (937, 61.00),\n",
       "  (946, 61.00),\n",
       "  (953, 61.00),\n",
       "  (181, 60.00),\n",
       "  (372, 60.00),\n",
       "  (401, 60.00),\n",
       "  (532, 60.00),\n",
       "  (539, 60.00),\n",
       "  (619, 60.00),\n",
       "  (697, 60.00),\n",
       "  (781, 60.00),\n",
       "  (884, 60.00),\n",
       "  (113, 59.00),\n",
       "  (327, 59.00),\n",
       "  (391, 59.00),\n",
       "  (404, 59.00),\n",
       "  (481, 59.00),\n",
       "  (490, 59.00),\n",
       "  (522, 59.00),\n",
       "  (581, 59.00),\n",
       "  (607, 59.00),\n",
       "  (655, 59.00),\n",
       "  (724, 59.00),\n",
       "  (774, 59.00),\n",
       "  (912, 59.00),\n",
       "  (952, 59.00),\n",
       "  (987, 59.00),\n",
       "  (60, 58.00),\n",
       "  (211, 58.00),\n",
       "  (249, 58.00),\n",
       "  (284, 58.00),\n",
       "  (317, 58.00),\n",
       "  (407, 58.00),\n",
       "  (410, 58.00),\n",
       "  (423, 58.00),\n",
       "  (458, 58.00),\n",
       "  (563, 58.00),\n",
       "  (594, 58.00),\n",
       "  (595, 58.00),\n",
       "  (597, 58.00),\n",
       "  (603, 58.00),\n",
       "  (641, 58.00),\n",
       "  (696, 58.00),\n",
       "  (709, 58.00),\n",
       "  (790, 58.00),\n",
       "  (857, 58.00),\n",
       "  (21, 57.00),\n",
       "  (307, 57.00),\n",
       "  (419, 57.00),\n",
       "  (441, 57.00),\n",
       "  (457, 57.00),\n",
       "  (612, 57.00),\n",
       "  (618, 57.00),\n",
       "  (636, 57.00),\n",
       "  (801, 57.00),\n",
       "  (69, 56.00),\n",
       "  (118, 56.00),\n",
       "  (231, 56.00),\n",
       "  (232, 56.00),\n",
       "  (263, 56.00),\n",
       "  (275, 56.00),\n",
       "  (308, 56.00),\n",
       "  (334, 56.00),\n",
       "  (367, 56.00),\n",
       "  (452, 56.00),\n",
       "  (477, 56.00),\n",
       "  (527, 56.00),\n",
       "  (547, 56.00),\n",
       "  (564, 56.00),\n",
       "  (635, 56.00),\n",
       "  (642, 56.00),\n",
       "  (653, 56.00),\n",
       "  (657, 56.00),\n",
       "  (661, 56.00),\n",
       "  (780, 56.00),\n",
       "  (816, 56.00),\n",
       "  (822, 56.00),\n",
       "  (957, 56.00),\n",
       "  (962, 56.00),\n",
       "  (975, 56.00),\n",
       "  (25, 55.00),\n",
       "  (31, 55.00),\n",
       "  (33, 55.00),\n",
       "  (57, 55.00),\n",
       "  (88, 55.00),\n",
       "  (104, 55.00),\n",
       "  (115, 55.00),\n",
       "  (129, 55.00),\n",
       "  (228, 55.00),\n",
       "  (328, 55.00),\n",
       "  (383, 55.00),\n",
       "  (396, 55.00),\n",
       "  (428, 55.00),\n",
       "  (509, 55.00),\n",
       "  (614, 55.00),\n",
       "  (738, 55.00),\n",
       "  (748, 55.00),\n",
       "  (777, 55.00),\n",
       "  (834, 55.00),\n",
       "  (852, 55.00),\n",
       "  (877, 55.00),\n",
       "  (985, 55.00),\n",
       "  (24, 54.00),\n",
       "  (58, 54.00),\n",
       "  (67, 54.00),\n",
       "  (70, 54.00),\n",
       "  (85, 54.00),\n",
       "  (170, 54.00),\n",
       "  (250, 54.00),\n",
       "  (259, 54.00),\n",
       "  (274, 54.00),\n",
       "  (444, 54.00),\n",
       "  (448, 54.00),\n",
       "  (451, 54.00),\n",
       "  (519, 54.00),\n",
       "  (524, 54.00),\n",
       "  (552, 54.00),\n",
       "  (569, 54.00),\n",
       "  (601, 54.00),\n",
       "  (766, 54.00),\n",
       "  (770, 54.00),\n",
       "  (818, 54.00),\n",
       "  (829, 54.00),\n",
       "  (855, 54.00),\n",
       "  (858, 54.00),\n",
       "  (950, 54.00),\n",
       "  (991, 54.00),\n",
       "  (995, 54.00),\n",
       "  (89, 53.00),\n",
       "  (171, 53.00),\n",
       "  (196, 53.00),\n",
       "  (216, 53.00),\n",
       "  (218, 53.00),\n",
       "  (256, 53.00),\n",
       "  (269, 53.00),\n",
       "  (276, 53.00),\n",
       "  (283, 53.00),\n",
       "  (289, 53.00),\n",
       "  (352, 53.00),\n",
       "  (375, 53.00),\n",
       "  (608, 53.00),\n",
       "  (626, 53.00),\n",
       "  (701, 53.00),\n",
       "  (768, 53.00),\n",
       "  (832, 53.00),\n",
       "  (903, 53.00),\n",
       "  (924, 53.00),\n",
       "  (936, 53.00),\n",
       "  (986, 53.00),\n",
       "  (0, 52.00),\n",
       "  (3, 52.00),\n",
       "  (36, 52.00),\n",
       "  (92, 52.00),\n",
       "  (96, 52.00),\n",
       "  (164, 52.00),\n",
       "  (172, 52.00),\n",
       "  (209, 52.00),\n",
       "  (241, 52.00),\n",
       "  (251, 52.00),\n",
       "  (254, 52.00),\n",
       "  (277, 52.00),\n",
       "  (291, 52.00),\n",
       "  (467, 52.00),\n",
       "  (528, 52.00),\n",
       "  (685, 52.00),\n",
       "  (757, 52.00),\n",
       "  (765, 52.00),\n",
       "  (792, 52.00),\n",
       "  (840, 52.00),\n",
       "  (865, 52.00),\n",
       "  (888, 52.00),\n",
       "  (922, 52.00),\n",
       "  (939, 52.00),\n",
       "  (990, 52.00),\n",
       "  (997, 52.00),\n",
       "  (15, 51.00),\n",
       "  (78, 51.00),\n",
       "  (114, 51.00),\n",
       "  (206, 51.00),\n",
       "  (229, 51.00),\n",
       "  (239, 51.00),\n",
       "  (285, 51.00),\n",
       "  (286, 51.00),\n",
       "  (336, 51.00),\n",
       "  (358, 51.00),\n",
       "  (378, 51.00),\n",
       "  (386, 51.00),\n",
       "  (395, 51.00),\n",
       "  (413, 51.00),\n",
       "  (431, 51.00),\n",
       "  (487, 51.00),\n",
       "  (488, 51.00),\n",
       "  (512, 51.00),\n",
       "  (526, 51.00),\n",
       "  (533, 51.00),\n",
       "  (535, 51.00),\n",
       "  (592, 51.00),\n",
       "  (667, 51.00),\n",
       "  (752, 51.00),\n",
       "  (753, 51.00),\n",
       "  (835, 51.00),\n",
       "  (886, 51.00),\n",
       "  (902, 51.00),\n",
       "  (1, 50.00),\n",
       "  (41, 50.00),\n",
       "  (126, 50.00),\n",
       "  (138, 50.00),\n",
       "  (176, 50.00),\n",
       "  (177, 50.00),\n",
       "  (194, 50.00),\n",
       "  (253, 50.00),\n",
       "  (261, 50.00),\n",
       "  (267, 50.00),\n",
       "  (294, 50.00),\n",
       "  (362, 50.00),\n",
       "  (373, 50.00),\n",
       "  (443, 50.00),\n",
       "  (449, 50.00),\n",
       "  (508, 50.00),\n",
       "  (514, 50.00),\n",
       "  (523, 50.00),\n",
       "  (545, 50.00),\n",
       "  (602, 50.00),\n",
       "  (606, 50.00),\n",
       "  (616, 50.00),\n",
       "  (665, 50.00),\n",
       "  (746, 50.00),\n",
       "  (771, 50.00),\n",
       "  (779, 50.00),\n",
       "  (797, 50.00),\n",
       "  (874, 50.00),\n",
       "  (917, 50.00),\n",
       "  (9, 49.00),\n",
       "  (12, 49.00),\n",
       "  (13, 49.00),\n",
       "  (35, 49.00),\n",
       "  (71, 49.00),\n",
       "  (102, 49.00),\n",
       "  (121, 49.00),\n",
       "  (159, 49.00),\n",
       "  (160, 49.00),\n",
       "  (214, 49.00),\n",
       "  (266, 49.00),\n",
       "  (301, 49.00),\n",
       "  (340, 49.00),\n",
       "  (387, 49.00),\n",
       "  (388, 49.00),\n",
       "  (541, 49.00),\n",
       "  (604, 49.00),\n",
       "  (639, 49.00),\n",
       "  (664, 49.00),\n",
       "  (739, 49.00),\n",
       "  (764, 49.00),\n",
       "  (776, 49.00),\n",
       "  (833, 49.00),\n",
       "  (890, 49.00),\n",
       "  (915, 49.00),\n",
       "  (927, 49.00),\n",
       "  (11, 48.00),\n",
       "  (14, 48.00),\n",
       "  (18, 48.00),\n",
       "  (72, 48.00),\n",
       "  (95, 48.00),\n",
       "  (100, 48.00),\n",
       "  (133, 48.00),\n",
       "  (134, 48.00),\n",
       "  (149, 48.00),\n",
       "  (156, 48.00),\n",
       "  (169, 48.00),\n",
       "  (221, 48.00),\n",
       "  (247, 48.00),\n",
       "  (255, 48.00),\n",
       "  (295, 48.00),\n",
       "  (320, 48.00),\n",
       "  (321, 48.00),\n",
       "  (325, 48.00),\n",
       "  (337, 48.00),\n",
       "  (376, 48.00),\n",
       "  (398, 48.00),\n",
       "  (432, 48.00),\n",
       "  (433, 48.00),\n",
       "  (478, 48.00),\n",
       "  (503, 48.00),\n",
       "  (517, 48.00),\n",
       "  (583, 48.00),\n",
       "  (628, 48.00),\n",
       "  (668, 48.00),\n",
       "  (683, 48.00),\n",
       "  (704, 48.00),\n",
       "  (727, 48.00),\n",
       "  (787, 48.00),\n",
       "  (848, 48.00),\n",
       "  (881, 48.00),\n",
       "  (943, 48.00),\n",
       "  (951, 48.00),\n",
       "  (989, 48.00),\n",
       "  (994, 48.00),\n",
       "  (22, 47.00),\n",
       "  (56, 47.00),\n",
       "  (79, 47.00),\n",
       "  (87, 47.00),\n",
       "  (123, 47.00),\n",
       "  (137, 47.00),\n",
       "  (139, 47.00),\n",
       "  (141, 47.00),\n",
       "  (193, 47.00),\n",
       "  (273, 47.00),\n",
       "  (288, 47.00),\n",
       "  (290, 47.00),\n",
       "  (326, 47.00),\n",
       "  (333, 47.00),\n",
       "  (335, 47.00),\n",
       "  (344, 47.00),\n",
       "  (351, 47.00),\n",
       "  (354, 47.00),\n",
       "  (366, 47.00),\n",
       "  (416, 47.00),\n",
       "  (422, 47.00),\n",
       "  (445, 47.00),\n",
       "  (518, 47.00),\n",
       "  (571, 47.00),\n",
       "  (576, 47.00),\n",
       "  (625, 47.00),\n",
       "  (637, 47.00),\n",
       "  (666, 47.00),\n",
       "  (684, 47.00),\n",
       "  (707, 47.00),\n",
       "  (751, 47.00),\n",
       "  (763, 47.00),\n",
       "  (769, 47.00),\n",
       "  (807, 47.00),\n",
       "  (889, 47.00),\n",
       "  (891, 47.00),\n",
       "  (918, 47.00),\n",
       "  (983, 47.00),\n",
       "  (5, 46.00),\n",
       "  (28, 46.00),\n",
       "  (53, 46.00),\n",
       "  (66, 46.00),\n",
       "  (130, 46.00),\n",
       "  (136, 46.00),\n",
       "  (144, 46.00),\n",
       "  (157, 46.00),\n",
       "  (191, 46.00),\n",
       "  (201, 46.00),\n",
       "  (212, 46.00),\n",
       "  (243, 46.00),\n",
       "  (257, 46.00),\n",
       "  (264, 46.00),\n",
       "  (279, 46.00),\n",
       "  (312, 46.00),\n",
       "  (323, 46.00),\n",
       "  (350, 46.00),\n",
       "  (377, 46.00),\n",
       "  (384, 46.00),\n",
       "  (392, 46.00),\n",
       "  (560, 46.00),\n",
       "  (566, 46.00),\n",
       "  (579, 46.00),\n",
       "  (659, 46.00),\n",
       "  (702, 46.00),\n",
       "  (706, 46.00),\n",
       "  (732, 46.00),\n",
       "  (823, 46.00),\n",
       "  (872, 46.00),\n",
       "  (993, 46.00),\n",
       "  (16, 45.00),\n",
       "  (40, 45.00),\n",
       "  (75, 45.00),\n",
       "  (105, 45.00),\n",
       "  (110, 45.00),\n",
       "  (131, 45.00),\n",
       "  (188, 45.00),\n",
       "  (215, 45.00),\n",
       "  (252, 45.00),\n",
       "  (324, 45.00),\n",
       "  (339, 45.00),\n",
       "  (355, 45.00),\n",
       "  (365, 45.00),\n",
       "  (381, 45.00),\n",
       "  (389, 45.00),\n",
       "  (397, 45.00),\n",
       "  (426, 45.00),\n",
       "  (475, 45.00),\n",
       "  (537, 45.00),\n",
       "  (613, 45.00),\n",
       "  (674, 45.00),\n",
       "  (758, 45.00),\n",
       "  (759, 45.00),\n",
       "  (796, 45.00),\n",
       "  (853, 45.00),\n",
       "  (867, 45.00),\n",
       "  (882, 45.00),\n",
       "  (900, 45.00),\n",
       "  (968, 45.00),\n",
       "  (981, 45.00),\n",
       "  (2, 44.00),\n",
       "  (17, 44.00),\n",
       "  (42, 44.00),\n",
       "  (44, 44.00),\n",
       "  (101, 44.00),\n",
       "  (107, 44.00),\n",
       "  (132, 44.00),\n",
       "  (227, 44.00),\n",
       "  (245, 44.00),\n",
       "  (287, 44.00),\n",
       "  (370, 44.00),\n",
       "  (412, 44.00),\n",
       "  (414, 44.00),\n",
       "  (447, 44.00),\n",
       "  (817, 44.00),\n",
       "  (821, 44.00),\n",
       "  (929, 44.00),\n",
       "  (941, 44.00),\n",
       "  (959, 44.00),\n",
       "  (63, 43.00),\n",
       "  (65, 43.00),\n",
       "  (83, 43.00),\n",
       "  (93, 43.00),\n",
       "  (174, 43.00),\n",
       "  (244, 43.00),\n",
       "  (299, 43.00),\n",
       "  (330, 43.00),\n",
       "  (332, 43.00),\n",
       "  (349, 43.00),\n",
       "  (437, 43.00),\n",
       "  (456, 43.00),\n",
       "  (485, 43.00),\n",
       "  (520, 43.00),\n",
       "  (574, 43.00),\n",
       "  (575, 43.00),\n",
       "  (599, 43.00),\n",
       "  (658, 43.00),\n",
       "  (682, 43.00),\n",
       "  (710, 43.00),\n",
       "  (717, 43.00),\n",
       "  (723, 43.00),\n",
       "  (755, 43.00),\n",
       "  (756, 43.00),\n",
       "  (761, 43.00),\n",
       "  (782, 43.00),\n",
       "  (795, 43.00),\n",
       "  (873, 43.00),\n",
       "  (898, 43.00),\n",
       "  (907, 43.00),\n",
       "  (910, 43.00),\n",
       "  (919, 43.00),\n",
       "  (984, 43.00),\n",
       "  (999, 43.00),\n",
       "  (108, 42.00),\n",
       "  (112, 42.00),\n",
       "  (153, 42.00),\n",
       "  (223, 42.00),\n",
       "  (260, 42.00),\n",
       "  (265, 42.00),\n",
       "  (268, 42.00),\n",
       "  (305, 42.00),\n",
       "  (329, 42.00),\n",
       "  (353, 42.00),\n",
       "  (429, 42.00),\n",
       "  (430, 42.00),\n",
       "  (434, 42.00),\n",
       "  (473, 42.00),\n",
       "  (495, 42.00),\n",
       "  (510, 42.00),\n",
       "  (558, 42.00),\n",
       "  (633, 42.00),\n",
       "  (643, 42.00),\n",
       "  (672, 42.00),\n",
       "  (690, 42.00),\n",
       "  (996, 42.00),\n",
       "  (7, 41.00),\n",
       "  (19, 41.00),\n",
       "  (80, 41.00),\n",
       "  (142, 41.00),\n",
       "  (154, 41.00),\n",
       "  (173, 41.00),\n",
       "  (190, 41.00),\n",
       "  (258, 41.00),\n",
       "  (302, 41.00),\n",
       "  (306, 41.00),\n",
       "  (322, 41.00),\n",
       "  (360, 41.00),\n",
       "  (390, 41.00),\n",
       "  (393, 41.00),\n",
       "  (421, 41.00),\n",
       "  (462, 41.00),\n",
       "  (486, 41.00),\n",
       "  (501, 41.00),\n",
       "  (507, 41.00),\n",
       "  (530, 41.00),\n",
       "  (573, 41.00),\n",
       "  (712, 41.00),\n",
       "  (714, 41.00),\n",
       "  (734, 41.00),\n",
       "  (837, 41.00),\n",
       "  (844, 41.00),\n",
       "  (862, 41.00),\n",
       "  (933, 41.00),\n",
       "  (998, 41.00),\n",
       "  (27, 40.00),\n",
       "  (117, 40.00),\n",
       "  (127, 40.00),\n",
       "  (186, 40.00),\n",
       "  (210, 40.00),\n",
       "  (297, 40.00),\n",
       "  (368, 40.00),\n",
       "  (380, 40.00),\n",
       "  (417, 40.00),\n",
       "  (442, 40.00),\n",
       "  (466, 40.00),\n",
       "  (538, 40.00),\n",
       "  (630, 40.00),\n",
       "  (670, 40.00),\n",
       "  (713, 40.00),\n",
       "  (722, 40.00),\n",
       "  (749, 40.00),\n",
       "  (799, 40.00),\n",
       "  (812, 40.00),\n",
       "  (851, 40.00),\n",
       "  (920, 40.00),\n",
       "  (925, 40.00),\n",
       "  (52, 39.00),\n",
       "  (91, 39.00),\n",
       "  (140, 39.00),\n",
       "  (183, 39.00),\n",
       "  (224, 39.00),\n",
       "  (236, 39.00),\n",
       "  (242, 39.00),\n",
       "  (248, 39.00),\n",
       "  (309, 39.00),\n",
       "  (346, 39.00),\n",
       "  (357, 39.00),\n",
       "  (405, 39.00),\n",
       "  (500, 39.00),\n",
       "  (548, 39.00),\n",
       "  (559, 39.00),\n",
       "  (694, 39.00),\n",
       "  (699, 39.00),\n",
       "  (708, 39.00),\n",
       "  (726, 39.00),\n",
       "  (894, 39.00),\n",
       "  (932, 39.00),\n",
       "  (10, 38.00),\n",
       "  (43, 38.00),\n",
       "  (262, 38.00),\n",
       "  (347, 38.00),\n",
       "  (427, 38.00),\n",
       "  (464, 38.00),\n",
       "  (480, 38.00),\n",
       "  (540, 38.00),\n",
       "  (542, 38.00),\n",
       "  (555, 38.00),\n",
       "  (584, 38.00),\n",
       "  (656, 38.00),\n",
       "  (719, 38.00),\n",
       "  (736, 38.00),\n",
       "  (814, 38.00),\n",
       "  (859, 38.00),\n",
       "  (921, 38.00),\n",
       "  (928, 38.00),\n",
       "  (38, 37.00),\n",
       "  (98, 37.00),\n",
       "  (111, 37.00),\n",
       "  (198, 37.00),\n",
       "  (200, 37.00),\n",
       "  (207, 37.00),\n",
       "  (230, 37.00),\n",
       "  (235, 37.00),\n",
       "  (278, 37.00),\n",
       "  (296, 37.00),\n",
       "  (338, 37.00),\n",
       "  (374, 37.00),\n",
       "  (379, 37.00),\n",
       "  (418, 37.00),\n",
       "  (439, 37.00),\n",
       "  (513, 37.00),\n",
       "  (529, 37.00),\n",
       "  (553, 37.00),\n",
       "  (554, 37.00),\n",
       "  (786, 37.00),\n",
       "  (854, 37.00),\n",
       "  (945, 37.00),\n",
       "  (45, 36.00),\n",
       "  (62, 36.00),\n",
       "  (143, 36.00),\n",
       "  (145, 36.00),\n",
       "  (148, 36.00),\n",
       "  (152, 36.00),\n",
       "  (175, 36.00),\n",
       "  (204, 36.00),\n",
       "  (205, 36.00),\n",
       "  (314, 36.00),\n",
       "  (385, 36.00),\n",
       "  (399, 36.00),\n",
       "  (408, 36.00),\n",
       "  (435, 36.00),\n",
       "  (453, 36.00),\n",
       "  (470, 36.00),\n",
       "  (546, 36.00),\n",
       "  (588, 36.00),\n",
       "  (605, 36.00),\n",
       "  (610, 36.00),\n",
       "  (615, 36.00),\n",
       "  (678, 36.00),\n",
       "  (687, 36.00),\n",
       "  (720, 36.00),\n",
       "  (793, 36.00),\n",
       "  (802, 36.00),\n",
       "  (934, 36.00),\n",
       "  (20, 35.00),\n",
       "  (120, 35.00),\n",
       "  (213, 35.00),\n",
       "  (226, 35.00),\n",
       "  (359, 35.00),\n",
       "  (600, 35.00),\n",
       "  (632, 35.00),\n",
       "  (827, 35.00),\n",
       "  (831, 35.00),\n",
       "  (846, 35.00),\n",
       "  (883, 35.00),\n",
       "  (958, 35.00),\n",
       "  (26, 34.00),\n",
       "  (73, 34.00),\n",
       "  (220, 34.00),\n",
       "  (356, 34.00),\n",
       "  (409, 34.00),\n",
       "  (415, 34.00),\n",
       "  (450, 34.00),\n",
       "  (494, 34.00),\n",
       "  (511, 34.00),\n",
       "  (543, 34.00),\n",
       "  (585, 34.00),\n",
       "  (627, 34.00),\n",
       "  (677, 34.00),\n",
       "  (693, 34.00),\n",
       "  (861, 34.00),\n",
       "  (897, 34.00),\n",
       "  (923, 34.00),\n",
       "  (947, 34.00),\n",
       "  (86, 33.00),\n",
       "  (403, 33.00),\n",
       "  (465, 33.00),\n",
       "  (650, 33.00),\n",
       "  (798, 33.00),\n",
       "  (964, 33.00),\n",
       "  (980, 33.00),\n",
       "  (158, 32.00),\n",
       "  (371, 32.00),\n",
       "  (402, 32.00),\n",
       "  (461, 32.00),\n",
       "  (629, 32.00),\n",
       "  (634, 32.00),\n",
       "  (644, 32.00),\n",
       "  (647, 32.00),\n",
       "  (760, 32.00),\n",
       "  (948, 32.00),\n",
       "  (954, 32.00),\n",
       "  (59, 31.00),\n",
       "  (64, 31.00),\n",
       "  (81, 31.00),\n",
       "  (459, 31.00),\n",
       "  (498, 31.00),\n",
       "  (521, 31.00),\n",
       "  (531, 31.00),\n",
       "  (578, 31.00),\n",
       "  (589, 31.00),\n",
       "  (745, 31.00),\n",
       "  (803, 31.00),\n",
       "  (860, 31.00),\n",
       "  (966, 31.00),\n",
       "  (146, 30.00),\n",
       "  (179, 30.00),\n",
       "  (303, 30.00),\n",
       "  (567, 30.00),\n",
       "  (676, 30.00),\n",
       "  (744, 30.00),\n",
       "  (789, 30.00),\n",
       "  (4, 29.00),\n",
       "  (122, 29.00),\n",
       "  (166, 29.00),\n",
       "  (187, 29.00),\n",
       "  (233, 29.00),\n",
       "  (345, 29.00),\n",
       "  (484, 29.00),\n",
       "  (551, 29.00),\n",
       "  (557, 29.00),\n",
       "  (568, 29.00),\n",
       "  (617, 29.00),\n",
       "  (733, 29.00),\n",
       "  (914, 29.00),\n",
       "  (29, 28.00),\n",
       "  (32, 28.00),\n",
       "  (106, 28.00),\n",
       "  (147, 28.00),\n",
       "  (469, 28.00),\n",
       "  (482, 28.00),\n",
       "  (502, 28.00),\n",
       "  (544, 28.00),\n",
       "  (663, 28.00),\n",
       "  (686, 28.00),\n",
       "  (691, 28.00),\n",
       "  (747, 28.00),\n",
       "  (804, 28.00),\n",
       "  (931, 28.00),\n",
       "  (54, 27.00),\n",
       "  (150, 27.00),\n",
       "  (271, 27.00),\n",
       "  (587, 27.00),\n",
       "  (596, 27.00),\n",
       "  (624, 27.00),\n",
       "  (660, 27.00),\n",
       "  (705, 27.00),\n",
       "  (869, 27.00),\n",
       "  (369, 26.00),\n",
       "  (718, 26.00),\n",
       "  (731, 26.00),\n",
       "  (942, 26.00),\n",
       "  (965, 26.00),\n",
       "  (202, 25.00),\n",
       "  (240, 25.00),\n",
       "  (536, 25.00),\n",
       "  (767, 25.00),\n",
       "  (911, 25.00),\n",
       "  (974, 25.00),\n",
       "  (163, 24.00),\n",
       "  (168, 24.00),\n",
       "  (479, 24.00),\n",
       "  (516, 24.00),\n",
       "  (590, 24.00),\n",
       "  (631, 24.00),\n",
       "  (680, 24.00),\n",
       "  (876, 24.00),\n",
       "  (901, 24.00),\n",
       "  (909, 24.00),\n",
       "  (49, 23.00),\n",
       "  (165, 23.00),\n",
       "  (185, 23.00),\n",
       "  (623, 23.00),\n",
       "  (785, 23.00),\n",
       "  (811, 23.00),\n",
       "  (838, 23.00),\n",
       "  (856, 23.00),\n",
       "  (68, 22.00),\n",
       "  (315, 22.00),\n",
       "  (438, 22.00),\n",
       "  (499, 22.00),\n",
       "  (773, 22.00),\n",
       "  (394, 21.00),\n",
       "  (622, 21.00),\n",
       "  (638, 21.00),\n",
       "  (675, 21.00),\n",
       "  (729, 21.00),\n",
       "  (976, 21.00),\n",
       "  (525, 20.00),\n",
       "  (598, 20.00),\n",
       "  (662, 20.00),\n",
       "  (728, 20.00),\n",
       "  (908, 20.00),\n",
       "  (103, 19.00),\n",
       "  (246, 19.00),\n",
       "  (400, 19.00),\n",
       "  (550, 19.00),\n",
       "  (715, 19.00),\n",
       "  (740, 19.00),\n",
       "  (885, 19.00),\n",
       "  (282, 18.00),\n",
       "  (648, 18.00),\n",
       "  (836, 18.00),\n",
       "  (970, 18.00),\n",
       "  (651, 17.00),\n",
       "  (906, 17.00),\n",
       "  (930, 17.00),\n",
       "  (977, 17.00),\n",
       "  (446, 16.00),\n",
       "  (841, 16.00),\n",
       "  (460, 15.00),\n",
       "  (534, 15.00),\n",
       "  (969, 15.00),\n",
       "  (493, 14.00),\n",
       "  (504, 14.00),\n",
       "  (742, 14.00),\n",
       "  (813, 14.00),\n",
       "  (689, 13.00),\n",
       "  (34, 12.00),\n",
       "  (940, 12.00),\n",
       "  (967, 12.00),\n",
       "  (167, 11.00),\n",
       "  (899, 11.00),\n",
       "  (673, 10.00),\n",
       "  (960, 10.00),\n",
       "  (978, 9.00),\n",
       "  (681, 8.00),\n",
       "  (935, 8.00),\n",
       "  (810, 7.00),\n",
       "  (961, 2.00)])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = targeted_diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f59b994b4e0>]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# on test\n",
    "sorted_hist = sorted(hist, key=lambda x: x[0], reverse = False)\n",
    "values = [elem[1] for elem in sorted_hist]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.8493)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "entropy(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fCvFG0VMKts"
   },
   "outputs": [],
   "source": [
    "def make_triplet_samples(z, margin, r2, r3):\n",
    "  positive_sample = z + random_vector_volume(z.shape, 0, margin).cuda() \n",
    "  negative_sample = z + random_vector_volume(z.shape, r2, r3).cuda()\n",
    "  return positive_sample, negative_sample\n",
    "\n",
    "def random_vector_surface(shape, r = 1.):\n",
    "  mat = torch.randn(size=shape).cuda()\n",
    "  norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "  return (mat/norm) * r\n",
    "\n",
    "def random_vector_volume(shape, inner_r, outer_r):\n",
    "  fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "  fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "  fraction.unsqueeze_(-1)\n",
    "  return random_vector_surface(shape, 1) * fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PizmBkGqMKtu"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_frequent(x):\n",
    "  return Counter(x).most_common(1)[0]\n",
    "\n",
    "def preds_around(center, radius, n_preds, model, dummy_img):\n",
    "  z_s = random_vector_volume([n_preds, 10], radius, radius + 0.01) + center[None]\n",
    "  noises = model.forward_z(z_s)\n",
    "  perturbed_imgs = noises + dummy_img \n",
    "  return torch.argmax(arch(perturbed_imgs), 1)\n",
    "  \n",
    "def most_freq_pred_around(center, radius, n_preds, model, dummy_img):\n",
    "  preds = preds_around(center, radius, n_preds, model, dummy_img)\n",
    "  most_freq = most_frequent(preds.tolist())\n",
    "  return (class_index_to_label(most_freq[0]), most_freq[1]/n_preds)\n",
    "\n",
    "def investigate_neighborhood(z, step, model, dummy_img):\n",
    "  with torch.no_grad():\n",
    "    result = []\n",
    "    for radius in np.arange(0.1, 6., step):\n",
    "#       print(\"creating {} more preds\".format(int(10 + 5 * (radius ** 2))))\n",
    "      most_freq_pred = most_freq_pred_around(z, radius, int(10 + 5 * (radius ** 2)), model, dummy_img)\n",
    "      result.append((radius, most_freq_pred))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-btRW4qMKtw",
    "outputId": "90e81f6a-1b9e-45a4-ae82-bda370319bd9"
   },
   "outputs": [],
   "source": [
    "#experiment 1\n",
    "\n",
    "z = torch.tensor([0.5] * 10).cuda()\n",
    "# z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "# z_s = z[None]\n",
    "\n",
    "model = learn.model.eval()\n",
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "  \n",
    "for i in range(6):\n",
    "  z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "  print(\"investigation for: \", z)\n",
    "  for elem in investigate_neighborhood(z, 0.5, model, x_img):\n",
    "    print(elem)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 1-1: modified investigate_z\n",
    "z_investigate_path = '/root/Derakhshani/adversarial/textual_notes/investigate_z_{}.txt'.format(env.save_filename)\n",
    "if Path(z_investigate_path).exists(): raise FileExistsError(\"file already exists\")\n",
    "file = open(str(z_investigate_path), 'w')\n",
    "        \n",
    "for i, (z, noise) in enumerate(zip(pruned_z_s, pruned_noises)):\n",
    "  hist = compute_prediction_histogram(learn, noise)\n",
    "  indexed_hist = [(i, val) for i, val in enumerate(hist)]\n",
    "  sorted_hist = sorted(indexed_hist, key=lambda x: x[1], reverse=True)\n",
    "  labeled_hist = [(class_index_to_label(i), count) for i, count in sorted_hist]\n",
    "  print(\"result {}:\".format(i))\n",
    "  print(big_vector_to_str(z))\n",
    "  print(labeled_hist[:6])\n",
    "  print(\"\\n\\n\")\n",
    "  \n",
    "  file.write(\"result {}:\\n\".format(i))\n",
    "  file.write(big_vector_to_str(z) + \"\\n\")\n",
    "  file.write(str(labeled_hist[:6]))\n",
    "  file.write(\"\\n\\n\\n\")\n",
    "  file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bp6YOnipMKtz"
   },
   "outputs": [],
   "source": [
    "#experiment 2\n",
    "import itertools\n",
    "z_s = [torch.tensor(t).cuda() for t in itertools.product( *([[-0.33, 0.33]] * 10) )]\n",
    "model = learn.model.eval()\n",
    "noises = []\n",
    "with torch.no_grad():\n",
    "  for z in z_s:\n",
    "    noises.append(model.forward_single_z(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "55lErWDyMKt1",
    "outputId": "93d8fb71-3fd5-44a5-d3ec-8013e13f17ba"
   },
   "outputs": [],
   "source": [
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "\n",
    "preds = []\n",
    "for noise in noises:\n",
    "  perturbed_img = x_img + noise\n",
    "  preds.append(torch.argmax(arch(perturbed_img[None]), 1)[0].item())\n",
    "\n",
    "from collections import Counter\n",
    "result = [(class_index_to_label(index), count) for index, count in Counter(preds).most_common(5)]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4WSg-wBFMKt5"
   },
   "outputs": [],
   "source": [
    "#experiment 3\n",
    "import itertools\n",
    "dimension_values = [[-0.9, 0.9]] * z_dim\n",
    "for i in range(z_dim):\n",
    "  if i % 100 != 0:\n",
    "    dimension_values[i] = [0.]\n",
    "# dimension_values[0] = [0.]\n",
    "# dimension_values[3] = [0.]\n",
    "# dimension_values[6] = [0.]\n",
    "# dimension_values[9] = [0.]\n",
    "pruned_z_s = [torch.tensor(t).cuda() for t in itertools.product(*dimension_values)]\n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 3: for the targeted-attack case\n",
    "pruned_z_s = []\n",
    "for i in range(z_dim):\n",
    "  new_z = torch.zeros(z_dim).cuda()\n",
    "  new_z[i] = 1.\n",
    "  pruned_z_s.append(new_z)\n",
    "  \n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 3-1: noises for \n",
    "pruned_z_s = []\n",
    "# for i in range(z_dim):\n",
    "#   new_z = torch.empty(z_dim).uniform_(0,1).cuda().detach()\n",
    "#   pruned_z_s.append(new_z)\n",
    "\n",
    "for i in range(z_dim):\n",
    "  new_z = torch.zeros(z_dim).cuda().detach()\n",
    "  new_z[i] = 1.\n",
    "  pruned_z_s.append(new_z)\n",
    "  \n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise in pruned_noises[0:200]:\n",
    "  img = noise_to_image(noise)\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider web\n",
    "z_values = [\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33],\n",
    "  [-0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33,  0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33,  0.33,  0.33],\n",
    "  [-0.33,  0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_values = [\n",
    "  # window screen\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuAVZzmKMKt9",
    "outputId": "7e6643e0-ce55-438b-e0ae-79bde3ee4cef"
   },
   "outputs": [],
   "source": [
    "#vgg-16_12 most repeated labels:\n",
    "l = [(611, 215.0),\n",
    "  (474, 194.1),\n",
    "  (398, 120.3),\n",
    "  (721, 79.6),\n",
    "  (741, 73.5),\n",
    "  (510, 62.5)]\n",
    "\n",
    "[(class_index_to_label(index), count) for index, count in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAgk-YyWc3rG"
   },
   "outputs": [],
   "source": [
    "# learn.recorder.plot_losses()\n",
    "# learn.recorder.plot_lr()\n",
    "# learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTHG4Bt7VDYp"
   },
   "outputs": [],
   "source": [
    "fooling_rates = []\n",
    "model = learn.model.eval()\n",
    "learn.metrics = [validation_single_perturbation]\n",
    "for i in range(10):\n",
    "  global_perturbations = model(torch.rand(1, 3, 224, 244).cuda())[0]\n",
    "  nag_util.global_perturbations = global_perturbations\n",
    "  fooling_rates.append(learn.validate()[1].cpu().item())\n",
    "  print(\"%d : %f\"%(i, fooling_rates[-1]))\n",
    "\n",
    "mean = np.mean(fooling_rates)\n",
    "stddev = np.std(fooling_rates)\n",
    "print(mean, stddev); print(fooling_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "OFCjzI7UaY3C",
    "outputId": "740185b4-dd54-46f4-b0af-79ee452568e1"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[200][0]\n",
    "x = normalize(x_img.data.cuda())\n",
    "z = torch.tensor([-0.33,  0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33], dtype=torch.float32).cuda()\n",
    "# z = torch.empty(z_dim).uniform_(-1,1).cuda()\n",
    "p = model.forward_single_z(z).detach()\n",
    "\n",
    "p_x = x + p\n",
    "# print(\"img range, noise range\")\n",
    "# print_range(x); print_range(p)\n",
    "adv_label = class_index_to_label(arch(p_x[None]).argmax(1).item())\n",
    "print_big_vector(arch(p_x[None])[0])\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0., 1.])\n",
    "p_img = Image(p)\n",
    "x_img.show()\n",
    "p_img.show()\n",
    "p_x_img.show()\n",
    "\n",
    "\n",
    "# print_range(p)\n",
    "# print_range(denormalize(x))\n",
    "# print_range(p_x)\n",
    "\n",
    "benign_label = class_index_to_label(arch(x[None]).argmax(1).item())\n",
    "\n",
    "print_big_vector(arch(x[None])[0])\n",
    "print(benign_label, adv_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzwsI2P1ZANz"
   },
   "outputs": [],
   "source": [
    "z1 = torch.tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p1 = model.forward_single_z(z1)\n",
    "\n",
    "z2 = torch.tensor([1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p2 = model.forward_single_z(z2)\n",
    "\n",
    "z3 = torch.tensor([1, 1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p3 = model.forward_single_z(z3)\n",
    "\n",
    "l2_distance(p1, p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eroI82OKSnAL"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[4][0]\n",
    "x = x_img.data[None].cuda()\n",
    "p = model(x)[0].squeeze().detach() \n",
    "x = x.squeeze()\n",
    "x = normalize(x)\n",
    "\n",
    "p_x = x + p\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0.,1.])\n",
    "p_img = Image(p)\n",
    "# x_img.show()\n",
    "p_img.show()\n",
    "# p_x_img.show()\n",
    "\n",
    "print_range(p)\n",
    "print_range(x)\n",
    "print_range(p_x)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NAG-tripletLossExperiment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
