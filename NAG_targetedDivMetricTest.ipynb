{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ahmad-PH/nag-notebooks/blob/master/NAG_tripletLossExperiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqeZpz16do4y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up-to-date.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os; import subprocess\n",
    "\n",
    "def detect_env():\n",
    "    return 'colab' if 'content' in os.listdir('/') else 'IBM'\n",
    "  \n",
    "def run_shell_command(cmd):\n",
    "  p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "  print(str(p.communicate()[0], 'utf-8'))\n",
    "  \n",
    "if detect_env() == 'colab': root_folder = '/content'\n",
    "elif detect_env() == 'IBM' : root_folder = '/root/Derakhshani/adversarial'\n",
    "python_files_path = root_folder + '/nag-public'\n",
    "\n",
    "if os.path.isdir(python_files_path):\n",
    "  initial_dir = os.getcwd()\n",
    "  os.chdir(python_files_path)\n",
    "  run_shell_command('git pull')\n",
    "  os.chdir(initial_dir)\n",
    "else:\n",
    "  os.chdir('/root/Derakhshani/adversarial')\n",
    "  run_shell_command('git clone https://github.com/ahmad-PH/nag-public.git')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(python_files_path + '/NAG-11May-beforeDenoiser')\n",
    "\n",
    "from nag_util import *\n",
    "import nag_util\n",
    "from environment import *\n",
    "from visualization import *\n",
    "\n",
    "env = create_env()\n",
    "env.setup(cuda_index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Ev7jcRKoARg"
   },
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.imports import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import sys; import os; import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_arch = \"targeted\"\n",
    "# gen_arch = \"non-targeted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralRelu(nn.Module):\n",
    "  def __init__(self, leak=None, sub=None, maxv=None):\n",
    "    super().__init__()\n",
    "    self.leak,self.sub,self.maxv = leak,sub,maxv\n",
    "\n",
    "  def forward(self, x): \n",
    "    x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n",
    "    if self.sub is not None: x.sub_(self.sub)\n",
    "    if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "    return x\n",
    "  \n",
    "class deconv_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k_size = (4,4), s = (2,2), pad = (1,1), b = True, activation = True):\n",
    "        super(deconv_layer, self).__init__()\n",
    "\n",
    "        self.CT2d = nn.ConvTranspose2d(in_channels = in_ch,\n",
    "                                  out_channels = out_ch,\n",
    "                                  kernel_size = k_size,\n",
    "                                  stride = s, \n",
    "                                  padding = pad,\n",
    "                                  bias = b)\n",
    "        self.BN2d = nn.BatchNorm2d(out_ch)\n",
    "        \n",
    "        self.activation = activation\n",
    "        if self.activation:\n",
    "            self.relu = GeneralRelu(0, 0.2, 5)\n",
    "        \n",
    "        self.weight_init()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.activation:\n",
    "            return self.relu(self.BN2d(self.CT2d(input)))\n",
    "        else:\n",
    "            return self.BN2d(self.CT2d(input))\n",
    "\n",
    "    def weight_init(self):\n",
    "        self.CT2d.weight.data.normal_(mean = 0, std = 0.02)\n",
    "        self.CT2d.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gen_arch == \"targeted\":\n",
    "  class Gen(nn.Module):\n",
    "    def __init__(self, z_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "      super(Gen, self).__init__()\n",
    "\n",
    "      self.bs = None\n",
    "      self.z_dim = z_dim\n",
    "      self.gf_dim = gf_dim\n",
    "      self.y_dim = y_dim\n",
    "      self.df_dim = df_dim\n",
    "      self.image_shape = image_shape\n",
    "\n",
    "      self.z_ = nn.Linear(self.z_dim, self.gf_dim * 7 * 4 * 4, bias=True)\n",
    "      self.z_.bias.data.fill_(0)\n",
    "      self.BN_ = nn.BatchNorm2d(self.gf_dim * 7)\n",
    "\n",
    "      self.half = max(self.gf_dim // 2, 1) \n",
    "      self.quarter = max(self.gf_dim // 4, 1)\n",
    "      self.eighth = max(self.gf_dim // 8, 1)\n",
    "      # sixteenth = max(self.gf_dim // 16, 1)\n",
    "\n",
    "      self.CT2d_1 = deconv_layer(self.gf_dim * 8, self.gf_dim * 4, k_size = (5,5), pad = (2,2))\n",
    "      self.CT2d_2 = deconv_layer(self.gf_dim * 5, self.gf_dim * 2)    \n",
    "      self.CT2d_3 = deconv_layer(self.gf_dim * 2 + self.half, self.gf_dim * 1)\n",
    "      self.CT2d_4 = deconv_layer(self.gf_dim * 1 + self.quarter, self.gf_dim * 1)\n",
    "      self.CT2d_5 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "      self.CT2d_6 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "      self.CT2d_7 = deconv_layer(self.gf_dim * 1 + self.eighth, 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "    def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "      h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "      h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "      output = deconv_layer(h_input)\n",
    "      assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "              \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "              \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "      return output\n",
    "\n",
    "    def forward_z(self, z):\n",
    "      self.bs = z.shape[0]\n",
    "\n",
    "      h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "      assert h0.shape[2:] == (4, 4), \"Unexpected shape, it shoud be (4,4)\"\n",
    "\n",
    "      h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "      h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "      h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "      h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "      h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "      h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "      h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "      ksi = 10.0\n",
    "      output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "      # this coeff scales the output to be appropriate for images that are \n",
    "      # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "      # interval)\n",
    "      return output_coeff * torch.tanh(h7)\n",
    "\n",
    "  #   # blind-selection\n",
    "    def forward(self, inputs):\n",
    "      self.bs = inputs.shape[0]\n",
    "\n",
    "      benign_preds_onehot = arch(inputs)\n",
    "      benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "\n",
    "      z = torch.zeros([self.bs, 1000]).cuda()\n",
    "      for i in range(self.bs):\n",
    "        random_label = self.randint(0,1000, exclude = benign_preds[i].item())\n",
    "        z[i][random_label] = 1.\n",
    "\n",
    "      z_out = self.forward_z(z)\n",
    "\n",
    "      return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "\n",
    "  #   #second-best selection: made validation so much worse\n",
    "  #   def forward(self, inputs):\n",
    "  #     self.bs = inputs.shape[0]\n",
    "\n",
    "  #     benign_preds_onehot = arch(inputs)\n",
    "  #     target_preds = torch.topk(benign_preds_onehot, 2, dim = 1).indices[:, 1:]\n",
    "\n",
    "  #     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "  #     for i in range(self.bs):\n",
    "  #       z[i][target_preds[i]] = 1.\n",
    "\n",
    "  #     z_out = self.forward_z(z)\n",
    "\n",
    "  #     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "\n",
    "  #    def forward(self, inputs):\n",
    "  #     self.bs = inputs.shape[0]\n",
    "\n",
    "  #     benign_preds_onehot = arch(inputs)\n",
    "  #     benign_preds = torch.argmax(benign_preds_onehot, dim = 1)\n",
    "\n",
    "  #     z = torch.zeros([self.bs, 1000]).cuda()\n",
    "  #     random_label = self.randint(0,1000, exclude = benign_preds.tolist())\n",
    "  #     for i in range(self.bs):\n",
    "  #       z[i][random_label] = 1.\n",
    "\n",
    "  #     z_out = self.forward_z(z)\n",
    "\n",
    "  #     return z_out, None, None, inputs, benign_preds_onehot, z\n",
    "\n",
    "    @staticmethod\n",
    "    def randint(low, high, exclude):\n",
    "      if exclude >= low and exclude < high:\n",
    "        temp = np.random.randint(low, high - 1)\n",
    "        if temp >= exclude:\n",
    "          temp = temp + 1\n",
    "        return temp\n",
    "      else:\n",
    "        return np.random.randint(low, high)\n",
    "\n",
    "    def forward_single_z(self, z):\n",
    "      return self.forward_z(z[None]).squeeze()\n",
    "\n",
    "    def generate_single_noise(self):\n",
    "      z = torch.empty(self.z_dim).uniform_(-1,1).cuda()\n",
    "      return self.forward_single_z(z)         \n",
    "\n",
    "    def make_triplet_samples(self, z, margin, r2, r3):\n",
    "      positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "      negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "      return positive_sample, negative_sample\n",
    "\n",
    "    def random_vector_surface(self, shape, r = 1.):\n",
    "      mat = torch.randn(size=shape).cuda()\n",
    "      norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "      return (mat/norm) * r\n",
    "\n",
    "\n",
    "    def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "      fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "      fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "      fraction.unsqueeze_(-1)\n",
    "      return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "    def make_z(self, in_shape):\n",
    "      return torch.empty(in_shape).cuda().uniform_(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gen_arch == \"non-targeted\":\n",
    "  class Gen(nn.Module):\n",
    "    def __init__(self, z_dim, gf_dim=64, y_dim = None, df_dim = 64, image_shape = [3,128,128]):\n",
    "      super(Gen, self).__init__()\n",
    "\n",
    "      self.bs = None\n",
    "      self.z_dim = z_dim\n",
    "      self.gf_dim = gf_dim\n",
    "      self.y_dim = y_dim\n",
    "      self.df_dim = df_dim\n",
    "      self.image_shape = image_shape\n",
    "\n",
    "      self.z_ = nn.Linear(self.z_dim, self.gf_dim * 7 * 4 * 4, bias=True)\n",
    "      self.z_.bias.data.fill_(0)\n",
    "      self.BN_ = nn.BatchNorm2d(self.gf_dim * 7)\n",
    "\n",
    "      self.half = max(self.gf_dim // 2, 1) \n",
    "      self.quarter = max(self.gf_dim // 4, 1)\n",
    "      self.eighth = max(self.gf_dim // 8, 1)\n",
    "      # sixteenth = max(self.gf_dim // 16, 1)\n",
    "\n",
    "      self.CT2d_1 = deconv_layer(self.gf_dim * 8, self.gf_dim * 4, k_size = (5,5), pad = (2,2))\n",
    "      self.CT2d_2 = deconv_layer(self.gf_dim * 5, self.gf_dim * 2)    \n",
    "      self.CT2d_3 = deconv_layer(self.gf_dim * 2 + self.half, self.gf_dim * 1)\n",
    "      self.CT2d_4 = deconv_layer(self.gf_dim * 1 + self.quarter, self.gf_dim * 1)\n",
    "      self.CT2d_5 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "      self.CT2d_6 = deconv_layer(self.gf_dim * 1 + self.eighth, self.gf_dim * 1)\n",
    "      self.CT2d_7 = deconv_layer(self.gf_dim * 1 + self.eighth, 3, k_size = (5,5), s = (1,1), pad = (2,2), activation = False)\n",
    "\n",
    "    def randomized_deconv_layer(self, h_input, z_size_0, z_size_1, deconv_layer, expected_output_size):\n",
    "      h_input_z = self.make_z([self.bs, z_size_0, z_size_1, z_size_1])\n",
    "      h_input = torch.cat([h_input, h_input_z], dim = 1)\n",
    "      output = deconv_layer(h_input)\n",
    "      assert output.shape[2:] == (expected_output_size, expected_output_size), \\\n",
    "              \"Unexpected output shape at randomized_deconv_layer. expected\" + \\\n",
    "              \"({0},{0}), got {1}\".format(expected_output_size, output.shape[2:])\n",
    "      return output\n",
    "\n",
    "    def forward_z(self, z):\n",
    "      self.bs = z.shape[0]\n",
    "\n",
    "      h0 = F.relu(self.BN_(self.z_(z).contiguous().view(self.bs, -1, 4, 4)))\n",
    "      assert h0.shape[2:] == (4, 4), \"Non-expected shape, it shoud be (4,4)\"\n",
    "\n",
    "      h1 = self.randomized_deconv_layer(h0, self.gf_dim, 4, self.CT2d_1, 7)\n",
    "      h2 = self.randomized_deconv_layer(h1, self.gf_dim, 7, self.CT2d_2, 14)\n",
    "      h3 = self.randomized_deconv_layer(h2, self.half, 14, self.CT2d_3, 28)\n",
    "      h4 = self.randomized_deconv_layer(h3, self.quarter, 28, self.CT2d_4, 56)\n",
    "      h5 = self.randomized_deconv_layer(h4, self.eighth, 56, self.CT2d_5, 112)\n",
    "      h6 = self.randomized_deconv_layer(h5, self.eighth, 112, self.CT2d_6, 224)\n",
    "      h7 = self.randomized_deconv_layer(h6, self.eighth, 224, self.CT2d_7, 224)\n",
    "\n",
    "      ksi = 10.0\n",
    "      output_coeff = ksi / (255.0 * np.mean(imagenet_stats[1])) \n",
    "      # this coeff scales the output to be appropriate for images that are \n",
    "      # normalized using imagenet_stats (and are hence in the approximate [-2.5, 2.5]\n",
    "      # interval)\n",
    "      return output_coeff * torch.tanh(h7)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "      self.bs = inputs.shape[0]\n",
    "      z = inputs.new_empty([self.bs, self.z_dim]).uniform_(-1,1).cuda()\n",
    "      p, n = self.make_triplet_samples(z, 0.1, 0.1, 2.)\n",
    "\n",
    "      z_out = self.forward_z(z)\n",
    "#       p_out = self.forward_z(p)\n",
    "#       n_out = self.forward_z(n)\n",
    "\n",
    "#       return z_out, p_out, n_out, inputs, z\n",
    "      return z_out, None, None, inputs, z\n",
    "\n",
    "    def forward_single_z(self, z):\n",
    "      return self.forward_z(z[None]).squeeze()\n",
    "\n",
    "    def generate_single_noise(self):\n",
    "      z = torch.empty(self.z_dim).uniform_(-1,1).cuda()\n",
    "      return self.forward_single_z(z)\n",
    "\n",
    "\n",
    "    def make_triplet_samples(self, z, margin, r2, r3):\n",
    "      positive_sample = z + self.random_vector_volume(z.shape, 0, margin).cuda() \n",
    "      negative_sample = z + self.random_vector_volume(z.shape, r2, r3).cuda()\n",
    "      return positive_sample, negative_sample\n",
    "\n",
    "    def random_vector_surface(self, shape, r = 1.):\n",
    "      mat = torch.randn(size=shape).cuda()\n",
    "      norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "      return (mat/norm) * r\n",
    "\n",
    "\n",
    "    def random_vector_volume(self, shape, inner_r, outer_r):\n",
    "      fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "      fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "      fraction.unsqueeze_(-1)\n",
    "      return self.random_vector_surface(shape, 1) * fraction\n",
    "\n",
    "    def make_z(self, in_shape):\n",
    "      return torch.empty(in_shape).cuda().uniform_(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def js_distance(x1, x2):\n",
    "  m = 0.5 * (x1 + x2)\n",
    "  return 0.5 * (F.kl_div(x1, m) + F.kl_div(x2, m))\n",
    "\n",
    "def kl_distance(x1, x2):\n",
    "  inp = torch.log(x1)\n",
    "  target = x2\n",
    "  return F.kl_div(inp, target, reduction='batchmean')\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  x1 = tensorify(x1)\n",
    "  x2 = tensorify(x2)\n",
    "  x1 = x1 / torch.sum(x1)\n",
    "  x2 = x2 / torch.sum(x2)\n",
    "  return kl_distance(x1[None], x2[None])\n",
    "\n",
    "def distrib_distance(x1, x2):\n",
    "  if not isinstance(x1, torch.Tensor): x1 = torch.tensor(x1)\n",
    "  if not isinstance(x2, torch.Tensor): x2 = torch.tensor(x2)\n",
    "  x1 = x1 * 100. / torch.sum(x1)\n",
    "  x2 = x2 * 100. / torch.sum(x2)\n",
    "  return torch.norm(x1 - x2, 2)\n",
    "\n",
    "def distance_from_uniform(x):\n",
    "  return distrib_distance(x, [1.] * len(x))\n",
    "\n",
    "def wasserstein_distance(x1, x2):\n",
    "  return torch.mean(x1 - x2)\n",
    "\n",
    "def l1_distance(x1, x2):\n",
    "  return F.l1_loss(x1, x2)\n",
    "\n",
    "def l2_distance(x1, x2):\n",
    "  return F.mse_loss(x1 * 10, x2 * 10)\n",
    "\n",
    "def mse_loss(x1, x2):\n",
    "  return F.mse_loss(x1, x2)\n",
    "\n",
    "def cos_distance(x1, x2, dim = 1):\n",
    "  return -1 * torch.mean(F.cosine_similarity(x1, x2, dim=dim))\n",
    "\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, distance_func, margin):\n",
    "  # max distance when using l1_distance is 2\n",
    "  # max distacne when using l2-distance is sqrt(2)\n",
    "  ap_dist = distance_func(anchor, positive)\n",
    "  an_dist = distance_func(anchor, negative)\n",
    "\n",
    "  triplet_loss.call_count += 1\n",
    "  if triplet_loss.call_count % 200 == 0 : #and anchor.shape[1] == 1000:\n",
    "#     print(\"a: \", end=\"\"); print_big_vector(anchor[0])\n",
    "#     print(\"p: \", end=\"\"); print_big_vector(positive[0])\n",
    "#     print(\"n: \", end=\"\"); print_big_vector(negative[0])\n",
    "    print(\"func:{}, ap_dist: {}, an_dist: {}\".format(distance_func.__name__, ap_dist, an_dist))\n",
    "    \n",
    "  return torch.mean(F.relu(ap_dist - an_dist + margin))\n",
    "\n",
    "triplet_loss.call_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def diversity_loss(embeddings, z_s):\n",
    "#   size = z_s.shape[0]\n",
    "#   result = 0\n",
    "#   for i in range(size):\n",
    "#     for j in range(i+1, size):\n",
    "# #       a = F.cosine_similarity(embeddings[i], embeddings[j], dim = 0)\n",
    "# #       b = torch.norm(z_s[i] - z_s[j], 2, dim = 0)\n",
    "# #       print('embeddings: ')\n",
    "# #       print_big_vector(embeddings[i])\n",
    "# #       print_big_vector(embeddings[j])\n",
    "# #       print(f'a: {a}, b:{b}, multiple: {a*b}')\n",
    "#       result += F.cosine_similarity(embeddings[i], embeddings[j], dim = 0) * \\\n",
    "#                 torch.norm(z_s[i] - z_s[j], 2, dim = 0)\n",
    "#   n_pairs = (size * (size - 1)) / 2\n",
    "#   mean = result / n_pairs \n",
    "# #   print(f'result {result}, n_pairs {n_pairs}, mean {mean}')\n",
    "#   return mean\n",
    "  \n",
    "\n",
    "# # normalized with shuffling\n",
    "# def diversity_loss(embeddings, deranged_embeddings, z_s, deranged_z_s):\n",
    "#     cos_similarity = F.cosine_similarity(embeddings, deranged_embeddings)\n",
    "#     z_distance = torch.norm(z_s - deranged_z_s, 2, dim = 1)\n",
    "#     return torch.mean(cos_similarity * z_distance)\n",
    "  \n",
    "# # normalized with shuffling\n",
    "# def diversity_loss(embeddings, z_s):\n",
    "#     deranged_embeddings, deranged_z_s = derange(embeddings, z_s)\n",
    "#     cos_similarity = F.cosine_similarity(embeddings, deranged_embeddings)\n",
    "#     z_distance = torch.norm(z_s - deranged_z_s, dim = 1)\n",
    "#     max_possible_z_distance = 6.3246\n",
    "#     return torch.mean(cos_similarity * (z_distance/max_possible_z_distance))\n",
    "\n",
    "\n",
    "def diversity_loss(input, target):\n",
    "#   return -1 * torch.mean(torch.pow(f_x_a-f_x_s,2))\n",
    "  if input.shape[0] != batch_size:\n",
    "    print(\"input shape: \", input.shape)\n",
    "    print(\"target shape: \", target.shape, \"\\n\\n\")\n",
    "  return torch.mean(F.cosine_similarity(\n",
    "    input.view([batch_size, -1]),\n",
    "    target.view([batch_size, -1]), \n",
    "  ))\n",
    "\n",
    "\n",
    "if gen_arch == 'non-targeted':\n",
    "  def fool_loss(input, target):\n",
    "    true_class = torch.argmax(target, dim=1).view(-1,1).long().cuda()\n",
    "    target_probabilities = input.gather(1, true_class)\n",
    "    epsilon = 1e-10\n",
    "    result =  torch.mean(-1 * torch.log(1 - target_probabilities + epsilon))\n",
    "\n",
    "    fool_loss.call_count += 1\n",
    "    if fool_loss.call_count % 100 == 0:\n",
    "      print(\"target probs {}, loss: {}: \".format(target_probabilities, result))\n",
    "\n",
    "    return result\n",
    "\n",
    "  fool_loss.call_count = 0\n",
    "\n",
    "if gen_arch == 'targeted':\n",
    "  def fool_loss(model_output, target_labels):\n",
    "    target_labels = target_labels.view(-1, 1).long().cuda()\n",
    "    target_probabilities = model_output.gather(1, target_labels)\n",
    "    epsilon = 1e-10\n",
    "    # highest possible fool_loss is - log(1e-10) == 23\n",
    "    result = torch.mean(-1 * torch.log(target_probabilities + epsilon))\n",
    "\n",
    "    fool_loss.call_count += 1\n",
    "    if fool_loss.call_count % 200 == 0:\n",
    "      print(\"target probs {}, loss: {}: \".format(target_probabilities, result))\n",
    "\n",
    "    return result\n",
    "\n",
    "  fool_loss.call_count = 0\n",
    "\n",
    "\n",
    "def targeted_validation(gen_output, target):\n",
    "  perturbations, _, _, clean_images, _, z = gen_output\n",
    "  perturbed_images = clean_images + perturbations\n",
    "  target_labels = torch.argmax(z, 1)\n",
    "  adversary_preds = torch.argmax(arch(perturbed_images), 1)\n",
    "#   print('adv preds: ', adversary_preds.shape, adversary_preds)\n",
    "#   print('target_labels: ', target_labels.shape, target_labels)\n",
    "#   print('eq: ', (adversary_preds == target_labels))\n",
    "  return (adversary_preds == target_labels).float().mean()\n",
    "  \n",
    "\n",
    "# # targeted \n",
    "# def validation(gen_output, target):\n",
    "#   perturbations, _, _, clean_images, _, _ = gen_output\n",
    "#   return validation_(perturbations, clean_images)\n",
    "\n",
    "# # non-targeted\n",
    "# def validation(gen_output, target):\n",
    "#   perturbations, _, _, clean_images, _ = gen_output\n",
    "#   return validation_(perturbations, clean_images)\n",
    "\n",
    "# # general\n",
    "def validation(gen_output, target):\n",
    "  perturbations = gen_output[0]\n",
    "  clean_images = gen_output[3]\n",
    "  return validation_(perturbations, clean_images)\n",
    "\n",
    "unfooled_histogram = np.array([0.] * 1000)\n",
    "fooled_histogram = np.array([0.] * 1000)\n",
    "valid_cnt = 0\n",
    "\n",
    "def validation_(perturbations, clean_images):\n",
    "  perturbed_images = clean_images + perturbations\n",
    "  benign_preds = torch.argmax(arch(clean_images), 1)\n",
    "  adversary_preds = torch.argmax(arch(perturbed_images), 1)\n",
    "\n",
    "#   is_unfooled = (benign_preds == adversary_preds)\n",
    "#   for i , unfooled in enumerate(is_unfooled):\n",
    "#     if unfooled == 1:\n",
    "#       unfooled_histogram[benign_preds[i]] += 1\n",
    "#     else:\n",
    "#       fooled_histogram[benign_preds[i]] += 1\n",
    "  \n",
    "#   global valid_cnt\n",
    "#   valid_cnt += 1\n",
    "#   if valid_cnt % 10 == 0:\n",
    "#     indexed = [(i, u) for i, u in enumerate(unfooled_histogram)]\n",
    "#     summarized = list(filter(lambda x: x[1] > 0.0, indexed))\n",
    "#     total_histogram = fooled_histogram + unfooled_histogram\n",
    "\n",
    "#     percent_total = [(i, 100. * u / (total_histogram[i] + 1e-10), total_histogram[i]) for i, u in enumerate(unfooled_histogram)]\n",
    "#     sorted_percent_total = sorted(percent_total, key =lambda x: x[1], reverse = True)\n",
    "    \n",
    "#     print('\\npercent_total: ')\n",
    "#     print(list(filter(lambda x: x[1] > 0.0, sorted_percent_total)))\n",
    "#     print('\\n')\n",
    "    \n",
    "  return (benign_preds != adversary_preds).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gen_arch == 'targeted':\n",
    "  class FeatureLoss(nn.Module):\n",
    "      def __name__(self):\n",
    "        return \"feature_loss\"\n",
    "\n",
    "      def __init__(self, dis, layers, layer_weights):\n",
    "          super().__init__()\n",
    "\n",
    "          # define generator here \n",
    "          self.dis = dis\n",
    "          self.diversity_layers = layers\n",
    "          self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "          self.weights = layer_weights\n",
    "          self.metric_names = [\"fool_loss\"] #+ [f\"div_loss_{i}\" for i in range(len(layers))] #maybe Gram\n",
    "  #         self.triplet_hooks = hook_outputs([arch.m.features[4]], detach=False)\n",
    "\n",
    "      def make_features(self, x, clone=False):\n",
    "          y = self.dis(x)\n",
    "          return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "\n",
    "      def forward(self, inp, target):\n",
    "        sigma_B, _, _, X_B, B_Y, z = inp\n",
    "\n",
    "        X_A = X_B + sigma_B\n",
    "  #       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "        A_Y, A_feat = self.make_features(X_A)\n",
    "  #       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "        chosen_labels = z.argmax(dim=1)\n",
    "        fooling_loss =  fool_loss(A_Y, chosen_labels)\n",
    "\n",
    "  #       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "  #       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "  #       self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "  #       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "        self.losses = [fooling_loss]\n",
    "        self.metrics = dict(zip(self.metric_names, [fooling_loss]))\n",
    "\n",
    "        return sum(self.losses)\n",
    "\n",
    "      def add_perturbation_shuffled(self, inp, perturbation):\n",
    "  #       j = torch.randperm(inp.shape[0])\n",
    "          j = derangement(inp.shape[0])\n",
    "          return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derange(*args):\n",
    "  if len(args) == 0: raise ValueError('shuffle function needs atleast one argument')\n",
    "  deranged_indexes = derangement(args[0].shape[0])\n",
    "  if not all([args[0].shape[0] == arg.shape[0] for arg in args]): \n",
    "    raise ValueError('inputs to shuffle must all have the same 0th dimension')\n",
    "  return [arg[deranged_indexes] for arg in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gen_arch == 'non-targeted':\n",
    "  class FeatureLoss(nn.Module):\n",
    "      def __name__(self):\n",
    "        return \"feature_loss\"\n",
    "\n",
    "      def __init__(self, dis, layers, layer_weights):\n",
    "          super().__init__()\n",
    "\n",
    "          self.dis = dis\n",
    "          self.diversity_layers = layers\n",
    "          self.hooks = hook_outputs(self.diversity_layers, detach=False)\n",
    "          self.weights = layer_weights\n",
    "\n",
    "  #         self.metric_names = [\"fool_loss\"] + [f\"div_loss_{i}\" for i in range(len(layers))] + ['triplet_loss']# Maybe Gram\n",
    "  #         self.metric_names = [\"div_loss\"] + ['triplet_loss']# Maybe Gram\n",
    "          self.metric_names = [\"fool_loss\"] + ['div_loss']# Maybe Gram\n",
    "          self.triplet_weight = 4.\n",
    "          self.div_weight = 1.\n",
    "          self.fooling_weight = 1.\n",
    "\n",
    "      def make_features(self, x, clone=False):\n",
    "          y = self.dis(x)\n",
    "          return y, [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "\n",
    "      # contrastive loss\n",
    "      def forward(self, inp, target):\n",
    "          sigma_B, sigma_pos, sigma_neg, X_B, z_B = inp\n",
    "\n",
    "          deranged_perturbations, deranged_z_s = derange(sigma_B, z_B)\n",
    "\n",
    "          X_A = X_B + sigma_B\n",
    "          X_S = X_B + deranged_perturbations\n",
    "#           X_A_pos = X_B + sigma_pos\n",
    "#           X_A_neg = X_B + sigma_neg\n",
    "\n",
    "          B_Y, _ = self.make_features(X_B)\n",
    "          A_Y, A_feat = self.make_features(X_A)\n",
    "          _, S_feat = self.make_features(X_S)\n",
    "#           pos_softmax, _ = self.make_features(X_A_pos)\n",
    "#           neg_softmax, _ = self.make_features(X_A_neg)\n",
    "\n",
    "          raw_fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "          weighted_fooling_loss = self.fooling_weight * raw_fooling_loss\n",
    "\n",
    "#           raw_diversity_loss = diversity_loss(A_feat[0], S_feat[0], z_B, deranged_z_s)\n",
    "          raw_diversity_loss = diversity_loss(A_feat[0], S_feat[0])\n",
    "          weighted_diversity_loss = raw_diversity_loss * self.div_weight\n",
    "\n",
    "#           raw_triplet_loss = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "#           weighted_triplet_loss = raw_triplet_loss * self.triplet_weight\n",
    "\n",
    "  #         self.losses = weighted_diversity_losses + [weighted_triplet_loss]\n",
    "  #         raw_losses = raw_diversity_losses + [raw_triplet_loss]\n",
    "\n",
    "          self.losses = [weighted_fooling_loss] + [weighted_diversity_loss] #+ [weighted_triplet_loss]\n",
    "          raw_losses = [raw_fooling_loss] + [raw_diversity_loss] #+ [raw_triplet_loss]\n",
    "\n",
    "  #         self.losses = [fooling_loss] + [weighted_triplet_loss]\n",
    "  #         self.metrics = dict(zip(self.metric_names, [fooling_loss] + [raw_triplet_loss]))\n",
    "\n",
    "          if len(self.metric_names) != len(raw_losses):\n",
    "            raise Exception(\"length of metric names unequals length of losses\")\n",
    "\n",
    "          self.metrics = dict(zip(self.metric_names, raw_losses))\n",
    "          return sum(self.losses)\n",
    "\n",
    "\n",
    "\n",
    "  # #     triplet loss\n",
    "  #     def forward(self, inp, target):\n",
    "  #         sigma_B, sigma_pos, sigma_neg, X_B, _ = inp\n",
    "\n",
    "  #         X_A = self.add_perturbation(X_B, sigma_B) \n",
    "  #         X_A_pos = self.add_perturbation(X_B, sigma_pos)\n",
    "  #         X_A_neg = self.add_perturbation(X_B, sigma_neg) \n",
    "  #         X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "  # #         B_Y, _ = self.make_features(X_B)\n",
    "  #         A_Y, A_feat = self.make_features(X_A)\n",
    "  # #         _, S_feat = self.make_features(X_S)\n",
    "  #         pos_softmax, _ = self.make_features(X_A_pos)\n",
    "  #         neg_softmax, _ = self.make_features(X_A_neg)\n",
    "\n",
    "  # #         raw_fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "  # #         weighted_fooling_loss = self.fooling_weight * raw_fooling_loss\n",
    "\n",
    "  #         raw_diversity_losses = [diversity_loss(a_f, s_f, sigma_B, ) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "  #         weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "  #         raw_triplet_loss = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "  #         weighted_triplet_loss = raw_triplet_loss * self.triplet_weight\n",
    "\n",
    "  #         self.losses = weighted_diversity_losses + [weighted_triplet_loss]\n",
    "  #         raw_losses = raw_diversity_losses + [raw_triplet_loss]\n",
    "\n",
    "  # #         self.losses = [weighted_fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss]\n",
    "  # #         raw_losses = [raw_fooling_loss] + raw_diversity_losses + [raw_triplet_loss]\n",
    "\n",
    "  # #         self.losses = [fooling_loss] + [weighted_triplet_loss]\n",
    "  # #         self.metrics = dict(zip(self.metric_names, [fooling_loss] + [raw_triplet_loss]))\n",
    "\n",
    "  #         if len(self.metric_names) != len(raw_losses):\n",
    "  #           raise Exception(\"length of metric names unequals length of losses\")\n",
    "\n",
    "  #         self.metrics = dict(zip(self.metric_names, raw_losses))\n",
    "  #         return sum(self.losses)\n",
    "\n",
    "\n",
    "  #     #use two types of triplet losses\n",
    "  #     def forward(self, inp, target):\n",
    "  #       sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "  #       X_A = self.add_perturbation(X_B, sigma_B) \n",
    "  #       X_A_pos = self.add_perturbation(X_B, sigma_pos)\n",
    "  #       X_A_neg = self.add_perturbation(X_B, sigma_neg) \n",
    "\n",
    "  #       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "  #       B_Y, _ = self.make_features(X_B)\n",
    "  #       A_Y, A_feat = self.make_features(X_A)\n",
    "  #       _, S_feat = self.make_features(X_S)\n",
    "  #       pos_softmax, _ = self.make_features(X_A_pos)\n",
    "  #       neg_softmax, _ = self.make_features(X_A_neg)\n",
    "\n",
    "  #       fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "  #       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "  #       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "  #       raw_triplet_loss_sm = triplet_loss(A_Y, pos_softmax, neg_softmax, cos_distance, 1.4)\n",
    "  #       weighted_triplet_loss_sm = raw_triplet_loss_sm * self.triplet_weight_sm\n",
    "\n",
    "  #       raw_triplet_loss_noise = triplet_loss(sigma_B, sigma_pos, sigma_neg, l2_distance, 5.)\n",
    "  #       weighted_triplet_loss_noise = raw_triplet_loss_noise * self.triplet_weight_noise\n",
    "\n",
    "  #       self.losses = [fooling_loss] + weighted_diversity_losses + [weighted_triplet_loss_sm, weighted_triplet_loss_noise] \n",
    "  #       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses + [weighted_triplet_loss_sm, weighted_triplet_loss_noise]))\n",
    "\n",
    "  #       return sum(self.losses)\n",
    "\n",
    "  #     # just fooling and diversity\n",
    "  #     def forward(self, inp, target):\n",
    "  #       sigma_B, sigma_pos, sigma_neg, X_B = inp\n",
    "\n",
    "  #       X_A = self.add_perturbation(X_B, sigma_B) \n",
    "\n",
    "  #       X_S = self.add_perturbation_shuffled(X_B, sigma_B) # Shuffled Addversarial Examples\n",
    "\n",
    "  #       B_Y, _ = self.make_features(X_B)\n",
    "  #       A_Y, A_feat = self.make_features(X_A)\n",
    "  #       _, S_feat = self.make_features(X_S)\n",
    "\n",
    "  #       fooling_loss =  fool_loss(A_Y, B_Y)\n",
    "\n",
    "  #       raw_diversity_losses = [diversity_loss(a_f, s_f) for a_f, s_f in zip(A_feat, S_feat)]\n",
    "  #       weighted_diversity_losses = [diversity_loss(a_f, s_f) * weight * self.div_weight for a_f, s_f, weight in zip(A_feat, S_feat, self.weights)]\n",
    "\n",
    "  #       self.losses = [fooling_loss] + weighted_diversity_losses\n",
    "  #       self.metrics = dict(zip(self.metric_names, [fooling_loss] + raw_diversity_losses))\n",
    "\n",
    "  #       return sum(self.losses)\n",
    "\n",
    "\n",
    "      def add_perturbation_shuffled(self, inp, perturbation):\n",
    "        j = derangement(inp.shape[0])\n",
    "        return inp.add(perturbation[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def produce_summary(root_folder, n_files):\n",
    "  def writeline(file, values, fmt_string):\n",
    "    file.write(', '.join(fmt_string.format(v) for v in values) + '\\n')\n",
    "  \n",
    "  last_rows = []\n",
    "  for i in range(n_files):\n",
    "    prefix = '/root/Derakhshani/adversarial/textual_notes/CSVs'\n",
    "    df = pd.read_csv(\"{}/{}/{}.csv\".format(prefix, root_folder, i))\n",
    "    last_rows.append(df.iloc[-1][1:-1].values.tolist())\n",
    "  \n",
    "  last_rows = np.array(last_rows)\n",
    "  \n",
    "  labels = list(df.columns[1:-1])\n",
    "  means = np.mean(last_rows, axis=0).tolist()\n",
    "  outfile = open('{}/{}/summary.txt'.format(prefix, root_folder), 'w+')\n",
    "  outfile.write('means: \\n')\n",
    "  writeline(outfile, labels, '{: >20}')\n",
    "  writeline(outfile, means, '{: >20.3}')\n",
    "  outfile.write('\\n')\n",
    "      \n",
    "  operations = []\n",
    "  for column in df.columns[1:-1]:\n",
    "    if column in ['train_loss', 'valid_loss', 'fool_loss', 'triplet_loss'] or column[:8] == 'div_loss':\n",
    "      operations.append('min')\n",
    "    elif column in ['validation', 'targeted_validation', 'div_metric', 'entropy']:\n",
    "      operations.append('max')\n",
    "    else:\n",
    "      raise ValueError('column {} is not recognized'.format(column))\n",
    "    \n",
    "  results = []\n",
    "  indexes = []\n",
    "  \n",
    "  for i in range(len(operations)):\n",
    "    values = last_rows[:, i]\n",
    "    if operations[i] == 'max': operation = np.max\n",
    "    elif operations[i] == 'min': operation = np.min\n",
    "    result = operation(values)\n",
    "    results.append(result)\n",
    "    indexes.append(values.tolist().index(result))\n",
    "  \n",
    "  outfile.write('bests: \\n')\n",
    "  writeline(outfile, labels, '{: >20}')\n",
    "  writeline(outfile, operations, '{: >20}')\n",
    "  writeline(outfile, results, '{: >20.3}')\n",
    "  writeline(outfile, indexes, '{: >20}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils import dir_util \n",
    "\n",
    "def investigate_initial_settings(n_settings, n_epochs, lr, wd, results_dir):\n",
    "  os.mkdir(env.get_csv_dir() + results_dir)\n",
    "  os.mkdir(env.get_models_dir() + results_dir)\n",
    "  \n",
    "  for setting_ind in range(n_settings):\n",
    "    print(f\"investigation no: {setting_ind}\")\n",
    "    learn = None; gen = None; gc.collect()\n",
    "    gen = Gen(z_dim = z_dim)\n",
    "    init_cnn(gen, True)\n",
    "    \n",
    "    tmp_csv_filename =  env.temp_csv_path + '/' + results_dir + '/' + str(setting_ind)\n",
    "    csv_logger = partial(ImmediateCSVLogger, filename=tmp_csv_filename)\n",
    "\n",
    "    if gen_arch == 'non-targeted':\n",
    "      metrics = [validation]\n",
    "    elif gen_arch == 'targeted':\n",
    "      metrics = [validation, targeted_validation]\n",
    "      \n",
    "    learn = Learner(data, gen, loss_func = feat_loss, metrics=metrics, \n",
    "                    model_dir = env.get_learner_models_dir(), \n",
    "                    callback_fns=[DiversityMetric, LossMetrics, csv_logger])\n",
    "    \n",
    "    saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=model.__name__ + \"-best\")\n",
    "    saver_every_epoch = SaveModelCallback(learn, every='epoch', name=model.__name__)\n",
    "\n",
    "    learn.fit(n_epochs, lr=lr, wd = wd, callbacks=[saver_best, saver_every_epoch])\n",
    "    \n",
    "    shutil.copyfile(tmp_csv_filename + \".csv\", env.get_csv_dir() + results_dir + '/' + str(setting_ind) + '.csv')\n",
    "    \n",
    "    model_dest = env.get_models_dir() + results_dir + '/' + str(setting_ind)\n",
    "    os.mkdir(model_dest)\n",
    "    dir_util.copy_tree(env.data_path/env.get_learner_models_dir(), model_dest)\n",
    "    shutil.rmtree(env.data_path/env.get_learner_models_dir())  \n",
    "    \n",
    "  produce_summary(results_dir, n_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_perturbations(learn, n_perturbations):\n",
    "  initial_training_mode = learn.model.training\n",
    "  gen = learn.model.eval()\n",
    "  perturbations = [gen.generate_single_noise() for _ in range(n_perturbations)]\n",
    "  learn.model.train(initial_training_mode)  \n",
    "  return perturbations\n",
    "\n",
    "def compute_prediction_histogram(learn, perturbation, verbose=False):\n",
    "  pred_hist = [0] * 1000\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 and verbose: print (\"at batch no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbation[None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      pred_hist[pred] += 1\n",
    "  return pred_hist\n",
    "\n",
    "\n",
    "def compute_mean_prediction_histogram(learn, perturbations, verbose=False):\n",
    "  pred_hist = torch.tensor([0] * 1000).detach_()\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 5 == 0 and verbose: print(f\"at batch no {batch_no}\")\n",
    "    for j, perturbation in enumerate(perturbations):\n",
    "      perturbed_batch = batch + perturbation[None]\n",
    "      preds = arch(perturbed_batch).argmax(1)\n",
    "      for pred in preds:\n",
    "        pred_hist[pred] += 1\n",
    "  pred_hist = pred_hist.float() / len(perturbations)\n",
    "  return pred_hist.tolist()\n",
    "\n",
    "\n",
    "def classes_needed_to_reach(percentage, hist):\n",
    "  hist_sum = np.sum(hist)\n",
    "  indexed_hist = [(i, hist_element) for i,hist_element in  \n",
    "                          enumerate(hist)]\n",
    "  sorted_hist = sorted(indexed_hist, key=lambda x: x[1], reverse = True)\n",
    "  \n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = sorted_hist[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / hist_sum) * 100.\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, sorted_hist\n",
    "\n",
    "def diversity(learn, n_perturbations, percentage = 95, verbose = True):\n",
    "  pred_histogram = compute_mean_prediction_histogram(\n",
    "      learn, generate_perturbations(learn, n_perturbations), verbose\n",
    "  )\n",
    "  print(\"finished creating the prediction histogram\")\n",
    "\n",
    "  return classes_needed_to_reach(95, pred_histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiversityMetric(LearnerCallback):\n",
    "  _order = -20 # Needs to run before the recorder\n",
    "  \n",
    "  def __init__(self, learn):\n",
    "    super().__init__(learn)\n",
    "    self.average_over = 4\n",
    "    self.n_perturbations = 10\n",
    "    self.percentage = 95\n",
    "  \n",
    "  def on_train_begin(self, **kwargs):\n",
    "    self.learn.recorder.add_metric_names(['div_metric', 'entropy'])\n",
    "    \n",
    "  def on_epoch_begin(self, **kwargs):\n",
    "    global learn\n",
    "    self.perturbations_list = [generate_perturbations(self.learn, self.n_perturbations) \\\n",
    "                          for _ in range(self.average_over)]\n",
    "    self.pred_hist_list = [torch.tensor([0] * 1000).detach_() for _ in range(self.average_over)]\n",
    "    \n",
    "  def on_batch_end(self, last_output, train, **kwargs):\n",
    "    if not train:\n",
    "      images = last_output[3]; assert(images.shape[1:] == (3,224, 224))\n",
    "      for perturbations, pred_hist in zip(self.perturbations_list, self.pred_hist_list):\n",
    "        for j, perturbation in enumerate(perturbations):\n",
    "          perturbed_batch = images + perturbation[None]\n",
    "          preds = arch(perturbed_batch).argmax(1)\n",
    "          for pred in preds:\n",
    "            pred_hist[pred] += 1\n",
    "  \n",
    "  def on_epoch_end(self, last_metrics, **kwargs):\n",
    "    for i in range(len(self.pred_hist_list)):\n",
    "      self.pred_hist_list[i] = (self.pred_hist_list[i].float() / self.n_perturbations).tolist()\n",
    "    \n",
    "    div_metric_list = [classes_needed_to_reach(self.percentage, pred_hist)[0] \\\n",
    "                          for pred_hist in self.pred_hist_list]\n",
    "    entropy_list = [entropy(pred_hist) for pred_hist in self.pred_hist_list]\n",
    "    return add_metrics(last_metrics, [np.mean(div_metric_list), np.mean(entropy_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetedDiversityMetric(DiversityMetric):\n",
    "    def __init__(self, n_perturbations, percentage):\n",
    "      super().__init__(n_perturbations, percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tltucTv2ep9-"
   },
   "outputs": [],
   "source": [
    "# mode = 'sanity_check'\n",
    "mode = 'normal'\n",
    "# mode = 'div_metric_calc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.resnet50\n",
    "# model = models.resnet152\n",
    "# model = models.vgg16_bn\n",
    "model = torchvision.models.googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SO1h55obXzOv",
    "outputId": "54414cc5-84d5-4f45-ecab-87374a58dd33"
   },
   "outputs": [],
   "source": [
    "if mode == \"normal\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "elif mode == \"sanity_check\":\n",
    "  env.load_dataset('dataset_sanity_check_small', 'dataset_sanity_check_small')  \n",
    "  env.set_data_path('dataset_sanity_check_small')\n",
    "elif mode == \"div_metric_calc\":\n",
    "  env.load_dataset('dataset','data')\n",
    "  env.set_data_path('dataset')\n",
    "  env.load_test_dataset(str(env.data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "koaQZmjMom7w"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "gpu_flag = True\n",
    "nag_util.batch_size = batch_size; nag_util.gpu_flag = gpu_flag;\n",
    "tfms = get_transforms(do_flip=False, max_rotate=0)\n",
    "data = (ImageList.from_folder(env.data_path)\n",
    "        .split_by_folder(valid=('test' if mode == 'div_metric_calc' else 'valid'))\n",
    "        .label_from_folder()\n",
    "        .transform(tfms, size=224)\n",
    "        .databunch(bs=batch_size, num_workers=1)\n",
    "        .normalize(imagenet_stats))\n",
    "\n",
    "# data.show_batch(rows=2, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDBkRV8yovwV"
   },
   "outputs": [],
   "source": [
    "if gen_arch == \"non-targeted\":\n",
    "  z_dim = 10\n",
    "elif gen_arch == \"targeted\":\n",
    "  z_dim = 1000\n",
    "  \n",
    "arch = SoftmaxWrapper(model(pretrained=True).cuda().eval())\n",
    "nag_util.arch = arch\n",
    "requires_grad(arch, False)\n",
    "\n",
    "# vgg:\n",
    "# layers = []\n",
    "# blocks = [i-1 for i,o in enumerate(children(arch.features)) if isinstance(o, nn.MaxPool2d)]\n",
    "# layers = [arch.features[i] for i in blocks]\n",
    "# layer_weights = [1] * len(layers)\n",
    "\n",
    "layers = [\n",
    "    arch.softmax\n",
    "]\n",
    "\n",
    "layer_weights = [1.] * len(layers)\n",
    "\n",
    "# inception:\n",
    "# layers = [\n",
    "#     arch.Conv2d_1a_3x3,\n",
    "#     arch.Mixed_6e,\n",
    "#     arch.Mixed_7a,\n",
    "#     arch.fc    \n",
    "# ]\n",
    "# layer_weights = [1.0/4.0] * len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qd9gXUy_ovww"
   },
   "outputs": [],
   "source": [
    "feat_loss = FeatureLoss(arch, layers, layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfZKdYD2MSdi"
   },
   "outputs": [],
   "source": [
    "# env.save_filename = 'resnet50_65' #resnet50_64\n",
    "# env.save_filename = 'resnet50_17'\n",
    "env.save_filename = 'googlenet_16'\n",
    "\n",
    "if Path(env.get_csv_path() + '.csv').exists(): raise FileExistsError(\"csv_path already exists\")\n",
    "if Path(env.get_models_path()).exists(): raise FileExistsError(\"models_path already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRAnneal(LearnerCallback):\n",
    "  _order = -20 # Needs to run before the recorder\n",
    "  \n",
    "  def __init__(self, learn, final_value):\n",
    "    super().__init__(learn)\n",
    "    self.final_value = final_value\n",
    "  \n",
    "  def on_train_begin(self, **kwargs):\n",
    "    self.initial_value = self.opt.lr\n",
    "    self.learn.recorder.add_metric_names(['lr'])\n",
    "  \n",
    "  def on_epoch_end(self, epoch, n_epochs, last_metrics, **kwargs):\n",
    "    self.opt.lr = annealing_linear(self.initial_value, self.final_value, float(epoch) / n_epochs)\n",
    "    return add_metrics(last_metrics, self.opt.lr)\n",
    "  \n",
    "# class LRMonitor(LearnerCallBack):\n",
    "#   def __init__(self, learn):\n",
    "#     super().__init__(learn)\n",
    "#     self.name = 'lr'\n",
    "    \n",
    "#   def on_epoch_end(self, last_metrics, **kwargs):\n",
    "#     return add_metrics(last_metrics, self.opt.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9J20CBLS8S9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models_directory returned is:  models/461\n"
     ]
    }
   ],
   "source": [
    "learn = None; gen = None; gc.collect()\n",
    "csv_logger = partial(ImmediateCSVLogger, filename= env.temp_csv_path + '/' + env.save_filename)\n",
    "gen = Gen(z_dim=z_dim)\n",
    "init_cnn(gen, True)\n",
    "\n",
    "if gen_arch == 'non-targeted':\n",
    "  metrics = [validation]\n",
    "elif gen_arch == 'targeted':\n",
    "  metrics = [validation, targeted_validation]\n",
    "    \n",
    "learn = Learner(data, gen, loss_func = feat_loss, model_dir = env.get_learner_models_dir(),\n",
    "                metrics=metrics, callback_fns=[DiversityMetric, LossMetrics, csv_logger])\n",
    "\n",
    "# learn = Learner(data, Gen(z_dim=10), loss_func = feat_loss, metrics=[validation], callback_fns=LossMetrics, opt_func = optim.SGD)\n",
    "# learn = Learner(data, Gen(z_dim=z_dim), loss_func = feat_loss, metrics=[validation], callback_fns=[LossMetrics, DiversityWeightsScheduler])\n",
    "\n",
    "# load_starting_point(learn, model.__name__, z_dim)\n",
    "# random_seed(42, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wOZYzOHDEdB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GeneralRelu. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type FeatureLoss. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Gen. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/anaconda3/envs/mmderakhshani/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type deconv_layer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (9000 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "n02454379,n02454379,n02454379,n02454379,n02454379\n",
       "Path: /root/Derakhshani/adversarial/datasets/dataset;\n",
       "\n",
       "Valid: LabelList (50000 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "n02454379,n02454379,n02454379,n02454379,n02454379\n",
       "Path: /root/Derakhshani/adversarial/datasets/dataset;\n",
       "\n",
       "Test: None, model=Gen(\n",
       "  (z_): Linear(in_features=1000, out_features=7168, bias=True)\n",
       "  (BN_): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (CT2d_1): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (BN2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_2): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(320, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_3): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(160, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_4): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(80, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_5): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_6): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (BN2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (CT2d_7): deconv_layer(\n",
       "    (CT2d): ConvTranspose2d(72, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (BN2d): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FeatureLoss(\n",
       "  (dis): SoftmaxWrapper(\n",
       "    (m): GoogLeNet(\n",
       "      (conv1): BasicConv2d(\n",
       "        (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      (conv2): BasicConv2d(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv3): BasicConv2d(\n",
       "        (conv): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      (inception3a): Inception(\n",
       "        (branch1): BasicConv2d(\n",
       "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch3): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch4): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (inception3b): Inception(\n",
       "        (branch1): BasicConv2d(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch3): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch4): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (maxpool3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      (inception4a): Inception(\n",
       "        (branch1): BasicConv2d(\n",
       "          (conv): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(208, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch3): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch4): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (inception4b): Inception(\n",
       "        (branch1): BasicConv2d(\n",
       "          (conv): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch3): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch4): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (inception4c): Inception(\n",
       "        (branch1): BasicConv2d(\n",
       "          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch3): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch4): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (inception4d): Inception(\n",
       "        (branch1): BasicConv2d(\n",
       "          (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch3): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch4): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (inception4e): Inception(\n",
       "        (branch1): BasicConv2d(\n",
       "          (conv): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch3): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch4): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      (inception5a): Inception(\n",
       "        (branch1): BasicConv2d(\n",
       "          (conv): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch3): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch4): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (inception5b): Inception(\n",
       "        (branch1): BasicConv2d(\n",
       "          (conv): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch3): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (branch4): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (dropout): Dropout(p=0.2)\n",
       "      (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax()\n",
       "  )\n",
       "), metrics=[<function validation at 0x7f332a70be18>, <function targeted_validation at 0x7f332a721510>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/Derakhshani/adversarial/datasets/dataset'), model_dir='models/461', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), <class '__main__.DiversityMetric'>, <class 'fastai.callbacks.loss_metrics.LossMetrics'>, functools.partial(<class 'nag_util.ImmediateCSVLogger'>, filename='/root/Derakhshani/adversarial/temp/googlenet_16')], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): Linear(in_features=1000, out_features=7168, bias=True)\n",
       "  (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): GeneralRelu()\n",
       "  (5): ConvTranspose2d(320, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): GeneralRelu()\n",
       "  (8): ConvTranspose2d(160, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): GeneralRelu()\n",
       "  (11): ConvTranspose2d(80, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): GeneralRelu()\n",
       "  (14): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (16): GeneralRelu()\n",
       "  (17): ConvTranspose2d(72, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): GeneralRelu()\n",
       "  (20): ConvTranspose2d(72, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (21): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")], add_time=True, silent=None)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !cp \"/content/gdrive/My Drive/DL/models/vgg16_12-last.pth\"  \"/content/\"\n",
    "# learn.load('/content/vgg16_12-last')\n",
    "\n",
    "# load_filename = 'resnet50-11_39'\n",
    "# load_filename = 'resnet50_startpoint_0'\n",
    "# load_filename = 'googlenet_13_attempt5/googlenet_13_attempt5_29'\n",
    "load_filename = 'investigate_googlenet_2/1/googlenet_2'\n",
    "# load_filename = 'googlenet_17/googlenet_17_64'\n",
    "# load_filename = 'vgg16_12-last'\n",
    "# load_filename = None\n",
    "\n",
    "learn.load('/root/Derakhshani/adversarial/models/' + load_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find(1e-6, 100)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dir = 'investigate_vgg16_1'\n",
    "# investigate_initial_settings(8, 3, lr = 1e-2, wd = 0.0, results_dir = results_dir)\n",
    "# shutil.rmtree(env.get_models_dir() + results_dir)\n",
    "# shutil.rmtree(env.get_csv_dir() + results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WeightTuner(LearnerCallback):\n",
    "#   def __init__(self, learn: Learner):\n",
    "#     super().__init__(learn)\n",
    "#     self.fooling_weight = learn.loss_func.fooling_weight\n",
    "    \n",
    "#   def on_epoch_begin(self, **kwargs):\n",
    "#     fooling_rate = \n",
    "#     print(\"by how much to increase the fooling_weight? (current value: {})\".format(fooling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoolingWeightScheduler(LearnerCallback):\n",
    "  def __init__(self, learn: Learner, fooling_loss_index):\n",
    "    super().__init__(learn)\n",
    "    self.fooling_loss_index = fooling_loss_index\n",
    "    self.weights_history = []\n",
    "    self.fooling_loss_history = []\n",
    "  \n",
    "  def on_epoch_end(self, last_metrics, **kwargs):\n",
    "    # history keeping\n",
    "    self.weights_history.append((kwargs['epoch'], self.learn.loss_func.fooling_weight))\n",
    "    \n",
    "    # the actualy functionality\n",
    "    fooling_loss = last_metrics[self.fooling_loss_index]\n",
    "    if fooling_loss < -2 or fooling_loss > 2:\n",
    "      raise ValueError('fooling loss is outside the range [-2, 2]. the fooling index is probably wrong.')\n",
    "    if kwargs['epoch'] <= 1:\n",
    "      print((\"\\n\\nchecking validity of fooling_loss_index. if the {}th index of this\" + \n",
    "            \"list is not fool_loss, then you must restart: {}\\n\\n\").format(self.fooling_loss_index, last_metrics))\n",
    "    \n",
    "    self.fooling_loss_history.append(fooling_loss)\n",
    "    \n",
    "    if len(self.weights_history) < 2:\n",
    "      return\n",
    "    \n",
    "    if self.fooling_loss_history[-1] > self.fooling_loss_history[-2]:\n",
    "      self.learn.loss_func.fooling_weight += 0.3    \n",
    "      print('fooling weight increased to {} at the end of epoch {}'.format(\n",
    "        self.learn.loss_func.fooling_weight, kwargs['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the selected settings are : \n",
      "\tmode: div_metric_calc \n",
      "\tnetw-under-attack: googlenet \n",
      "\tload filename: googlenet_17/googlenet_17_64 \n",
      "      \tsave filename: googlenet_16\n",
      "\tmetric names: ['fool_loss']\n",
      "\tgen arch: targeted\n",
      "\n",
      "please MAKE SURE that the config is correct.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  load_filename\n",
    "except NameError:\n",
    "  load_filename = None\n",
    "\n",
    "print(\"the selected settings are : \")\n",
    "print('''\\tmode: {} \\n\\tnetw-under-attack: {} \\n\\tload filename: {} \n",
    "      \\tsave filename: {}\\n\\tmetric names: {}\\n\\tgen arch: {}\\n'''.format(\n",
    "      mode, model.__name__, load_filename , env.save_filename, feat_loss.metric_names,\n",
    "      gen_arch\n",
    "))\n",
    "print(\"please MAKE SURE that the config is correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find(1e-6, 100)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='154' class='' max='300', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      51.33% [154/300 11:32:27<10:56:29]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>validation</th>\n",
       "      <th>targeted_validation</th>\n",
       "      <th>div_metric</th>\n",
       "      <th>entropy</th>\n",
       "      <th>fool_loss</th>\n",
       "      <th>lr</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.199475</td>\n",
       "      <td>9.203181</td>\n",
       "      <td>0.511000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>775.000000</td>\n",
       "      <td>9.634100</td>\n",
       "      <td>9.203183</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.975951</td>\n",
       "      <td>8.979214</td>\n",
       "      <td>0.511000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>523.250000</td>\n",
       "      <td>8.440783</td>\n",
       "      <td>8.979215</td>\n",
       "      <td>0.009967</td>\n",
       "      <td>04:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.685092</td>\n",
       "      <td>8.778375</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>517.500000</td>\n",
       "      <td>8.402109</td>\n",
       "      <td>8.778376</td>\n",
       "      <td>0.009934</td>\n",
       "      <td>04:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8.625844</td>\n",
       "      <td>8.702210</td>\n",
       "      <td>0.589000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>544.750000</td>\n",
       "      <td>8.721867</td>\n",
       "      <td>8.702208</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8.669984</td>\n",
       "      <td>8.709808</td>\n",
       "      <td>0.592000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>494.000000</td>\n",
       "      <td>8.508518</td>\n",
       "      <td>8.709809</td>\n",
       "      <td>0.009868</td>\n",
       "      <td>04:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8.492018</td>\n",
       "      <td>8.595423</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>537.750000</td>\n",
       "      <td>8.531683</td>\n",
       "      <td>8.595422</td>\n",
       "      <td>0.009835</td>\n",
       "      <td>04:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>8.506056</td>\n",
       "      <td>8.587947</td>\n",
       "      <td>0.608000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>539.750000</td>\n",
       "      <td>8.587811</td>\n",
       "      <td>8.587946</td>\n",
       "      <td>0.009802</td>\n",
       "      <td>04:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8.458735</td>\n",
       "      <td>8.554623</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>8.535191</td>\n",
       "      <td>8.554623</td>\n",
       "      <td>0.009769</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8.341434</td>\n",
       "      <td>8.499042</td>\n",
       "      <td>0.623000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>518.500000</td>\n",
       "      <td>8.345800</td>\n",
       "      <td>8.499041</td>\n",
       "      <td>0.009736</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>8.358216</td>\n",
       "      <td>8.496431</td>\n",
       "      <td>0.619000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>516.750000</td>\n",
       "      <td>8.465167</td>\n",
       "      <td>8.496431</td>\n",
       "      <td>0.009703</td>\n",
       "      <td>04:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>8.402821</td>\n",
       "      <td>8.512463</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>8.429620</td>\n",
       "      <td>8.512463</td>\n",
       "      <td>0.009670</td>\n",
       "      <td>04:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>8.390862</td>\n",
       "      <td>8.458193</td>\n",
       "      <td>0.628000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>518.500000</td>\n",
       "      <td>8.412821</td>\n",
       "      <td>8.458193</td>\n",
       "      <td>0.009637</td>\n",
       "      <td>04:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>8.327023</td>\n",
       "      <td>8.392145</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>505.250000</td>\n",
       "      <td>8.363990</td>\n",
       "      <td>8.392146</td>\n",
       "      <td>0.009604</td>\n",
       "      <td>04:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>8.347869</td>\n",
       "      <td>8.386729</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>487.500000</td>\n",
       "      <td>8.289625</td>\n",
       "      <td>8.386729</td>\n",
       "      <td>0.009571</td>\n",
       "      <td>04:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>8.311331</td>\n",
       "      <td>8.355925</td>\n",
       "      <td>0.636000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>8.335283</td>\n",
       "      <td>8.355925</td>\n",
       "      <td>0.009538</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>8.326675</td>\n",
       "      <td>8.306893</td>\n",
       "      <td>0.647000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>495.750000</td>\n",
       "      <td>8.316674</td>\n",
       "      <td>8.306892</td>\n",
       "      <td>0.009505</td>\n",
       "      <td>04:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>8.296013</td>\n",
       "      <td>8.238502</td>\n",
       "      <td>0.653000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>510.750000</td>\n",
       "      <td>8.417744</td>\n",
       "      <td>8.238501</td>\n",
       "      <td>0.009472</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>8.183421</td>\n",
       "      <td>8.247651</td>\n",
       "      <td>0.655000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>541.000000</td>\n",
       "      <td>8.664337</td>\n",
       "      <td>8.247651</td>\n",
       "      <td>0.009439</td>\n",
       "      <td>04:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>8.212006</td>\n",
       "      <td>8.255275</td>\n",
       "      <td>0.662000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>499.500000</td>\n",
       "      <td>8.320322</td>\n",
       "      <td>8.255275</td>\n",
       "      <td>0.009406</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>8.169740</td>\n",
       "      <td>8.247992</td>\n",
       "      <td>0.653000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>483.250000</td>\n",
       "      <td>8.184696</td>\n",
       "      <td>8.247992</td>\n",
       "      <td>0.009373</td>\n",
       "      <td>04:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>8.108485</td>\n",
       "      <td>8.301184</td>\n",
       "      <td>0.655000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>475.750000</td>\n",
       "      <td>8.125779</td>\n",
       "      <td>8.301184</td>\n",
       "      <td>0.009340</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>8.102424</td>\n",
       "      <td>8.248942</td>\n",
       "      <td>0.655000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>480.500000</td>\n",
       "      <td>8.131064</td>\n",
       "      <td>8.248942</td>\n",
       "      <td>0.009307</td>\n",
       "      <td>04:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>8.141055</td>\n",
       "      <td>8.177631</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>511.750000</td>\n",
       "      <td>8.394599</td>\n",
       "      <td>8.177632</td>\n",
       "      <td>0.009274</td>\n",
       "      <td>04:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>8.099418</td>\n",
       "      <td>8.187940</td>\n",
       "      <td>0.659000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>515.250000</td>\n",
       "      <td>8.387353</td>\n",
       "      <td>8.187941</td>\n",
       "      <td>0.009241</td>\n",
       "      <td>04:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>8.118025</td>\n",
       "      <td>8.160850</td>\n",
       "      <td>0.664000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>583.750000</td>\n",
       "      <td>8.901648</td>\n",
       "      <td>8.160850</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>04:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>8.103735</td>\n",
       "      <td>8.100914</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>517.750000</td>\n",
       "      <td>8.381920</td>\n",
       "      <td>8.100913</td>\n",
       "      <td>0.009175</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>8.093385</td>\n",
       "      <td>8.123698</td>\n",
       "      <td>0.662000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>529.250000</td>\n",
       "      <td>8.491493</td>\n",
       "      <td>8.123699</td>\n",
       "      <td>0.009142</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>8.039931</td>\n",
       "      <td>8.078261</td>\n",
       "      <td>0.658000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>529.500000</td>\n",
       "      <td>8.504660</td>\n",
       "      <td>8.078260</td>\n",
       "      <td>0.009109</td>\n",
       "      <td>04:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>7.986593</td>\n",
       "      <td>8.155081</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>517.250000</td>\n",
       "      <td>8.440604</td>\n",
       "      <td>8.155081</td>\n",
       "      <td>0.009076</td>\n",
       "      <td>04:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>8.106354</td>\n",
       "      <td>8.139317</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>525.250000</td>\n",
       "      <td>8.471572</td>\n",
       "      <td>8.139318</td>\n",
       "      <td>0.009043</td>\n",
       "      <td>04:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>8.034567</td>\n",
       "      <td>8.106065</td>\n",
       "      <td>0.662000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>515.750000</td>\n",
       "      <td>8.399136</td>\n",
       "      <td>8.106066</td>\n",
       "      <td>0.009010</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>8.054870</td>\n",
       "      <td>8.134342</td>\n",
       "      <td>0.663000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>526.750000</td>\n",
       "      <td>8.510918</td>\n",
       "      <td>8.134343</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>04:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>8.064052</td>\n",
       "      <td>8.093186</td>\n",
       "      <td>0.673000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>572.500000</td>\n",
       "      <td>8.791194</td>\n",
       "      <td>8.093186</td>\n",
       "      <td>0.008944</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>7.993203</td>\n",
       "      <td>8.067417</td>\n",
       "      <td>0.666000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>570.250000</td>\n",
       "      <td>8.813788</td>\n",
       "      <td>8.067417</td>\n",
       "      <td>0.008911</td>\n",
       "      <td>04:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>7.972868</td>\n",
       "      <td>8.066757</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>540.500000</td>\n",
       "      <td>8.617255</td>\n",
       "      <td>8.066758</td>\n",
       "      <td>0.008878</td>\n",
       "      <td>04:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>7.854024</td>\n",
       "      <td>8.073113</td>\n",
       "      <td>0.682000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>555.500000</td>\n",
       "      <td>8.709885</td>\n",
       "      <td>8.073113</td>\n",
       "      <td>0.008845</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>7.977285</td>\n",
       "      <td>8.054011</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>548.000000</td>\n",
       "      <td>8.637423</td>\n",
       "      <td>8.054010</td>\n",
       "      <td>0.008812</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>7.959695</td>\n",
       "      <td>7.986286</td>\n",
       "      <td>0.682000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>541.000000</td>\n",
       "      <td>8.617800</td>\n",
       "      <td>7.986285</td>\n",
       "      <td>0.008779</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>7.933329</td>\n",
       "      <td>8.045377</td>\n",
       "      <td>0.679000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>542.000000</td>\n",
       "      <td>8.577048</td>\n",
       "      <td>8.045378</td>\n",
       "      <td>0.008746</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>7.963741</td>\n",
       "      <td>8.135034</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>532.500000</td>\n",
       "      <td>8.509072</td>\n",
       "      <td>8.135034</td>\n",
       "      <td>0.008713</td>\n",
       "      <td>04:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>8.026730</td>\n",
       "      <td>8.023495</td>\n",
       "      <td>0.688000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>8.468384</td>\n",
       "      <td>8.023495</td>\n",
       "      <td>0.008680</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>7.936101</td>\n",
       "      <td>8.036869</td>\n",
       "      <td>0.682000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>484.500000</td>\n",
       "      <td>8.211323</td>\n",
       "      <td>8.036868</td>\n",
       "      <td>0.008647</td>\n",
       "      <td>04:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>7.967117</td>\n",
       "      <td>7.997777</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>496.000000</td>\n",
       "      <td>8.275049</td>\n",
       "      <td>7.997777</td>\n",
       "      <td>0.008614</td>\n",
       "      <td>04:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>7.780284</td>\n",
       "      <td>7.974747</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>8.250600</td>\n",
       "      <td>7.974747</td>\n",
       "      <td>0.008581</td>\n",
       "      <td>04:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>7.917612</td>\n",
       "      <td>7.941567</td>\n",
       "      <td>0.701000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>497.500000</td>\n",
       "      <td>8.311341</td>\n",
       "      <td>7.941568</td>\n",
       "      <td>0.008548</td>\n",
       "      <td>04:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>7.873788</td>\n",
       "      <td>7.973596</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>486.500000</td>\n",
       "      <td>8.218283</td>\n",
       "      <td>7.973595</td>\n",
       "      <td>0.008515</td>\n",
       "      <td>04:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>7.868811</td>\n",
       "      <td>8.040606</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>8.306005</td>\n",
       "      <td>8.040606</td>\n",
       "      <td>0.008482</td>\n",
       "      <td>04:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>7.878739</td>\n",
       "      <td>8.001189</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>497.500000</td>\n",
       "      <td>8.277188</td>\n",
       "      <td>8.001189</td>\n",
       "      <td>0.008449</td>\n",
       "      <td>04:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>7.877689</td>\n",
       "      <td>7.957468</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>514.750000</td>\n",
       "      <td>8.379704</td>\n",
       "      <td>7.957467</td>\n",
       "      <td>0.008416</td>\n",
       "      <td>04:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>7.858537</td>\n",
       "      <td>7.817009</td>\n",
       "      <td>0.697000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>523.500000</td>\n",
       "      <td>8.412794</td>\n",
       "      <td>7.817009</td>\n",
       "      <td>0.008383</td>\n",
       "      <td>04:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>7.855598</td>\n",
       "      <td>7.918439</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>531.250000</td>\n",
       "      <td>8.472220</td>\n",
       "      <td>7.918438</td>\n",
       "      <td>0.008350</td>\n",
       "      <td>04:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>7.803025</td>\n",
       "      <td>7.932932</td>\n",
       "      <td>0.709000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>524.750000</td>\n",
       "      <td>8.439746</td>\n",
       "      <td>7.932931</td>\n",
       "      <td>0.008317</td>\n",
       "      <td>04:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>7.832505</td>\n",
       "      <td>7.817805</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>8.405823</td>\n",
       "      <td>7.817806</td>\n",
       "      <td>0.008284</td>\n",
       "      <td>04:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>7.833617</td>\n",
       "      <td>7.924608</td>\n",
       "      <td>0.698000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>545.250000</td>\n",
       "      <td>8.579651</td>\n",
       "      <td>7.924609</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>04:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>7.818986</td>\n",
       "      <td>7.875701</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>537.250000</td>\n",
       "      <td>8.526644</td>\n",
       "      <td>7.875701</td>\n",
       "      <td>0.008218</td>\n",
       "      <td>04:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>7.766294</td>\n",
       "      <td>7.876260</td>\n",
       "      <td>0.724000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>516.500000</td>\n",
       "      <td>8.386042</td>\n",
       "      <td>7.876259</td>\n",
       "      <td>0.008185</td>\n",
       "      <td>04:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>7.742660</td>\n",
       "      <td>7.850282</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>523.500000</td>\n",
       "      <td>8.423911</td>\n",
       "      <td>7.850282</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>04:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>7.782901</td>\n",
       "      <td>7.897940</td>\n",
       "      <td>0.724000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>512.250000</td>\n",
       "      <td>8.327158</td>\n",
       "      <td>7.897940</td>\n",
       "      <td>0.008119</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>7.752210</td>\n",
       "      <td>7.866784</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>8.472479</td>\n",
       "      <td>7.866783</td>\n",
       "      <td>0.008086</td>\n",
       "      <td>04:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>7.733784</td>\n",
       "      <td>7.756688</td>\n",
       "      <td>0.724000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>547.250000</td>\n",
       "      <td>8.661749</td>\n",
       "      <td>7.756690</td>\n",
       "      <td>0.008053</td>\n",
       "      <td>04:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>7.748917</td>\n",
       "      <td>7.846357</td>\n",
       "      <td>0.727000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>524.250000</td>\n",
       "      <td>8.480562</td>\n",
       "      <td>7.846357</td>\n",
       "      <td>0.008020</td>\n",
       "      <td>04:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>7.679263</td>\n",
       "      <td>7.865458</td>\n",
       "      <td>0.708000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>517.500000</td>\n",
       "      <td>8.428359</td>\n",
       "      <td>7.865458</td>\n",
       "      <td>0.007987</td>\n",
       "      <td>04:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>7.713552</td>\n",
       "      <td>7.796378</td>\n",
       "      <td>0.726000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>546.000000</td>\n",
       "      <td>8.564461</td>\n",
       "      <td>7.796380</td>\n",
       "      <td>0.007954</td>\n",
       "      <td>04:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>7.756547</td>\n",
       "      <td>7.833426</td>\n",
       "      <td>0.719000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>535.250000</td>\n",
       "      <td>8.565443</td>\n",
       "      <td>7.833426</td>\n",
       "      <td>0.007921</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>7.658340</td>\n",
       "      <td>7.812523</td>\n",
       "      <td>0.726000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>528.500000</td>\n",
       "      <td>8.480944</td>\n",
       "      <td>7.812522</td>\n",
       "      <td>0.007888</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>7.660933</td>\n",
       "      <td>7.747742</td>\n",
       "      <td>0.724000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>499.500000</td>\n",
       "      <td>8.275000</td>\n",
       "      <td>7.747742</td>\n",
       "      <td>0.007855</td>\n",
       "      <td>04:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>7.698746</td>\n",
       "      <td>7.760494</td>\n",
       "      <td>0.723000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>520.250000</td>\n",
       "      <td>8.480661</td>\n",
       "      <td>7.760495</td>\n",
       "      <td>0.007822</td>\n",
       "      <td>04:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>7.611561</td>\n",
       "      <td>7.734913</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>526.000000</td>\n",
       "      <td>8.488903</td>\n",
       "      <td>7.734912</td>\n",
       "      <td>0.007789</td>\n",
       "      <td>04:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>7.673154</td>\n",
       "      <td>7.832211</td>\n",
       "      <td>0.731000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>534.250000</td>\n",
       "      <td>8.528320</td>\n",
       "      <td>7.832211</td>\n",
       "      <td>0.007756</td>\n",
       "      <td>04:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>7.723097</td>\n",
       "      <td>7.770729</td>\n",
       "      <td>0.736000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>570.750000</td>\n",
       "      <td>8.740560</td>\n",
       "      <td>7.770729</td>\n",
       "      <td>0.007723</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>7.719886</td>\n",
       "      <td>7.696643</td>\n",
       "      <td>0.731000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>566.250000</td>\n",
       "      <td>8.715129</td>\n",
       "      <td>7.696643</td>\n",
       "      <td>0.007690</td>\n",
       "      <td>04:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>7.721422</td>\n",
       "      <td>7.787831</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>566.750000</td>\n",
       "      <td>8.722178</td>\n",
       "      <td>7.787830</td>\n",
       "      <td>0.007657</td>\n",
       "      <td>04:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>7.672343</td>\n",
       "      <td>7.673317</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>556.000000</td>\n",
       "      <td>8.661257</td>\n",
       "      <td>7.673316</td>\n",
       "      <td>0.007624</td>\n",
       "      <td>04:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>7.682676</td>\n",
       "      <td>7.761106</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>566.750000</td>\n",
       "      <td>8.696191</td>\n",
       "      <td>7.761106</td>\n",
       "      <td>0.007591</td>\n",
       "      <td>04:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>7.582072</td>\n",
       "      <td>7.808496</td>\n",
       "      <td>0.729000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>576.250000</td>\n",
       "      <td>8.771729</td>\n",
       "      <td>7.808497</td>\n",
       "      <td>0.007558</td>\n",
       "      <td>04:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>7.584916</td>\n",
       "      <td>7.687944</td>\n",
       "      <td>0.737000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>8.761556</td>\n",
       "      <td>7.687945</td>\n",
       "      <td>0.007525</td>\n",
       "      <td>04:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>7.711078</td>\n",
       "      <td>7.693946</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>585.750000</td>\n",
       "      <td>8.842855</td>\n",
       "      <td>7.693945</td>\n",
       "      <td>0.007492</td>\n",
       "      <td>04:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>7.672552</td>\n",
       "      <td>7.732306</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>544.000000</td>\n",
       "      <td>8.597557</td>\n",
       "      <td>7.732307</td>\n",
       "      <td>0.007459</td>\n",
       "      <td>04:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>7.613333</td>\n",
       "      <td>7.688628</td>\n",
       "      <td>0.739000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>570.500000</td>\n",
       "      <td>8.771714</td>\n",
       "      <td>7.688627</td>\n",
       "      <td>0.007426</td>\n",
       "      <td>04:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>7.629042</td>\n",
       "      <td>7.683312</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>570.750000</td>\n",
       "      <td>8.749920</td>\n",
       "      <td>7.683312</td>\n",
       "      <td>0.007393</td>\n",
       "      <td>04:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>7.648256</td>\n",
       "      <td>7.653133</td>\n",
       "      <td>0.741000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>551.000000</td>\n",
       "      <td>8.587175</td>\n",
       "      <td>7.653132</td>\n",
       "      <td>0.007360</td>\n",
       "      <td>04:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>7.646446</td>\n",
       "      <td>7.671184</td>\n",
       "      <td>0.743000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>560.750000</td>\n",
       "      <td>8.648518</td>\n",
       "      <td>7.671184</td>\n",
       "      <td>0.007327</td>\n",
       "      <td>04:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>7.616429</td>\n",
       "      <td>7.671604</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>551.750000</td>\n",
       "      <td>8.661141</td>\n",
       "      <td>7.671604</td>\n",
       "      <td>0.007294</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>7.535349</td>\n",
       "      <td>7.642673</td>\n",
       "      <td>0.747000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>565.250000</td>\n",
       "      <td>8.686004</td>\n",
       "      <td>7.642673</td>\n",
       "      <td>0.007261</td>\n",
       "      <td>04:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>7.548214</td>\n",
       "      <td>7.643622</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>559.250000</td>\n",
       "      <td>8.655789</td>\n",
       "      <td>7.643621</td>\n",
       "      <td>0.007228</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>7.616577</td>\n",
       "      <td>7.639870</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>546.750000</td>\n",
       "      <td>8.584290</td>\n",
       "      <td>7.639871</td>\n",
       "      <td>0.007195</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>7.638740</td>\n",
       "      <td>7.662282</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>535.250000</td>\n",
       "      <td>8.487433</td>\n",
       "      <td>7.662282</td>\n",
       "      <td>0.007162</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>7.560879</td>\n",
       "      <td>7.614841</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>526.750000</td>\n",
       "      <td>8.424457</td>\n",
       "      <td>7.614841</td>\n",
       "      <td>0.007129</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>7.498406</td>\n",
       "      <td>7.679070</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>536.750000</td>\n",
       "      <td>8.472052</td>\n",
       "      <td>7.679070</td>\n",
       "      <td>0.007096</td>\n",
       "      <td>04:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>7.514164</td>\n",
       "      <td>7.598824</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>541.500000</td>\n",
       "      <td>8.511462</td>\n",
       "      <td>7.598824</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>7.578559</td>\n",
       "      <td>7.702515</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>553.500000</td>\n",
       "      <td>8.577789</td>\n",
       "      <td>7.702515</td>\n",
       "      <td>0.007030</td>\n",
       "      <td>04:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>7.526561</td>\n",
       "      <td>7.658095</td>\n",
       "      <td>0.739000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>548.750000</td>\n",
       "      <td>8.550779</td>\n",
       "      <td>7.658094</td>\n",
       "      <td>0.006997</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>7.525045</td>\n",
       "      <td>7.566941</td>\n",
       "      <td>0.734000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>556.000000</td>\n",
       "      <td>8.605816</td>\n",
       "      <td>7.566941</td>\n",
       "      <td>0.006964</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>7.500762</td>\n",
       "      <td>7.636811</td>\n",
       "      <td>0.743000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>557.250000</td>\n",
       "      <td>8.644506</td>\n",
       "      <td>7.636811</td>\n",
       "      <td>0.006931</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>7.481819</td>\n",
       "      <td>7.593695</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>541.250000</td>\n",
       "      <td>8.516235</td>\n",
       "      <td>7.593696</td>\n",
       "      <td>0.006898</td>\n",
       "      <td>04:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>7.473671</td>\n",
       "      <td>7.550272</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>543.250000</td>\n",
       "      <td>8.521295</td>\n",
       "      <td>7.550272</td>\n",
       "      <td>0.006865</td>\n",
       "      <td>04:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>7.500116</td>\n",
       "      <td>7.607670</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>544.250000</td>\n",
       "      <td>8.537749</td>\n",
       "      <td>7.607670</td>\n",
       "      <td>0.006832</td>\n",
       "      <td>04:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>7.487347</td>\n",
       "      <td>7.597827</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>559.000000</td>\n",
       "      <td>8.632771</td>\n",
       "      <td>7.597827</td>\n",
       "      <td>0.006799</td>\n",
       "      <td>04:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>7.483907</td>\n",
       "      <td>7.516847</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>557.500000</td>\n",
       "      <td>8.638379</td>\n",
       "      <td>7.516845</td>\n",
       "      <td>0.006766</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>7.449820</td>\n",
       "      <td>7.616355</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>8.616201</td>\n",
       "      <td>7.616355</td>\n",
       "      <td>0.006733</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7.489778</td>\n",
       "      <td>7.491984</td>\n",
       "      <td>0.743000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>573.000000</td>\n",
       "      <td>8.769046</td>\n",
       "      <td>7.491985</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>04:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>7.407585</td>\n",
       "      <td>7.500682</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>561.250000</td>\n",
       "      <td>8.695135</td>\n",
       "      <td>7.500681</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>04:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>7.466794</td>\n",
       "      <td>7.548660</td>\n",
       "      <td>0.743000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>551.750000</td>\n",
       "      <td>8.631062</td>\n",
       "      <td>7.548660</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>04:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>7.496151</td>\n",
       "      <td>7.520792</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>569.250000</td>\n",
       "      <td>8.731194</td>\n",
       "      <td>7.520792</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>04:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>7.446672</td>\n",
       "      <td>7.520388</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>565.500000</td>\n",
       "      <td>8.636072</td>\n",
       "      <td>7.520387</td>\n",
       "      <td>0.006568</td>\n",
       "      <td>04:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>7.370359</td>\n",
       "      <td>7.536460</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>559.000000</td>\n",
       "      <td>8.600445</td>\n",
       "      <td>7.536458</td>\n",
       "      <td>0.006535</td>\n",
       "      <td>04:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>7.494708</td>\n",
       "      <td>7.516976</td>\n",
       "      <td>0.758000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>563.750000</td>\n",
       "      <td>8.670636</td>\n",
       "      <td>7.516977</td>\n",
       "      <td>0.006502</td>\n",
       "      <td>04:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>7.456848</td>\n",
       "      <td>7.460060</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>572.000000</td>\n",
       "      <td>8.698863</td>\n",
       "      <td>7.460060</td>\n",
       "      <td>0.006469</td>\n",
       "      <td>04:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>7.430043</td>\n",
       "      <td>7.500918</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>564.750000</td>\n",
       "      <td>8.633571</td>\n",
       "      <td>7.500917</td>\n",
       "      <td>0.006436</td>\n",
       "      <td>04:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>7.486521</td>\n",
       "      <td>7.455341</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>578.750000</td>\n",
       "      <td>8.706837</td>\n",
       "      <td>7.455342</td>\n",
       "      <td>0.006403</td>\n",
       "      <td>04:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>7.405308</td>\n",
       "      <td>7.495768</td>\n",
       "      <td>0.743000</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>538.250000</td>\n",
       "      <td>8.471404</td>\n",
       "      <td>7.495768</td>\n",
       "      <td>0.006370</td>\n",
       "      <td>04:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>7.392202</td>\n",
       "      <td>7.472012</td>\n",
       "      <td>0.758000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>8.475199</td>\n",
       "      <td>7.472011</td>\n",
       "      <td>0.006337</td>\n",
       "      <td>04:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>7.466258</td>\n",
       "      <td>7.543531</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>542.500000</td>\n",
       "      <td>8.502880</td>\n",
       "      <td>7.543531</td>\n",
       "      <td>0.006304</td>\n",
       "      <td>04:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>7.336548</td>\n",
       "      <td>7.483327</td>\n",
       "      <td>0.747000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>542.750000</td>\n",
       "      <td>8.500204</td>\n",
       "      <td>7.483327</td>\n",
       "      <td>0.006271</td>\n",
       "      <td>04:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>7.368824</td>\n",
       "      <td>7.498144</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>555.250000</td>\n",
       "      <td>8.591968</td>\n",
       "      <td>7.498145</td>\n",
       "      <td>0.006238</td>\n",
       "      <td>04:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>7.418448</td>\n",
       "      <td>7.427878</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>562.000000</td>\n",
       "      <td>8.619082</td>\n",
       "      <td>7.427878</td>\n",
       "      <td>0.006205</td>\n",
       "      <td>04:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>7.387801</td>\n",
       "      <td>7.485806</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>540.250000</td>\n",
       "      <td>8.483051</td>\n",
       "      <td>7.485806</td>\n",
       "      <td>0.006172</td>\n",
       "      <td>04:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>7.375075</td>\n",
       "      <td>7.481396</td>\n",
       "      <td>0.747000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>544.750000</td>\n",
       "      <td>8.503202</td>\n",
       "      <td>7.481397</td>\n",
       "      <td>0.006139</td>\n",
       "      <td>04:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>7.394910</td>\n",
       "      <td>7.460744</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>553.500000</td>\n",
       "      <td>8.549680</td>\n",
       "      <td>7.460743</td>\n",
       "      <td>0.006106</td>\n",
       "      <td>04:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>7.351398</td>\n",
       "      <td>7.480481</td>\n",
       "      <td>0.754000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>555.500000</td>\n",
       "      <td>8.540482</td>\n",
       "      <td>7.480481</td>\n",
       "      <td>0.006073</td>\n",
       "      <td>04:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>7.398022</td>\n",
       "      <td>7.481115</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>545.500000</td>\n",
       "      <td>8.496152</td>\n",
       "      <td>7.481114</td>\n",
       "      <td>0.006040</td>\n",
       "      <td>04:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>7.359078</td>\n",
       "      <td>7.485583</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>541.250000</td>\n",
       "      <td>8.470209</td>\n",
       "      <td>7.485583</td>\n",
       "      <td>0.006007</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>7.383066</td>\n",
       "      <td>7.443902</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>544.750000</td>\n",
       "      <td>8.494241</td>\n",
       "      <td>7.443902</td>\n",
       "      <td>0.005974</td>\n",
       "      <td>04:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>7.396608</td>\n",
       "      <td>7.338559</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>557.250000</td>\n",
       "      <td>8.591288</td>\n",
       "      <td>7.338559</td>\n",
       "      <td>0.005941</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>7.396627</td>\n",
       "      <td>7.445016</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>581.000000</td>\n",
       "      <td>8.725463</td>\n",
       "      <td>7.445016</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>7.318918</td>\n",
       "      <td>7.426468</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>600.250000</td>\n",
       "      <td>8.876741</td>\n",
       "      <td>7.426467</td>\n",
       "      <td>0.005875</td>\n",
       "      <td>04:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>7.422248</td>\n",
       "      <td>7.383907</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>599.500000</td>\n",
       "      <td>8.833235</td>\n",
       "      <td>7.383907</td>\n",
       "      <td>0.005842</td>\n",
       "      <td>04:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>7.332553</td>\n",
       "      <td>7.469241</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>562.500000</td>\n",
       "      <td>8.621835</td>\n",
       "      <td>7.469240</td>\n",
       "      <td>0.005809</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>7.450225</td>\n",
       "      <td>7.472275</td>\n",
       "      <td>0.747000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>584.000000</td>\n",
       "      <td>8.736770</td>\n",
       "      <td>7.472275</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>04:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>7.412276</td>\n",
       "      <td>7.401683</td>\n",
       "      <td>0.759000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>545.250000</td>\n",
       "      <td>8.499659</td>\n",
       "      <td>7.401684</td>\n",
       "      <td>0.005743</td>\n",
       "      <td>04:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>7.366374</td>\n",
       "      <td>7.365104</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>579.500000</td>\n",
       "      <td>8.690267</td>\n",
       "      <td>7.365106</td>\n",
       "      <td>0.005710</td>\n",
       "      <td>04:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>7.243502</td>\n",
       "      <td>7.441023</td>\n",
       "      <td>0.741000</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>558.250000</td>\n",
       "      <td>8.587724</td>\n",
       "      <td>7.441023</td>\n",
       "      <td>0.005677</td>\n",
       "      <td>04:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>7.461787</td>\n",
       "      <td>7.425273</td>\n",
       "      <td>0.758000</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>563.250000</td>\n",
       "      <td>8.648736</td>\n",
       "      <td>7.425274</td>\n",
       "      <td>0.005644</td>\n",
       "      <td>04:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>7.337040</td>\n",
       "      <td>7.446754</td>\n",
       "      <td>0.743000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>549.750000</td>\n",
       "      <td>8.551287</td>\n",
       "      <td>7.446754</td>\n",
       "      <td>0.005611</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>7.334147</td>\n",
       "      <td>7.353924</td>\n",
       "      <td>0.747000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>547.500000</td>\n",
       "      <td>8.547132</td>\n",
       "      <td>7.353923</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>04:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>7.360682</td>\n",
       "      <td>7.359558</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>556.000000</td>\n",
       "      <td>8.571537</td>\n",
       "      <td>7.359559</td>\n",
       "      <td>0.005545</td>\n",
       "      <td>04:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>7.295626</td>\n",
       "      <td>7.384748</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>551.500000</td>\n",
       "      <td>8.533356</td>\n",
       "      <td>7.384748</td>\n",
       "      <td>0.005512</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>7.341208</td>\n",
       "      <td>7.370287</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>567.500000</td>\n",
       "      <td>8.627754</td>\n",
       "      <td>7.370287</td>\n",
       "      <td>0.005479</td>\n",
       "      <td>04:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>7.318606</td>\n",
       "      <td>7.362034</td>\n",
       "      <td>0.762000</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>560.250000</td>\n",
       "      <td>8.584478</td>\n",
       "      <td>7.362034</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>04:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>7.290531</td>\n",
       "      <td>7.356919</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>563.000000</td>\n",
       "      <td>8.608905</td>\n",
       "      <td>7.356919</td>\n",
       "      <td>0.005413</td>\n",
       "      <td>04:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>7.337369</td>\n",
       "      <td>7.420472</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>565.500000</td>\n",
       "      <td>8.608970</td>\n",
       "      <td>7.420473</td>\n",
       "      <td>0.005380</td>\n",
       "      <td>04:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>7.352620</td>\n",
       "      <td>7.372697</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>573.750000</td>\n",
       "      <td>8.657478</td>\n",
       "      <td>7.372697</td>\n",
       "      <td>0.005347</td>\n",
       "      <td>04:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>7.343617</td>\n",
       "      <td>7.319755</td>\n",
       "      <td>0.753000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>561.500000</td>\n",
       "      <td>8.582781</td>\n",
       "      <td>7.319754</td>\n",
       "      <td>0.005314</td>\n",
       "      <td>04:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>7.338847</td>\n",
       "      <td>7.318701</td>\n",
       "      <td>0.747000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>560.750000</td>\n",
       "      <td>8.571268</td>\n",
       "      <td>7.318700</td>\n",
       "      <td>0.005281</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>7.295774</td>\n",
       "      <td>7.415954</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>566.500000</td>\n",
       "      <td>8.630415</td>\n",
       "      <td>7.415954</td>\n",
       "      <td>0.005248</td>\n",
       "      <td>04:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>7.304703</td>\n",
       "      <td>7.365501</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>577.500000</td>\n",
       "      <td>8.696463</td>\n",
       "      <td>7.365502</td>\n",
       "      <td>0.005215</td>\n",
       "      <td>04:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>7.266482</td>\n",
       "      <td>7.368520</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>566.250000</td>\n",
       "      <td>8.632872</td>\n",
       "      <td>7.368518</td>\n",
       "      <td>0.005182</td>\n",
       "      <td>04:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>7.314550</td>\n",
       "      <td>7.399385</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>571.750000</td>\n",
       "      <td>8.623522</td>\n",
       "      <td>7.399385</td>\n",
       "      <td>0.005149</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>7.286623</td>\n",
       "      <td>7.317584</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>560.250000</td>\n",
       "      <td>8.574715</td>\n",
       "      <td>7.317584</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>04:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>7.365320</td>\n",
       "      <td>7.353630</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>546.500000</td>\n",
       "      <td>8.497187</td>\n",
       "      <td>7.353629</td>\n",
       "      <td>0.005083</td>\n",
       "      <td>04:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>7.295312</td>\n",
       "      <td>7.270205</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>582.750000</td>\n",
       "      <td>8.697544</td>\n",
       "      <td>7.270204</td>\n",
       "      <td>0.005050</td>\n",
       "      <td>04:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>7.259504</td>\n",
       "      <td>7.342604</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>8.686893</td>\n",
       "      <td>7.342604</td>\n",
       "      <td>0.005017</td>\n",
       "      <td>04:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>7.223194</td>\n",
       "      <td>7.285019</td>\n",
       "      <td>0.747000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>8.848425</td>\n",
       "      <td>7.285019</td>\n",
       "      <td>0.004984</td>\n",
       "      <td>04:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>7.269383</td>\n",
       "      <td>7.354444</td>\n",
       "      <td>0.753000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>8.801534</td>\n",
       "      <td>7.354444</td>\n",
       "      <td>0.004951</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='216' class='' max='562', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      38.43% [216/562 01:24<02:14 7.2068]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[1.2571e-05],\n",
      "        [1.8596e-04],\n",
      "        [1.2446e-03],\n",
      "        [9.2343e-04],\n",
      "        [2.0649e-04],\n",
      "        [3.2474e-05],\n",
      "        [1.9488e-04],\n",
      "        [4.4806e-04],\n",
      "        [5.0289e-04],\n",
      "        [2.2066e-06],\n",
      "        [6.1601e-04],\n",
      "        [4.5806e-04],\n",
      "        [4.0867e-07],\n",
      "        [5.0947e-04],\n",
      "        [6.4659e-04],\n",
      "        [5.8737e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.125341415405273: \n",
      "target probs tensor([[9.8129e-05],\n",
      "        [5.8617e-05],\n",
      "        [2.7890e-04],\n",
      "        [1.7620e-05],\n",
      "        [1.0357e-04],\n",
      "        [2.1083e-04],\n",
      "        [1.7516e-04],\n",
      "        [4.9089e-06],\n",
      "        [2.9838e-04],\n",
      "        [8.3745e-05],\n",
      "        [9.9175e-05],\n",
      "        [8.8664e-06],\n",
      "        [1.5860e-06],\n",
      "        [3.0042e-04],\n",
      "        [9.3173e-04],\n",
      "        [1.4006e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.374341011047363: \n",
      "target probs tensor([[1.3017e-04],\n",
      "        [2.2575e-04],\n",
      "        [6.2150e-05],\n",
      "        [5.2545e-05],\n",
      "        [1.9121e-04],\n",
      "        [3.2195e-05],\n",
      "        [9.2251e-05],\n",
      "        [3.4869e-04],\n",
      "        [4.5853e-05],\n",
      "        [5.0262e-04],\n",
      "        [6.0559e-05],\n",
      "        [5.0921e-05],\n",
      "        [8.1727e-04],\n",
      "        [4.8406e-04],\n",
      "        [8.6807e-04],\n",
      "        [1.4436e-05]], device='cuda:0'), loss: 8.947586059570312: \n",
      "Better model found at epoch 0 with validation value: 0.5109999775886536.\n",
      "target probs tensor([[3.9817e-04],\n",
      "        [2.2097e-05],\n",
      "        [3.1697e-04],\n",
      "        [6.6923e-04],\n",
      "        [1.9090e-04],\n",
      "        [1.3334e-04],\n",
      "        [1.8284e-05],\n",
      "        [4.7900e-05],\n",
      "        [2.2529e-03],\n",
      "        [1.0508e-04],\n",
      "        [2.7050e-04],\n",
      "        [3.6445e-04],\n",
      "        [3.8353e-05],\n",
      "        [1.0453e-04],\n",
      "        [1.2247e-02],\n",
      "        [2.2975e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.485090255737305: \n",
      "target probs tensor([[2.2817e-04],\n",
      "        [5.7146e-04],\n",
      "        [3.2935e-04],\n",
      "        [6.5655e-04],\n",
      "        [5.9273e-05],\n",
      "        [7.4047e-04],\n",
      "        [5.3979e-05],\n",
      "        [9.5577e-05],\n",
      "        [1.3736e-04],\n",
      "        [1.6875e-04],\n",
      "        [4.6017e-04],\n",
      "        [9.6813e-05],\n",
      "        [5.7794e-05],\n",
      "        [4.7523e-05],\n",
      "        [1.6685e-04],\n",
      "        [7.3768e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.584604263305664: \n",
      "target probs tensor([[3.1719e-04],\n",
      "        [4.9516e-06],\n",
      "        [2.4100e-04],\n",
      "        [3.9141e-05],\n",
      "        [1.4206e-04],\n",
      "        [4.7746e-05],\n",
      "        [8.1621e-05],\n",
      "        [1.2500e-04],\n",
      "        [1.0100e-05],\n",
      "        [7.9149e-05],\n",
      "        [2.5505e-03],\n",
      "        [2.6333e-04],\n",
      "        [7.7164e-04],\n",
      "        [3.9929e-04],\n",
      "        [4.9632e-05],\n",
      "        [5.2089e-06]], device='cuda:0'), loss: 9.261863708496094: \n",
      "target probs tensor([[3.7416e-05],\n",
      "        [8.5238e-05],\n",
      "        [2.6078e-04],\n",
      "        [1.3303e-03],\n",
      "        [7.5296e-05],\n",
      "        [3.9285e-04],\n",
      "        [1.4162e-03],\n",
      "        [4.6699e-04],\n",
      "        [9.4082e-05],\n",
      "        [6.4203e-05],\n",
      "        [1.3140e-04],\n",
      "        [2.0732e-04],\n",
      "        [1.7401e-03],\n",
      "        [6.0642e-04],\n",
      "        [2.6479e-05],\n",
      "        [2.2524e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.44033432006836: \n",
      "target probs tensor([[1.3725e-05],\n",
      "        [1.7551e-05],\n",
      "        [2.5024e-04],\n",
      "        [9.2639e-05],\n",
      "        [3.1635e-05],\n",
      "        [3.7986e-05],\n",
      "        [1.4487e-04],\n",
      "        [1.3298e-04],\n",
      "        [3.3711e-05],\n",
      "        [2.8984e-05],\n",
      "        [5.1275e-05],\n",
      "        [6.5418e-05],\n",
      "        [1.4025e-04],\n",
      "        [2.4567e-04],\n",
      "        [2.4261e-04],\n",
      "        [3.9561e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.477079391479492: \n",
      "target probs tensor([[3.9276e-05],\n",
      "        [3.6838e-04],\n",
      "        [1.7274e-04],\n",
      "        [7.1481e-03],\n",
      "        [4.9772e-04],\n",
      "        [2.2818e-06],\n",
      "        [2.7803e-06],\n",
      "        [5.5596e-04],\n",
      "        [1.0179e-03],\n",
      "        [1.5683e-04],\n",
      "        [6.2165e-05],\n",
      "        [8.6954e-06],\n",
      "        [1.4443e-04],\n",
      "        [2.4263e-05],\n",
      "        [2.2384e-06],\n",
      "        [3.6467e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.658065795898438: \n",
      "Better model found at epoch 2 with validation value: 0.5540000200271606.\n",
      "target probs tensor([[1.7914e-04],\n",
      "        [7.8016e-04],\n",
      "        [1.1524e-03],\n",
      "        [1.3003e-04],\n",
      "        [1.1611e-03],\n",
      "        [7.6140e-03],\n",
      "        [3.3009e-04],\n",
      "        [3.8352e-05],\n",
      "        [3.3320e-04],\n",
      "        [7.2362e-04],\n",
      "        [2.2625e-04],\n",
      "        [2.9883e-04],\n",
      "        [2.5574e-04],\n",
      "        [2.0534e-04],\n",
      "        [3.3449e-04],\n",
      "        [1.4800e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.771646499633789: \n",
      "target probs tensor([[3.2318e-04],\n",
      "        [2.5690e-04],\n",
      "        [1.2840e-03],\n",
      "        [7.9225e-04],\n",
      "        [5.7709e-04],\n",
      "        [7.0124e-05],\n",
      "        [2.2591e-04],\n",
      "        [4.2528e-06],\n",
      "        [2.2622e-04],\n",
      "        [2.5007e-04],\n",
      "        [2.9094e-04],\n",
      "        [1.9444e-04],\n",
      "        [6.6008e-05],\n",
      "        [8.2013e-05],\n",
      "        [1.1735e-04],\n",
      "        [1.0362e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.657732963562012: \n",
      "target probs tensor([[4.0232e-05],\n",
      "        [3.4214e-04],\n",
      "        [1.1960e-04],\n",
      "        [2.0473e-04],\n",
      "        [2.5099e-05],\n",
      "        [4.5200e-04],\n",
      "        [4.9504e-05],\n",
      "        [1.3381e-04],\n",
      "        [2.6349e-05],\n",
      "        [1.6304e-04],\n",
      "        [1.2232e-05],\n",
      "        [1.1400e-04],\n",
      "        [3.3877e-04],\n",
      "        [1.0467e-04],\n",
      "        [4.0192e-05],\n",
      "        [3.5024e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.371625900268555: \n",
      "Better model found at epoch 3 with validation value: 0.5889999866485596.\n",
      "target probs tensor([[3.5332e-05],\n",
      "        [2.5880e-05],\n",
      "        [7.2278e-04],\n",
      "        [2.4686e-04],\n",
      "        [1.7636e-05],\n",
      "        [5.5363e-04],\n",
      "        [1.8409e-04],\n",
      "        [6.5311e-03],\n",
      "        [3.1884e-04],\n",
      "        [2.0834e-04],\n",
      "        [6.7024e-04],\n",
      "        [2.1430e-05],\n",
      "        [2.1172e-04],\n",
      "        [1.3222e-03],\n",
      "        [6.0811e-03],\n",
      "        [2.2902e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.224150657653809: \n",
      "target probs tensor([[5.7318e-04],\n",
      "        [9.4389e-05],\n",
      "        [3.0619e-04],\n",
      "        [1.6238e-04],\n",
      "        [2.7943e-03],\n",
      "        [9.0415e-04],\n",
      "        [4.3518e-04],\n",
      "        [1.0239e-04],\n",
      "        [2.7884e-04],\n",
      "        [6.6872e-04],\n",
      "        [2.7795e-04],\n",
      "        [1.1454e-04],\n",
      "        [4.1911e-04],\n",
      "        [1.2422e-05],\n",
      "        [2.5187e-04],\n",
      "        [5.1973e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.33417797088623: \n",
      "target probs tensor([[6.3217e-05],\n",
      "        [1.0523e-04],\n",
      "        [1.5560e-04],\n",
      "        [1.1805e-05],\n",
      "        [9.8201e-05],\n",
      "        [3.5573e-04],\n",
      "        [1.5807e-04],\n",
      "        [5.5747e-06],\n",
      "        [4.5195e-03],\n",
      "        [3.6614e-04],\n",
      "        [9.8695e-05],\n",
      "        [5.5403e-03],\n",
      "        [1.7899e-03],\n",
      "        [1.2909e-03],\n",
      "        [3.3323e-04],\n",
      "        [2.4685e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.518006324768066: \n",
      "Better model found at epoch 4 with validation value: 0.5920000076293945.\n",
      "target probs tensor([[2.5050e-04],\n",
      "        [6.0412e-05],\n",
      "        [1.7316e-04],\n",
      "        [1.0571e-04],\n",
      "        [4.2939e-05],\n",
      "        [8.9228e-04],\n",
      "        [1.1641e-03],\n",
      "        [4.2667e-04],\n",
      "        [1.2364e-04],\n",
      "        [3.6641e-04],\n",
      "        [7.1293e-05],\n",
      "        [1.2686e-06],\n",
      "        [3.8433e-04],\n",
      "        [2.4427e-05],\n",
      "        [3.3925e-06],\n",
      "        [2.6780e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.459973335266113: \n",
      "target probs tensor([[2.1567e-04],\n",
      "        [2.2202e-04],\n",
      "        [2.6026e-04],\n",
      "        [8.0837e-04],\n",
      "        [6.7315e-05],\n",
      "        [9.7220e-06],\n",
      "        [5.3733e-05],\n",
      "        [4.8994e-04],\n",
      "        [9.5743e-04],\n",
      "        [1.9734e-04],\n",
      "        [1.2654e-04],\n",
      "        [1.7694e-05],\n",
      "        [8.8632e-04],\n",
      "        [8.7407e-06],\n",
      "        [3.1142e-04],\n",
      "        [1.0676e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.73878002166748: \n",
      "target probs tensor([[5.5991e-05],\n",
      "        [1.5128e-03],\n",
      "        [1.6179e-04],\n",
      "        [5.4596e-04],\n",
      "        [1.3127e-04],\n",
      "        [8.0083e-05],\n",
      "        [1.0671e-03],\n",
      "        [1.7157e-04],\n",
      "        [5.4623e-04],\n",
      "        [3.2201e-04],\n",
      "        [2.4212e-04],\n",
      "        [2.8260e-04],\n",
      "        [2.4649e-04],\n",
      "        [2.7562e-04],\n",
      "        [1.9175e-03],\n",
      "        [3.3756e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.076033592224121: \n",
      "Better model found at epoch 5 with validation value: 0.6050000190734863.\n",
      "target probs tensor([[3.5028e-04],\n",
      "        [1.5923e-07],\n",
      "        [1.4149e-04],\n",
      "        [2.0111e-04],\n",
      "        [5.9469e-04],\n",
      "        [5.0317e-06],\n",
      "        [2.7259e-04],\n",
      "        [7.7705e-04],\n",
      "        [1.8382e-04],\n",
      "        [1.1191e-03],\n",
      "        [2.6989e-05],\n",
      "        [4.1423e-04],\n",
      "        [1.7420e-05],\n",
      "        [2.5059e-07],\n",
      "        [1.5364e-04],\n",
      "        [1.8929e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.430756568908691: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[4.9988e-04],\n",
      "        [1.1407e-04],\n",
      "        [2.9888e-05],\n",
      "        [1.6002e-04],\n",
      "        [1.8742e-04],\n",
      "        [1.6340e-04],\n",
      "        [6.9487e-04],\n",
      "        [6.1032e-04],\n",
      "        [3.4898e-04],\n",
      "        [3.4842e-04],\n",
      "        [2.5171e-05],\n",
      "        [1.2666e-03],\n",
      "        [5.8777e-04],\n",
      "        [2.1901e-04],\n",
      "        [3.1256e-04],\n",
      "        [8.0894e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.109380722045898: \n",
      "target probs tensor([[1.8143e-04],\n",
      "        [2.7316e-05],\n",
      "        [1.9847e-04],\n",
      "        [5.5304e-04],\n",
      "        [1.3042e-04],\n",
      "        [3.0649e-04],\n",
      "        [1.7341e-04],\n",
      "        [8.2402e-04],\n",
      "        [2.4745e-05],\n",
      "        [1.2924e-04],\n",
      "        [5.8590e-04],\n",
      "        [6.2948e-04],\n",
      "        [2.3817e-05],\n",
      "        [6.3646e-04],\n",
      "        [2.3451e-04],\n",
      "        [1.5146e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.448308944702148: \n",
      "Better model found at epoch 6 with validation value: 0.6079999804496765.\n",
      "target probs tensor([[1.4653e-04],\n",
      "        [7.2003e-05],\n",
      "        [3.7622e-04],\n",
      "        [2.2628e-05],\n",
      "        [1.1464e-04],\n",
      "        [3.2355e-04],\n",
      "        [1.5060e-02],\n",
      "        [6.9676e-05],\n",
      "        [5.5953e-05],\n",
      "        [3.0245e-04],\n",
      "        [1.0207e-03],\n",
      "        [3.5330e-05],\n",
      "        [2.4542e-05],\n",
      "        [3.1123e-05],\n",
      "        [4.2651e-04],\n",
      "        [1.7895e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.764970779418945: \n",
      "target probs tensor([[3.8062e-05],\n",
      "        [1.8648e-04],\n",
      "        [6.6809e-05],\n",
      "        [5.4044e-04],\n",
      "        [4.4178e-04],\n",
      "        [5.9881e-04],\n",
      "        [1.8744e-04],\n",
      "        [1.4896e-04],\n",
      "        [2.7207e-04],\n",
      "        [2.3710e-04],\n",
      "        [4.6474e-04],\n",
      "        [3.1117e-04],\n",
      "        [2.9991e-05],\n",
      "        [4.0496e-04],\n",
      "        [1.6160e-04],\n",
      "        [1.1475e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.548412322998047: \n",
      "target probs tensor([[3.3697e-04],\n",
      "        [1.1953e-04],\n",
      "        [9.3483e-04],\n",
      "        [6.5441e-03],\n",
      "        [4.6727e-04],\n",
      "        [2.0504e-04],\n",
      "        [9.3326e-05],\n",
      "        [1.2221e-04],\n",
      "        [8.0040e-03],\n",
      "        [8.9397e-04],\n",
      "        [7.2668e-05],\n",
      "        [3.0994e-03],\n",
      "        [9.5550e-05],\n",
      "        [5.6928e-05],\n",
      "        [3.4018e-05],\n",
      "        [1.8069e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.035785675048828: \n",
      "target probs tensor([[2.0433e-03],\n",
      "        [1.6804e-03],\n",
      "        [5.9497e-04],\n",
      "        [2.0485e-04],\n",
      "        [8.4806e-05],\n",
      "        [2.4265e-04],\n",
      "        [1.5960e-03],\n",
      "        [1.9835e-04]], device='cuda:0'), loss: 7.645858287811279: \n",
      "target probs tensor([[5.7390e-05],\n",
      "        [2.1864e-04],\n",
      "        [3.1950e-04],\n",
      "        [2.8066e-04],\n",
      "        [1.9873e-04],\n",
      "        [2.3587e-04],\n",
      "        [7.8214e-05],\n",
      "        [2.3676e-04],\n",
      "        [3.7859e-04],\n",
      "        [1.0964e-03],\n",
      "        [2.1510e-04],\n",
      "        [1.5333e-04],\n",
      "        [3.1113e-04],\n",
      "        [5.5279e-04],\n",
      "        [1.7587e-04],\n",
      "        [1.6534e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.372024536132812: \n",
      "target probs tensor([[5.8053e-06],\n",
      "        [4.6795e-05],\n",
      "        [7.1201e-04],\n",
      "        [6.4124e-04],\n",
      "        [2.1728e-04],\n",
      "        [5.1491e-05],\n",
      "        [1.8740e-05],\n",
      "        [5.2417e-04],\n",
      "        [1.5787e-03],\n",
      "        [4.6101e-04],\n",
      "        [2.7687e-05],\n",
      "        [2.5361e-04],\n",
      "        [3.2945e-04],\n",
      "        [1.6382e-04],\n",
      "        [3.4950e-05],\n",
      "        [2.3772e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.851325988769531: \n",
      "target probs tensor([[2.5300e-04],\n",
      "        [9.3587e-04],\n",
      "        [1.9791e-04],\n",
      "        [1.9519e-04],\n",
      "        [6.2987e-04],\n",
      "        [1.3015e-05],\n",
      "        [3.7596e-04],\n",
      "        [3.2231e-04],\n",
      "        [1.3679e-03],\n",
      "        [4.3969e-04],\n",
      "        [1.6558e-04],\n",
      "        [4.2944e-04],\n",
      "        [2.0042e-03],\n",
      "        [7.0643e-05],\n",
      "        [3.3025e-04],\n",
      "        [4.9058e-05]], device='cuda:0'), loss: 8.2101469039917: \n",
      "Better model found at epoch 8 with validation value: 0.6230000257492065.\n",
      "target probs tensor([[2.7325e-04],\n",
      "        [6.7066e-04],\n",
      "        [3.2581e-05],\n",
      "        [3.8618e-04],\n",
      "        [7.6990e-04],\n",
      "        [5.8046e-04],\n",
      "        [1.6661e-04],\n",
      "        [8.7831e-04],\n",
      "        [9.2640e-05],\n",
      "        [1.2998e-04],\n",
      "        [3.3763e-04],\n",
      "        [4.2520e-05],\n",
      "        [1.2752e-04],\n",
      "        [5.4763e-06],\n",
      "        [2.4609e-04],\n",
      "        [1.4013e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.519882202148438: \n",
      "target probs tensor([[2.7226e-04],\n",
      "        [2.0363e-04],\n",
      "        [3.9898e-04],\n",
      "        [6.4185e-04],\n",
      "        [6.3444e-05],\n",
      "        [1.3967e-04],\n",
      "        [3.0597e-04],\n",
      "        [2.1574e-02],\n",
      "        [6.7366e-04],\n",
      "        [1.2904e-03],\n",
      "        [5.9983e-04],\n",
      "        [6.3855e-04],\n",
      "        [8.0403e-04],\n",
      "        [4.2784e-05],\n",
      "        [2.8810e-04],\n",
      "        [3.8671e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.911507606506348: \n",
      "target probs tensor([[2.9936e-04],\n",
      "        [3.4255e-04],\n",
      "        [6.6168e-05],\n",
      "        [1.5263e-04],\n",
      "        [1.9532e-04],\n",
      "        [2.3522e-04],\n",
      "        [3.3577e-04],\n",
      "        [1.3433e-04],\n",
      "        [7.6153e-07],\n",
      "        [3.6336e-04],\n",
      "        [5.4639e-05],\n",
      "        [7.2949e-04],\n",
      "        [2.0656e-02],\n",
      "        [1.2145e-04],\n",
      "        [3.0003e-05],\n",
      "        [3.4753e-04]], device='cuda:0'), loss: 8.66464614868164: \n",
      "target probs tensor([[3.6782e-04],\n",
      "        [9.6491e-07],\n",
      "        [6.3336e-04],\n",
      "        [8.5687e-05],\n",
      "        [1.5139e-04],\n",
      "        [4.6430e-04],\n",
      "        [6.6727e-05],\n",
      "        [4.1302e-05],\n",
      "        [6.1988e-04],\n",
      "        [1.8690e-03],\n",
      "        [3.9971e-04],\n",
      "        [7.9274e-05],\n",
      "        [1.1252e-04],\n",
      "        [7.0486e-04],\n",
      "        [7.8956e-05],\n",
      "        [4.5546e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.693429946899414: \n",
      "target probs tensor([[2.6834e-04],\n",
      "        [1.3915e-04],\n",
      "        [1.8105e-04],\n",
      "        [1.3391e-03],\n",
      "        [1.4022e-02],\n",
      "        [1.1823e-04],\n",
      "        [1.0731e-04],\n",
      "        [1.1679e-03],\n",
      "        [3.5261e-06],\n",
      "        [6.5635e-05],\n",
      "        [2.5350e-04],\n",
      "        [1.3757e-04],\n",
      "        [8.1461e-04],\n",
      "        [1.8665e-04],\n",
      "        [1.7035e-02],\n",
      "        [4.1260e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.028804779052734: \n",
      "target probs tensor([[4.8424e-04],\n",
      "        [3.7730e-04],\n",
      "        [2.7951e-04],\n",
      "        [1.1077e-04],\n",
      "        [1.1976e-03],\n",
      "        [1.0572e-04],\n",
      "        [9.4320e-05],\n",
      "        [1.0641e-04],\n",
      "        [3.4140e-05],\n",
      "        [2.7125e-05],\n",
      "        [6.6726e-04],\n",
      "        [2.2694e-05],\n",
      "        [9.3517e-05],\n",
      "        [1.5925e-04],\n",
      "        [2.0912e-04],\n",
      "        [1.8580e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.812299728393555: \n",
      "Better model found at epoch 10 with validation value: 0.6430000066757202.\n",
      "target probs tensor([[1.3931e-04],\n",
      "        [5.9785e-04],\n",
      "        [1.6554e-04],\n",
      "        [2.7045e-04],\n",
      "        [7.1369e-06],\n",
      "        [1.1430e-03],\n",
      "        [2.7666e-04],\n",
      "        [4.9726e-03],\n",
      "        [1.1093e-04],\n",
      "        [1.5492e-03],\n",
      "        [2.2342e-04],\n",
      "        [6.8864e-05],\n",
      "        [2.6455e-04],\n",
      "        [2.8808e-05],\n",
      "        [7.4269e-04],\n",
      "        [1.3106e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.359213829040527: \n",
      "target probs tensor([[1.1931e-03],\n",
      "        [6.7984e-04],\n",
      "        [1.2015e-04],\n",
      "        [1.9727e-02],\n",
      "        [4.9696e-04],\n",
      "        [1.0526e-05],\n",
      "        [9.5664e-05],\n",
      "        [6.9722e-04],\n",
      "        [3.7782e-05],\n",
      "        [1.0037e-04],\n",
      "        [2.8651e-03],\n",
      "        [1.0318e-04],\n",
      "        [8.3425e-04],\n",
      "        [3.3015e-04],\n",
      "        [3.0822e-03],\n",
      "        [1.8867e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.047422409057617: \n",
      "target probs tensor([[5.0025e-04],\n",
      "        [1.0180e-03],\n",
      "        [4.4527e-05],\n",
      "        [1.1039e-04],\n",
      "        [1.8202e-04],\n",
      "        [1.2557e-04],\n",
      "        [5.7579e-07],\n",
      "        [1.6375e-04],\n",
      "        [1.2009e-03],\n",
      "        [6.6801e-05],\n",
      "        [4.3665e-04],\n",
      "        [1.4046e-03],\n",
      "        [5.2444e-05],\n",
      "        [5.2798e-04],\n",
      "        [4.5249e-04],\n",
      "        [3.5663e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.767939567565918: \n",
      "target probs tensor([[2.9860e-04],\n",
      "        [1.0751e-04],\n",
      "        [1.8419e-04],\n",
      "        [2.7045e-03],\n",
      "        [2.6623e-04],\n",
      "        [2.0251e-04],\n",
      "        [4.0281e-04],\n",
      "        [6.8423e-04],\n",
      "        [5.0050e-05],\n",
      "        [2.2421e-05],\n",
      "        [9.7103e-04],\n",
      "        [1.5029e-02],\n",
      "        [8.3032e-04],\n",
      "        [5.4808e-05],\n",
      "        [6.3023e-04],\n",
      "        [6.5299e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.078821182250977: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[1.2114e-04],\n",
      "        [8.6481e-04],\n",
      "        [4.0294e-04],\n",
      "        [4.9330e-05],\n",
      "        [4.9274e-04],\n",
      "        [1.6925e-04],\n",
      "        [1.8098e-04],\n",
      "        [2.3832e-04],\n",
      "        [2.6931e-05],\n",
      "        [1.1657e-04],\n",
      "        [8.5698e-05],\n",
      "        [2.0465e-04],\n",
      "        [1.1360e-04],\n",
      "        [6.9868e-04],\n",
      "        [1.1634e-04],\n",
      "        [1.0126e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.550342559814453: \n",
      "target probs tensor([[2.0037e-04],\n",
      "        [3.3792e-04],\n",
      "        [4.4393e-05],\n",
      "        [1.3009e-04],\n",
      "        [4.6130e-04],\n",
      "        [1.7268e-03],\n",
      "        [2.3357e-04],\n",
      "        [3.2024e-04],\n",
      "        [3.5274e-04],\n",
      "        [3.4732e-04],\n",
      "        [3.3738e-04],\n",
      "        [2.7601e-03],\n",
      "        [1.4703e-04],\n",
      "        [2.5094e-04],\n",
      "        [1.2053e-04],\n",
      "        [2.4948e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.135369300842285: \n",
      "Better model found at epoch 12 with validation value: 0.6510000228881836.\n",
      "target probs tensor([[2.1714e-04],\n",
      "        [4.6081e-04],\n",
      "        [7.6602e-05],\n",
      "        [1.2366e-04],\n",
      "        [1.0047e-03],\n",
      "        [9.0484e-05],\n",
      "        [6.2624e-05],\n",
      "        [4.9906e-04],\n",
      "        [2.2737e-04],\n",
      "        [1.1345e-03],\n",
      "        [2.0743e-04],\n",
      "        [7.7762e-07],\n",
      "        [4.8677e-03],\n",
      "        [9.6935e-05],\n",
      "        [6.7475e-04],\n",
      "        [4.0774e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.467351913452148: \n",
      "target probs tensor([[1.1455e-04],\n",
      "        [6.9827e-04],\n",
      "        [5.1993e-04],\n",
      "        [2.6526e-04],\n",
      "        [5.5607e-05],\n",
      "        [1.1087e-04],\n",
      "        [1.5755e-04],\n",
      "        [1.1320e-04],\n",
      "        [4.0475e-05],\n",
      "        [2.2780e-04],\n",
      "        [1.5383e-02],\n",
      "        [3.1350e-04],\n",
      "        [2.3910e-04],\n",
      "        [3.2858e-04],\n",
      "        [6.7133e-04],\n",
      "        [1.7588e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.390158653259277: \n",
      "target probs tensor([[4.2720e-04],\n",
      "        [5.3765e-04],\n",
      "        [2.2989e-05],\n",
      "        [1.1979e-03],\n",
      "        [1.7581e-05],\n",
      "        [3.6427e-04],\n",
      "        [5.9727e-04],\n",
      "        [3.5466e-07],\n",
      "        [2.0665e-04],\n",
      "        [1.2895e-04],\n",
      "        [1.2307e-04],\n",
      "        [3.4231e-04],\n",
      "        [1.8970e-04],\n",
      "        [1.1520e-04],\n",
      "        [2.9812e-04],\n",
      "        [5.6519e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.987281799316406: \n",
      "target probs tensor([[2.3849e-04],\n",
      "        [2.9142e-05],\n",
      "        [2.3097e-04],\n",
      "        [4.7587e-04],\n",
      "        [4.0307e-04],\n",
      "        [8.5797e-04],\n",
      "        [8.1306e-05],\n",
      "        [2.0742e-02],\n",
      "        [5.0129e-04],\n",
      "        [4.8703e-04],\n",
      "        [4.2521e-04],\n",
      "        [5.3939e-05],\n",
      "        [2.3995e-05],\n",
      "        [2.1659e-04],\n",
      "        [1.8707e-05],\n",
      "        [2.9829e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.367103576660156: \n",
      "target probs tensor([[6.5701e-04],\n",
      "        [2.3558e-04],\n",
      "        [1.6427e-03],\n",
      "        [3.4509e-04],\n",
      "        [1.9136e-03],\n",
      "        [8.5825e-05],\n",
      "        [4.4807e-05],\n",
      "        [1.4992e-03],\n",
      "        [4.0994e-04],\n",
      "        [1.6729e-04],\n",
      "        [5.0679e-05],\n",
      "        [4.6264e-04],\n",
      "        [4.9470e-04],\n",
      "        [3.6970e-03],\n",
      "        [1.2515e-03],\n",
      "        [1.3254e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.818121433258057: \n",
      "target probs tensor([[2.4799e-03],\n",
      "        [1.7761e-04],\n",
      "        [5.5193e-04],\n",
      "        [5.0141e-04],\n",
      "        [1.5496e-04],\n",
      "        [3.1651e-04],\n",
      "        [1.3467e-04],\n",
      "        [8.1564e-04],\n",
      "        [1.6818e-03],\n",
      "        [1.4071e-04],\n",
      "        [9.8646e-03],\n",
      "        [2.9995e-05],\n",
      "        [2.4734e-04],\n",
      "        [4.7837e-05],\n",
      "        [3.2405e-05],\n",
      "        [1.0637e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.163662910461426: \n",
      "target probs tensor([[4.5429e-06],\n",
      "        [8.2544e-05],\n",
      "        [1.7520e-04],\n",
      "        [1.3326e-06],\n",
      "        [1.1626e-04],\n",
      "        [8.1258e-04],\n",
      "        [3.3946e-04],\n",
      "        [1.2037e-04],\n",
      "        [1.1847e-05],\n",
      "        [9.8335e-05],\n",
      "        [4.8262e-04],\n",
      "        [1.3927e-04],\n",
      "        [2.0022e-04],\n",
      "        [7.0299e-04],\n",
      "        [4.2490e-04],\n",
      "        [3.7838e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.098468780517578: \n",
      "target probs tensor([[2.4331e-04],\n",
      "        [4.7750e-04],\n",
      "        [7.3440e-04],\n",
      "        [6.9430e-04],\n",
      "        [5.7215e-04],\n",
      "        [1.5558e-04],\n",
      "        [1.6246e-04],\n",
      "        [5.1839e-05],\n",
      "        [2.1312e-04],\n",
      "        [2.8022e-03],\n",
      "        [1.5737e-04],\n",
      "        [1.0960e-04],\n",
      "        [4.5486e-05],\n",
      "        [2.7274e-04],\n",
      "        [1.4870e-04],\n",
      "        [6.3730e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.241722106933594: \n",
      "target probs tensor([[5.8708e-04],\n",
      "        [1.3925e-04],\n",
      "        [4.7705e-04],\n",
      "        [6.2529e-04],\n",
      "        [2.6244e-04],\n",
      "        [3.4411e-04],\n",
      "        [3.1262e-04],\n",
      "        [2.5575e-04],\n",
      "        [2.0794e-03],\n",
      "        [6.4051e-05],\n",
      "        [4.2942e-04],\n",
      "        [7.4076e-04],\n",
      "        [1.1155e-04],\n",
      "        [9.1006e-05],\n",
      "        [8.2942e-05],\n",
      "        [7.2090e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.108563423156738: \n",
      "target probs tensor([[0.0007],\n",
      "        [0.0006],\n",
      "        [0.0008],\n",
      "        [0.0004],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0048],\n",
      "        [0.0002]], device='cuda:0'), loss: 7.562021732330322: \n",
      "target probs tensor([[1.9150e-04],\n",
      "        [1.4620e-03],\n",
      "        [2.5719e-03],\n",
      "        [2.1442e-05],\n",
      "        [3.3122e-04],\n",
      "        [1.8029e-04],\n",
      "        [4.4105e-04],\n",
      "        [3.7466e-03],\n",
      "        [2.4373e-04],\n",
      "        [1.2907e-04],\n",
      "        [4.0045e-04],\n",
      "        [2.6905e-04],\n",
      "        [1.3672e-03],\n",
      "        [1.7466e-05],\n",
      "        [4.9787e-04],\n",
      "        [8.1582e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.102266311645508: \n",
      "target probs tensor([[1.2453e-04],\n",
      "        [1.0223e-04],\n",
      "        [2.0625e-04],\n",
      "        [1.2489e-03],\n",
      "        [1.7480e-05],\n",
      "        [2.3474e-03],\n",
      "        [1.8129e-05],\n",
      "        [4.3911e-04],\n",
      "        [2.4889e-04],\n",
      "        [1.1263e-05],\n",
      "        [1.1436e-04],\n",
      "        [6.4625e-04],\n",
      "        [1.1307e-04],\n",
      "        [4.3513e-06],\n",
      "        [1.5028e-04],\n",
      "        [4.5716e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.084397315979004: \n",
      "target probs tensor([[3.2707e-04],\n",
      "        [6.1144e-04],\n",
      "        [1.3245e-04],\n",
      "        [2.6144e-04],\n",
      "        [1.6861e-04],\n",
      "        [5.2630e-05],\n",
      "        [4.4717e-04],\n",
      "        [1.9432e-04],\n",
      "        [1.1424e-04],\n",
      "        [8.3001e-04],\n",
      "        [3.0146e-04],\n",
      "        [5.7550e-04],\n",
      "        [5.8459e-04],\n",
      "        [8.6406e-05],\n",
      "        [1.2735e-03],\n",
      "        [4.8939e-05]], device='cuda:0'), loss: 8.283295631408691: \n",
      "Better model found at epoch 16 with validation value: 0.652999997138977.\n",
      "target probs tensor([[2.8467e-04],\n",
      "        [3.2593e-04],\n",
      "        [3.4314e-08],\n",
      "        [3.0947e-05],\n",
      "        [3.5627e-04],\n",
      "        [1.7238e-04],\n",
      "        [1.3238e-04],\n",
      "        [6.4017e-04],\n",
      "        [3.8592e-04],\n",
      "        [2.2754e-04],\n",
      "        [1.3380e-03],\n",
      "        [3.5242e-03],\n",
      "        [1.9643e-06],\n",
      "        [2.5842e-04],\n",
      "        [3.7675e-04],\n",
      "        [4.2148e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.888744354248047: \n",
      "target probs tensor([[6.8127e-05],\n",
      "        [1.3210e-03],\n",
      "        [1.4195e-03],\n",
      "        [1.5322e-04],\n",
      "        [6.0888e-04],\n",
      "        [9.7006e-04],\n",
      "        [1.1406e-03],\n",
      "        [7.0795e-04],\n",
      "        [4.4894e-05],\n",
      "        [6.5076e-04],\n",
      "        [5.5351e-04],\n",
      "        [8.9937e-05],\n",
      "        [1.4220e-04],\n",
      "        [2.6935e-05],\n",
      "        [2.8945e-04],\n",
      "        [1.2322e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.164351463317871: \n",
      "target probs tensor([[1.6551e-04],\n",
      "        [1.4548e-04],\n",
      "        [3.4113e-05],\n",
      "        [1.3731e-04],\n",
      "        [7.4003e-04],\n",
      "        [6.5832e-04],\n",
      "        [4.2765e-04],\n",
      "        [2.2887e-03],\n",
      "        [2.8046e-06],\n",
      "        [4.5641e-04],\n",
      "        [4.7758e-04],\n",
      "        [3.2208e-04],\n",
      "        [4.3373e-04],\n",
      "        [8.2858e-05],\n",
      "        [3.6495e-04],\n",
      "        [1.7136e-04]], device='cuda:0'), loss: 8.436589241027832: \n",
      "Better model found at epoch 17 with validation value: 0.6549999713897705.\n",
      "target probs tensor([[2.6433e-04],\n",
      "        [2.3130e-04],\n",
      "        [9.1555e-06],\n",
      "        [2.5864e-04],\n",
      "        [1.3978e-04],\n",
      "        [3.6796e-04],\n",
      "        [5.8732e-04],\n",
      "        [1.2634e-04],\n",
      "        [1.4569e-04],\n",
      "        [2.0374e-02],\n",
      "        [1.3544e-04],\n",
      "        [3.7173e-06],\n",
      "        [3.1215e-04],\n",
      "        [1.3600e-04],\n",
      "        [2.9417e-04],\n",
      "        [6.9567e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.511534690856934: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[6.4430e-04],\n",
      "        [2.1510e-04],\n",
      "        [2.4578e-04],\n",
      "        [3.7233e-04],\n",
      "        [3.3952e-04],\n",
      "        [3.7319e-05],\n",
      "        [5.3551e-04],\n",
      "        [2.9739e-04],\n",
      "        [7.2232e-04],\n",
      "        [6.3756e-06],\n",
      "        [1.5232e-03],\n",
      "        [3.8870e-03],\n",
      "        [2.6175e-04],\n",
      "        [2.4320e-03],\n",
      "        [8.9013e-05],\n",
      "        [2.1725e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.924638748168945: \n",
      "target probs tensor([[2.1384e-06],\n",
      "        [4.4009e-04],\n",
      "        [2.2852e-04],\n",
      "        [2.0938e-04],\n",
      "        [1.6723e-02],\n",
      "        [4.8820e-06],\n",
      "        [8.5250e-05],\n",
      "        [1.4945e-04],\n",
      "        [3.5075e-05],\n",
      "        [7.2777e-04],\n",
      "        [1.0281e-04],\n",
      "        [9.2739e-05],\n",
      "        [2.9066e-04],\n",
      "        [1.9412e-04],\n",
      "        [1.5533e-04],\n",
      "        [4.2471e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.120010375976562: \n",
      "Better model found at epoch 18 with validation value: 0.6620000004768372.\n",
      "target probs tensor([[4.2789e-04],\n",
      "        [4.2054e-05],\n",
      "        [1.3582e-02],\n",
      "        [2.7036e-04],\n",
      "        [9.1479e-05],\n",
      "        [7.3100e-04],\n",
      "        [1.8057e-06],\n",
      "        [5.3108e-03],\n",
      "        [5.1466e-04],\n",
      "        [2.4723e-05],\n",
      "        [4.5272e-04],\n",
      "        [4.1312e-04],\n",
      "        [5.6847e-04],\n",
      "        [5.2921e-04],\n",
      "        [8.6200e-05],\n",
      "        [2.3951e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.088287353515625: \n",
      "target probs tensor([[9.6754e-05],\n",
      "        [8.9473e-04],\n",
      "        [3.4692e-04],\n",
      "        [3.2384e-04],\n",
      "        [3.6302e-04],\n",
      "        [1.6036e-04],\n",
      "        [4.2829e-04],\n",
      "        [1.8298e-04],\n",
      "        [5.5496e-04],\n",
      "        [2.3215e-04],\n",
      "        [1.1202e-04],\n",
      "        [3.2144e-04],\n",
      "        [1.1402e-05],\n",
      "        [7.7959e-04],\n",
      "        [5.9160e-04],\n",
      "        [1.5164e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.171991348266602: \n",
      "target probs tensor([[3.6302e-04],\n",
      "        [3.6187e-05],\n",
      "        [1.8955e-04],\n",
      "        [3.5206e-04],\n",
      "        [1.0522e-03],\n",
      "        [1.2570e-04],\n",
      "        [1.8784e-04],\n",
      "        [1.7122e-04],\n",
      "        [2.8499e-04],\n",
      "        [2.1033e-04],\n",
      "        [1.0831e-03],\n",
      "        [2.2503e-05],\n",
      "        [4.0896e-04],\n",
      "        [2.0687e-04],\n",
      "        [3.6504e-04],\n",
      "        [3.8656e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.373748779296875: \n",
      "target probs tensor([[2.3285e-04],\n",
      "        [1.2814e-04],\n",
      "        [4.1182e-04],\n",
      "        [4.1957e-05],\n",
      "        [3.9772e-03],\n",
      "        [2.3714e-04],\n",
      "        [3.4879e-03],\n",
      "        [9.6841e-04],\n",
      "        [6.1647e-06],\n",
      "        [1.6837e-04],\n",
      "        [1.6025e-05],\n",
      "        [4.5002e-04],\n",
      "        [1.9068e-04],\n",
      "        [4.5701e-04],\n",
      "        [5.4908e-04],\n",
      "        [3.3339e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.304779052734375: \n",
      "target probs tensor([[6.1638e-04],\n",
      "        [8.8333e-04],\n",
      "        [3.4035e-04],\n",
      "        [3.3849e-05],\n",
      "        [5.4087e-04],\n",
      "        [4.6769e-05],\n",
      "        [8.0825e-04],\n",
      "        [6.7053e-04],\n",
      "        [2.1022e-03],\n",
      "        [2.8403e-04],\n",
      "        [5.0102e-04],\n",
      "        [3.8824e-04],\n",
      "        [2.4426e-02],\n",
      "        [8.3435e-03],\n",
      "        [1.2756e-04],\n",
      "        [1.6503e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.680278301239014: \n",
      "target probs tensor([[1.0561e-03],\n",
      "        [7.8864e-05],\n",
      "        [3.7126e-04],\n",
      "        [5.6472e-04],\n",
      "        [1.3030e-04],\n",
      "        [4.4537e-04],\n",
      "        [1.0356e-03],\n",
      "        [1.4567e-05],\n",
      "        [2.5037e-04],\n",
      "        [6.1245e-04],\n",
      "        [2.7542e-04],\n",
      "        [1.2501e-04],\n",
      "        [1.1149e-04],\n",
      "        [3.9319e-04],\n",
      "        [3.1276e-04],\n",
      "        [5.1510e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.238100051879883: \n",
      "target probs tensor([[4.6720e-04],\n",
      "        [7.3383e-04],\n",
      "        [4.0775e-04],\n",
      "        [2.4265e-04],\n",
      "        [1.3996e-04],\n",
      "        [5.7713e-04],\n",
      "        [3.3192e-04],\n",
      "        [1.5999e-03],\n",
      "        [8.9469e-04],\n",
      "        [8.4551e-04],\n",
      "        [5.8520e-04],\n",
      "        [2.5301e-04],\n",
      "        [4.5730e-04],\n",
      "        [9.8586e-07],\n",
      "        [1.4143e-04],\n",
      "        [1.7460e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.165721893310547: \n",
      "target probs tensor([[2.4528e-06],\n",
      "        [9.8670e-04],\n",
      "        [4.2730e-04],\n",
      "        [3.0472e-04],\n",
      "        [5.5789e-05],\n",
      "        [2.6566e-03],\n",
      "        [1.2909e-02],\n",
      "        [1.4648e-04],\n",
      "        [7.4449e-03],\n",
      "        [2.3649e-04],\n",
      "        [7.0802e-04],\n",
      "        [6.8316e-04],\n",
      "        [7.4312e-05],\n",
      "        [1.3620e-03],\n",
      "        [1.0933e-03],\n",
      "        [8.5305e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.648723125457764: \n",
      "target probs tensor([[1.5720e-04],\n",
      "        [4.0704e-04],\n",
      "        [5.1686e-04],\n",
      "        [1.6369e-03],\n",
      "        [6.8350e-04],\n",
      "        [2.4663e-02],\n",
      "        [3.8437e-04],\n",
      "        [1.9745e-07],\n",
      "        [4.5471e-04],\n",
      "        [4.2434e-05],\n",
      "        [1.3435e-04],\n",
      "        [3.1608e-04],\n",
      "        [1.9717e-04],\n",
      "        [4.2508e-04],\n",
      "        [9.0096e-04],\n",
      "        [1.1616e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.102615356445312: \n",
      "target probs tensor([[1.1193e-04],\n",
      "        [7.8186e-05],\n",
      "        [1.4377e-04],\n",
      "        [1.9584e-05],\n",
      "        [8.4058e-05],\n",
      "        [1.1058e-04],\n",
      "        [2.1685e-03],\n",
      "        [1.7018e-03],\n",
      "        [1.0818e-04],\n",
      "        [5.0698e-04],\n",
      "        [3.4793e-04],\n",
      "        [6.4850e-04],\n",
      "        [3.6220e-05],\n",
      "        [2.8795e-04],\n",
      "        [3.5186e-03],\n",
      "        [9.0619e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.125053405761719: \n",
      "target probs tensor([[5.7962e-04],\n",
      "        [1.5221e-05],\n",
      "        [3.1262e-04],\n",
      "        [2.0838e-04],\n",
      "        [8.7428e-04],\n",
      "        [8.3278e-04],\n",
      "        [5.2304e-04],\n",
      "        [7.5181e-04],\n",
      "        [7.1730e-06],\n",
      "        [1.9270e-04],\n",
      "        [3.0332e-04],\n",
      "        [2.4738e-03],\n",
      "        [2.1677e-04],\n",
      "        [4.9306e-04],\n",
      "        [6.9353e-05],\n",
      "        [5.5656e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.65660572052002: \n",
      "target probs tensor([[2.2618e-03],\n",
      "        [1.3398e-04],\n",
      "        [1.9686e-04],\n",
      "        [4.2699e-04],\n",
      "        [6.5262e-04],\n",
      "        [2.3819e-04],\n",
      "        [1.7886e-04],\n",
      "        [6.3206e-04],\n",
      "        [7.0840e-05],\n",
      "        [1.2976e-04],\n",
      "        [4.6014e-05],\n",
      "        [1.4839e-03],\n",
      "        [1.5195e-04],\n",
      "        [1.9494e-04],\n",
      "        [4.6808e-04],\n",
      "        [3.0471e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.192245483398438: \n",
      "Better model found at epoch 22 with validation value: 0.6700000166893005.\n",
      "target probs tensor([[5.8936e-05],\n",
      "        [2.5596e-04],\n",
      "        [1.1152e-04],\n",
      "        [3.0496e-04],\n",
      "        [1.5797e-04],\n",
      "        [6.2432e-04],\n",
      "        [2.9902e-04],\n",
      "        [2.7020e-04],\n",
      "        [1.3327e-03],\n",
      "        [1.7240e-03],\n",
      "        [4.3990e-04],\n",
      "        [1.7009e-04],\n",
      "        [1.0415e-05],\n",
      "        [3.8274e-04],\n",
      "        [5.7230e-04],\n",
      "        [1.1319e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.165712356567383: \n",
      "target probs tensor([[3.8820e-04],\n",
      "        [5.7295e-03],\n",
      "        [4.9479e-04],\n",
      "        [2.0669e-05],\n",
      "        [1.1402e-02],\n",
      "        [1.7776e-03],\n",
      "        [1.9129e-04],\n",
      "        [2.3333e-03],\n",
      "        [2.8709e-04],\n",
      "        [2.3546e-05],\n",
      "        [2.0482e-04],\n",
      "        [8.9496e-04],\n",
      "        [6.4233e-04],\n",
      "        [1.2944e-04],\n",
      "        [4.8831e-04],\n",
      "        [6.1049e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.799900054931641: \n",
      "target probs tensor([[5.6661e-04],\n",
      "        [2.9023e-04],\n",
      "        [5.6430e-05],\n",
      "        [1.4073e-04],\n",
      "        [2.0619e-04],\n",
      "        [4.6513e-05],\n",
      "        [4.9032e-05],\n",
      "        [1.1432e-03],\n",
      "        [8.8437e-04],\n",
      "        [2.4948e-04],\n",
      "        [8.2563e-06],\n",
      "        [2.8880e-04],\n",
      "        [4.0738e-04],\n",
      "        [1.4828e-04],\n",
      "        [3.1441e-04],\n",
      "        [1.7419e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.622169494628906: \n",
      "target probs tensor([[3.9575e-04],\n",
      "        [8.4600e-05],\n",
      "        [2.0595e-03],\n",
      "        [2.9787e-03],\n",
      "        [3.3924e-03],\n",
      "        [9.1717e-04],\n",
      "        [9.5274e-04],\n",
      "        [2.5873e-03]], device='cuda:0'), loss: 6.850953578948975: \n",
      "target probs tensor([[1.4303e-04],\n",
      "        [5.6053e-04],\n",
      "        [3.3992e-04],\n",
      "        [3.4555e-04],\n",
      "        [2.4738e-04],\n",
      "        [5.2881e-05],\n",
      "        [8.8144e-04],\n",
      "        [3.4838e-04],\n",
      "        [8.9450e-03],\n",
      "        [2.7472e-03],\n",
      "        [2.6887e-05],\n",
      "        [2.1979e-04],\n",
      "        [5.1185e-04],\n",
      "        [1.3971e-04],\n",
      "        [1.4973e-04],\n",
      "        [5.9995e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.980235576629639: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[9.0490e-07],\n",
      "        [8.8807e-04],\n",
      "        [1.0619e-04],\n",
      "        [1.3240e-03],\n",
      "        [4.9347e-04],\n",
      "        [6.2965e-04],\n",
      "        [1.1274e-03],\n",
      "        [1.7867e-04],\n",
      "        [1.1353e-04],\n",
      "        [1.1265e-03],\n",
      "        [5.2160e-04],\n",
      "        [2.9444e-04],\n",
      "        [3.4312e-04],\n",
      "        [2.1092e-04],\n",
      "        [1.8045e-04],\n",
      "        [1.6084e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.136028289794922: \n",
      "target probs tensor([[7.5920e-04],\n",
      "        [1.2144e-03],\n",
      "        [1.9033e-05],\n",
      "        [2.2123e-04],\n",
      "        [2.4664e-02],\n",
      "        [6.0381e-05],\n",
      "        [3.2494e-03],\n",
      "        [7.2131e-05],\n",
      "        [7.8752e-04],\n",
      "        [2.4658e-04],\n",
      "        [6.3911e-04],\n",
      "        [1.6936e-04],\n",
      "        [2.5442e-04],\n",
      "        [1.1551e-04],\n",
      "        [1.0209e-03],\n",
      "        [6.4372e-05]], device='cuda:0'), loss: 7.952492713928223: \n",
      "target probs tensor([[0.0001],\n",
      "        [0.0021],\n",
      "        [0.0003],\n",
      "        [0.0003],\n",
      "        [0.0051],\n",
      "        [0.0001],\n",
      "        [0.0001],\n",
      "        [0.0054],\n",
      "        [0.0001],\n",
      "        [0.0050],\n",
      "        [0.0024],\n",
      "        [0.0001],\n",
      "        [0.0003],\n",
      "        [0.0009],\n",
      "        [0.0005],\n",
      "        [0.0002]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.526028633117676: \n",
      "target probs tensor([[7.9967e-04],\n",
      "        [9.8482e-04],\n",
      "        [1.0034e-04],\n",
      "        [5.0375e-04],\n",
      "        [5.0442e-04],\n",
      "        [8.7121e-05],\n",
      "        [3.9639e-03],\n",
      "        [1.2781e-04],\n",
      "        [6.1358e-04],\n",
      "        [1.1600e-04],\n",
      "        [3.7928e-06],\n",
      "        [9.0184e-04],\n",
      "        [6.1623e-04],\n",
      "        [2.7492e-05],\n",
      "        [4.7338e-03],\n",
      "        [5.2274e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.065319061279297: \n",
      "target probs tensor([[5.8177e-04],\n",
      "        [9.0083e-06],\n",
      "        [9.8821e-06],\n",
      "        [2.4472e-04],\n",
      "        [3.5117e-04],\n",
      "        [1.2855e-03],\n",
      "        [4.2028e-04],\n",
      "        [7.8842e-05],\n",
      "        [2.7336e-07],\n",
      "        [8.6846e-04],\n",
      "        [4.3078e-04],\n",
      "        [2.5222e-04],\n",
      "        [8.0526e-04],\n",
      "        [3.1900e-04],\n",
      "        [3.2916e-04],\n",
      "        [1.1480e-03]], device='cuda:0'), loss: 8.681244850158691: \n",
      "Better model found at epoch 25 with validation value: 0.671999990940094.\n",
      "target probs tensor([[1.2595e-03],\n",
      "        [3.5597e-04],\n",
      "        [2.4954e-04],\n",
      "        [1.0151e-03],\n",
      "        [2.6041e-04],\n",
      "        [6.6082e-04],\n",
      "        [6.7737e-05],\n",
      "        [6.5914e-04],\n",
      "        [1.1666e-03],\n",
      "        [1.2698e-04],\n",
      "        [1.0111e-04],\n",
      "        [5.6831e-04],\n",
      "        [1.5329e-05],\n",
      "        [7.5579e-04],\n",
      "        [1.4059e-03],\n",
      "        [4.1034e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.9589056968688965: \n",
      "target probs tensor([[1.0738e-03],\n",
      "        [1.8847e-05],\n",
      "        [2.4752e-05],\n",
      "        [1.1214e-03],\n",
      "        [6.7054e-04],\n",
      "        [6.6396e-04],\n",
      "        [3.4174e-04],\n",
      "        [6.9190e-04],\n",
      "        [2.0939e-04],\n",
      "        [4.3000e-04],\n",
      "        [9.7998e-04],\n",
      "        [7.2614e-05],\n",
      "        [9.8876e-05],\n",
      "        [1.3518e-05],\n",
      "        [5.2571e-04],\n",
      "        [9.3906e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.289596557617188: \n",
      "target probs tensor([[9.7786e-05],\n",
      "        [2.1083e-04],\n",
      "        [8.7554e-03],\n",
      "        [4.1640e-04],\n",
      "        [2.3292e-05],\n",
      "        [2.6070e-04],\n",
      "        [3.3101e-04],\n",
      "        [2.9546e-04],\n",
      "        [4.7437e-04],\n",
      "        [2.7916e-04],\n",
      "        [1.0202e-03],\n",
      "        [1.0508e-04],\n",
      "        [3.4702e-04],\n",
      "        [5.3104e-05],\n",
      "        [2.4088e-04],\n",
      "        [3.9776e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.339851379394531: \n",
      "target probs tensor([[5.0525e-04],\n",
      "        [9.0150e-05],\n",
      "        [3.4544e-04],\n",
      "        [5.3308e-04],\n",
      "        [7.5268e-04],\n",
      "        [6.5443e-04],\n",
      "        [4.2248e-04],\n",
      "        [5.5411e-05],\n",
      "        [1.2952e-04],\n",
      "        [5.4300e-04],\n",
      "        [4.4987e-04],\n",
      "        [1.7440e-04],\n",
      "        [7.4199e-05],\n",
      "        [1.6881e-03],\n",
      "        [2.0432e-04],\n",
      "        [3.6119e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.103209495544434: \n",
      "target probs tensor([[8.0229e-04],\n",
      "        [2.0778e-03],\n",
      "        [4.3574e-02],\n",
      "        [2.8535e-04],\n",
      "        [5.0009e-05],\n",
      "        [2.8887e-04],\n",
      "        [3.5590e-04],\n",
      "        [4.5401e-04],\n",
      "        [8.0557e-04],\n",
      "        [2.4684e-04],\n",
      "        [8.3319e-04],\n",
      "        [4.6994e-06],\n",
      "        [9.4255e-05],\n",
      "        [1.7309e-04],\n",
      "        [8.3121e-04],\n",
      "        [2.3623e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.903385639190674: \n",
      "target probs tensor([[1.6118e-04],\n",
      "        [2.9183e-04],\n",
      "        [1.1030e-04],\n",
      "        [1.7158e-04],\n",
      "        [6.3633e-04],\n",
      "        [4.8329e-05],\n",
      "        [6.4971e-04],\n",
      "        [2.0410e-03],\n",
      "        [5.5756e-04],\n",
      "        [1.7965e-04],\n",
      "        [3.9850e-04],\n",
      "        [1.8389e-06],\n",
      "        [9.6449e-04],\n",
      "        [4.6946e-04],\n",
      "        [6.8625e-04],\n",
      "        [7.0155e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.236906051635742: \n",
      "target probs tensor([[9.4010e-04],\n",
      "        [2.7694e-04],\n",
      "        [3.9741e-04],\n",
      "        [7.5569e-04],\n",
      "        [5.1269e-04],\n",
      "        [3.4225e-04],\n",
      "        [5.5749e-04],\n",
      "        [2.7226e-04],\n",
      "        [3.5191e-05],\n",
      "        [1.6608e-01],\n",
      "        [4.9145e-06],\n",
      "        [4.7835e-02],\n",
      "        [8.9443e-04],\n",
      "        [3.2903e-03],\n",
      "        [7.0081e-04],\n",
      "        [3.6313e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.291852951049805: \n",
      "target probs tensor([[3.7502e-04],\n",
      "        [8.7601e-04],\n",
      "        [1.2952e-04],\n",
      "        [5.8403e-04],\n",
      "        [1.9320e-03],\n",
      "        [2.6829e-05],\n",
      "        [2.1472e-04],\n",
      "        [7.7231e-05],\n",
      "        [6.8097e-04],\n",
      "        [1.3414e-03],\n",
      "        [5.1330e-05],\n",
      "        [3.8085e-06],\n",
      "        [2.6800e-04],\n",
      "        [3.4484e-04],\n",
      "        [1.9401e-02],\n",
      "        [7.1672e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.24752140045166: \n",
      "target probs tensor([[2.2973e-04],\n",
      "        [3.5003e-04],\n",
      "        [4.5145e-04],\n",
      "        [9.3743e-04],\n",
      "        [1.8759e-04],\n",
      "        [1.0443e-03],\n",
      "        [1.5537e-03],\n",
      "        [3.3887e-04],\n",
      "        [1.6814e-04],\n",
      "        [1.1960e-04],\n",
      "        [1.8277e-02],\n",
      "        [2.9352e-05],\n",
      "        [7.2269e-04],\n",
      "        [8.0106e-04],\n",
      "        [8.8337e-04],\n",
      "        [3.6397e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.649175643920898: \n",
      "target probs tensor([[2.9283e-05],\n",
      "        [3.2425e-04],\n",
      "        [1.3551e-03],\n",
      "        [4.5376e-03],\n",
      "        [4.5421e-04],\n",
      "        [2.1245e-02],\n",
      "        [5.4866e-02],\n",
      "        [1.1552e-03],\n",
      "        [7.2040e-04],\n",
      "        [7.4522e-04],\n",
      "        [1.6762e-04],\n",
      "        [5.4829e-04],\n",
      "        [4.2776e-05],\n",
      "        [1.1644e-02],\n",
      "        [8.9820e-05],\n",
      "        [3.3426e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.135034084320068: \n",
      "target probs tensor([[5.5617e-04],\n",
      "        [1.8341e-03],\n",
      "        [2.2413e-04],\n",
      "        [1.7558e-04],\n",
      "        [1.5096e-04],\n",
      "        [2.2636e-04],\n",
      "        [1.1956e-04],\n",
      "        [2.9345e-04],\n",
      "        [2.5843e-05],\n",
      "        [1.7088e-04],\n",
      "        [4.2641e-04],\n",
      "        [3.5715e-05],\n",
      "        [4.0455e-04],\n",
      "        [7.3378e-04],\n",
      "        [5.6981e-04],\n",
      "        [7.0973e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.262041091918945: \n",
      "target probs tensor([[2.8555e-04],\n",
      "        [8.4194e-05],\n",
      "        [7.2236e-05],\n",
      "        [4.1396e-04],\n",
      "        [5.3616e-06],\n",
      "        [9.5479e-07],\n",
      "        [1.5875e-03],\n",
      "        [6.3129e-03],\n",
      "        [1.0662e-03],\n",
      "        [1.8101e-06],\n",
      "        [6.3818e-04],\n",
      "        [1.4470e-03],\n",
      "        [5.6992e-04],\n",
      "        [3.9596e-05],\n",
      "        [1.1093e-03],\n",
      "        [3.6420e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.810600280761719: \n",
      "target probs tensor([[6.4817e-04],\n",
      "        [3.8117e-03],\n",
      "        [1.5562e-03],\n",
      "        [3.6157e-04],\n",
      "        [1.6444e-04],\n",
      "        [4.3305e-04],\n",
      "        [1.9045e-03],\n",
      "        [2.0394e-04],\n",
      "        [2.1876e-03],\n",
      "        [9.6868e-06],\n",
      "        [3.1829e-03],\n",
      "        [2.2142e-04],\n",
      "        [4.8097e-04],\n",
      "        [3.4115e-05],\n",
      "        [1.4036e-04],\n",
      "        [6.6348e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.779398441314697: \n",
      "target probs tensor([[6.1310e-03],\n",
      "        [3.9955e-04],\n",
      "        [1.9952e-04],\n",
      "        [7.7572e-03],\n",
      "        [2.7918e-04],\n",
      "        [7.2708e-03],\n",
      "        [7.5804e-06],\n",
      "        [6.4056e-04],\n",
      "        [2.6424e-05],\n",
      "        [4.2126e-04],\n",
      "        [9.6961e-04],\n",
      "        [9.5271e-05],\n",
      "        [2.1417e-04],\n",
      "        [1.8913e-03],\n",
      "        [5.3600e-04],\n",
      "        [1.7368e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.748043060302734: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[1.2918e-04],\n",
      "        [3.0555e-03],\n",
      "        [6.3656e-04],\n",
      "        [5.7807e-04],\n",
      "        [1.8513e-04],\n",
      "        [2.0828e-03],\n",
      "        [2.5218e-03],\n",
      "        [7.4358e-04],\n",
      "        [1.2586e-03],\n",
      "        [4.5468e-04],\n",
      "        [2.3278e-04],\n",
      "        [9.2478e-05],\n",
      "        [4.4817e-04],\n",
      "        [1.3465e-04],\n",
      "        [6.9896e-04],\n",
      "        [1.7775e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.629226207733154: \n",
      "target probs tensor([[6.6867e-04],\n",
      "        [3.5971e-04],\n",
      "        [1.3465e-04],\n",
      "        [7.1932e-04],\n",
      "        [7.6616e-04],\n",
      "        [3.1918e-05],\n",
      "        [7.2982e-05],\n",
      "        [1.7069e-04],\n",
      "        [7.9714e-03],\n",
      "        [6.4411e-04],\n",
      "        [8.6851e-04],\n",
      "        [1.7686e-03],\n",
      "        [1.7655e-04],\n",
      "        [4.8762e-04],\n",
      "        [1.4341e-04],\n",
      "        [1.9631e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.896076679229736: \n",
      "target probs tensor([[8.7561e-06],\n",
      "        [2.2845e-03],\n",
      "        [3.2945e-03],\n",
      "        [1.6779e-05],\n",
      "        [6.1165e-04],\n",
      "        [8.7649e-05],\n",
      "        [6.1607e-04],\n",
      "        [7.6824e-04],\n",
      "        [2.6482e-04],\n",
      "        [7.2154e-04],\n",
      "        [5.7974e-04],\n",
      "        [1.7477e-03],\n",
      "        [3.4343e-04],\n",
      "        [3.1470e-04],\n",
      "        [2.8650e-04],\n",
      "        [6.0993e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.913536071777344: \n",
      "target probs tensor([[5.7992e-04],\n",
      "        [5.0392e-04],\n",
      "        [1.0466e-04],\n",
      "        [6.8968e-04],\n",
      "        [9.4746e-05],\n",
      "        [2.9811e-03],\n",
      "        [3.6360e-04],\n",
      "        [5.7079e-04],\n",
      "        [1.8972e-03],\n",
      "        [3.0651e-06],\n",
      "        [1.2908e-03],\n",
      "        [8.3070e-04],\n",
      "        [2.4660e-04],\n",
      "        [1.8678e-03],\n",
      "        [1.7132e-04],\n",
      "        [5.5804e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.838740348815918: \n",
      "target probs tensor([[0.0005],\n",
      "        [0.0005],\n",
      "        [0.0533],\n",
      "        [0.0005],\n",
      "        [0.0002],\n",
      "        [0.0004],\n",
      "        [0.0011],\n",
      "        [0.0007]], device='cuda:0'), loss: 6.997758865356445: \n",
      "target probs tensor([[2.4006e-04],\n",
      "        [2.0602e-04],\n",
      "        [3.0650e-04],\n",
      "        [4.8266e-04],\n",
      "        [6.1170e-05],\n",
      "        [4.8235e-04],\n",
      "        [3.7122e-03],\n",
      "        [2.4013e-04],\n",
      "        [2.5235e-05],\n",
      "        [1.9423e-04],\n",
      "        [2.2658e-06],\n",
      "        [3.5151e-03],\n",
      "        [9.0210e-05],\n",
      "        [1.4906e-04],\n",
      "        [3.7270e-04],\n",
      "        [3.4756e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.47397518157959: \n",
      "target probs tensor([[1.5321e-04],\n",
      "        [9.3744e-04],\n",
      "        [3.9582e-05],\n",
      "        [3.7447e-03],\n",
      "        [8.3715e-04],\n",
      "        [4.0066e-03],\n",
      "        [2.0295e-04],\n",
      "        [1.4974e-03],\n",
      "        [6.5814e-04],\n",
      "        [3.7527e-04],\n",
      "        [8.3684e-06],\n",
      "        [1.0049e-04],\n",
      "        [8.9565e-03],\n",
      "        [1.9706e-04],\n",
      "        [4.3388e-04],\n",
      "        [1.2872e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.821934700012207: \n",
      "target probs tensor([[1.5457e-03],\n",
      "        [2.5115e-03],\n",
      "        [1.4361e-05],\n",
      "        [9.0595e-04],\n",
      "        [1.8415e-04],\n",
      "        [1.9052e-04],\n",
      "        [5.3224e-04],\n",
      "        [4.5663e-04],\n",
      "        [1.4921e-04],\n",
      "        [2.5739e-04],\n",
      "        [2.1427e-03],\n",
      "        [2.6433e-05],\n",
      "        [3.6832e-04],\n",
      "        [8.3007e-05],\n",
      "        [2.2498e-03],\n",
      "        [6.0584e-05]], device='cuda:0'), loss: 8.117835998535156: \n",
      "Better model found at epoch 32 with validation value: 0.6729999780654907.\n",
      "target probs tensor([[3.8058e-04],\n",
      "        [1.0923e-03],\n",
      "        [3.5408e-04],\n",
      "        [1.0979e-04],\n",
      "        [1.9844e-04],\n",
      "        [1.7511e-04],\n",
      "        [6.8914e-06],\n",
      "        [2.7092e-03],\n",
      "        [4.6842e-04],\n",
      "        [1.7463e-04],\n",
      "        [3.3760e-04],\n",
      "        [2.1800e-04],\n",
      "        [5.7493e-05],\n",
      "        [1.4703e-04],\n",
      "        [8.5414e-04],\n",
      "        [5.0732e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.438400268554688: \n",
      "target probs tensor([[1.5144e-03],\n",
      "        [6.3686e-05],\n",
      "        [3.1048e-03],\n",
      "        [1.2974e-04],\n",
      "        [3.4967e-05],\n",
      "        [7.8919e-05],\n",
      "        [1.6748e-04],\n",
      "        [6.6930e-05],\n",
      "        [3.2258e-04],\n",
      "        [1.9066e-03],\n",
      "        [2.3892e-03],\n",
      "        [2.9121e-03],\n",
      "        [8.7599e-04],\n",
      "        [6.5020e-04],\n",
      "        [3.2655e-04],\n",
      "        [2.3369e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.864867210388184: \n",
      "target probs tensor([[3.5575e-04],\n",
      "        [1.1447e-04],\n",
      "        [4.4539e-04],\n",
      "        [3.1613e-04],\n",
      "        [1.8375e-03],\n",
      "        [1.6263e-03],\n",
      "        [8.2109e-04],\n",
      "        [1.0217e-04],\n",
      "        [1.6930e-07],\n",
      "        [7.1266e-04],\n",
      "        [1.4524e-04],\n",
      "        [6.9646e-05],\n",
      "        [6.4598e-04],\n",
      "        [2.2517e-03],\n",
      "        [2.0871e-04],\n",
      "        [6.0131e-04]], device='cuda:0'), loss: 8.274084091186523: \n",
      "target probs tensor([[9.5511e-04],\n",
      "        [4.7566e-04],\n",
      "        [1.5841e-03],\n",
      "        [7.0176e-04],\n",
      "        [1.0335e-03],\n",
      "        [2.6279e-03],\n",
      "        [1.7439e-03],\n",
      "        [2.3671e-04],\n",
      "        [7.7769e-05],\n",
      "        [5.8653e-05],\n",
      "        [1.7294e-03],\n",
      "        [1.1513e-04],\n",
      "        [3.4915e-05],\n",
      "        [4.9233e-04],\n",
      "        [8.5021e-04],\n",
      "        [6.0566e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.820416450500488: \n",
      "target probs tensor([[4.5958e-04],\n",
      "        [5.1297e-04],\n",
      "        [2.5247e-04],\n",
      "        [3.4159e-08],\n",
      "        [6.3192e-04],\n",
      "        [2.8903e-04],\n",
      "        [8.4349e-04],\n",
      "        [2.2096e-04],\n",
      "        [2.5148e-05],\n",
      "        [1.2524e-03],\n",
      "        [9.6238e-05],\n",
      "        [1.0367e-03],\n",
      "        [5.3362e-04],\n",
      "        [5.5916e-04],\n",
      "        [1.2176e-03],\n",
      "        [9.5601e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.50813102722168: \n",
      "target probs tensor([[3.7790e-05],\n",
      "        [7.8249e-04],\n",
      "        [8.7588e-04],\n",
      "        [3.4738e-04],\n",
      "        [1.1410e-04],\n",
      "        [3.2412e-04],\n",
      "        [2.6797e-04],\n",
      "        [8.5327e-04],\n",
      "        [4.2044e-05],\n",
      "        [2.2051e-03],\n",
      "        [2.2827e-05],\n",
      "        [1.2872e-03],\n",
      "        [1.0865e-04],\n",
      "        [3.2302e-04],\n",
      "        [1.6755e-03],\n",
      "        [1.5353e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.163803100585938: \n",
      "target probs tensor([[6.2069e-04],\n",
      "        [4.0816e-04],\n",
      "        [8.2970e-06],\n",
      "        [6.8749e-04],\n",
      "        [5.9542e-05],\n",
      "        [1.1383e-03],\n",
      "        [3.6933e-04],\n",
      "        [6.3488e-02],\n",
      "        [6.7620e-04],\n",
      "        [1.5465e-03],\n",
      "        [4.4057e-04],\n",
      "        [3.9682e-04],\n",
      "        [6.7653e-06],\n",
      "        [4.3724e-04],\n",
      "        [1.2029e-03],\n",
      "        [5.9985e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.778073310852051: \n",
      "target probs tensor([[1.8289e-04],\n",
      "        [6.7411e-04],\n",
      "        [2.9799e-04],\n",
      "        [7.3397e-05],\n",
      "        [1.2704e-04],\n",
      "        [3.8691e-03],\n",
      "        [1.4377e-04],\n",
      "        [4.0742e-04],\n",
      "        [1.6245e-04],\n",
      "        [4.5408e-04],\n",
      "        [2.1808e-04],\n",
      "        [1.2279e-04],\n",
      "        [1.7838e-04],\n",
      "        [1.9251e-01],\n",
      "        [9.1625e-04],\n",
      "        [1.0115e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.672147750854492: \n",
      "target probs tensor([[2.6406e-04],\n",
      "        [7.0989e-04],\n",
      "        [2.0684e-05],\n",
      "        [1.7298e-02],\n",
      "        [6.0030e-04],\n",
      "        [5.4939e-04],\n",
      "        [7.9147e-04],\n",
      "        [6.7430e-04],\n",
      "        [5.1514e-05],\n",
      "        [1.5467e-04],\n",
      "        [2.2952e-04],\n",
      "        [4.6681e-04],\n",
      "        [1.4929e-04],\n",
      "        [6.5774e-03],\n",
      "        [3.3291e-04],\n",
      "        [2.9108e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.773843288421631: \n",
      "Better model found at epoch 35 with validation value: 0.6819999814033508.\n",
      "target probs tensor([[1.5113e-04],\n",
      "        [1.3913e-03],\n",
      "        [1.3734e-04],\n",
      "        [6.0142e-04],\n",
      "        [1.1350e-04],\n",
      "        [3.1610e-04],\n",
      "        [2.1581e-04],\n",
      "        [9.3280e-05],\n",
      "        [4.6738e-04],\n",
      "        [6.5149e-04],\n",
      "        [7.5979e-04],\n",
      "        [3.4813e-02],\n",
      "        [9.0605e-05],\n",
      "        [6.5291e-04],\n",
      "        [3.2516e-04],\n",
      "        [1.6880e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.840884685516357: \n",
      "target probs tensor([[3.2743e-04],\n",
      "        [4.7733e-03],\n",
      "        [1.5066e-04],\n",
      "        [3.4184e-05],\n",
      "        [9.1472e-04],\n",
      "        [3.0171e-05],\n",
      "        [1.2817e-05],\n",
      "        [9.2354e-05],\n",
      "        [1.3167e-04],\n",
      "        [2.0025e-04],\n",
      "        [4.8605e-04],\n",
      "        [2.1462e-04],\n",
      "        [3.2801e-07],\n",
      "        [1.0083e-04],\n",
      "        [7.8583e-06],\n",
      "        [1.2318e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.301763534545898: \n",
      "target probs tensor([[5.3208e-04],\n",
      "        [2.5033e-03],\n",
      "        [7.1881e-04],\n",
      "        [4.3385e-04],\n",
      "        [4.9442e-04],\n",
      "        [2.9832e-04],\n",
      "        [1.0023e-04],\n",
      "        [5.9311e-03],\n",
      "        [8.1962e-04],\n",
      "        [8.3339e-05],\n",
      "        [3.5897e-04],\n",
      "        [8.9935e-04],\n",
      "        [1.5413e-03],\n",
      "        [4.1814e-05],\n",
      "        [1.1497e-04],\n",
      "        [1.4038e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.7824578285217285: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 36 with validation value: 0.6859999895095825.\n",
      "target probs tensor([[1.3310e-04],\n",
      "        [5.8744e-05],\n",
      "        [6.4066e-05],\n",
      "        [7.9100e-04],\n",
      "        [7.2075e-04],\n",
      "        [7.8679e-05],\n",
      "        [6.4320e-04],\n",
      "        [1.0883e-04],\n",
      "        [8.1352e-04],\n",
      "        [4.9336e-04],\n",
      "        [1.2808e-05],\n",
      "        [1.7259e-04],\n",
      "        [6.0442e-03],\n",
      "        [3.0214e-04],\n",
      "        [3.9754e-04],\n",
      "        [2.8613e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.280356407165527: \n",
      "target probs tensor([[7.8210e-05],\n",
      "        [1.7277e-03],\n",
      "        [1.2933e-04],\n",
      "        [1.5871e-04],\n",
      "        [3.2407e-04],\n",
      "        [4.6207e-05],\n",
      "        [2.0129e-05],\n",
      "        [7.1124e-05],\n",
      "        [7.0235e-05],\n",
      "        [2.0754e-04],\n",
      "        [1.1534e-03],\n",
      "        [7.5063e-04],\n",
      "        [5.7577e-04],\n",
      "        [1.0444e-03],\n",
      "        [1.4707e-04],\n",
      "        [7.3317e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.391899108886719: \n",
      "target probs tensor([[1.1549e-04],\n",
      "        [6.9524e-04],\n",
      "        [6.4817e-05],\n",
      "        [4.6739e-04],\n",
      "        [4.3704e-05],\n",
      "        [8.0961e-05],\n",
      "        [3.8729e-04],\n",
      "        [2.2004e-04],\n",
      "        [1.5543e-04],\n",
      "        [3.8286e-04],\n",
      "        [1.5535e-03],\n",
      "        [6.5582e-03],\n",
      "        [5.0327e-04],\n",
      "        [6.6818e-04],\n",
      "        [9.2002e-05],\n",
      "        [7.0079e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.917418479919434: \n",
      "target probs tensor([[1.2737e-03],\n",
      "        [1.1968e-04],\n",
      "        [1.0486e-03],\n",
      "        [1.1266e-03],\n",
      "        [3.1276e-04],\n",
      "        [7.4206e-05],\n",
      "        [2.4268e-03],\n",
      "        [5.8576e-04],\n",
      "        [3.9041e-06],\n",
      "        [8.8237e-04],\n",
      "        [5.0144e-04],\n",
      "        [5.7438e-04],\n",
      "        [1.8958e-04],\n",
      "        [1.1660e-03],\n",
      "        [2.0885e-04],\n",
      "        [1.5811e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.823922634124756: \n",
      "target probs tensor([[1.8015e-04],\n",
      "        [2.6970e-04],\n",
      "        [3.6410e-04],\n",
      "        [1.7133e-04],\n",
      "        [2.5886e-03],\n",
      "        [9.3051e-04],\n",
      "        [1.5750e-04],\n",
      "        [3.9273e-04],\n",
      "        [1.7908e-04],\n",
      "        [2.5868e-03],\n",
      "        [2.0502e-04],\n",
      "        [7.2538e-02],\n",
      "        [8.7633e-05],\n",
      "        [4.5559e-04],\n",
      "        [1.2449e-04],\n",
      "        [5.7555e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.634600639343262: \n",
      "target probs tensor([[0.0012],\n",
      "        [0.0007],\n",
      "        [0.0001],\n",
      "        [0.0001],\n",
      "        [0.0005],\n",
      "        [0.0002],\n",
      "        [0.0004],\n",
      "        [0.0006],\n",
      "        [0.0003],\n",
      "        [0.0042],\n",
      "        [0.0004],\n",
      "        [0.0026],\n",
      "        [0.0005],\n",
      "        [0.0007],\n",
      "        [0.0002],\n",
      "        [0.0010]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.570119857788086: \n",
      "target probs tensor([[9.2406e-04],\n",
      "        [2.5814e-03],\n",
      "        [1.8090e-04],\n",
      "        [4.8483e-04],\n",
      "        [2.8612e-04],\n",
      "        [1.7787e-05],\n",
      "        [3.4880e-04],\n",
      "        [3.4127e-04],\n",
      "        [1.7258e-05],\n",
      "        [1.8861e-04],\n",
      "        [1.5466e-04],\n",
      "        [8.0215e-04],\n",
      "        [1.2223e-04],\n",
      "        [2.2330e-04],\n",
      "        [1.7282e-04],\n",
      "        [5.9673e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.324061393737793: \n",
      "target probs tensor([[9.1882e-04],\n",
      "        [1.0632e-04],\n",
      "        [5.6032e-05],\n",
      "        [1.4184e-05],\n",
      "        [1.8581e-03],\n",
      "        [5.5471e-04],\n",
      "        [5.0524e-04],\n",
      "        [2.7797e-04],\n",
      "        [2.2636e-04],\n",
      "        [3.5562e-04],\n",
      "        [1.5658e-04],\n",
      "        [2.7563e-05],\n",
      "        [1.0003e-04],\n",
      "        [4.9323e-05],\n",
      "        [3.6392e-04],\n",
      "        [5.5061e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.55027961730957: \n",
      "target probs tensor([[0.0034],\n",
      "        [0.0005],\n",
      "        [0.0003],\n",
      "        [0.0006],\n",
      "        [0.0004],\n",
      "        [0.0001],\n",
      "        [0.0003],\n",
      "        [0.0007],\n",
      "        [0.0003],\n",
      "        [0.0015],\n",
      "        [0.0002],\n",
      "        [0.0001],\n",
      "        [0.0002],\n",
      "        [0.0003],\n",
      "        [0.0002],\n",
      "        [0.0010]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.84559965133667: \n",
      "target probs tensor([[0.0019],\n",
      "        [0.0035],\n",
      "        [0.0005],\n",
      "        [0.0007],\n",
      "        [0.0001],\n",
      "        [0.0004],\n",
      "        [0.0003],\n",
      "        [0.0007]], device='cuda:0'), loss: 7.396108627319336: \n",
      "target probs tensor([[8.1786e-03],\n",
      "        [5.8918e-04],\n",
      "        [3.1589e-03],\n",
      "        [1.0661e-03],\n",
      "        [1.6933e-04],\n",
      "        [4.1777e-04],\n",
      "        [1.1475e-03],\n",
      "        [8.0284e-04],\n",
      "        [3.4395e-04],\n",
      "        [8.6509e-05],\n",
      "        [6.2642e-04],\n",
      "        [8.5127e-05],\n",
      "        [1.1840e-03],\n",
      "        [5.3379e-04],\n",
      "        [6.1885e-05],\n",
      "        [1.2143e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.6415019035339355: \n",
      "target probs tensor([[8.0320e-04],\n",
      "        [9.2942e-04],\n",
      "        [5.5501e-04],\n",
      "        [1.4563e-03],\n",
      "        [1.7755e-04],\n",
      "        [2.5730e-04],\n",
      "        [5.9720e-04],\n",
      "        [1.0362e-03],\n",
      "        [3.4469e-04],\n",
      "        [5.3685e-06],\n",
      "        [1.0615e-04],\n",
      "        [6.5145e-04],\n",
      "        [1.9672e-04],\n",
      "        [3.2101e-06],\n",
      "        [4.2833e-04],\n",
      "        [5.2735e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.275879859924316: \n",
      "target probs tensor([[3.3144e-04],\n",
      "        [6.4213e-04],\n",
      "        [2.0976e-05],\n",
      "        [7.7863e-04],\n",
      "        [4.3441e-04],\n",
      "        [1.0666e-03],\n",
      "        [5.9500e-04],\n",
      "        [5.1823e-03],\n",
      "        [5.0782e-04],\n",
      "        [1.8788e-03],\n",
      "        [9.9599e-04],\n",
      "        [4.2689e-05],\n",
      "        [6.0055e-04],\n",
      "        [8.1753e-04],\n",
      "        [1.8065e-03],\n",
      "        [4.5843e-05]], device='cuda:0'), loss: 7.639782905578613: \n",
      "Better model found at epoch 40 with validation value: 0.6880000233650208.\n",
      "target probs tensor([[2.0449e-04],\n",
      "        [2.3696e-04],\n",
      "        [2.9807e-04],\n",
      "        [1.1874e-03],\n",
      "        [1.2381e-03],\n",
      "        [6.8980e-05],\n",
      "        [5.2818e-04],\n",
      "        [6.6593e-04],\n",
      "        [3.7019e-04],\n",
      "        [2.6811e-04],\n",
      "        [2.6436e-04],\n",
      "        [2.1590e-02],\n",
      "        [6.4594e-05],\n",
      "        [2.1745e-03],\n",
      "        [5.3301e-05],\n",
      "        [1.1586e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.713175296783447: \n",
      "target probs tensor([[4.8540e-04],\n",
      "        [3.7381e-06],\n",
      "        [2.9129e-04],\n",
      "        [1.1460e-04],\n",
      "        [1.0474e-04],\n",
      "        [1.5152e-04],\n",
      "        [2.2880e-04],\n",
      "        [1.0813e-03],\n",
      "        [1.8208e-04],\n",
      "        [1.1764e-04],\n",
      "        [8.6163e-05],\n",
      "        [1.4339e-04],\n",
      "        [1.9721e-04],\n",
      "        [2.9810e-04],\n",
      "        [3.1410e-04],\n",
      "        [6.2466e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.798620223999023: \n",
      "target probs tensor([[1.1067e-03],\n",
      "        [1.2080e-04],\n",
      "        [2.0030e-04],\n",
      "        [1.3544e-04],\n",
      "        [2.0239e-03],\n",
      "        [1.0830e-03],\n",
      "        [5.6658e-03],\n",
      "        [1.0505e-03],\n",
      "        [2.8486e-06],\n",
      "        [8.8600e-04],\n",
      "        [5.4498e-04],\n",
      "        [1.6563e-04],\n",
      "        [1.7499e-03],\n",
      "        [1.5239e-03],\n",
      "        [6.8720e-05],\n",
      "        [3.6620e-04]], device='cuda:0'), loss: 7.791463851928711: \n",
      "target probs tensor([[1.0916e-03],\n",
      "        [2.0762e-04],\n",
      "        [9.7056e-05],\n",
      "        [1.1060e-03],\n",
      "        [1.9383e-04],\n",
      "        [5.3170e-04],\n",
      "        [9.5899e-04],\n",
      "        [1.5644e-04],\n",
      "        [1.4433e-03],\n",
      "        [5.8603e-05],\n",
      "        [1.5338e-04],\n",
      "        [7.4649e-05],\n",
      "        [2.8400e-04],\n",
      "        [3.3145e-04],\n",
      "        [4.4912e-05],\n",
      "        [4.1335e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.231191635131836: \n",
      "target probs tensor([[5.0125e-04],\n",
      "        [1.2884e-03],\n",
      "        [1.8821e-04],\n",
      "        [2.1749e-04],\n",
      "        [6.8035e-04],\n",
      "        [9.8465e-06],\n",
      "        [5.5826e-04],\n",
      "        [3.2854e-04],\n",
      "        [9.5104e-05],\n",
      "        [1.6561e-04],\n",
      "        [6.8860e-04],\n",
      "        [1.1674e-07],\n",
      "        [1.0858e-03],\n",
      "        [6.8785e-04],\n",
      "        [9.1244e-04],\n",
      "        [3.3496e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.638591766357422: \n",
      "target probs tensor([[7.9223e-04],\n",
      "        [2.4579e-02],\n",
      "        [1.2273e-04],\n",
      "        [1.6142e-05],\n",
      "        [2.5987e-04],\n",
      "        [3.3235e-04],\n",
      "        [3.7112e-04],\n",
      "        [5.6786e-04],\n",
      "        [6.7893e-04],\n",
      "        [5.3976e-04],\n",
      "        [8.7339e-05],\n",
      "        [2.0126e-05],\n",
      "        [7.0266e-06],\n",
      "        [2.0965e-05],\n",
      "        [1.0157e-05],\n",
      "        [5.3584e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.698081970214844: \n",
      "Better model found at epoch 42 with validation value: 0.6909999847412109.\n",
      "target probs tensor([[7.0059e-04],\n",
      "        [4.4507e-04],\n",
      "        [2.4369e-04],\n",
      "        [5.9187e-04],\n",
      "        [6.7157e-04],\n",
      "        [5.3255e-05],\n",
      "        [1.6476e-04],\n",
      "        [5.4443e-04],\n",
      "        [8.7359e-05],\n",
      "        [4.7081e-04],\n",
      "        [4.3669e-05],\n",
      "        [4.6192e-04],\n",
      "        [4.1211e-04],\n",
      "        [1.2617e-03],\n",
      "        [4.7430e-04],\n",
      "        [1.2346e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.978218078613281: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[3.6130e-04],\n",
      "        [9.3380e-06],\n",
      "        [1.1214e-04],\n",
      "        [1.5056e-03],\n",
      "        [5.1127e-04],\n",
      "        [2.8512e-03],\n",
      "        [2.1995e-04],\n",
      "        [3.1122e-04],\n",
      "        [2.8171e-04],\n",
      "        [1.5375e-02],\n",
      "        [3.6487e-04],\n",
      "        [1.3804e-04],\n",
      "        [1.4195e-03],\n",
      "        [1.4815e-04],\n",
      "        [1.1836e-03],\n",
      "        [4.1095e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.756360054016113: \n",
      "target probs tensor([[1.0618e-05],\n",
      "        [8.0406e-04],\n",
      "        [3.4080e-04],\n",
      "        [6.9351e-04],\n",
      "        [2.9363e-04],\n",
      "        [4.2524e-04],\n",
      "        [2.4545e-04],\n",
      "        [2.7666e-04],\n",
      "        [1.3520e-04],\n",
      "        [7.8372e-04],\n",
      "        [2.0841e-03],\n",
      "        [1.1419e-04],\n",
      "        [3.8608e-04],\n",
      "        [1.7237e-04],\n",
      "        [3.7627e-04],\n",
      "        [4.0611e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.966626167297363: \n",
      "target probs tensor([[3.5202e-04],\n",
      "        [8.7393e-03],\n",
      "        [2.1183e-03],\n",
      "        [2.3059e-04],\n",
      "        [4.6777e-04],\n",
      "        [6.5071e-02],\n",
      "        [7.7824e-05],\n",
      "        [8.5388e-04],\n",
      "        [2.1927e-03],\n",
      "        [5.0700e-04],\n",
      "        [1.0623e-04],\n",
      "        [1.7682e-04],\n",
      "        [6.4690e-04],\n",
      "        [6.7552e-05],\n",
      "        [8.8853e-04],\n",
      "        [9.2907e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.143802642822266: \n",
      "target probs tensor([[1.2443e-04],\n",
      "        [8.9310e-04],\n",
      "        [1.7270e-05],\n",
      "        [2.9278e-06],\n",
      "        [2.6541e-03],\n",
      "        [2.5600e-04],\n",
      "        [7.0212e-04],\n",
      "        [5.9640e-05],\n",
      "        [4.4860e-05],\n",
      "        [3.5386e-05],\n",
      "        [6.7081e-05],\n",
      "        [1.4616e-03],\n",
      "        [4.2394e-04],\n",
      "        [1.4159e-04],\n",
      "        [3.2698e-02],\n",
      "        [5.3563e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.430680274963379: \n",
      "target probs tensor([[1.4738e-01],\n",
      "        [5.3802e-04],\n",
      "        [1.6106e-04],\n",
      "        [2.4444e-04],\n",
      "        [6.7289e-04],\n",
      "        [1.3219e-02],\n",
      "        [2.2706e-04],\n",
      "        [1.9480e-03],\n",
      "        [6.1517e-06],\n",
      "        [2.5613e-04],\n",
      "        [9.9027e-05],\n",
      "        [1.0817e-03],\n",
      "        [3.1560e-04],\n",
      "        [4.2897e-04],\n",
      "        [7.8870e-04],\n",
      "        [1.0768e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.716948509216309: \n",
      "Better model found at epoch 44 with validation value: 0.7009999752044678.\n",
      "target probs tensor([[7.5409e-04],\n",
      "        [7.8187e-04],\n",
      "        [2.8817e-04],\n",
      "        [2.1019e-04],\n",
      "        [4.9398e-05],\n",
      "        [5.0583e-04],\n",
      "        [1.0735e-03],\n",
      "        [2.5051e-04],\n",
      "        [4.5029e-04],\n",
      "        [2.8359e-04],\n",
      "        [4.7756e-04],\n",
      "        [4.3023e-04],\n",
      "        [8.6075e-04],\n",
      "        [6.3056e-03],\n",
      "        [1.5173e-03],\n",
      "        [7.2902e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.6881184577941895: \n",
      "target probs tensor([[1.1144e-03],\n",
      "        [3.3862e-04],\n",
      "        [1.5132e-04],\n",
      "        [4.2983e-04],\n",
      "        [4.6676e-05],\n",
      "        [1.9377e-08],\n",
      "        [1.0900e-03],\n",
      "        [2.6161e-06],\n",
      "        [9.8095e-06],\n",
      "        [2.0942e-04],\n",
      "        [2.8505e-04],\n",
      "        [2.8361e-04],\n",
      "        [1.0422e-04],\n",
      "        [1.5361e-04],\n",
      "        [3.9739e-05],\n",
      "        [8.1417e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.391921997070312: \n",
      "target probs tensor([[8.3606e-04],\n",
      "        [5.2525e-05],\n",
      "        [6.0164e-04],\n",
      "        [3.1210e-04],\n",
      "        [1.3917e-03],\n",
      "        [5.3235e-04],\n",
      "        [8.7387e-04],\n",
      "        [1.7724e-04],\n",
      "        [5.6981e-04],\n",
      "        [1.4027e-04],\n",
      "        [7.2292e-04],\n",
      "        [6.0561e-04],\n",
      "        [4.8685e-04],\n",
      "        [1.2090e-04],\n",
      "        [3.5143e-04],\n",
      "        [2.8245e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.72998571395874: \n",
      "target probs tensor([[3.5371e-04],\n",
      "        [5.6082e-04],\n",
      "        [5.7886e-04],\n",
      "        [2.0366e-03],\n",
      "        [2.2769e-04],\n",
      "        [2.0569e-02],\n",
      "        [1.8106e-04],\n",
      "        [2.2618e-03],\n",
      "        [1.1418e-04],\n",
      "        [1.3982e-04],\n",
      "        [5.1504e-04],\n",
      "        [5.1880e-04],\n",
      "        [5.5478e-04],\n",
      "        [1.0372e-03],\n",
      "        [7.2712e-04],\n",
      "        [7.1746e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.518064022064209: \n",
      "target probs tensor([[2.9116e-04],\n",
      "        [1.1520e-04],\n",
      "        [1.0509e-04],\n",
      "        [4.7979e-04],\n",
      "        [2.0417e-03],\n",
      "        [2.1648e-04],\n",
      "        [3.1929e-03],\n",
      "        [3.7551e-04],\n",
      "        [5.1072e-04],\n",
      "        [3.1905e-05],\n",
      "        [7.1594e-05],\n",
      "        [2.3124e-05],\n",
      "        [1.5021e-03],\n",
      "        [4.1213e-04],\n",
      "        [1.6581e-04],\n",
      "        [1.8707e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.96309757232666: \n",
      "target probs tensor([[6.8463e-06],\n",
      "        [1.0252e-03],\n",
      "        [7.5790e-05],\n",
      "        [6.6178e-02],\n",
      "        [1.0847e-02],\n",
      "        [2.2736e-04],\n",
      "        [4.2876e-05],\n",
      "        [5.8259e-04],\n",
      "        [1.2440e-03],\n",
      "        [4.4556e-04],\n",
      "        [3.8432e-04],\n",
      "        [4.0606e-04],\n",
      "        [7.2213e-04],\n",
      "        [2.6381e-04],\n",
      "        [5.2355e-04],\n",
      "        [4.7060e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.635265350341797: \n",
      "target probs tensor([[1.7370e-04],\n",
      "        [1.0731e-03],\n",
      "        [3.0464e-03],\n",
      "        [1.9106e-04],\n",
      "        [2.4600e-04],\n",
      "        [7.0100e-04],\n",
      "        [1.6587e-04],\n",
      "        [3.8416e-03],\n",
      "        [5.3756e-04],\n",
      "        [3.6419e-04],\n",
      "        [9.6215e-05],\n",
      "        [2.1886e-04],\n",
      "        [2.9084e-04],\n",
      "        [5.1261e-04],\n",
      "        [2.3000e-04],\n",
      "        [1.4656e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.714715957641602: \n",
      "target probs tensor([[2.2919e-04],\n",
      "        [3.4481e-04],\n",
      "        [2.1836e-03],\n",
      "        [6.2963e-05],\n",
      "        [1.3979e-03],\n",
      "        [1.5254e-02],\n",
      "        [6.3667e-06],\n",
      "        [2.6302e-04],\n",
      "        [1.5534e-02],\n",
      "        [3.1591e-03],\n",
      "        [8.0768e-04],\n",
      "        [3.7515e-04],\n",
      "        [7.7751e-04],\n",
      "        [5.6184e-04],\n",
      "        [5.3858e-03],\n",
      "        [3.5578e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.241075038909912: \n",
      "target probs tensor([[3.1680e-04],\n",
      "        [9.2471e-04],\n",
      "        [1.2359e-04],\n",
      "        [6.0448e-04],\n",
      "        [1.2043e-04],\n",
      "        [1.8395e-05],\n",
      "        [8.7101e-03],\n",
      "        [7.3420e-04],\n",
      "        [1.5357e-03],\n",
      "        [8.0084e-05],\n",
      "        [3.6969e-04],\n",
      "        [4.4067e-06],\n",
      "        [5.0302e-05],\n",
      "        [6.9550e-04],\n",
      "        [4.8731e-04],\n",
      "        [8.5694e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.352933883666992: \n",
      "target probs tensor([[1.6296e-03],\n",
      "        [5.4311e-04],\n",
      "        [3.3725e-03],\n",
      "        [4.3460e-05],\n",
      "        [2.5128e-04],\n",
      "        [4.8168e-04],\n",
      "        [4.4546e-04],\n",
      "        [1.7931e-03]], device='cuda:0'), loss: 7.4550886154174805: \n",
      "target probs tensor([[2.9206e-04],\n",
      "        [4.6139e-04],\n",
      "        [2.4391e-04],\n",
      "        [4.3982e-04],\n",
      "        [1.2361e-04],\n",
      "        [5.4058e-04],\n",
      "        [1.6234e-03],\n",
      "        [1.4526e-03],\n",
      "        [6.2555e-06],\n",
      "        [7.0539e-05],\n",
      "        [1.0414e-03],\n",
      "        [2.4847e-04],\n",
      "        [2.9186e-04],\n",
      "        [5.2890e-03],\n",
      "        [7.9153e-04],\n",
      "        [4.5903e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.891536712646484: \n",
      "target probs tensor([[4.5389e-04],\n",
      "        [2.3296e-04],\n",
      "        [1.5280e-04],\n",
      "        [3.0239e-05],\n",
      "        [1.6117e-04],\n",
      "        [4.8362e-04],\n",
      "        [1.2833e-04],\n",
      "        [3.0817e-04],\n",
      "        [1.9367e-03],\n",
      "        [4.8827e-03],\n",
      "        [1.8620e-04],\n",
      "        [1.9658e-04],\n",
      "        [6.6009e-05],\n",
      "        [1.2622e-04],\n",
      "        [1.4970e-03],\n",
      "        [1.6788e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.197481155395508: \n",
      "target probs tensor([[1.7938e-04],\n",
      "        [3.6534e-04],\n",
      "        [1.1141e-04],\n",
      "        [3.5862e-04],\n",
      "        [4.2167e-04],\n",
      "        [1.1869e-03],\n",
      "        [1.1275e-03],\n",
      "        [1.9997e-04],\n",
      "        [1.2550e-04],\n",
      "        [1.1021e-03],\n",
      "        [1.1850e-03],\n",
      "        [3.0969e-04],\n",
      "        [2.6924e-03],\n",
      "        [4.8085e-05],\n",
      "        [3.7282e-04],\n",
      "        [8.2389e-04]], device='cuda:0'), loss: 7.80352783203125: \n",
      "target probs tensor([[4.4318e-05],\n",
      "        [4.0495e-05],\n",
      "        [1.4343e-04],\n",
      "        [4.3508e-04],\n",
      "        [5.3886e-04],\n",
      "        [5.1623e-05],\n",
      "        [3.8644e-04],\n",
      "        [5.3865e-03],\n",
      "        [1.7974e-03],\n",
      "        [2.2737e-05],\n",
      "        [1.0644e-03],\n",
      "        [2.9856e-04],\n",
      "        [8.3762e-05],\n",
      "        [5.6858e-04],\n",
      "        [1.1291e-05],\n",
      "        [2.1583e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.492198944091797: \n",
      "target probs tensor([[3.8315e-04],\n",
      "        [1.2554e-03],\n",
      "        [2.5255e-04],\n",
      "        [1.7177e-03],\n",
      "        [1.5629e-05],\n",
      "        [3.2798e-03],\n",
      "        [6.4245e-03],\n",
      "        [1.8523e-03],\n",
      "        [1.8067e-03],\n",
      "        [4.2151e-04],\n",
      "        [5.8539e-04],\n",
      "        [2.0073e-04],\n",
      "        [2.9309e-04],\n",
      "        [6.8140e-04],\n",
      "        [2.3179e-04],\n",
      "        [3.5161e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.156993865966797: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[4.8704e-04],\n",
      "        [5.5244e-03],\n",
      "        [2.3005e-04],\n",
      "        [2.6886e-05],\n",
      "        [1.8802e-04],\n",
      "        [1.2099e-03],\n",
      "        [3.7929e-04],\n",
      "        [5.1063e-04],\n",
      "        [3.0751e-06],\n",
      "        [4.5298e-04],\n",
      "        [7.3014e-04],\n",
      "        [1.9176e-04],\n",
      "        [1.0414e-03],\n",
      "        [4.1021e-04],\n",
      "        [4.5050e-04],\n",
      "        [1.1777e-01]], device='cuda:0'), loss: 7.697727203369141: \n",
      "target probs tensor([[9.8279e-04],\n",
      "        [5.0100e-03],\n",
      "        [3.2363e-04],\n",
      "        [9.8865e-04],\n",
      "        [9.3542e-04],\n",
      "        [1.4013e-04],\n",
      "        [8.6292e-04],\n",
      "        [5.0819e-04],\n",
      "        [1.4298e-03],\n",
      "        [5.1292e-05],\n",
      "        [7.2158e-05],\n",
      "        [5.4063e-04],\n",
      "        [1.6230e-04],\n",
      "        [1.8758e-03],\n",
      "        [1.9286e-04],\n",
      "        [3.4064e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.812314033508301: \n",
      "target probs tensor([[1.3920e-03],\n",
      "        [6.4000e-06],\n",
      "        [4.7465e-04],\n",
      "        [6.7374e-04],\n",
      "        [1.7262e-04],\n",
      "        [4.3140e-04],\n",
      "        [7.0609e-04],\n",
      "        [1.0345e-04],\n",
      "        [2.3387e-04],\n",
      "        [2.7426e-04],\n",
      "        [8.8569e-04],\n",
      "        [2.4782e-03],\n",
      "        [4.4806e-03],\n",
      "        [8.0814e-05],\n",
      "        [8.9052e-04],\n",
      "        [5.4816e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.830777168273926: \n",
      "target probs tensor([[4.1468e-04],\n",
      "        [9.6137e-05],\n",
      "        [8.2470e-04],\n",
      "        [6.7128e-06],\n",
      "        [1.3429e-04],\n",
      "        [3.2651e-04],\n",
      "        [3.0450e-04],\n",
      "        [1.0554e-03],\n",
      "        [1.0469e-03],\n",
      "        [5.4436e-06],\n",
      "        [2.3522e-04],\n",
      "        [2.2803e-03],\n",
      "        [3.7951e-04],\n",
      "        [3.2881e-04],\n",
      "        [7.9617e-04],\n",
      "        [2.2295e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.156413078308105: \n",
      "target probs tensor([[4.7649e-05],\n",
      "        [2.8557e-04],\n",
      "        [1.0017e-04],\n",
      "        [7.9738e-04],\n",
      "        [3.0263e-06],\n",
      "        [6.4781e-04],\n",
      "        [3.9478e-04],\n",
      "        [1.2239e-03],\n",
      "        [1.5210e-04],\n",
      "        [2.8285e-05],\n",
      "        [5.5271e-04],\n",
      "        [6.5389e-04],\n",
      "        [2.5965e-03],\n",
      "        [2.8273e-04],\n",
      "        [9.7203e-05],\n",
      "        [3.6608e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.257450103759766: \n",
      "target probs tensor([[2.2241e-03],\n",
      "        [1.5237e-04],\n",
      "        [7.2533e-04],\n",
      "        [2.9868e-04],\n",
      "        [1.3044e-04],\n",
      "        [5.1520e-04],\n",
      "        [8.9198e-07],\n",
      "        [4.7269e-04],\n",
      "        [1.1476e-03],\n",
      "        [4.3722e-04],\n",
      "        [1.5363e-02],\n",
      "        [5.8336e-04],\n",
      "        [7.0807e-06],\n",
      "        [6.2124e-05],\n",
      "        [4.7500e-04],\n",
      "        [3.5984e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.224946022033691: \n",
      "target probs tensor([[3.0731e-05],\n",
      "        [4.6737e-04],\n",
      "        [4.1610e-04],\n",
      "        [3.5180e-05],\n",
      "        [1.7761e-04],\n",
      "        [1.0973e-03],\n",
      "        [7.0043e-04],\n",
      "        [3.0195e-04],\n",
      "        [3.8601e-03],\n",
      "        [6.5988e-05],\n",
      "        [1.1642e-03],\n",
      "        [4.4248e-04],\n",
      "        [3.4388e-04],\n",
      "        [4.6905e-04],\n",
      "        [4.6076e-04],\n",
      "        [1.8421e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.031383514404297: \n",
      "Better model found at epoch 51 with validation value: 0.7089999914169312.\n",
      "target probs tensor([[2.1114e-03],\n",
      "        [1.5719e-05],\n",
      "        [6.5012e-02],\n",
      "        [2.2304e-04],\n",
      "        [1.8773e-05],\n",
      "        [1.6905e-03],\n",
      "        [5.3698e-04],\n",
      "        [2.3189e-04],\n",
      "        [1.1215e-03],\n",
      "        [7.6904e-04],\n",
      "        [4.5610e-04],\n",
      "        [1.9265e-04],\n",
      "        [1.6765e-04],\n",
      "        [1.6578e-05],\n",
      "        [1.3759e-03],\n",
      "        [5.9259e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.841165065765381: \n",
      "target probs tensor([[1.2266e-02],\n",
      "        [5.8038e-04],\n",
      "        [2.0396e-04],\n",
      "        [5.1412e-04],\n",
      "        [5.4155e-05],\n",
      "        [3.3180e-05],\n",
      "        [6.6945e-04],\n",
      "        [5.1218e-03],\n",
      "        [5.3995e-04],\n",
      "        [4.3507e-04],\n",
      "        [4.2792e-04],\n",
      "        [1.2509e-03],\n",
      "        [7.8572e-04],\n",
      "        [5.5626e-04],\n",
      "        [7.5787e-04],\n",
      "        [4.9077e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.48725700378418: \n",
      "target probs tensor([[5.0449e-04],\n",
      "        [2.5757e-03],\n",
      "        [1.2596e-04],\n",
      "        [3.0260e-04],\n",
      "        [1.8772e-04],\n",
      "        [1.3144e-05],\n",
      "        [2.1683e-04],\n",
      "        [1.3291e-04],\n",
      "        [1.3871e-02],\n",
      "        [8.1763e-04],\n",
      "        [3.3509e-03],\n",
      "        [1.2955e-03],\n",
      "        [8.0779e-05],\n",
      "        [2.3906e-04],\n",
      "        [3.1333e-04],\n",
      "        [4.7547e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.814692497253418: \n",
      "Better model found at epoch 52 with validation value: 0.7120000123977661.\n",
      "target probs tensor([[2.0590e-03],\n",
      "        [8.1877e-04],\n",
      "        [8.4061e-04],\n",
      "        [2.5590e-03],\n",
      "        [8.4038e-04],\n",
      "        [7.4831e-05],\n",
      "        [7.9433e-04],\n",
      "        [7.3137e-04],\n",
      "        [7.5866e-03],\n",
      "        [1.4351e-03],\n",
      "        [1.2281e-03],\n",
      "        [6.5701e-04],\n",
      "        [9.7377e-05],\n",
      "        [6.8559e-04],\n",
      "        [4.4199e-04],\n",
      "        [4.6677e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.166083335876465: \n",
      "target probs tensor([[4.6721e-04],\n",
      "        [3.0215e-04],\n",
      "        [1.4054e-04],\n",
      "        [7.6143e-04],\n",
      "        [6.7583e-04],\n",
      "        [8.2826e-04],\n",
      "        [1.7287e-04],\n",
      "        [1.1238e-03],\n",
      "        [4.3582e-05],\n",
      "        [3.5553e-04],\n",
      "        [1.4802e-04],\n",
      "        [9.6450e-04],\n",
      "        [1.5909e-03],\n",
      "        [4.4921e-03],\n",
      "        [6.5922e-04],\n",
      "        [2.0088e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.694019794464111: \n",
      "target probs tensor([[2.2713e-04],\n",
      "        [1.7103e-04],\n",
      "        [2.5126e-07],\n",
      "        [6.2795e-06],\n",
      "        [5.1559e-04],\n",
      "        [6.4541e-04],\n",
      "        [7.3704e-05],\n",
      "        [2.4687e-04],\n",
      "        [1.8090e-04],\n",
      "        [1.0454e-04],\n",
      "        [2.9075e-05],\n",
      "        [3.0615e-04],\n",
      "        [1.7634e-04],\n",
      "        [5.6539e-04],\n",
      "        [4.1048e-03],\n",
      "        [9.6206e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 9.010148048400879: \n",
      "target probs tensor([[9.5284e-04],\n",
      "        [9.3613e-05],\n",
      "        [1.8401e-04],\n",
      "        [4.7892e-04],\n",
      "        [6.8573e-04],\n",
      "        [5.2824e-04],\n",
      "        [6.7355e-04],\n",
      "        [1.0511e-03],\n",
      "        [3.1353e-04],\n",
      "        [6.8650e-05],\n",
      "        [1.6495e-03],\n",
      "        [4.4161e-04],\n",
      "        [3.8664e-05],\n",
      "        [1.5757e-04],\n",
      "        [6.5223e-04],\n",
      "        [5.2081e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.229490280151367: \n",
      "target probs tensor([[3.1788e-04],\n",
      "        [1.5188e-04],\n",
      "        [1.0758e-03],\n",
      "        [1.5850e-03],\n",
      "        [4.1571e-04],\n",
      "        [7.2461e-04],\n",
      "        [1.8416e-03],\n",
      "        [1.3511e-02],\n",
      "        [2.3705e-02],\n",
      "        [3.3049e-04],\n",
      "        [3.8628e-04],\n",
      "        [3.9773e-04],\n",
      "        [2.5382e-04],\n",
      "        [8.8947e-05],\n",
      "        [1.1102e-04],\n",
      "        [8.2098e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.312988758087158: \n",
      "target probs tensor([[1.6716e-03],\n",
      "        [3.7721e-04],\n",
      "        [1.1787e-04],\n",
      "        [5.6496e-04],\n",
      "        [8.7460e-04],\n",
      "        [3.7219e-04],\n",
      "        [3.6897e-04],\n",
      "        [1.0188e-05],\n",
      "        [2.2210e-04],\n",
      "        [8.5759e-07],\n",
      "        [3.6758e-04],\n",
      "        [1.2687e-03],\n",
      "        [5.2243e-04],\n",
      "        [1.7197e-03],\n",
      "        [8.2884e-04],\n",
      "        [1.7312e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.092185020446777: \n",
      "Better model found at epoch 54 with validation value: 0.7200000286102295.\n",
      "target probs tensor([[3.3941e-04],\n",
      "        [3.0755e-04],\n",
      "        [4.6378e-04],\n",
      "        [5.2516e-04],\n",
      "        [5.8152e-04],\n",
      "        [1.1136e-03],\n",
      "        [6.4887e-04],\n",
      "        [2.7971e-04],\n",
      "        [7.6079e-02],\n",
      "        [3.7792e-05],\n",
      "        [5.4559e-04],\n",
      "        [4.2462e-04],\n",
      "        [7.5072e-04],\n",
      "        [9.6853e-05],\n",
      "        [3.2746e-05],\n",
      "        [3.3386e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.742551326751709: \n",
      "target probs tensor([[8.6085e-05],\n",
      "        [4.1257e-04],\n",
      "        [3.0385e-03],\n",
      "        [7.9243e-05],\n",
      "        [3.6817e-03],\n",
      "        [1.6465e-04],\n",
      "        [2.5335e-05],\n",
      "        [9.5194e-04],\n",
      "        [3.8414e-04],\n",
      "        [5.5511e-04],\n",
      "        [6.7257e-05],\n",
      "        [1.3432e-03],\n",
      "        [2.4771e-03],\n",
      "        [3.7293e-03],\n",
      "        [6.6185e-03],\n",
      "        [1.2369e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.445926666259766: \n",
      "target probs tensor([[4.7379e-02],\n",
      "        [1.5719e-04],\n",
      "        [1.8516e-04],\n",
      "        [1.9298e-04],\n",
      "        [6.1638e-05],\n",
      "        [1.3519e-03],\n",
      "        [7.4005e-05],\n",
      "        [4.1216e-04],\n",
      "        [3.4714e-03],\n",
      "        [1.4484e-02],\n",
      "        [1.6670e-03],\n",
      "        [1.1340e-04],\n",
      "        [5.1353e-04],\n",
      "        [9.7107e-04],\n",
      "        [3.6832e-04],\n",
      "        [5.6765e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.364498138427734: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[5.0760e-04],\n",
      "        [4.4606e-03],\n",
      "        [1.0616e-03],\n",
      "        [9.7247e-05],\n",
      "        [2.8277e-04],\n",
      "        [3.8155e-04],\n",
      "        [6.5361e-04],\n",
      "        [9.0719e-04]], device='cuda:0'), loss: 7.433103561401367: \n",
      "Better model found at epoch 55 with validation value: 0.7239999771118164.\n",
      "target probs tensor([[5.8763e-04],\n",
      "        [7.1020e-04],\n",
      "        [8.0344e-04],\n",
      "        [3.9294e-04],\n",
      "        [1.9413e-03],\n",
      "        [5.3828e-04],\n",
      "        [1.5315e-03],\n",
      "        [4.7096e-05],\n",
      "        [2.1577e-03],\n",
      "        [3.7916e-05],\n",
      "        [1.2409e-04],\n",
      "        [1.5676e-04],\n",
      "        [1.6150e-04],\n",
      "        [4.7335e-04],\n",
      "        [1.3554e-04],\n",
      "        [1.0955e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.878604412078857: \n",
      "target probs tensor([[1.1903e-03],\n",
      "        [9.7060e-05],\n",
      "        [3.4887e-05],\n",
      "        [3.0806e-05],\n",
      "        [6.9216e-04],\n",
      "        [4.7884e-04],\n",
      "        [2.4197e-05],\n",
      "        [6.1240e-04],\n",
      "        [3.3563e-04],\n",
      "        [1.3351e-04],\n",
      "        [8.8180e-04],\n",
      "        [9.1309e-05],\n",
      "        [4.3592e-05],\n",
      "        [4.0811e-04],\n",
      "        [4.6928e-04],\n",
      "        [5.5453e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.489631652832031: \n",
      "target probs tensor([[9.3984e-04],\n",
      "        [2.2012e-04],\n",
      "        [7.8881e-04],\n",
      "        [8.0178e-04],\n",
      "        [1.7976e-03],\n",
      "        [8.7198e-05],\n",
      "        [2.8971e-02],\n",
      "        [6.8498e-05],\n",
      "        [5.9222e-04],\n",
      "        [3.8796e-03],\n",
      "        [7.7622e-04],\n",
      "        [1.0845e-04],\n",
      "        [4.2410e-04],\n",
      "        [9.2545e-04],\n",
      "        [9.4811e-03],\n",
      "        [3.0895e-05]], device='cuda:0'), loss: 7.345728397369385: \n",
      "Better model found at epoch 56 with validation value: 0.7300000190734863.\n",
      "target probs tensor([[1.3293e-04],\n",
      "        [2.5071e-05],\n",
      "        [9.8434e-05],\n",
      "        [6.2545e-04],\n",
      "        [1.5782e-03],\n",
      "        [6.9118e-04],\n",
      "        [8.9456e-03],\n",
      "        [1.3544e-04],\n",
      "        [6.4603e-04],\n",
      "        [9.8157e-04],\n",
      "        [8.7996e-03],\n",
      "        [4.0077e-04],\n",
      "        [2.4802e-04],\n",
      "        [8.0058e-04],\n",
      "        [2.9223e-04],\n",
      "        [3.5469e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.613460540771484: \n",
      "target probs tensor([[3.8643e-04],\n",
      "        [2.8519e-04],\n",
      "        [2.6439e-04],\n",
      "        [4.1613e-04],\n",
      "        [3.3575e-03],\n",
      "        [2.0898e-04],\n",
      "        [1.9502e-04],\n",
      "        [8.5328e-04],\n",
      "        [1.2773e-04],\n",
      "        [5.6116e-04],\n",
      "        [2.2351e-03],\n",
      "        [4.9364e-04],\n",
      "        [8.0665e-05],\n",
      "        [4.7183e-04],\n",
      "        [1.0817e-02],\n",
      "        [4.6866e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.5791730880737305: \n",
      "target probs tensor([[4.2485e-04],\n",
      "        [8.0430e-05],\n",
      "        [2.4822e-04],\n",
      "        [4.3779e-04],\n",
      "        [3.2268e-04],\n",
      "        [6.2423e-04],\n",
      "        [3.7212e-04],\n",
      "        [7.5771e-04],\n",
      "        [4.3879e-07],\n",
      "        [1.0259e-03],\n",
      "        [3.6844e-04],\n",
      "        [1.7140e-03],\n",
      "        [1.4498e-03],\n",
      "        [1.8280e-04],\n",
      "        [1.2990e-04],\n",
      "        [3.9749e-04]], device='cuda:0'), loss: 8.215325355529785: \n",
      "target probs tensor([[0.0001],\n",
      "        [0.0004],\n",
      "        [0.0003],\n",
      "        [0.0002],\n",
      "        [0.0025],\n",
      "        [0.0003],\n",
      "        [0.0077],\n",
      "        [0.0003],\n",
      "        [0.0016],\n",
      "        [0.0006],\n",
      "        [0.0003],\n",
      "        [0.0003],\n",
      "        [0.0004],\n",
      "        [0.0006],\n",
      "        [0.0033],\n",
      "        [0.0005]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.429691314697266: \n",
      "target probs tensor([[3.1314e-03],\n",
      "        [1.2248e-04],\n",
      "        [1.5816e-02],\n",
      "        [3.7965e-04],\n",
      "        [5.7269e-05],\n",
      "        [5.9087e-04],\n",
      "        [2.6014e-04],\n",
      "        [2.8051e-04],\n",
      "        [9.5573e-04],\n",
      "        [1.4862e-04],\n",
      "        [3.1106e-04],\n",
      "        [6.8145e-04],\n",
      "        [1.5067e-03],\n",
      "        [1.1248e-03],\n",
      "        [1.3870e-03],\n",
      "        [3.7327e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.4579596519470215: \n",
      "target probs tensor([[1.5531e-04],\n",
      "        [2.4187e-04],\n",
      "        [2.6273e-04],\n",
      "        [1.4384e-04],\n",
      "        [5.3537e-06],\n",
      "        [8.8330e-04],\n",
      "        [6.2616e-04],\n",
      "        [8.1048e-04],\n",
      "        [4.0052e-02],\n",
      "        [4.1713e-05],\n",
      "        [3.5678e-06],\n",
      "        [3.2358e-04],\n",
      "        [4.5225e-05],\n",
      "        [4.9109e-03],\n",
      "        [1.5887e-03],\n",
      "        [3.1558e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.078566551208496: \n",
      "target probs tensor([[1.4239e-04],\n",
      "        [6.7371e-04],\n",
      "        [1.6649e-04],\n",
      "        [1.9343e-06],\n",
      "        [4.0421e-04],\n",
      "        [1.0113e-03],\n",
      "        [3.5911e-04],\n",
      "        [3.9858e-04],\n",
      "        [4.4308e-04],\n",
      "        [6.2280e-05],\n",
      "        [8.5505e-04],\n",
      "        [2.2893e-04],\n",
      "        [9.7573e-04],\n",
      "        [4.3236e-06],\n",
      "        [5.8348e-04],\n",
      "        [5.9355e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.468570709228516: \n",
      "target probs tensor([[0.0011],\n",
      "        [0.0010],\n",
      "        [0.0007],\n",
      "        [0.0007],\n",
      "        [0.0003],\n",
      "        [0.0013],\n",
      "        [0.0002],\n",
      "        [0.0065],\n",
      "        [0.0011],\n",
      "        [0.0050],\n",
      "        [0.0015],\n",
      "        [0.0002],\n",
      "        [0.0003],\n",
      "        [0.0005],\n",
      "        [0.0048],\n",
      "        [0.0006]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.985949993133545: \n",
      "target probs tensor([[1.0921e-03],\n",
      "        [2.6981e-04],\n",
      "        [3.6992e-04],\n",
      "        [4.9323e-04],\n",
      "        [3.9020e-03],\n",
      "        [2.2278e-03],\n",
      "        [2.1777e-06],\n",
      "        [4.0437e-05],\n",
      "        [6.6471e-04],\n",
      "        [1.9491e-04],\n",
      "        [1.9195e-04],\n",
      "        [1.0957e-06],\n",
      "        [9.7561e-05],\n",
      "        [1.5455e-02],\n",
      "        [2.9269e-04],\n",
      "        [3.1799e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.318519592285156: \n",
      "target probs tensor([[8.7584e-05],\n",
      "        [4.0717e-04],\n",
      "        [1.2552e-04],\n",
      "        [1.0002e-04],\n",
      "        [1.3042e-04],\n",
      "        [2.6656e-04],\n",
      "        [2.0281e-03],\n",
      "        [3.0799e-05],\n",
      "        [2.0840e-03],\n",
      "        [4.3740e-05],\n",
      "        [4.2305e-03],\n",
      "        [7.5372e-04],\n",
      "        [1.3542e-03],\n",
      "        [1.6209e-04],\n",
      "        [3.3939e-04],\n",
      "        [1.0291e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.010730743408203: \n",
      "target probs tensor([[1.0076e-03],\n",
      "        [3.0106e-04],\n",
      "        [2.6269e-04],\n",
      "        [5.5468e-04],\n",
      "        [1.0113e-03],\n",
      "        [4.7388e-04],\n",
      "        [1.7414e-03],\n",
      "        [3.6577e-04],\n",
      "        [1.7870e-03],\n",
      "        [8.4904e-05],\n",
      "        [1.0117e-03],\n",
      "        [4.5083e-04],\n",
      "        [3.6832e-05],\n",
      "        [1.2955e-03],\n",
      "        [3.2304e-04],\n",
      "        [8.3961e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.615480422973633: \n",
      "target probs tensor([[2.6694e-04],\n",
      "        [7.0577e-04],\n",
      "        [6.6939e-03],\n",
      "        [3.3851e-04],\n",
      "        [8.4167e-04],\n",
      "        [3.2096e-04],\n",
      "        [8.7801e-04],\n",
      "        [3.2716e-04],\n",
      "        [1.7524e-05],\n",
      "        [1.8560e-04],\n",
      "        [7.2720e-03],\n",
      "        [1.2037e-04],\n",
      "        [1.4018e-02],\n",
      "        [2.0512e-04],\n",
      "        [3.9206e-04],\n",
      "        [9.8730e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.62428617477417: \n",
      "target probs tensor([[6.1656e-05],\n",
      "        [1.0960e-03],\n",
      "        [3.1202e-04],\n",
      "        [6.8779e-04],\n",
      "        [7.6655e-04],\n",
      "        [1.1863e-03],\n",
      "        [3.3779e-05],\n",
      "        [1.6142e-04],\n",
      "        [1.5055e-02],\n",
      "        [3.9785e-03],\n",
      "        [3.9819e-04],\n",
      "        [1.0231e-04],\n",
      "        [1.5937e-03],\n",
      "        [1.3138e-04],\n",
      "        [1.8731e-04],\n",
      "        [4.1617e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.561550140380859: \n",
      "target probs tensor([[1.4329e-04],\n",
      "        [4.3661e-04],\n",
      "        [8.6871e-04],\n",
      "        [5.5941e-04],\n",
      "        [9.6694e-05],\n",
      "        [2.0232e-03],\n",
      "        [9.3097e-04],\n",
      "        [1.0128e-03],\n",
      "        [1.4950e-03],\n",
      "        [3.0567e-04],\n",
      "        [4.5263e-05],\n",
      "        [5.8571e-04],\n",
      "        [2.9898e-02],\n",
      "        [4.7761e-04],\n",
      "        [4.9608e-04],\n",
      "        [9.8616e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.529980659484863: \n",
      "target probs tensor([[2.5818e-04],\n",
      "        [8.2003e-03],\n",
      "        [5.0353e-04],\n",
      "        [1.3688e-04],\n",
      "        [3.2186e-04],\n",
      "        [4.4300e-04],\n",
      "        [9.9506e-04],\n",
      "        [1.6670e-05],\n",
      "        [1.6355e-04],\n",
      "        [1.7788e-03],\n",
      "        [3.9912e-04],\n",
      "        [5.3918e-04],\n",
      "        [5.6493e-04],\n",
      "        [6.1142e-04],\n",
      "        [3.0431e-04],\n",
      "        [9.3877e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.723906993865967: \n",
      "target probs tensor([[1.4060e-04],\n",
      "        [1.2821e-03],\n",
      "        [3.2823e-04],\n",
      "        [1.2223e-04],\n",
      "        [5.0290e-04],\n",
      "        [3.1550e-05],\n",
      "        [2.5075e-04],\n",
      "        [5.9244e-04],\n",
      "        [8.8964e-05],\n",
      "        [4.7686e-04],\n",
      "        [1.8991e-04],\n",
      "        [3.4770e-04],\n",
      "        [2.5833e-04],\n",
      "        [3.6319e-04],\n",
      "        [6.5427e-04],\n",
      "        [7.3474e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.29892635345459: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[2.2044e-03],\n",
      "        [4.1051e-05],\n",
      "        [1.2914e-03],\n",
      "        [1.5135e-04],\n",
      "        [5.1345e-04],\n",
      "        [4.7952e-04],\n",
      "        [2.8694e-04],\n",
      "        [1.3053e-03],\n",
      "        [6.9396e-03],\n",
      "        [5.7810e-04],\n",
      "        [2.3870e-04],\n",
      "        [6.7478e-04],\n",
      "        [5.9103e-04],\n",
      "        [8.2348e-04],\n",
      "        [1.1111e-03],\n",
      "        [2.8151e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.309945106506348: \n",
      "target probs tensor([[0.0001],\n",
      "        [0.0009],\n",
      "        [0.0003],\n",
      "        [0.0005],\n",
      "        [0.0014],\n",
      "        [0.0020],\n",
      "        [0.0004],\n",
      "        [0.0004],\n",
      "        [0.0078],\n",
      "        [0.0009],\n",
      "        [0.0009],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0001],\n",
      "        [0.0006],\n",
      "        [0.0005]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.248430252075195: \n",
      "target probs tensor([[3.6974e-04],\n",
      "        [5.6772e-04],\n",
      "        [1.1442e-03],\n",
      "        [3.4106e-04],\n",
      "        [6.0288e-05],\n",
      "        [3.8121e-04],\n",
      "        [4.7918e-04],\n",
      "        [4.6909e-05],\n",
      "        [5.7945e-04],\n",
      "        [2.1484e-04],\n",
      "        [1.0592e-04],\n",
      "        [5.1064e-04],\n",
      "        [3.2590e-02],\n",
      "        [1.0158e-03],\n",
      "        [1.7483e-03],\n",
      "        [1.9550e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.554143905639648: \n",
      "target probs tensor([[2.3793e-04],\n",
      "        [8.0173e-04],\n",
      "        [2.8567e-03],\n",
      "        [2.3570e-04],\n",
      "        [7.2708e-04],\n",
      "        [1.3738e-03],\n",
      "        [4.3667e-04],\n",
      "        [3.9077e-04],\n",
      "        [6.1679e-04],\n",
      "        [1.1633e-03],\n",
      "        [1.4596e-03],\n",
      "        [2.3334e-05],\n",
      "        [5.3141e-03],\n",
      "        [1.4247e-04],\n",
      "        [5.4587e-04],\n",
      "        [9.7930e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.435119152069092: \n",
      "target probs tensor([[3.8148e-04],\n",
      "        [6.4478e-04],\n",
      "        [1.6026e-03],\n",
      "        [6.1402e-04],\n",
      "        [2.1591e-04],\n",
      "        [6.8058e-04],\n",
      "        [1.3337e-03],\n",
      "        [4.7302e-04],\n",
      "        [7.2346e-05],\n",
      "        [7.5311e-04],\n",
      "        [2.5921e-04],\n",
      "        [4.3596e-04],\n",
      "        [8.1691e-04],\n",
      "        [8.9253e-05],\n",
      "        [1.5748e-04],\n",
      "        [3.8076e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.802741050720215: \n",
      "target probs tensor([[0.0003],\n",
      "        [0.0014],\n",
      "        [0.0007],\n",
      "        [0.0003],\n",
      "        [0.0016],\n",
      "        [0.0007],\n",
      "        [0.0020],\n",
      "        [0.0010]], device='cuda:0'), loss: 7.093181610107422: \n",
      "target probs tensor([[8.5569e-04],\n",
      "        [8.2953e-04],\n",
      "        [1.9570e-04],\n",
      "        [1.1562e-03],\n",
      "        [5.9345e-05],\n",
      "        [5.5866e-04],\n",
      "        [6.9745e-04],\n",
      "        [4.4967e-04],\n",
      "        [3.5739e-04],\n",
      "        [1.9867e-04],\n",
      "        [2.4540e-04],\n",
      "        [2.2560e-04],\n",
      "        [1.1175e-02],\n",
      "        [1.1160e-03],\n",
      "        [8.9398e-03],\n",
      "        [2.3058e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.450701713562012: \n",
      "target probs tensor([[1.3685e-05],\n",
      "        [5.0833e-03],\n",
      "        [7.1316e-04],\n",
      "        [4.6689e-04],\n",
      "        [3.1708e-05],\n",
      "        [1.7201e-05],\n",
      "        [1.3377e-04],\n",
      "        [8.7826e-05],\n",
      "        [3.1984e-04],\n",
      "        [5.9788e-04],\n",
      "        [2.5250e-04],\n",
      "        [4.3745e-04],\n",
      "        [6.2301e-04],\n",
      "        [6.4605e-03],\n",
      "        [1.5771e-02],\n",
      "        [2.0208e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.972075462341309: \n",
      "target probs tensor([[1.3408e-03],\n",
      "        [2.7379e-04],\n",
      "        [1.7171e-04],\n",
      "        [1.3104e-03],\n",
      "        [1.0141e-03],\n",
      "        [3.6979e-04],\n",
      "        [9.1784e-04],\n",
      "        [2.7649e-03],\n",
      "        [6.1191e-04],\n",
      "        [6.8580e-04],\n",
      "        [9.8241e-04],\n",
      "        [8.0511e-04],\n",
      "        [1.6899e-03],\n",
      "        [1.4553e-02],\n",
      "        [2.2552e-04],\n",
      "        [5.6156e-05]], device='cuda:0'), loss: 7.208544731140137: \n",
      "target probs tensor([[9.0612e-04],\n",
      "        [2.5653e-04],\n",
      "        [2.1800e-03],\n",
      "        [4.4850e-04],\n",
      "        [1.2138e-03],\n",
      "        [9.1491e-04],\n",
      "        [1.1190e-03],\n",
      "        [1.6756e-04],\n",
      "        [3.3338e-04],\n",
      "        [1.0415e-04],\n",
      "        [7.4944e-03],\n",
      "        [8.0177e-04],\n",
      "        [7.4981e-04],\n",
      "        [1.4335e-01],\n",
      "        [4.6144e-04],\n",
      "        [2.4752e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.895730972290039: \n",
      "target probs tensor([[8.2726e-04],\n",
      "        [2.6545e-05],\n",
      "        [4.1860e-05],\n",
      "        [1.4138e-03],\n",
      "        [9.9886e-04],\n",
      "        [6.3680e-04],\n",
      "        [6.1151e-04],\n",
      "        [8.6662e-04],\n",
      "        [6.9194e-05],\n",
      "        [2.6805e-03],\n",
      "        [3.8849e-04],\n",
      "        [4.0873e-03],\n",
      "        [2.6912e-04],\n",
      "        [2.3313e-04],\n",
      "        [3.1386e-04],\n",
      "        [3.6243e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.776379585266113: \n",
      "target probs tensor([[1.7098e-03],\n",
      "        [4.3815e-02],\n",
      "        [2.1655e-04],\n",
      "        [2.6268e-04],\n",
      "        [6.0899e-04],\n",
      "        [7.6122e-04],\n",
      "        [9.0805e-04],\n",
      "        [3.0171e-03],\n",
      "        [1.4467e-07],\n",
      "        [2.0731e-03],\n",
      "        [2.6743e-04],\n",
      "        [1.6983e-03],\n",
      "        [2.2613e-03],\n",
      "        [1.5663e-01],\n",
      "        [2.0286e-04],\n",
      "        [2.2237e-03]], device='cuda:0'), loss: 7.041384220123291: \n",
      "target probs tensor([[3.6493e-04],\n",
      "        [2.4971e-04],\n",
      "        [4.2060e-04],\n",
      "        [3.6669e-04],\n",
      "        [8.1294e-04],\n",
      "        [2.1138e-04],\n",
      "        [2.6463e-04],\n",
      "        [2.5953e-04],\n",
      "        [4.3089e-05],\n",
      "        [2.7895e-04],\n",
      "        [1.7050e-04],\n",
      "        [1.5675e-04],\n",
      "        [7.0071e-04],\n",
      "        [2.0879e-03],\n",
      "        [1.4216e-02],\n",
      "        [5.1078e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.806787490844727: \n",
      "target probs tensor([[9.8410e-04],\n",
      "        [1.3648e-04],\n",
      "        [2.0643e-01],\n",
      "        [4.3261e-03],\n",
      "        [1.9760e-03],\n",
      "        [3.1639e-05],\n",
      "        [5.6218e-04],\n",
      "        [1.2423e-04],\n",
      "        [2.0031e-03],\n",
      "        [2.5389e-04],\n",
      "        [1.8249e-05],\n",
      "        [1.1942e-03],\n",
      "        [1.2481e-03],\n",
      "        [1.8369e-03],\n",
      "        [1.9375e-04],\n",
      "        [1.6160e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.3942060470581055: \n",
      "target probs tensor([[3.6667e-05],\n",
      "        [9.9076e-05],\n",
      "        [1.0097e-03],\n",
      "        [2.9255e-04],\n",
      "        [5.3317e-04],\n",
      "        [3.8495e-04],\n",
      "        [1.3384e-03],\n",
      "        [2.4836e-04],\n",
      "        [2.9190e-04],\n",
      "        [2.3377e-03],\n",
      "        [1.7864e-04],\n",
      "        [7.8857e-04],\n",
      "        [6.7411e-04],\n",
      "        [1.5333e-03],\n",
      "        [2.8956e-04],\n",
      "        [4.0849e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.780621528625488: \n",
      "target probs tensor([[1.2084e-04],\n",
      "        [5.2934e-04],\n",
      "        [6.9077e-04],\n",
      "        [8.6166e-04],\n",
      "        [3.2001e-04],\n",
      "        [2.7097e-03],\n",
      "        [2.5662e-04],\n",
      "        [2.2563e-05],\n",
      "        [2.7474e-04],\n",
      "        [2.3948e-04],\n",
      "        [1.1820e-04],\n",
      "        [2.7555e-04],\n",
      "        [5.9842e-04],\n",
      "        [3.6302e-04],\n",
      "        [7.5794e-04],\n",
      "        [2.1473e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.891983509063721: \n",
      "target probs tensor([[1.0806e-01],\n",
      "        [3.0322e-04],\n",
      "        [2.9899e-04],\n",
      "        [3.6468e-04],\n",
      "        [5.7690e-04],\n",
      "        [9.7065e-04],\n",
      "        [2.5572e-05],\n",
      "        [2.7610e-04],\n",
      "        [3.6428e-04],\n",
      "        [1.2533e-04],\n",
      "        [1.3723e-03],\n",
      "        [4.2375e-04],\n",
      "        [9.8651e-05],\n",
      "        [5.0779e-04],\n",
      "        [1.5084e-03],\n",
      "        [1.8239e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.668566703796387: \n",
      "target probs tensor([[5.0249e-04],\n",
      "        [8.8611e-04],\n",
      "        [6.8662e-05],\n",
      "        [7.5911e-04],\n",
      "        [5.1759e-04],\n",
      "        [1.4585e-03],\n",
      "        [2.9602e-04],\n",
      "        [3.9293e-03],\n",
      "        [7.8505e-04],\n",
      "        [7.7184e-04],\n",
      "        [9.0234e-05],\n",
      "        [1.2566e-02],\n",
      "        [5.9088e-04],\n",
      "        [5.3855e-04],\n",
      "        [1.1019e-03],\n",
      "        [1.4559e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.216565132141113: \n",
      "target probs tensor([[6.0727e-04],\n",
      "        [1.6741e-04],\n",
      "        [5.2074e-04],\n",
      "        [1.7330e-04],\n",
      "        [9.9218e-04],\n",
      "        [2.3868e-04],\n",
      "        [3.3161e-04],\n",
      "        [1.9430e-03],\n",
      "        [2.0486e-04],\n",
      "        [1.0167e-03],\n",
      "        [6.0081e-04],\n",
      "        [1.2336e-04],\n",
      "        [5.1168e-06],\n",
      "        [9.7274e-04],\n",
      "        [1.3921e-03],\n",
      "        [9.6178e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.748430252075195: \n",
      "target probs tensor([[1.1280e-03],\n",
      "        [2.5090e-04],\n",
      "        [4.1616e-04],\n",
      "        [3.9521e-04],\n",
      "        [7.1902e-04],\n",
      "        [6.4768e-04],\n",
      "        [1.6292e-03],\n",
      "        [7.1228e-03],\n",
      "        [6.7292e-04],\n",
      "        [3.0245e-04],\n",
      "        [7.0094e-04],\n",
      "        [2.8676e-04],\n",
      "        [2.3464e-04],\n",
      "        [5.5683e-05],\n",
      "        [2.7207e-03],\n",
      "        [3.8816e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.461487293243408: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[0.0002],\n",
      "        [0.0001],\n",
      "        [0.0008],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0011],\n",
      "        [0.0008],\n",
      "        [0.0003],\n",
      "        [0.0002],\n",
      "        [0.0027],\n",
      "        [0.0003],\n",
      "        [0.0001],\n",
      "        [0.0005],\n",
      "        [0.0019],\n",
      "        [0.0005]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.8076910972595215: \n",
      "Better model found at epoch 68 with validation value: 0.7310000061988831.\n",
      "target probs tensor([[1.0008e-04],\n",
      "        [2.4441e-04],\n",
      "        [9.3905e-03],\n",
      "        [2.2859e-04],\n",
      "        [1.1564e-03],\n",
      "        [3.8683e-04],\n",
      "        [1.1961e-03],\n",
      "        [1.0163e-04],\n",
      "        [2.1431e-03],\n",
      "        [4.9732e-06],\n",
      "        [1.0384e-04],\n",
      "        [3.4053e-04],\n",
      "        [1.2688e-03],\n",
      "        [1.0450e-02],\n",
      "        [4.4014e-04],\n",
      "        [4.9895e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.69984769821167: \n",
      "target probs tensor([[7.2788e-04],\n",
      "        [4.1641e-04],\n",
      "        [9.5640e-04],\n",
      "        [1.0082e-03],\n",
      "        [3.0746e-04],\n",
      "        [8.7326e-05],\n",
      "        [3.6446e-04],\n",
      "        [5.5291e-04],\n",
      "        [6.8219e-04],\n",
      "        [2.0489e-03],\n",
      "        [2.0312e-04],\n",
      "        [1.9919e-05],\n",
      "        [1.8406e-03],\n",
      "        [4.8625e-03],\n",
      "        [2.2518e-04],\n",
      "        [1.3398e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.572220802307129: \n",
      "target probs tensor([[1.7122e-04],\n",
      "        [5.6839e-04],\n",
      "        [6.3916e-03],\n",
      "        [8.7638e-04],\n",
      "        [4.4805e-04],\n",
      "        [5.3656e-04],\n",
      "        [1.2121e-03],\n",
      "        [1.1434e-03],\n",
      "        [2.5601e-05],\n",
      "        [2.9938e-04],\n",
      "        [5.7758e-04],\n",
      "        [7.3159e-04],\n",
      "        [6.3772e-04],\n",
      "        [1.6962e-04],\n",
      "        [4.5552e-04],\n",
      "        [1.0458e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.5580010414123535: \n",
      "Better model found at epoch 69 with validation value: 0.7360000014305115.\n",
      "target probs tensor([[1.0489e-03],\n",
      "        [3.3925e-04],\n",
      "        [7.3723e-04],\n",
      "        [5.2877e-04],\n",
      "        [5.5198e-04],\n",
      "        [3.2797e-04],\n",
      "        [5.8655e-03],\n",
      "        [2.5291e-04],\n",
      "        [1.2717e-02],\n",
      "        [1.4961e-04],\n",
      "        [1.7616e-04],\n",
      "        [2.0367e-05],\n",
      "        [1.6595e-03],\n",
      "        [1.4733e-03],\n",
      "        [2.0482e-06],\n",
      "        [1.5263e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.729682922363281: \n",
      "target probs tensor([[2.0490e-04],\n",
      "        [6.2484e-04],\n",
      "        [6.0203e-04],\n",
      "        [1.1335e-04],\n",
      "        [5.6713e-04],\n",
      "        [2.3628e-02],\n",
      "        [2.0527e-04],\n",
      "        [2.6647e-04],\n",
      "        [2.0903e-04],\n",
      "        [4.2041e-04],\n",
      "        [1.2157e-04],\n",
      "        [3.3703e-05],\n",
      "        [3.8417e-04],\n",
      "        [3.1108e-03],\n",
      "        [4.5331e-04],\n",
      "        [1.1851e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.746704578399658: \n",
      "target probs tensor([[1.4339e-05],\n",
      "        [4.7316e-04],\n",
      "        [1.3170e-04],\n",
      "        [3.0508e-03],\n",
      "        [1.0216e-03],\n",
      "        [4.5296e-03],\n",
      "        [1.2941e-04],\n",
      "        [3.9821e-03],\n",
      "        [7.0518e-03],\n",
      "        [4.9478e-04],\n",
      "        [1.5583e-05],\n",
      "        [1.7152e-04],\n",
      "        [2.2669e-04],\n",
      "        [1.4269e-03],\n",
      "        [2.1011e-03],\n",
      "        [6.1209e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.857326030731201: \n",
      "target probs tensor([[7.0871e-04],\n",
      "        [6.5828e-04],\n",
      "        [2.9797e-04],\n",
      "        [1.6373e-03],\n",
      "        [1.4670e-04],\n",
      "        [1.1491e-03],\n",
      "        [1.4449e-02],\n",
      "        [7.0122e-04],\n",
      "        [2.4567e-04],\n",
      "        [5.6416e-05],\n",
      "        [4.0106e-04],\n",
      "        [1.4272e-04],\n",
      "        [5.9136e-04],\n",
      "        [1.9374e-05],\n",
      "        [9.2066e-04],\n",
      "        [2.6934e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.779487609863281: \n",
      "target probs tensor([[9.6318e-04],\n",
      "        [1.5994e-03],\n",
      "        [2.0870e-03],\n",
      "        [3.8051e-03],\n",
      "        [2.8334e-04],\n",
      "        [2.7786e-04],\n",
      "        [9.9722e-05],\n",
      "        [1.9654e-05],\n",
      "        [1.8459e-04],\n",
      "        [8.2883e-04],\n",
      "        [3.5348e-04],\n",
      "        [1.1022e-03],\n",
      "        [8.8200e-04],\n",
      "        [2.8155e-03],\n",
      "        [2.3385e-03],\n",
      "        [2.4393e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.310354709625244: \n",
      "target probs tensor([[1.4729e-04],\n",
      "        [8.8495e-04],\n",
      "        [9.5524e-04],\n",
      "        [1.4515e-03],\n",
      "        [2.1710e-04],\n",
      "        [7.1467e-04],\n",
      "        [1.7128e-04],\n",
      "        [4.4114e-03],\n",
      "        [1.2753e-03],\n",
      "        [6.1606e-04],\n",
      "        [3.0028e-04],\n",
      "        [3.8412e-04],\n",
      "        [6.2379e-04],\n",
      "        [5.9431e-04],\n",
      "        [1.1415e-03],\n",
      "        [1.3594e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.621099472045898: \n",
      "target probs tensor([[0.0004],\n",
      "        [0.0005],\n",
      "        [0.0003],\n",
      "        [0.0006],\n",
      "        [0.0004],\n",
      "        [0.0008],\n",
      "        [0.0005],\n",
      "        [0.0006]], device='cuda:0'), loss: 7.6194562911987305: \n",
      "Better model found at epoch 71 with validation value: 0.7490000128746033.\n",
      "target probs tensor([[4.1435e-04],\n",
      "        [4.4287e-04],\n",
      "        [2.6168e-04],\n",
      "        [1.3411e-03],\n",
      "        [8.3193e-04],\n",
      "        [9.9658e-05],\n",
      "        [1.3008e-03],\n",
      "        [3.9211e-03],\n",
      "        [2.3662e-04],\n",
      "        [2.8801e-03],\n",
      "        [6.5726e-04],\n",
      "        [2.2707e-05],\n",
      "        [2.5519e-04],\n",
      "        [2.3199e-04],\n",
      "        [4.6248e-05],\n",
      "        [1.8866e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.748838901519775: \n",
      "target probs tensor([[4.7905e-04],\n",
      "        [8.5129e-05],\n",
      "        [1.4176e-04],\n",
      "        [4.7998e-04],\n",
      "        [5.9434e-04],\n",
      "        [4.7034e-05],\n",
      "        [1.2874e-03],\n",
      "        [3.0819e-04],\n",
      "        [7.9462e-04],\n",
      "        [2.3672e-04],\n",
      "        [1.7239e-04],\n",
      "        [1.1990e-03],\n",
      "        [4.9141e-04],\n",
      "        [7.4603e-03],\n",
      "        [2.3859e-04],\n",
      "        [1.2310e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.755384922027588: \n",
      "target probs tensor([[2.1842e-04],\n",
      "        [2.6028e-03],\n",
      "        [1.0065e-03],\n",
      "        [4.2636e-04],\n",
      "        [1.0728e-03],\n",
      "        [3.8478e-04],\n",
      "        [7.7823e-04],\n",
      "        [4.8877e-04],\n",
      "        [2.5535e-04],\n",
      "        [6.6468e-04],\n",
      "        [1.4619e-02],\n",
      "        [8.5077e-05],\n",
      "        [6.2961e-04],\n",
      "        [4.5060e-04],\n",
      "        [9.7759e-04],\n",
      "        [9.0428e-05]], device='cuda:0'), loss: 7.439211368560791: \n",
      "target probs tensor([[8.0091e-06],\n",
      "        [1.7763e-05],\n",
      "        [6.5274e-04],\n",
      "        [1.0524e-03],\n",
      "        [1.7524e-03],\n",
      "        [1.0698e-03],\n",
      "        [2.6198e-03],\n",
      "        [3.8275e-07],\n",
      "        [5.4137e-04],\n",
      "        [3.9095e-04],\n",
      "        [8.4785e-04],\n",
      "        [1.9832e-03],\n",
      "        [3.1196e-04],\n",
      "        [6.3243e-04],\n",
      "        [6.1748e-04],\n",
      "        [8.1728e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.085869789123535: \n",
      "target probs tensor([[1.6720e-04],\n",
      "        [1.1432e-03],\n",
      "        [3.2712e-04],\n",
      "        [6.9729e-04],\n",
      "        [5.8656e-04],\n",
      "        [6.1461e-05],\n",
      "        [1.2386e-03],\n",
      "        [2.5932e-05],\n",
      "        [3.5364e-04],\n",
      "        [4.5114e-04],\n",
      "        [1.0770e-03],\n",
      "        [2.4717e-04],\n",
      "        [3.1431e-04],\n",
      "        [8.3191e-03],\n",
      "        [1.2680e-04],\n",
      "        [1.2267e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.635874271392822: \n",
      "target probs tensor([[8.4807e-04],\n",
      "        [3.4985e-04],\n",
      "        [4.8251e-03],\n",
      "        [2.6891e-05],\n",
      "        [2.8123e-04],\n",
      "        [1.1066e-03],\n",
      "        [3.0033e-04],\n",
      "        [2.7918e-04],\n",
      "        [1.2357e-05],\n",
      "        [8.2684e-04],\n",
      "        [6.2761e-05],\n",
      "        [2.1917e-02],\n",
      "        [8.1562e-04],\n",
      "        [5.2949e-03],\n",
      "        [2.5543e-04],\n",
      "        [6.5898e-04]], device='cuda:0'), loss: 7.625690937042236: \n",
      "target probs tensor([[1.3982e-03],\n",
      "        [6.6688e-04],\n",
      "        [8.1306e-03],\n",
      "        [5.3571e-04],\n",
      "        [1.4009e-03],\n",
      "        [9.2549e-05],\n",
      "        [1.2989e-03],\n",
      "        [6.9572e-04],\n",
      "        [2.0987e-03],\n",
      "        [9.2598e-04],\n",
      "        [9.3075e-04],\n",
      "        [1.4507e-04],\n",
      "        [3.2612e-04],\n",
      "        [2.2297e-05],\n",
      "        [5.5212e-04],\n",
      "        [8.2855e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.394411087036133: \n",
      "target probs tensor([[5.7409e-04],\n",
      "        [5.0765e-04],\n",
      "        [1.2725e-03],\n",
      "        [2.0423e-03],\n",
      "        [7.1426e-06],\n",
      "        [8.9862e-04],\n",
      "        [4.2749e-04],\n",
      "        [5.6024e-04],\n",
      "        [1.3480e-03],\n",
      "        [2.2456e-04],\n",
      "        [5.3933e-04],\n",
      "        [1.7716e-04],\n",
      "        [2.8823e-03],\n",
      "        [4.6908e-04],\n",
      "        [6.4745e-05],\n",
      "        [7.0893e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.725155830383301: \n",
      "target probs tensor([[9.7759e-05],\n",
      "        [1.3283e-03],\n",
      "        [3.2458e-04],\n",
      "        [1.1186e-02],\n",
      "        [2.1087e-04],\n",
      "        [1.4836e-04],\n",
      "        [7.8006e-04],\n",
      "        [1.8445e-03],\n",
      "        [2.7472e-04],\n",
      "        [3.3054e-03],\n",
      "        [2.6502e-04],\n",
      "        [9.9299e-04],\n",
      "        [2.6276e-04],\n",
      "        [1.3681e-03],\n",
      "        [1.4235e-03],\n",
      "        [3.3544e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.20423698425293: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[4.1877e-04],\n",
      "        [3.9449e-04],\n",
      "        [4.3762e-04],\n",
      "        [6.1837e-04],\n",
      "        [9.8303e-04],\n",
      "        [1.7687e-03],\n",
      "        [1.8200e-04],\n",
      "        [2.4997e-04],\n",
      "        [3.3409e-05],\n",
      "        [2.9031e-04],\n",
      "        [6.2750e-04],\n",
      "        [2.7347e-04],\n",
      "        [4.0966e-04],\n",
      "        [5.6222e-04],\n",
      "        [2.6639e-02],\n",
      "        [1.1096e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.540560722351074: \n",
      "target probs tensor([[5.9756e-04],\n",
      "        [3.7482e-03],\n",
      "        [1.3972e-04],\n",
      "        [1.0635e-04],\n",
      "        [5.2925e-04],\n",
      "        [7.1120e-04],\n",
      "        [8.7978e-04],\n",
      "        [5.4367e-04],\n",
      "        [2.8569e-03],\n",
      "        [1.9054e-04],\n",
      "        [3.4768e-04],\n",
      "        [7.4211e-04],\n",
      "        [1.2066e-04],\n",
      "        [8.3941e-05],\n",
      "        [7.9922e-04],\n",
      "        [4.8101e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.697045803070068: \n",
      "target probs tensor([[6.1623e-05],\n",
      "        [5.4492e-04],\n",
      "        [2.0777e-05],\n",
      "        [7.2501e-04],\n",
      "        [2.0257e-03],\n",
      "        [9.3709e-04],\n",
      "        [1.8509e-04],\n",
      "        [9.1421e-04],\n",
      "        [4.4904e-04],\n",
      "        [8.6930e-05],\n",
      "        [6.7442e-04],\n",
      "        [2.9293e-04],\n",
      "        [1.6885e-04],\n",
      "        [5.3492e-05],\n",
      "        [6.0532e-04],\n",
      "        [4.5709e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.275527954101562: \n",
      "target probs tensor([[5.8611e-04],\n",
      "        [2.7187e-04],\n",
      "        [1.0932e-03],\n",
      "        [2.3655e-05],\n",
      "        [1.2590e-03],\n",
      "        [4.7536e-03],\n",
      "        [4.1861e-04],\n",
      "        [3.3030e-05],\n",
      "        [7.9919e-04],\n",
      "        [2.4712e-04],\n",
      "        [5.4329e-04],\n",
      "        [6.7944e-04],\n",
      "        [4.9790e-04],\n",
      "        [1.5695e-03],\n",
      "        [4.8136e-04],\n",
      "        [2.1368e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.584049224853516: \n",
      "target probs tensor([[1.1134e-03],\n",
      "        [1.1236e-03],\n",
      "        [1.7588e-04],\n",
      "        [3.7835e-04],\n",
      "        [1.7914e-04],\n",
      "        [1.0376e-03],\n",
      "        [3.8817e-04],\n",
      "        [5.1472e-04],\n",
      "        [4.2008e-04],\n",
      "        [9.9440e-04],\n",
      "        [9.3887e-04],\n",
      "        [8.2302e-05],\n",
      "        [1.5727e-03],\n",
      "        [1.1685e-03],\n",
      "        [1.9605e-03],\n",
      "        [3.2175e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.474301338195801: \n",
      "target probs tensor([[4.2783e-04],\n",
      "        [5.4517e-04],\n",
      "        [1.5195e-04],\n",
      "        [2.9652e-04],\n",
      "        [6.8023e-04],\n",
      "        [6.6143e-05],\n",
      "        [4.8814e-03],\n",
      "        [4.1762e-06],\n",
      "        [3.8669e-04],\n",
      "        [3.4816e-04],\n",
      "        [1.6647e-03],\n",
      "        [2.9583e-04],\n",
      "        [4.4932e-04],\n",
      "        [2.3537e-03],\n",
      "        [7.1537e-04],\n",
      "        [6.0848e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.847695350646973: \n",
      "target probs tensor([[0.0004],\n",
      "        [0.0037],\n",
      "        [0.0047],\n",
      "        [0.0002],\n",
      "        [0.0012],\n",
      "        [0.0003],\n",
      "        [0.0004],\n",
      "        [0.0004],\n",
      "        [0.0002],\n",
      "        [0.0008],\n",
      "        [0.0001],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0001],\n",
      "        [0.0007],\n",
      "        [0.0022]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.647531509399414: \n",
      "target probs tensor([[6.2062e-04],\n",
      "        [1.2934e-03],\n",
      "        [2.7802e-03],\n",
      "        [1.9190e-03],\n",
      "        [2.3720e-04],\n",
      "        [5.6259e-04],\n",
      "        [6.1097e-05],\n",
      "        [7.0386e-07],\n",
      "        [6.4970e-05],\n",
      "        [4.2132e-04],\n",
      "        [9.9962e-04],\n",
      "        [1.9604e-03],\n",
      "        [6.1642e-04],\n",
      "        [4.5857e-03],\n",
      "        [9.2111e-04],\n",
      "        [3.9133e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.608859062194824: \n",
      "target probs tensor([[7.6832e-04],\n",
      "        [6.8314e-04],\n",
      "        [8.5056e-05],\n",
      "        [1.4286e-03],\n",
      "        [5.6219e-04],\n",
      "        [5.7063e-04],\n",
      "        [2.0038e-04],\n",
      "        [4.7498e-04],\n",
      "        [4.0532e-04],\n",
      "        [5.6190e-04],\n",
      "        [3.0516e-04],\n",
      "        [7.1713e-04],\n",
      "        [1.5508e-03],\n",
      "        [5.3551e-04],\n",
      "        [1.2984e-03],\n",
      "        [9.4391e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.484165191650391: \n",
      "target probs tensor([[6.3826e-06],\n",
      "        [1.4321e-03],\n",
      "        [1.0008e-04],\n",
      "        [2.1783e-03],\n",
      "        [3.5156e-04],\n",
      "        [4.4902e-04],\n",
      "        [4.1750e-02],\n",
      "        [1.0080e-03],\n",
      "        [1.5898e-04],\n",
      "        [4.5216e-04],\n",
      "        [3.4632e-05],\n",
      "        [1.5847e-03],\n",
      "        [2.9308e-04],\n",
      "        [3.8798e-04],\n",
      "        [1.8000e-04],\n",
      "        [6.0559e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.510583877563477: \n",
      "target probs tensor([[1.0356e-03],\n",
      "        [5.9588e-04],\n",
      "        [4.6056e-05],\n",
      "        [3.4552e-05],\n",
      "        [2.6004e-04],\n",
      "        [2.9294e-04],\n",
      "        [1.3280e-04],\n",
      "        [1.3592e-03],\n",
      "        [4.4269e-04],\n",
      "        [1.3591e-04],\n",
      "        [1.1808e-04],\n",
      "        [1.4009e-05],\n",
      "        [9.2333e-04],\n",
      "        [7.5141e-04],\n",
      "        [3.8435e-04],\n",
      "        [4.9221e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.311399459838867: \n",
      "target probs tensor([[7.8245e-04],\n",
      "        [9.9650e-04],\n",
      "        [5.3367e-04],\n",
      "        [1.8118e-04],\n",
      "        [1.0748e-03],\n",
      "        [1.2375e-04],\n",
      "        [1.1592e-03],\n",
      "        [1.2263e-03],\n",
      "        [1.8209e-03],\n",
      "        [1.4368e-04],\n",
      "        [3.6532e-04],\n",
      "        [4.5284e-04],\n",
      "        [5.9079e-04],\n",
      "        [4.2589e-05],\n",
      "        [2.5009e-04],\n",
      "        [3.4613e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.7527570724487305: \n",
      "target probs tensor([[0.0002],\n",
      "        [0.0006],\n",
      "        [0.0007],\n",
      "        [0.0008],\n",
      "        [0.0008],\n",
      "        [0.0005],\n",
      "        [0.0011],\n",
      "        [0.0009],\n",
      "        [0.0006],\n",
      "        [0.0001],\n",
      "        [0.0029],\n",
      "        [0.0001],\n",
      "        [0.0008],\n",
      "        [0.0018],\n",
      "        [0.0011],\n",
      "        [0.0005]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.4096198081970215: \n",
      "target probs tensor([[0.0004],\n",
      "        [0.0052],\n",
      "        [0.0002],\n",
      "        [0.0007],\n",
      "        [0.0005],\n",
      "        [0.0006],\n",
      "        [0.0009],\n",
      "        [0.0030],\n",
      "        [0.0019],\n",
      "        [0.0030],\n",
      "        [0.0003],\n",
      "        [0.0001],\n",
      "        [0.0063],\n",
      "        [0.0003],\n",
      "        [0.0005],\n",
      "        [0.0007]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.1348772048950195: \n",
      "target probs tensor([[1.0414e-03],\n",
      "        [7.2903e-04],\n",
      "        [1.3014e-03],\n",
      "        [6.8014e-04],\n",
      "        [4.1448e-04],\n",
      "        [7.6921e-04],\n",
      "        [5.0077e-04],\n",
      "        [1.4289e-03],\n",
      "        [2.6821e-05],\n",
      "        [5.2068e-04],\n",
      "        [8.2300e-04],\n",
      "        [1.8044e-03],\n",
      "        [1.0318e-03],\n",
      "        [3.0549e-02],\n",
      "        [5.1880e-04],\n",
      "        [6.8139e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.404350280761719: \n",
      "target probs tensor([[0.0010],\n",
      "        [0.0009],\n",
      "        [0.0015],\n",
      "        [0.0001],\n",
      "        [0.0013],\n",
      "        [0.0002],\n",
      "        [0.0006],\n",
      "        [0.0007]], device='cuda:0'), loss: 7.383702754974365: \n",
      "target probs tensor([[3.7948e-04],\n",
      "        [1.5235e-04],\n",
      "        [1.8137e-03],\n",
      "        [1.6984e-04],\n",
      "        [8.1768e-04],\n",
      "        [4.9318e-05],\n",
      "        [3.7596e-03],\n",
      "        [1.4673e-02],\n",
      "        [1.0622e-04],\n",
      "        [1.1317e-03],\n",
      "        [1.7912e-03],\n",
      "        [3.4706e-04],\n",
      "        [6.0111e-04],\n",
      "        [3.0538e-04],\n",
      "        [1.2114e-02],\n",
      "        [1.5808e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.193080425262451: \n",
      "target probs tensor([[2.6640e-04],\n",
      "        [6.3273e-04],\n",
      "        [7.2285e-04],\n",
      "        [4.2077e-03],\n",
      "        [4.9178e-04],\n",
      "        [1.9637e-05],\n",
      "        [3.5247e-04],\n",
      "        [8.7813e-04],\n",
      "        [1.2727e-02],\n",
      "        [2.6068e-04],\n",
      "        [3.5893e-04],\n",
      "        [5.7993e-04],\n",
      "        [3.5960e-03],\n",
      "        [5.0762e-04],\n",
      "        [2.5434e-04],\n",
      "        [1.5128e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.35803747177124: \n",
      "target probs tensor([[9.3635e-04],\n",
      "        [9.3060e-03],\n",
      "        [3.3113e-04],\n",
      "        [3.0480e-03],\n",
      "        [6.6530e-04],\n",
      "        [1.1839e-03],\n",
      "        [1.5690e-03],\n",
      "        [3.0880e-03],\n",
      "        [8.5352e-04],\n",
      "        [1.9788e-03],\n",
      "        [7.4998e-04],\n",
      "        [3.2223e-04],\n",
      "        [1.1862e-03],\n",
      "        [5.5307e-04],\n",
      "        [2.3243e-04],\n",
      "        [8.5266e-05]], device='cuda:0'), loss: 7.015591621398926: \n",
      "target probs tensor([[9.9926e-04],\n",
      "        [2.5377e-04],\n",
      "        [3.0510e-04],\n",
      "        [3.3388e-05],\n",
      "        [9.7670e-04],\n",
      "        [3.2614e-04],\n",
      "        [1.6362e-03],\n",
      "        [8.3066e-06],\n",
      "        [1.2329e-04],\n",
      "        [5.7659e-04],\n",
      "        [1.2010e-03],\n",
      "        [2.0293e-04],\n",
      "        [1.3751e-03],\n",
      "        [4.3830e-05],\n",
      "        [5.7430e-06],\n",
      "        [8.1112e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.384918212890625: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[5.9758e-04],\n",
      "        [6.7038e-04],\n",
      "        [1.9740e-03],\n",
      "        [4.5359e-04],\n",
      "        [1.8864e-03],\n",
      "        [9.4150e-05],\n",
      "        [1.1669e-03],\n",
      "        [9.9694e-04],\n",
      "        [6.3505e-03],\n",
      "        [7.6838e-06],\n",
      "        [1.0697e-03],\n",
      "        [8.5936e-04],\n",
      "        [2.1658e-03],\n",
      "        [9.3842e-05],\n",
      "        [1.3692e-04],\n",
      "        [7.3808e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.507246494293213: \n",
      "target probs tensor([[6.3679e-04],\n",
      "        [3.6018e-04],\n",
      "        [1.7642e-04],\n",
      "        [1.4674e-04],\n",
      "        [4.1450e-04],\n",
      "        [1.0174e-03],\n",
      "        [2.9525e-04],\n",
      "        [3.8368e-04],\n",
      "        [2.6152e-05],\n",
      "        [7.6669e-04],\n",
      "        [6.4930e-04],\n",
      "        [2.0689e-04],\n",
      "        [2.9099e-03],\n",
      "        [4.3505e-04],\n",
      "        [1.9463e-04],\n",
      "        [6.6444e-04]], device='cuda:0'), loss: 7.901147365570068: \n",
      "target probs tensor([[0.0036],\n",
      "        [0.0002],\n",
      "        [0.0043],\n",
      "        [0.0002],\n",
      "        [0.0001],\n",
      "        [0.0005],\n",
      "        [0.0163],\n",
      "        [0.0002],\n",
      "        [0.0013],\n",
      "        [0.0007],\n",
      "        [0.0002],\n",
      "        [0.0003],\n",
      "        [0.0006],\n",
      "        [0.0013],\n",
      "        [0.0007],\n",
      "        [0.0014]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.227071762084961: \n",
      "target probs tensor([[1.6280e-03],\n",
      "        [4.6003e-03],\n",
      "        [5.7706e-03],\n",
      "        [1.0694e-03],\n",
      "        [1.5760e-04],\n",
      "        [1.9457e-04],\n",
      "        [1.9172e-06],\n",
      "        [1.6282e-04],\n",
      "        [6.5274e-04],\n",
      "        [1.1311e-02],\n",
      "        [2.5444e-04],\n",
      "        [8.7442e-04],\n",
      "        [5.9652e-04],\n",
      "        [1.8790e-03],\n",
      "        [1.1473e-04],\n",
      "        [4.6793e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.535091876983643: \n",
      "target probs tensor([[2.2929e-04],\n",
      "        [3.0189e-04],\n",
      "        [2.4624e-05],\n",
      "        [4.7621e-04],\n",
      "        [1.1491e-04],\n",
      "        [1.1988e-04],\n",
      "        [5.2313e-04],\n",
      "        [3.7585e-04],\n",
      "        [3.4825e-04],\n",
      "        [3.4202e-06],\n",
      "        [4.4982e-04],\n",
      "        [2.8784e-03],\n",
      "        [5.4883e-04],\n",
      "        [5.7553e-04],\n",
      "        [5.3974e-04],\n",
      "        [2.0590e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.335993766784668: \n",
      "target probs tensor([[1.0685e-03],\n",
      "        [3.8947e-04],\n",
      "        [2.7742e-05],\n",
      "        [4.5560e-04],\n",
      "        [4.7255e-04],\n",
      "        [1.5224e-04],\n",
      "        [2.4424e-05],\n",
      "        [2.3386e-05],\n",
      "        [2.2171e-04],\n",
      "        [3.7841e-04],\n",
      "        [5.1007e-04],\n",
      "        [2.3771e-03],\n",
      "        [6.0164e-04],\n",
      "        [5.0342e-03],\n",
      "        [3.8198e-03],\n",
      "        [3.3883e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.924429893493652: \n",
      "target probs tensor([[3.2806e-04],\n",
      "        [1.9273e-04],\n",
      "        [3.5988e-06],\n",
      "        [6.0064e-04],\n",
      "        [6.3354e-05],\n",
      "        [9.6402e-04],\n",
      "        [1.8130e-04],\n",
      "        [1.1421e-04],\n",
      "        [4.7908e-04],\n",
      "        [1.0507e-04],\n",
      "        [1.7321e-03],\n",
      "        [2.1111e-04],\n",
      "        [3.6420e-04],\n",
      "        [4.4619e-04],\n",
      "        [1.4202e-05],\n",
      "        [9.9151e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.510625839233398: \n",
      "target probs tensor([[2.5117e-07],\n",
      "        [7.2806e-04],\n",
      "        [5.9168e-04],\n",
      "        [6.7917e-06],\n",
      "        [3.4677e-04],\n",
      "        [3.8290e-04],\n",
      "        [4.2958e-04],\n",
      "        [2.8168e-04],\n",
      "        [2.5020e-03],\n",
      "        [1.1602e-03],\n",
      "        [1.0283e-04],\n",
      "        [6.4002e-04],\n",
      "        [1.3578e-05],\n",
      "        [4.0661e-04],\n",
      "        [5.4936e-04],\n",
      "        [1.1481e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.361946105957031: \n",
      "target probs tensor([[3.4258e-03],\n",
      "        [4.1327e-04],\n",
      "        [3.9649e-04],\n",
      "        [1.7692e-04],\n",
      "        [1.7291e-04],\n",
      "        [1.2556e-03],\n",
      "        [1.1592e-04],\n",
      "        [1.4962e-03],\n",
      "        [4.6326e-04],\n",
      "        [1.8677e-03],\n",
      "        [1.6557e-05],\n",
      "        [1.5247e-03],\n",
      "        [3.3579e-04],\n",
      "        [7.5327e-04],\n",
      "        [1.9956e-04],\n",
      "        [6.3992e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.710596084594727: \n",
      "target probs tensor([[6.5074e-05],\n",
      "        [6.7079e-05],\n",
      "        [8.3724e-03],\n",
      "        [2.3341e-04],\n",
      "        [5.7959e-04],\n",
      "        [7.5043e-04],\n",
      "        [4.5196e-06],\n",
      "        [1.6101e-04],\n",
      "        [4.0211e-04],\n",
      "        [5.9346e-04],\n",
      "        [1.1882e-04],\n",
      "        [4.0169e-04],\n",
      "        [1.3283e-02],\n",
      "        [3.9332e-04],\n",
      "        [6.6215e-05],\n",
      "        [1.1060e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.192790985107422: \n",
      "target probs tensor([[1.8101e-04],\n",
      "        [1.2694e-04],\n",
      "        [1.1978e-03],\n",
      "        [3.8887e-04],\n",
      "        [8.1695e-04],\n",
      "        [1.1685e-03],\n",
      "        [8.0046e-04],\n",
      "        [1.8655e-04],\n",
      "        [5.9303e-04],\n",
      "        [4.1825e-06],\n",
      "        [8.5229e-04],\n",
      "        [2.9006e-04],\n",
      "        [2.4408e-04],\n",
      "        [6.7292e-05],\n",
      "        [9.1490e-04],\n",
      "        [1.2473e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.023950576782227: \n",
      "target probs tensor([[3.6785e-04],\n",
      "        [1.4696e-04],\n",
      "        [1.8545e-03],\n",
      "        [2.7085e-04],\n",
      "        [4.8673e-04],\n",
      "        [5.7065e-04],\n",
      "        [1.1428e-03],\n",
      "        [8.5739e-04],\n",
      "        [5.0896e-04],\n",
      "        [2.8187e-04],\n",
      "        [4.4223e-04],\n",
      "        [7.7417e-04],\n",
      "        [1.9273e-03],\n",
      "        [5.6422e-04],\n",
      "        [1.9883e-04],\n",
      "        [4.9487e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.68643045425415: \n",
      "target probs tensor([[7.8852e-05],\n",
      "        [9.0602e-04],\n",
      "        [1.8694e-05],\n",
      "        [9.6961e-05],\n",
      "        [1.8562e-05],\n",
      "        [5.4198e-04],\n",
      "        [2.5420e-04],\n",
      "        [1.3637e-03],\n",
      "        [5.3813e-04],\n",
      "        [3.4815e-03],\n",
      "        [2.7636e-04],\n",
      "        [1.5067e-04],\n",
      "        [8.2552e-03],\n",
      "        [1.2306e-04],\n",
      "        [4.7935e-04],\n",
      "        [4.9484e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.069280624389648: \n",
      "target probs tensor([[1.0130e-03],\n",
      "        [7.1015e-04],\n",
      "        [4.3573e-04],\n",
      "        [2.7863e-04],\n",
      "        [3.4284e-04],\n",
      "        [4.3616e-03],\n",
      "        [5.4880e-05],\n",
      "        [7.4200e-04],\n",
      "        [5.8195e-05],\n",
      "        [5.5312e-04],\n",
      "        [1.2324e-04],\n",
      "        [2.2902e-04],\n",
      "        [4.3255e-05],\n",
      "        [3.1618e-04],\n",
      "        [1.3647e-05],\n",
      "        [1.7287e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.175213813781738: \n",
      "target probs tensor([[7.6529e-04],\n",
      "        [1.3162e-03],\n",
      "        [1.3498e-03],\n",
      "        [3.9194e-03],\n",
      "        [1.4517e-03],\n",
      "        [4.3184e-04],\n",
      "        [1.0691e-03],\n",
      "        [3.5136e-05],\n",
      "        [3.6885e-02],\n",
      "        [1.4918e-03],\n",
      "        [1.6899e-03],\n",
      "        [2.5201e-04],\n",
      "        [4.7437e-05],\n",
      "        [1.5636e-04],\n",
      "        [1.5518e-03],\n",
      "        [6.1871e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.149382591247559: \n",
      "target probs tensor([[5.0909e-03],\n",
      "        [1.0982e-03],\n",
      "        [9.0752e-04],\n",
      "        [1.6167e-04],\n",
      "        [3.3687e-03],\n",
      "        [2.2856e-04],\n",
      "        [3.5149e-04],\n",
      "        [1.7513e-03],\n",
      "        [6.6769e-04],\n",
      "        [6.4197e-04],\n",
      "        [9.4536e-05],\n",
      "        [2.8510e-04],\n",
      "        [3.0292e-05],\n",
      "        [1.9201e-03],\n",
      "        [3.0488e-03],\n",
      "        [2.3299e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.300851821899414: \n",
      "target probs tensor([[8.8759e-04],\n",
      "        [8.4793e-05],\n",
      "        [1.1718e-03],\n",
      "        [1.0239e-04],\n",
      "        [1.4109e-05],\n",
      "        [1.3929e-03],\n",
      "        [7.2380e-04],\n",
      "        [9.8751e-04],\n",
      "        [2.8265e-03],\n",
      "        [1.0102e-03],\n",
      "        [3.8805e-05],\n",
      "        [7.0451e-04],\n",
      "        [1.0483e-03],\n",
      "        [1.8359e-03],\n",
      "        [7.3836e-04],\n",
      "        [2.0973e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.703595161437988: \n",
      "target probs tensor([[6.0134e-04],\n",
      "        [9.7664e-04],\n",
      "        [7.1553e-04],\n",
      "        [7.5030e-04],\n",
      "        [4.2679e-04],\n",
      "        [2.6132e-03],\n",
      "        [4.9317e-04],\n",
      "        [9.2245e-05],\n",
      "        [7.2385e-05],\n",
      "        [4.5237e-04],\n",
      "        [4.8871e-04],\n",
      "        [2.8856e-04],\n",
      "        [3.8418e-03],\n",
      "        [2.5703e-04],\n",
      "        [8.8326e-03],\n",
      "        [5.0035e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.410243034362793: \n",
      "target probs tensor([[2.5475e-04],\n",
      "        [5.8201e-04],\n",
      "        [4.7677e-04],\n",
      "        [4.3143e-04],\n",
      "        [2.5374e-04],\n",
      "        [7.3693e-04],\n",
      "        [6.3745e-04],\n",
      "        [1.1190e-04],\n",
      "        [6.0564e-04],\n",
      "        [1.0955e-03],\n",
      "        [7.1639e-03],\n",
      "        [1.1567e-03],\n",
      "        [4.7208e-04],\n",
      "        [2.0058e-04],\n",
      "        [3.4412e-04],\n",
      "        [7.7195e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.66326904296875: \n",
      "target probs tensor([[7.8491e-04],\n",
      "        [5.8990e-03],\n",
      "        [2.9164e-02],\n",
      "        [3.2589e-03],\n",
      "        [2.7243e-04],\n",
      "        [8.7569e-05],\n",
      "        [1.2843e-03],\n",
      "        [2.2176e-04],\n",
      "        [1.1007e-04],\n",
      "        [1.5748e-03],\n",
      "        [6.1036e-04],\n",
      "        [4.3447e-04],\n",
      "        [4.7240e-04],\n",
      "        [3.9697e-04],\n",
      "        [6.0524e-04],\n",
      "        [8.9501e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.174720287322998: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[0.0005],\n",
      "        [0.0004],\n",
      "        [0.0025],\n",
      "        [0.0003],\n",
      "        [0.0035],\n",
      "        [0.0121],\n",
      "        [0.0038],\n",
      "        [0.0238]], device='cuda:0'), loss: 6.113409996032715: \n",
      "target probs tensor([[0.0011],\n",
      "        [0.0003],\n",
      "        [0.0006],\n",
      "        [0.0041],\n",
      "        [0.0011],\n",
      "        [0.0030],\n",
      "        [0.0004],\n",
      "        [0.0013],\n",
      "        [0.0009],\n",
      "        [0.0008],\n",
      "        [0.0002],\n",
      "        [0.0022],\n",
      "        [0.0006],\n",
      "        [0.0005],\n",
      "        [0.0219],\n",
      "        [0.0009]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.857074737548828: \n",
      "target probs tensor([[3.9095e-04],\n",
      "        [6.7102e-04],\n",
      "        [1.3064e-03],\n",
      "        [7.0532e-04],\n",
      "        [2.1918e-03],\n",
      "        [3.1611e-04],\n",
      "        [9.1539e-05],\n",
      "        [7.9721e-04],\n",
      "        [4.2540e-03],\n",
      "        [3.1855e-06],\n",
      "        [3.7257e-04],\n",
      "        [4.0400e-04],\n",
      "        [3.4004e-04],\n",
      "        [6.2795e-05],\n",
      "        [2.1701e-04],\n",
      "        [2.7544e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.986701488494873: \n",
      "target probs tensor([[1.4237e-04],\n",
      "        [2.6794e-04],\n",
      "        [3.2872e-04],\n",
      "        [1.0073e-03],\n",
      "        [2.9252e-04],\n",
      "        [3.1515e-05],\n",
      "        [7.5961e-03],\n",
      "        [1.4464e-03],\n",
      "        [4.4181e-04],\n",
      "        [3.6894e-03],\n",
      "        [7.7389e-04],\n",
      "        [1.1793e-04],\n",
      "        [1.6744e-04],\n",
      "        [5.5935e-04],\n",
      "        [1.9026e-03],\n",
      "        [7.2021e-04]], device='cuda:0'), loss: 7.571497917175293: \n",
      "Better model found at epoch 88 with validation value: 0.7549999952316284.\n",
      "target probs tensor([[0.0022],\n",
      "        [0.0005],\n",
      "        [0.0011],\n",
      "        [0.0007],\n",
      "        [0.0012],\n",
      "        [0.0001],\n",
      "        [0.0018],\n",
      "        [0.0008],\n",
      "        [0.0002],\n",
      "        [0.0007],\n",
      "        [0.0005],\n",
      "        [0.0007],\n",
      "        [0.0015],\n",
      "        [0.0007],\n",
      "        [0.0018],\n",
      "        [0.0007]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.1979265213012695: \n",
      "target probs tensor([[1.3468e-03],\n",
      "        [7.5660e-03],\n",
      "        [1.1278e-04],\n",
      "        [7.6465e-04],\n",
      "        [1.4287e-03],\n",
      "        [3.0166e-04],\n",
      "        [5.1980e-04],\n",
      "        [1.3246e-02],\n",
      "        [1.0535e-03],\n",
      "        [2.1368e-04],\n",
      "        [2.3171e-02],\n",
      "        [5.5767e-05],\n",
      "        [3.3967e-04],\n",
      "        [1.7201e-03],\n",
      "        [1.8648e-03],\n",
      "        [1.9041e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.023331165313721: \n",
      "target probs tensor([[7.8306e-04],\n",
      "        [1.3149e-04],\n",
      "        [2.7822e-04],\n",
      "        [9.3559e-05],\n",
      "        [3.9509e-03],\n",
      "        [6.7236e-04],\n",
      "        [9.1515e-05],\n",
      "        [7.3822e-04],\n",
      "        [8.2147e-07],\n",
      "        [1.5531e-03],\n",
      "        [8.3775e-05],\n",
      "        [1.5709e-03],\n",
      "        [1.5921e-03],\n",
      "        [4.1175e-04],\n",
      "        [1.0266e-04],\n",
      "        [8.3136e-04]], device='cuda:0'), loss: 8.10869312286377: \n",
      "target probs tensor([[2.8623e-04],\n",
      "        [3.2486e-04],\n",
      "        [1.1606e-05],\n",
      "        [1.4357e-03],\n",
      "        [2.2994e-04],\n",
      "        [6.2360e-05],\n",
      "        [4.1116e-02],\n",
      "        [1.4819e-04],\n",
      "        [1.0339e-03],\n",
      "        [1.6450e-04],\n",
      "        [6.5937e-04],\n",
      "        [5.6387e-04],\n",
      "        [4.6337e-04],\n",
      "        [1.0560e-03],\n",
      "        [7.4937e-04],\n",
      "        [4.0149e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.756772994995117: \n",
      "target probs tensor([[0.0002],\n",
      "        [0.0009],\n",
      "        [0.0004],\n",
      "        [0.0006],\n",
      "        [0.0007],\n",
      "        [0.0002],\n",
      "        [0.0003],\n",
      "        [0.0003],\n",
      "        [0.0006],\n",
      "        [0.0014],\n",
      "        [0.0004],\n",
      "        [0.0002],\n",
      "        [0.0003],\n",
      "        [0.0028],\n",
      "        [0.0005],\n",
      "        [0.0004]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.67690896987915: \n",
      "target probs tensor([[3.2551e-06],\n",
      "        [1.0650e-03],\n",
      "        [2.3800e-04],\n",
      "        [1.8693e-04],\n",
      "        [3.9463e-03],\n",
      "        [3.4029e-03],\n",
      "        [5.0749e-04],\n",
      "        [3.7921e-04],\n",
      "        [4.3440e-04],\n",
      "        [5.7883e-04],\n",
      "        [5.5805e-04],\n",
      "        [3.5075e-04],\n",
      "        [3.3372e-04],\n",
      "        [1.2083e-04],\n",
      "        [1.0002e-01],\n",
      "        [9.9080e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.786437034606934: \n",
      "target probs tensor([[6.5360e-04],\n",
      "        [1.1190e-03],\n",
      "        [4.1498e-04],\n",
      "        [2.1305e-04],\n",
      "        [3.5794e-04],\n",
      "        [4.7580e-04],\n",
      "        [1.5141e-04],\n",
      "        [7.7640e-04],\n",
      "        [5.1010e-05],\n",
      "        [5.6807e-04],\n",
      "        [4.6348e-04],\n",
      "        [3.3466e-04],\n",
      "        [1.6855e-04],\n",
      "        [4.2458e-04],\n",
      "        [5.5572e-04],\n",
      "        [6.2479e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.892097473144531: \n",
      "target probs tensor([[2.5484e-04],\n",
      "        [2.4138e-03],\n",
      "        [1.0914e-03],\n",
      "        [5.1444e-04],\n",
      "        [2.9986e-02],\n",
      "        [2.1815e-04],\n",
      "        [8.8853e-03],\n",
      "        [5.6200e-04],\n",
      "        [1.7098e-03],\n",
      "        [2.6105e-06],\n",
      "        [7.9590e-04],\n",
      "        [2.8476e-04],\n",
      "        [5.5982e-04],\n",
      "        [6.3699e-04],\n",
      "        [3.0951e-04],\n",
      "        [2.2010e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.419661998748779: \n",
      "target probs tensor([[8.5442e-04],\n",
      "        [1.9765e-03],\n",
      "        [9.0475e-05],\n",
      "        [7.6508e-04],\n",
      "        [6.2766e-03],\n",
      "        [5.5672e-04],\n",
      "        [5.4157e-05],\n",
      "        [2.6611e-03],\n",
      "        [4.7470e-04],\n",
      "        [4.8133e-04],\n",
      "        [5.9425e-04],\n",
      "        [2.4354e-05],\n",
      "        [5.0406e-04],\n",
      "        [2.4278e-04],\n",
      "        [1.0809e-03],\n",
      "        [7.1174e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.589447021484375: \n",
      "target probs tensor([[1.9491e-04],\n",
      "        [4.9924e-03],\n",
      "        [6.7289e-05],\n",
      "        [2.1767e-03],\n",
      "        [1.7759e-04],\n",
      "        [3.1416e-04],\n",
      "        [3.8436e-04],\n",
      "        [6.5825e-04],\n",
      "        [1.8090e-03],\n",
      "        [1.6092e-04],\n",
      "        [3.5275e-05],\n",
      "        [3.2301e-04],\n",
      "        [2.1185e-04],\n",
      "        [5.2896e-04],\n",
      "        [1.6703e-04],\n",
      "        [1.9264e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.004176139831543: \n",
      "target probs tensor([[5.0552e-02],\n",
      "        [1.4458e-03],\n",
      "        [2.6561e-01],\n",
      "        [1.4251e-04],\n",
      "        [1.8924e-05],\n",
      "        [3.3757e-04],\n",
      "        [6.7841e-04],\n",
      "        [4.2685e-05],\n",
      "        [2.9438e-04],\n",
      "        [6.4521e-04],\n",
      "        [8.5279e-04],\n",
      "        [5.8657e-04],\n",
      "        [1.3875e-03],\n",
      "        [2.6385e-04],\n",
      "        [6.4546e-05],\n",
      "        [6.0631e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.362082004547119: \n",
      "target probs tensor([[0.0002],\n",
      "        [0.0003],\n",
      "        [0.0001],\n",
      "        [0.0032],\n",
      "        [0.0006],\n",
      "        [0.0005],\n",
      "        [0.0007],\n",
      "        [0.0012],\n",
      "        [0.0006],\n",
      "        [0.0027],\n",
      "        [0.0001],\n",
      "        [0.0022],\n",
      "        [0.0002],\n",
      "        [0.0066],\n",
      "        [0.0007],\n",
      "        [0.0004]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.346397399902344: \n",
      "target probs tensor([[7.0250e-04],\n",
      "        [4.7844e-04],\n",
      "        [2.3446e-04],\n",
      "        [8.0859e-04],\n",
      "        [4.5317e-04],\n",
      "        [1.1117e-03],\n",
      "        [1.2079e-04],\n",
      "        [1.7054e-03],\n",
      "        [2.6319e-04],\n",
      "        [7.1336e-05],\n",
      "        [2.0356e-03],\n",
      "        [9.3204e-04],\n",
      "        [2.9257e-04],\n",
      "        [1.0576e-03],\n",
      "        [7.6016e-06],\n",
      "        [1.6019e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.922590255737305: \n",
      "target probs tensor([[1.6992e-04],\n",
      "        [5.4404e-04],\n",
      "        [2.5611e-04],\n",
      "        [4.2404e-03],\n",
      "        [5.6117e-03],\n",
      "        [1.2981e-05],\n",
      "        [3.0582e-05],\n",
      "        [4.0633e-04],\n",
      "        [2.1799e-03],\n",
      "        [7.4379e-04],\n",
      "        [8.1715e-05],\n",
      "        [1.8031e-03],\n",
      "        [2.3344e-04],\n",
      "        [5.6753e-04],\n",
      "        [2.1771e-03],\n",
      "        [2.3517e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.74703311920166: \n",
      "target probs tensor([[2.0532e-03],\n",
      "        [9.2555e-04],\n",
      "        [1.2173e-03],\n",
      "        [1.9475e-04],\n",
      "        [7.2651e-05],\n",
      "        [3.2076e-04],\n",
      "        [2.7369e-04],\n",
      "        [4.9173e-04],\n",
      "        [9.5567e-04],\n",
      "        [3.4248e-04],\n",
      "        [9.3742e-04],\n",
      "        [4.2089e-06],\n",
      "        [1.2293e-03],\n",
      "        [7.8519e-04],\n",
      "        [3.9160e-04],\n",
      "        [4.4307e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.845319747924805: \n",
      "target probs tensor([[8.2523e-04],\n",
      "        [6.7332e-04],\n",
      "        [1.8858e-04],\n",
      "        [2.7998e-01],\n",
      "        [6.8429e-04],\n",
      "        [2.8125e-04],\n",
      "        [3.8004e-04],\n",
      "        [1.1143e-03],\n",
      "        [5.8689e-04],\n",
      "        [2.1403e-04],\n",
      "        [3.3000e-04],\n",
      "        [2.1087e-04],\n",
      "        [3.6358e-05],\n",
      "        [1.2023e-01],\n",
      "        [5.5753e-04],\n",
      "        [2.3418e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.184563636779785: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[2.5830e-04],\n",
      "        [8.4074e-05],\n",
      "        [1.1665e-03],\n",
      "        [4.0518e-04],\n",
      "        [2.1872e-04],\n",
      "        [7.0987e-04],\n",
      "        [5.9397e-04],\n",
      "        [1.9095e-03],\n",
      "        [5.7647e-04],\n",
      "        [4.0663e-04],\n",
      "        [4.0000e-05],\n",
      "        [1.1492e-03],\n",
      "        [1.1647e-03],\n",
      "        [2.1304e-04],\n",
      "        [5.9551e-04],\n",
      "        [2.0351e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.660748481750488: \n",
      "target probs tensor([[3.0066e-04],\n",
      "        [3.3039e-04],\n",
      "        [1.4769e-03],\n",
      "        [1.8693e-03],\n",
      "        [7.7840e-05],\n",
      "        [5.6728e-05],\n",
      "        [2.4388e-03],\n",
      "        [8.8187e-02],\n",
      "        [6.0377e-04],\n",
      "        [2.1991e-02],\n",
      "        [1.7747e-03],\n",
      "        [1.1582e-03],\n",
      "        [1.0961e-04],\n",
      "        [5.0123e-04],\n",
      "        [2.2834e-04],\n",
      "        [1.6198e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.0286865234375: \n",
      "target probs tensor([[1.6182e-04],\n",
      "        [4.9355e-05],\n",
      "        [2.1802e-03],\n",
      "        [2.4827e-03],\n",
      "        [4.1822e-04],\n",
      "        [2.8070e-04],\n",
      "        [4.0932e-04],\n",
      "        [4.6466e-03],\n",
      "        [2.5280e-04],\n",
      "        [1.7239e-04],\n",
      "        [1.3492e-03],\n",
      "        [9.6996e-04],\n",
      "        [4.3137e-04],\n",
      "        [9.7766e-04],\n",
      "        [1.0610e-04],\n",
      "        [3.0939e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.64429235458374: \n",
      "target probs tensor([[1.4581e-03],\n",
      "        [5.0326e-04],\n",
      "        [1.1572e-03],\n",
      "        [4.4661e-04],\n",
      "        [9.0170e-05],\n",
      "        [1.1035e-04],\n",
      "        [2.2226e-04],\n",
      "        [1.3431e-04],\n",
      "        [5.9012e-04],\n",
      "        [8.4894e-04],\n",
      "        [7.1276e-02],\n",
      "        [5.8437e-04],\n",
      "        [6.4226e-04],\n",
      "        [2.6414e-04],\n",
      "        [3.8418e-03],\n",
      "        [1.0090e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.31227445602417: \n",
      "target probs tensor([[2.0136e-03],\n",
      "        [7.8329e-04],\n",
      "        [7.1853e-03],\n",
      "        [5.2110e-04],\n",
      "        [3.5646e-03],\n",
      "        [4.5526e-03],\n",
      "        [9.2647e-04],\n",
      "        [6.3734e-04],\n",
      "        [1.3573e-04],\n",
      "        [3.1220e-04],\n",
      "        [1.0321e-04],\n",
      "        [5.5837e-02],\n",
      "        [7.2717e-04],\n",
      "        [6.7764e-05],\n",
      "        [1.4186e-03],\n",
      "        [3.2018e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.981096267700195: \n",
      "target probs tensor([[0.0007],\n",
      "        [0.0059],\n",
      "        [0.0026],\n",
      "        [0.0004],\n",
      "        [0.0003],\n",
      "        [0.0002],\n",
      "        [0.0022],\n",
      "        [0.0002]], device='cuda:0'), loss: 7.2092437744140625: \n",
      "target probs tensor([[4.3094e-04],\n",
      "        [1.6190e-04],\n",
      "        [1.2946e-03],\n",
      "        [7.0249e-05],\n",
      "        [8.4455e-04],\n",
      "        [2.5197e-02],\n",
      "        [3.5547e-04],\n",
      "        [1.2464e-05],\n",
      "        [2.4024e-04],\n",
      "        [1.1804e-03],\n",
      "        [5.2112e-03],\n",
      "        [1.2697e-04],\n",
      "        [3.3293e-04],\n",
      "        [4.5943e-05],\n",
      "        [1.0946e-03],\n",
      "        [1.1943e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.864602565765381: \n",
      "target probs tensor([[2.0431e-03],\n",
      "        [4.7202e-04],\n",
      "        [9.8166e-04],\n",
      "        [1.0323e-02],\n",
      "        [3.3075e-04],\n",
      "        [7.0759e-04],\n",
      "        [2.4241e-03],\n",
      "        [1.6518e-04],\n",
      "        [2.1073e-03],\n",
      "        [8.6959e-04],\n",
      "        [7.7263e-05],\n",
      "        [2.6308e-04],\n",
      "        [3.7412e-04],\n",
      "        [3.6636e-04],\n",
      "        [1.2084e-03],\n",
      "        [1.3329e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.213301658630371: \n",
      "target probs tensor([[0.0001],\n",
      "        [0.0040],\n",
      "        [0.0004],\n",
      "        [0.0014],\n",
      "        [0.0002],\n",
      "        [0.0001],\n",
      "        [0.0009],\n",
      "        [0.0002],\n",
      "        [0.0009],\n",
      "        [0.0006],\n",
      "        [0.0002],\n",
      "        [0.0001],\n",
      "        [0.0005],\n",
      "        [0.0004],\n",
      "        [0.0005],\n",
      "        [0.0004]], device='cuda:0'), loss: 7.827152252197266: \n",
      "target probs tensor([[1.7853e-05],\n",
      "        [5.1746e-05],\n",
      "        [6.5118e-04],\n",
      "        [1.7676e-03],\n",
      "        [1.0046e-03],\n",
      "        [6.4021e-04],\n",
      "        [2.8832e-04],\n",
      "        [1.4890e-03],\n",
      "        [1.5682e-06],\n",
      "        [2.1015e-04],\n",
      "        [1.5250e-04],\n",
      "        [8.8695e-05],\n",
      "        [1.2890e-02],\n",
      "        [2.6434e-04],\n",
      "        [1.1131e-04],\n",
      "        [4.6332e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.294819831848145: \n",
      "target probs tensor([[8.2199e-03],\n",
      "        [8.9066e-05],\n",
      "        [1.0595e-02],\n",
      "        [4.0695e-05],\n",
      "        [4.2533e-04],\n",
      "        [7.2724e-04],\n",
      "        [2.3692e-03],\n",
      "        [5.6961e-04],\n",
      "        [7.3132e-04],\n",
      "        [1.1251e-03],\n",
      "        [5.8242e-04],\n",
      "        [1.0907e-03],\n",
      "        [5.9448e-04],\n",
      "        [1.2430e-03],\n",
      "        [2.2422e-03],\n",
      "        [5.7416e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.078094482421875: \n",
      "target probs tensor([[1.0867e-03],\n",
      "        [1.1668e-03],\n",
      "        [2.3232e-02],\n",
      "        [2.4720e-05],\n",
      "        [5.9789e-04],\n",
      "        [7.1517e-04],\n",
      "        [4.5671e-04],\n",
      "        [9.0436e-04],\n",
      "        [7.7853e-07],\n",
      "        [1.5092e-03],\n",
      "        [1.9220e-03],\n",
      "        [3.4832e-04],\n",
      "        [1.0795e-03],\n",
      "        [5.5846e-04],\n",
      "        [6.7840e-05],\n",
      "        [3.3197e-04]], device='cuda:0'), loss: 7.7513861656188965: \n",
      "Better model found at epoch 97 with validation value: 0.7570000290870667.\n",
      "target probs tensor([[1.6076e-04],\n",
      "        [7.2563e-05],\n",
      "        [5.7908e-04],\n",
      "        [1.2175e-05],\n",
      "        [4.0980e-04],\n",
      "        [1.0251e-03],\n",
      "        [2.0528e-01],\n",
      "        [2.8224e-04],\n",
      "        [4.8280e-04],\n",
      "        [3.6104e-04],\n",
      "        [3.6492e-04],\n",
      "        [4.3642e-04],\n",
      "        [1.3390e-05],\n",
      "        [8.6818e-04],\n",
      "        [2.8998e-04],\n",
      "        [3.3420e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.944398403167725: \n",
      "target probs tensor([[2.7378e-03],\n",
      "        [8.5615e-06],\n",
      "        [3.7044e-03],\n",
      "        [4.9945e-03],\n",
      "        [8.1228e-04],\n",
      "        [1.3780e-05],\n",
      "        [8.5384e-05],\n",
      "        [5.3748e-04],\n",
      "        [3.6612e-01],\n",
      "        [8.9647e-04],\n",
      "        [6.5755e-04],\n",
      "        [5.8780e-04],\n",
      "        [1.4874e-04],\n",
      "        [1.0868e-03],\n",
      "        [1.6391e-04],\n",
      "        [2.9121e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.434676170349121: \n",
      "target probs tensor([[1.4599e-03],\n",
      "        [5.7002e-05],\n",
      "        [1.1409e-06],\n",
      "        [9.8259e-04],\n",
      "        [5.2890e-05],\n",
      "        [1.1872e-03],\n",
      "        [8.0522e-04],\n",
      "        [4.0477e-05],\n",
      "        [3.7652e-04],\n",
      "        [7.3306e-04],\n",
      "        [2.6677e-03],\n",
      "        [5.1368e-04],\n",
      "        [8.7645e-04],\n",
      "        [1.9302e-03],\n",
      "        [6.5125e-04],\n",
      "        [1.2204e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.060882568359375: \n",
      "target probs tensor([[5.1569e-03],\n",
      "        [1.6255e-03],\n",
      "        [3.0395e-04],\n",
      "        [9.9657e-04],\n",
      "        [2.4524e-04],\n",
      "        [3.6948e-04],\n",
      "        [2.5643e-02],\n",
      "        [2.6784e-04],\n",
      "        [2.4485e-04],\n",
      "        [1.0044e-02],\n",
      "        [3.6176e-04],\n",
      "        [1.5385e-03],\n",
      "        [3.3902e-04],\n",
      "        [5.5592e-04],\n",
      "        [2.8674e-05],\n",
      "        [5.4450e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.511638641357422: \n",
      "target probs tensor([[5.8874e-05],\n",
      "        [1.6199e-04],\n",
      "        [3.8873e-04],\n",
      "        [7.1369e-05],\n",
      "        [9.8931e-04],\n",
      "        [8.2150e-04],\n",
      "        [7.2237e-03],\n",
      "        [2.3549e-03],\n",
      "        [2.7698e-04],\n",
      "        [2.7319e-04],\n",
      "        [7.7194e-05],\n",
      "        [4.1785e-04],\n",
      "        [1.0643e-04],\n",
      "        [7.1043e-04],\n",
      "        [4.7854e-04],\n",
      "        [2.5464e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.783430099487305: \n",
      "target probs tensor([[6.3036e-04],\n",
      "        [1.4112e-04],\n",
      "        [3.9328e-04],\n",
      "        [6.9075e-04],\n",
      "        [1.1317e-03],\n",
      "        [2.6407e-06],\n",
      "        [8.5187e-04],\n",
      "        [4.6932e-04],\n",
      "        [5.4436e-05],\n",
      "        [5.9567e-04],\n",
      "        [2.4456e-04],\n",
      "        [4.2835e-04],\n",
      "        [1.0411e-03],\n",
      "        [5.2675e-04],\n",
      "        [4.9584e-04],\n",
      "        [6.3460e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.026161193847656: \n",
      "target probs tensor([[5.8484e-05],\n",
      "        [4.7041e-04],\n",
      "        [1.3562e-03],\n",
      "        [8.2499e-04],\n",
      "        [3.0675e-04],\n",
      "        [4.6818e-04],\n",
      "        [4.3971e-04],\n",
      "        [3.8105e-03],\n",
      "        [7.7928e-04],\n",
      "        [2.9784e-05],\n",
      "        [5.2243e-02],\n",
      "        [1.8142e-01],\n",
      "        [5.7547e-05],\n",
      "        [6.9067e-03],\n",
      "        [2.5119e-04],\n",
      "        [2.7666e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.101568698883057: \n",
      "target probs tensor([[1.2331e-03],\n",
      "        [1.1468e-03],\n",
      "        [7.7956e-04],\n",
      "        [5.2086e-04],\n",
      "        [7.8406e-04],\n",
      "        [1.1809e-03],\n",
      "        [5.6276e-04],\n",
      "        [2.5208e-03],\n",
      "        [1.4004e-03],\n",
      "        [4.2528e-04],\n",
      "        [4.6092e-04],\n",
      "        [5.8153e-03],\n",
      "        [1.3115e-04],\n",
      "        [1.1722e-03],\n",
      "        [2.0884e-04],\n",
      "        [1.9059e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.502497673034668: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[2.0870e-04],\n",
      "        [3.8558e-04],\n",
      "        [6.8056e-04],\n",
      "        [3.3595e-04],\n",
      "        [3.2323e-03],\n",
      "        [1.1341e-04],\n",
      "        [1.3900e-03],\n",
      "        [6.9402e-04],\n",
      "        [4.7253e-04],\n",
      "        [3.3671e-04],\n",
      "        [9.7508e-06],\n",
      "        [1.1101e-03],\n",
      "        [4.7331e-04],\n",
      "        [3.8101e-04],\n",
      "        [2.8780e-04],\n",
      "        [1.9518e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.619393348693848: \n",
      "target probs tensor([[1.0718e-02],\n",
      "        [7.3544e-04],\n",
      "        [1.1122e-03],\n",
      "        [9.7587e-04],\n",
      "        [5.6824e-04],\n",
      "        [8.0160e-04],\n",
      "        [6.8948e-02],\n",
      "        [4.1681e-04],\n",
      "        [9.7653e-04],\n",
      "        [5.7188e-06],\n",
      "        [5.9619e-04],\n",
      "        [7.4653e-05],\n",
      "        [3.9286e-03],\n",
      "        [1.7113e-04],\n",
      "        [1.8806e-02],\n",
      "        [1.1567e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.963929176330566: \n",
      "target probs tensor([[0.0018],\n",
      "        [0.0002],\n",
      "        [0.0031],\n",
      "        [0.0009],\n",
      "        [0.0039],\n",
      "        [0.0001],\n",
      "        [0.0002],\n",
      "        [0.0010],\n",
      "        [0.0014],\n",
      "        [0.0016],\n",
      "        [0.0015],\n",
      "        [0.0008],\n",
      "        [0.0040],\n",
      "        [0.0006],\n",
      "        [0.0002],\n",
      "        [0.0005]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.102389335632324: \n",
      "target probs tensor([[6.5633e-03],\n",
      "        [7.6386e-05],\n",
      "        [5.5309e-04],\n",
      "        [1.7903e-03],\n",
      "        [2.4508e-04],\n",
      "        [9.2589e-04],\n",
      "        [2.4901e-04],\n",
      "        [1.2333e-04],\n",
      "        [3.1048e-04],\n",
      "        [6.6962e-03],\n",
      "        [2.4613e-02],\n",
      "        [2.5890e-03],\n",
      "        [3.1131e-03],\n",
      "        [7.4731e-04],\n",
      "        [1.6985e-01],\n",
      "        [3.4738e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.504984378814697: \n",
      "target probs tensor([[0.0005],\n",
      "        [0.0004],\n",
      "        [0.0008],\n",
      "        [0.0004],\n",
      "        [0.0002],\n",
      "        [0.0006],\n",
      "        [0.0005],\n",
      "        [0.0003],\n",
      "        [0.0002],\n",
      "        [0.0011],\n",
      "        [0.0003],\n",
      "        [0.0001],\n",
      "        [0.0003],\n",
      "        [0.0021],\n",
      "        [0.0009],\n",
      "        [0.0008]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.684823036193848: \n",
      "target probs tensor([[2.4553e-05],\n",
      "        [7.4864e-03],\n",
      "        [3.9858e-03],\n",
      "        [3.6195e-03],\n",
      "        [9.4896e-04],\n",
      "        [1.5330e-03],\n",
      "        [1.3823e-03],\n",
      "        [6.6322e-05],\n",
      "        [1.0814e-03],\n",
      "        [8.6752e-04],\n",
      "        [4.0308e-03],\n",
      "        [3.4007e-02],\n",
      "        [2.3308e-04],\n",
      "        [1.0071e-03],\n",
      "        [9.5732e-04],\n",
      "        [2.0998e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.860029220581055: \n",
      "target probs tensor([[9.8474e-07],\n",
      "        [1.8562e-03],\n",
      "        [1.7870e-04],\n",
      "        [7.2969e-05],\n",
      "        [3.2776e-03],\n",
      "        [1.3683e-03],\n",
      "        [1.8210e-04],\n",
      "        [3.2476e-04],\n",
      "        [2.4481e-03],\n",
      "        [6.6656e-05],\n",
      "        [6.3061e-04],\n",
      "        [5.8487e-04],\n",
      "        [4.9464e-04],\n",
      "        [1.0979e-04],\n",
      "        [4.4640e-05],\n",
      "        [7.3941e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.370793342590332: \n",
      "target probs tensor([[1.9000e-02],\n",
      "        [2.3652e-04],\n",
      "        [4.1803e-04],\n",
      "        [1.0885e-03],\n",
      "        [4.7732e-05],\n",
      "        [6.1902e-04],\n",
      "        [1.2928e-03],\n",
      "        [3.0715e-04],\n",
      "        [3.2984e-02],\n",
      "        [1.2221e-03],\n",
      "        [5.1550e-03],\n",
      "        [7.0353e-04],\n",
      "        [4.6094e-04],\n",
      "        [5.7652e-04],\n",
      "        [4.9208e-03],\n",
      "        [1.2864e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.796818733215332: \n",
      "target probs tensor([[1.0911e-03],\n",
      "        [5.3035e-04],\n",
      "        [5.3618e-04],\n",
      "        [9.9814e-05],\n",
      "        [9.3206e-04],\n",
      "        [2.6796e-04],\n",
      "        [1.9493e-04],\n",
      "        [3.6739e-04],\n",
      "        [2.8364e-05],\n",
      "        [7.2262e-04],\n",
      "        [4.9216e-05],\n",
      "        [1.8798e-04],\n",
      "        [6.0434e-05],\n",
      "        [7.9571e-04],\n",
      "        [1.4395e-01],\n",
      "        [1.1996e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.77977180480957: \n",
      "target probs tensor([[3.6843e-04],\n",
      "        [9.8340e-05],\n",
      "        [6.5019e-04],\n",
      "        [3.6692e-04],\n",
      "        [6.5704e-04],\n",
      "        [8.9699e-04],\n",
      "        [3.4492e-04],\n",
      "        [3.3281e-06],\n",
      "        [1.7562e-04],\n",
      "        [8.5016e-04],\n",
      "        [3.1252e-03],\n",
      "        [1.7515e-01],\n",
      "        [1.4106e-04],\n",
      "        [3.2149e-02],\n",
      "        [2.5884e-04],\n",
      "        [1.5976e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.202420711517334: \n",
      "target probs tensor([[0.0006],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0018],\n",
      "        [0.0005],\n",
      "        [0.0001],\n",
      "        [0.0008],\n",
      "        [0.0001]], device='cuda:0'), loss: 7.451953411102295: \n",
      "target probs tensor([[2.4740e-03],\n",
      "        [8.5752e-04],\n",
      "        [7.5100e-04],\n",
      "        [3.3150e-04],\n",
      "        [9.9128e-05],\n",
      "        [1.8596e-03],\n",
      "        [8.6586e-04],\n",
      "        [1.1563e-04],\n",
      "        [7.2394e-04],\n",
      "        [1.1675e-04],\n",
      "        [6.6813e-04],\n",
      "        [1.4234e-04],\n",
      "        [5.6521e-04],\n",
      "        [1.5894e-03],\n",
      "        [5.5019e-04],\n",
      "        [3.3704e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.610661506652832: \n",
      "target probs tensor([[1.6326e-03],\n",
      "        [1.1139e-04],\n",
      "        [2.6130e-05],\n",
      "        [3.1273e-04],\n",
      "        [9.3502e-04],\n",
      "        [8.4004e-05],\n",
      "        [1.8939e-03],\n",
      "        [6.6861e-05],\n",
      "        [7.1082e-04],\n",
      "        [1.8500e-06],\n",
      "        [5.2717e-04],\n",
      "        [9.0943e-04],\n",
      "        [8.0802e-04],\n",
      "        [3.5203e-04],\n",
      "        [2.3814e-04],\n",
      "        [2.8149e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.310941696166992: \n",
      "target probs tensor([[9.1207e-04],\n",
      "        [1.2918e-03],\n",
      "        [3.7581e-05],\n",
      "        [4.5331e-04],\n",
      "        [3.7739e-03],\n",
      "        [1.1558e-04],\n",
      "        [6.7891e-03],\n",
      "        [1.1156e-04],\n",
      "        [6.5760e-04],\n",
      "        [2.0296e-03],\n",
      "        [3.9151e-04],\n",
      "        [2.3722e-04],\n",
      "        [6.9223e-04],\n",
      "        [1.0790e-03],\n",
      "        [1.7905e-03],\n",
      "        [4.2331e-04]], device='cuda:0'), loss: 7.387307167053223: \n",
      "target probs tensor([[8.3064e-04],\n",
      "        [3.4263e-04],\n",
      "        [8.8461e-05],\n",
      "        [1.2626e-03],\n",
      "        [7.4153e-04],\n",
      "        [5.7418e-04],\n",
      "        [5.0682e-04],\n",
      "        [2.9514e-03],\n",
      "        [7.8735e-02],\n",
      "        [1.7792e-05],\n",
      "        [1.2830e-03],\n",
      "        [4.1300e-04],\n",
      "        [4.1595e-04],\n",
      "        [2.3338e-04],\n",
      "        [5.3284e-04],\n",
      "        [5.9488e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.38770055770874: \n",
      "target probs tensor([[1.0309e-03],\n",
      "        [7.3229e-04],\n",
      "        [3.5574e-04],\n",
      "        [3.5959e-04],\n",
      "        [2.9239e-04],\n",
      "        [3.8068e-04],\n",
      "        [9.4991e-06],\n",
      "        [4.4617e-03],\n",
      "        [1.1620e-04],\n",
      "        [3.4030e-03],\n",
      "        [1.1126e-03],\n",
      "        [2.4842e-03],\n",
      "        [1.7527e-04],\n",
      "        [4.8255e-04],\n",
      "        [2.9741e-03],\n",
      "        [1.9599e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.57118558883667: \n",
      "target probs tensor([[5.1826e-04],\n",
      "        [8.1631e-03],\n",
      "        [1.4463e-02],\n",
      "        [3.1246e-03],\n",
      "        [8.9387e-04],\n",
      "        [3.5204e-04],\n",
      "        [4.7555e-04],\n",
      "        [3.2847e-03],\n",
      "        [3.2195e-06],\n",
      "        [1.0951e-02],\n",
      "        [3.3253e-04],\n",
      "        [2.3402e-04],\n",
      "        [2.8867e-03],\n",
      "        [2.3198e-04],\n",
      "        [2.1952e-04],\n",
      "        [7.5915e-04]], device='cuda:0'), loss: 7.129523277282715: \n",
      "target probs tensor([[1.1863e-03],\n",
      "        [8.1186e-04],\n",
      "        [4.8201e-05],\n",
      "        [2.3355e-03],\n",
      "        [3.2093e-04],\n",
      "        [5.4562e-03],\n",
      "        [1.1653e-03],\n",
      "        [1.4041e-03],\n",
      "        [4.0626e-05],\n",
      "        [3.6540e-04],\n",
      "        [2.3795e-04],\n",
      "        [7.7486e-04],\n",
      "        [1.3428e-03],\n",
      "        [1.3411e-03],\n",
      "        [3.8534e-04],\n",
      "        [7.5829e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.389761447906494: \n",
      "target probs tensor([[4.2873e-04],\n",
      "        [4.3633e-04],\n",
      "        [3.9484e-04],\n",
      "        [3.9141e-03],\n",
      "        [3.9420e-04],\n",
      "        [7.4178e-04],\n",
      "        [2.7117e-02],\n",
      "        [4.0246e-03],\n",
      "        [7.1148e-05],\n",
      "        [1.7258e-04],\n",
      "        [6.7118e-04],\n",
      "        [1.1626e-07],\n",
      "        [1.6613e-04],\n",
      "        [4.3386e-03],\n",
      "        [5.3787e-04],\n",
      "        [7.6822e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.710671424865723: \n",
      "target probs tensor([[5.2324e-04],\n",
      "        [5.3215e-04],\n",
      "        [2.3803e-02],\n",
      "        [3.8816e-03],\n",
      "        [2.4512e-02],\n",
      "        [3.1455e-03],\n",
      "        [1.3193e-03],\n",
      "        [1.1489e-03],\n",
      "        [1.0026e-03],\n",
      "        [1.0831e-03],\n",
      "        [1.4902e-03],\n",
      "        [1.0465e-02],\n",
      "        [5.0432e-04],\n",
      "        [2.6499e-04],\n",
      "        [1.6477e-03],\n",
      "        [9.4662e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.616215705871582: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 106 with validation value: 0.7580000162124634.\n",
      "target probs tensor([[1.5001e-03],\n",
      "        [1.5212e-04],\n",
      "        [1.5988e-03],\n",
      "        [1.8933e-03],\n",
      "        [1.0308e-03],\n",
      "        [1.2154e-03],\n",
      "        [1.6699e-04],\n",
      "        [8.3615e-04],\n",
      "        [4.8579e-02],\n",
      "        [1.6465e-04],\n",
      "        [6.9561e-05],\n",
      "        [2.0012e-03],\n",
      "        [5.9056e-04],\n",
      "        [5.7808e-04],\n",
      "        [1.1055e-03],\n",
      "        [9.0442e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.100305557250977: \n",
      "target probs tensor([[1.1167e-03],\n",
      "        [1.1118e-05],\n",
      "        [9.2078e-04],\n",
      "        [3.6988e-03],\n",
      "        [6.9034e-05],\n",
      "        [1.7466e-04],\n",
      "        [1.1060e-03],\n",
      "        [4.7801e-05],\n",
      "        [7.2518e-03],\n",
      "        [9.3184e-04],\n",
      "        [7.4195e-04],\n",
      "        [2.6683e-04],\n",
      "        [2.7269e-04],\n",
      "        [2.2554e-04],\n",
      "        [2.8314e-05],\n",
      "        [3.9075e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.002948760986328: \n",
      "target probs tensor([[4.2610e-04],\n",
      "        [3.1020e-05],\n",
      "        [3.4101e-03],\n",
      "        [8.7378e-04],\n",
      "        [1.6426e-04],\n",
      "        [1.3678e-04],\n",
      "        [2.1887e-03],\n",
      "        [1.3313e-03],\n",
      "        [8.8831e-04],\n",
      "        [1.1174e-03],\n",
      "        [1.1174e-03],\n",
      "        [3.5232e-05],\n",
      "        [4.9200e-04],\n",
      "        [1.9617e-03],\n",
      "        [6.0957e-04],\n",
      "        [3.5787e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.436402320861816: \n",
      "target probs tensor([[4.3243e-04],\n",
      "        [5.1027e-05],\n",
      "        [2.4885e-04],\n",
      "        [2.8493e-05],\n",
      "        [7.3943e-04],\n",
      "        [1.3910e-04],\n",
      "        [2.1494e-03],\n",
      "        [1.6160e-03],\n",
      "        [2.7058e-03],\n",
      "        [6.0268e-04],\n",
      "        [9.3210e-04],\n",
      "        [2.2962e-04],\n",
      "        [3.6896e-03],\n",
      "        [1.2899e-03],\n",
      "        [7.1137e-04],\n",
      "        [3.3197e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.5782470703125: \n",
      "target probs tensor([[9.4747e-04],\n",
      "        [2.6417e-04],\n",
      "        [3.0662e-03],\n",
      "        [9.1259e-04],\n",
      "        [9.7048e-04],\n",
      "        [3.2072e-04],\n",
      "        [1.3463e-05],\n",
      "        [1.8026e-04],\n",
      "        [1.4819e-04],\n",
      "        [4.9780e-05],\n",
      "        [1.1402e-03],\n",
      "        [1.1889e-03],\n",
      "        [2.7840e-04],\n",
      "        [2.0738e-03],\n",
      "        [9.7596e-04],\n",
      "        [2.0119e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.659272193908691: \n",
      "target probs tensor([[0.0001],\n",
      "        [0.0002],\n",
      "        [0.0008],\n",
      "        [0.0014],\n",
      "        [0.0006],\n",
      "        [0.0007],\n",
      "        [0.0061],\n",
      "        [0.0012],\n",
      "        [0.0023],\n",
      "        [0.0003],\n",
      "        [0.0004],\n",
      "        [0.0021],\n",
      "        [0.0002],\n",
      "        [0.0001],\n",
      "        [0.0005],\n",
      "        [0.0007]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.419228553771973: \n",
      "target probs tensor([[1.5939e-04],\n",
      "        [9.6723e-05],\n",
      "        [9.5085e-04],\n",
      "        [4.0700e-03],\n",
      "        [4.5690e-04],\n",
      "        [2.1414e-03],\n",
      "        [1.6377e-03],\n",
      "        [1.0623e-03],\n",
      "        [2.8413e-04],\n",
      "        [1.3592e-03],\n",
      "        [1.3837e-03],\n",
      "        [6.0255e-03],\n",
      "        [2.7330e-04],\n",
      "        [2.2618e-04],\n",
      "        [4.0162e-04],\n",
      "        [6.2974e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.237520217895508: \n",
      "target probs tensor([[1.1801e-03],\n",
      "        [6.0367e-04],\n",
      "        [6.3388e-04],\n",
      "        [6.1186e-04],\n",
      "        [1.2780e-02],\n",
      "        [1.8180e-04],\n",
      "        [2.1781e-04],\n",
      "        [4.5629e-04],\n",
      "        [2.7937e-03],\n",
      "        [9.7854e-04],\n",
      "        [1.0030e-03],\n",
      "        [5.7545e-04],\n",
      "        [4.6028e-05],\n",
      "        [3.0381e-04],\n",
      "        [9.0224e-05],\n",
      "        [6.7021e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.493468284606934: \n",
      "target probs tensor([[7.7194e-04],\n",
      "        [3.8628e-04],\n",
      "        [8.0358e-05],\n",
      "        [1.3548e-03],\n",
      "        [3.4611e-04],\n",
      "        [1.3705e-03],\n",
      "        [4.3323e-05],\n",
      "        [4.1844e-04],\n",
      "        [7.1746e-06],\n",
      "        [7.7500e-04],\n",
      "        [3.7782e-04],\n",
      "        [5.3689e-03],\n",
      "        [1.4876e-03],\n",
      "        [1.9980e-04],\n",
      "        [2.7651e-01],\n",
      "        [5.3554e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.606918811798096: \n",
      "target probs tensor([[3.9388e-04],\n",
      "        [2.8233e-06],\n",
      "        [1.2542e-03],\n",
      "        [7.9900e-04],\n",
      "        [7.7684e-04],\n",
      "        [8.4411e-04],\n",
      "        [5.3639e-04],\n",
      "        [3.1414e-04],\n",
      "        [3.6326e-05],\n",
      "        [2.0994e-03],\n",
      "        [2.1271e-02],\n",
      "        [3.4219e-04],\n",
      "        [3.6083e-04],\n",
      "        [3.5923e-04],\n",
      "        [2.8040e-03],\n",
      "        [2.8482e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.648930072784424: \n",
      "target probs tensor([[0.0030],\n",
      "        [0.0041],\n",
      "        [0.0001],\n",
      "        [0.0014],\n",
      "        [0.0003],\n",
      "        [0.0009],\n",
      "        [0.0014],\n",
      "        [0.0008],\n",
      "        [0.0012],\n",
      "        [0.0001],\n",
      "        [0.0001],\n",
      "        [0.0009],\n",
      "        [0.0014],\n",
      "        [0.0002],\n",
      "        [0.0003],\n",
      "        [0.0007]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.387997627258301: \n",
      "target probs tensor([[7.5826e-03],\n",
      "        [2.8302e-02],\n",
      "        [7.0142e-04],\n",
      "        [7.9601e-03],\n",
      "        [1.2003e-03],\n",
      "        [1.0306e-03],\n",
      "        [4.2454e-04],\n",
      "        [2.2861e-04],\n",
      "        [2.4316e-03],\n",
      "        [1.6821e-04],\n",
      "        [4.7456e-04],\n",
      "        [2.0700e-04],\n",
      "        [3.3381e-04],\n",
      "        [1.1446e-03],\n",
      "        [3.9069e-05],\n",
      "        [1.0691e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.056715965270996: \n",
      "target probs tensor([[6.5010e-04],\n",
      "        [6.7321e-04],\n",
      "        [5.2052e-06],\n",
      "        [3.3120e-04],\n",
      "        [4.5171e-02],\n",
      "        [1.0613e-01],\n",
      "        [2.6659e-04],\n",
      "        [1.5827e-05],\n",
      "        [2.9978e-04],\n",
      "        [8.7832e-04],\n",
      "        [2.1914e-04],\n",
      "        [6.4914e-04],\n",
      "        [1.7980e-03],\n",
      "        [7.1115e-03],\n",
      "        [1.0021e-05],\n",
      "        [1.9441e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.749134540557861: \n",
      "target probs tensor([[1.7037e-03],\n",
      "        [1.9805e-03],\n",
      "        [1.8666e-03],\n",
      "        [9.4799e-04],\n",
      "        [8.0449e-04],\n",
      "        [5.6633e-05],\n",
      "        [1.4242e-04],\n",
      "        [2.2982e-03],\n",
      "        [8.8984e-04],\n",
      "        [2.9074e-06],\n",
      "        [4.2886e-04],\n",
      "        [7.5247e-05],\n",
      "        [7.1417e-04],\n",
      "        [2.2810e-04],\n",
      "        [6.4981e-04],\n",
      "        [2.2346e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.592040538787842: \n",
      "target probs tensor([[2.6167e-04],\n",
      "        [2.5149e-04],\n",
      "        [1.1395e-04],\n",
      "        [1.3997e-03],\n",
      "        [2.2937e-04],\n",
      "        [1.0829e-04],\n",
      "        [1.0890e-03],\n",
      "        [4.2440e-04],\n",
      "        [5.6611e-05],\n",
      "        [8.9092e-04],\n",
      "        [6.9067e-04],\n",
      "        [6.5259e-07],\n",
      "        [1.7417e-04],\n",
      "        [1.5157e-03],\n",
      "        [1.0717e-03],\n",
      "        [2.2552e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.455877304077148: \n",
      "target probs tensor([[3.6396e-04],\n",
      "        [4.4927e-04],\n",
      "        [1.6033e-03],\n",
      "        [6.5078e-04],\n",
      "        [1.3530e-03],\n",
      "        [1.2516e-04],\n",
      "        [8.7650e-03],\n",
      "        [8.9520e-05]], device='cuda:0'), loss: 7.381082534790039: \n",
      "target probs tensor([[6.8027e-04],\n",
      "        [6.8755e-04],\n",
      "        [5.3955e-03],\n",
      "        [6.1037e-04],\n",
      "        [1.2683e-04],\n",
      "        [1.1942e-04],\n",
      "        [2.4479e-03],\n",
      "        [9.9225e-04],\n",
      "        [2.3000e-01],\n",
      "        [4.4544e-05],\n",
      "        [8.7575e-04],\n",
      "        [3.7094e-04],\n",
      "        [1.4546e-05],\n",
      "        [1.4715e-03],\n",
      "        [4.1448e-04],\n",
      "        [9.8702e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.308136940002441: \n",
      "target probs tensor([[1.1425e-01],\n",
      "        [1.0056e-03],\n",
      "        [8.9027e-04],\n",
      "        [1.1932e-03],\n",
      "        [1.7685e-05],\n",
      "        [2.0825e-03],\n",
      "        [3.7470e-03],\n",
      "        [1.6411e-03],\n",
      "        [2.1327e-03],\n",
      "        [6.7892e-04],\n",
      "        [7.7753e-04],\n",
      "        [2.8636e-04],\n",
      "        [4.2174e-04],\n",
      "        [4.0347e-04],\n",
      "        [1.4013e-04],\n",
      "        [3.6979e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.066741943359375: \n",
      "target probs tensor([[4.3877e-04],\n",
      "        [6.7234e-04],\n",
      "        [1.2804e-04],\n",
      "        [1.6344e-04],\n",
      "        [1.7237e-04],\n",
      "        [4.2245e-05],\n",
      "        [1.1931e-03],\n",
      "        [1.0094e-03],\n",
      "        [1.0201e-03],\n",
      "        [7.4784e-03],\n",
      "        [6.2449e-04],\n",
      "        [2.4530e-04],\n",
      "        [1.1244e-03],\n",
      "        [3.4694e-04],\n",
      "        [1.0015e-03],\n",
      "        [6.0177e-04]], device='cuda:0'), loss: 7.60250186920166: \n",
      "target probs tensor([[1.7208e-03],\n",
      "        [6.8643e-04],\n",
      "        [3.0740e-04],\n",
      "        [5.3125e-06],\n",
      "        [7.1662e-04],\n",
      "        [3.4565e-04],\n",
      "        [6.0210e-04],\n",
      "        [3.5548e-04],\n",
      "        [3.1854e-04],\n",
      "        [3.0006e-03],\n",
      "        [1.2646e-02],\n",
      "        [7.5849e-04],\n",
      "        [6.2904e-04],\n",
      "        [2.6021e-03],\n",
      "        [2.0614e-04],\n",
      "        [2.4305e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.355918884277344: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[0.0004],\n",
      "        [0.0008],\n",
      "        [0.0012],\n",
      "        [0.0003],\n",
      "        [0.0080],\n",
      "        [0.0006],\n",
      "        [0.0065],\n",
      "        [0.0008],\n",
      "        [0.0001],\n",
      "        [0.0050],\n",
      "        [0.0205],\n",
      "        [0.0028],\n",
      "        [0.0007],\n",
      "        [0.0052],\n",
      "        [0.0154],\n",
      "        [0.0011]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.3742828369140625: \n",
      "target probs tensor([[4.8473e-04],\n",
      "        [3.8493e-04],\n",
      "        [4.5624e-04],\n",
      "        [7.5337e-03],\n",
      "        [4.8470e-05],\n",
      "        [1.5603e-02],\n",
      "        [1.7186e-03],\n",
      "        [8.7168e-04],\n",
      "        [1.3974e-06],\n",
      "        [1.5009e-03],\n",
      "        [2.4574e-04],\n",
      "        [3.9080e-04],\n",
      "        [5.2648e-04],\n",
      "        [1.2198e-03],\n",
      "        [7.7149e-04],\n",
      "        [5.1971e-04]], device='cuda:0'), loss: 7.544395446777344: \n",
      "target probs tensor([[1.1145e-03],\n",
      "        [5.2523e-04],\n",
      "        [2.0753e-04],\n",
      "        [2.9256e-04],\n",
      "        [1.1358e-03],\n",
      "        [1.2981e-05],\n",
      "        [1.4291e-02],\n",
      "        [8.1522e-04],\n",
      "        [1.2020e-03],\n",
      "        [1.2874e-05],\n",
      "        [7.5347e-03],\n",
      "        [1.4911e-03],\n",
      "        [8.8320e-04],\n",
      "        [9.1257e-04],\n",
      "        [1.5274e-03],\n",
      "        [5.8548e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.500138759613037: \n",
      "target probs tensor([[6.2800e-04],\n",
      "        [4.8810e-04],\n",
      "        [4.9810e-04],\n",
      "        [4.1237e-04],\n",
      "        [5.9772e-04],\n",
      "        [8.4295e-05],\n",
      "        [1.2933e-03],\n",
      "        [1.2864e-03],\n",
      "        [1.7222e-05],\n",
      "        [7.3771e-05],\n",
      "        [1.0606e-03],\n",
      "        [9.9843e-05],\n",
      "        [9.2944e-05],\n",
      "        [1.4155e-04],\n",
      "        [4.5374e-04],\n",
      "        [9.3610e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.116815567016602: \n",
      "target probs tensor([[0.0003],\n",
      "        [0.0006],\n",
      "        [0.0005],\n",
      "        [0.0007],\n",
      "        [0.0006],\n",
      "        [0.0019],\n",
      "        [0.0003],\n",
      "        [0.0005],\n",
      "        [0.0007],\n",
      "        [0.0002],\n",
      "        [0.0010],\n",
      "        [0.0016],\n",
      "        [0.0015],\n",
      "        [0.0002],\n",
      "        [0.0003],\n",
      "        [0.0002]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.514067649841309: \n",
      "target probs tensor([[0.0004],\n",
      "        [0.0020],\n",
      "        [0.0034],\n",
      "        [0.0005],\n",
      "        [0.0006],\n",
      "        [0.0003],\n",
      "        [0.0002],\n",
      "        [0.0014],\n",
      "        [0.0039],\n",
      "        [0.0002],\n",
      "        [0.0242],\n",
      "        [0.0011],\n",
      "        [0.0012],\n",
      "        [0.0007],\n",
      "        [0.0006],\n",
      "        [0.0039]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.8486647605896: \n",
      "target probs tensor([[1.4238e-03],\n",
      "        [6.4034e-06],\n",
      "        [4.7559e-02],\n",
      "        [1.1704e-03],\n",
      "        [1.2650e-02],\n",
      "        [2.8868e-05],\n",
      "        [3.4524e-03],\n",
      "        [1.0739e-03],\n",
      "        [2.5525e-03],\n",
      "        [6.4210e-04],\n",
      "        [2.1968e-04],\n",
      "        [4.3575e-05],\n",
      "        [9.5624e-04],\n",
      "        [1.1107e-02],\n",
      "        [6.7571e-04],\n",
      "        [3.6736e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.986382961273193: \n",
      "target probs tensor([[0.0013],\n",
      "        [0.0007],\n",
      "        [0.0006],\n",
      "        [0.0001],\n",
      "        [0.0003],\n",
      "        [0.0010],\n",
      "        [0.0002],\n",
      "        [0.0015],\n",
      "        [0.0011],\n",
      "        [0.0004],\n",
      "        [0.0005],\n",
      "        [0.0011],\n",
      "        [0.0009],\n",
      "        [0.0005],\n",
      "        [0.0004],\n",
      "        [0.0005]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.456193923950195: \n",
      "target probs tensor([[1.3777e-05],\n",
      "        [5.4525e-04],\n",
      "        [1.3105e-03],\n",
      "        [2.2631e-04],\n",
      "        [2.9476e-04],\n",
      "        [7.2035e-04],\n",
      "        [6.3433e-01],\n",
      "        [2.6307e-03],\n",
      "        [1.2737e-03],\n",
      "        [6.7590e-04],\n",
      "        [4.0060e-04],\n",
      "        [9.4376e-04],\n",
      "        [4.9392e-04],\n",
      "        [1.4399e-02],\n",
      "        [1.4758e-03],\n",
      "        [4.9661e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.889503002166748: \n",
      "target probs tensor([[1.6766e-03],\n",
      "        [7.1961e-04],\n",
      "        [8.1731e-04],\n",
      "        [3.5920e-05],\n",
      "        [6.9831e-04],\n",
      "        [3.3749e-04],\n",
      "        [2.8414e-04],\n",
      "        [5.9783e-03],\n",
      "        [1.6059e-03],\n",
      "        [1.5858e-03],\n",
      "        [1.9158e-03],\n",
      "        [4.3034e-04],\n",
      "        [7.4582e-04],\n",
      "        [3.7353e-04],\n",
      "        [4.0389e-04],\n",
      "        [2.3854e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.209596633911133: \n",
      "target probs tensor([[6.7254e-03],\n",
      "        [3.8881e-04],\n",
      "        [1.1271e-03],\n",
      "        [4.4908e-03],\n",
      "        [7.1500e-02],\n",
      "        [3.7305e-04],\n",
      "        [7.3632e-04],\n",
      "        [3.2811e-02],\n",
      "        [4.9244e-02],\n",
      "        [2.8819e-04],\n",
      "        [1.9961e-03],\n",
      "        [4.5902e-04],\n",
      "        [6.2708e-04],\n",
      "        [1.0974e-03],\n",
      "        [5.7796e-05],\n",
      "        [1.8144e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.346033096313477: \n",
      "target probs tensor([[0.0003],\n",
      "        [0.0021],\n",
      "        [0.0009],\n",
      "        [0.0019],\n",
      "        [0.0038],\n",
      "        [0.0004],\n",
      "        [0.0011],\n",
      "        [0.0049],\n",
      "        [0.0011],\n",
      "        [0.0029],\n",
      "        [0.0013],\n",
      "        [0.0014],\n",
      "        [0.0002],\n",
      "        [0.0096],\n",
      "        [0.0008],\n",
      "        [0.0050]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.53749418258667: \n",
      "target probs tensor([[1.2356e-04],\n",
      "        [3.9862e-03],\n",
      "        [4.2228e-04],\n",
      "        [3.6207e-01],\n",
      "        [1.4887e-03],\n",
      "        [2.3975e-04],\n",
      "        [8.4306e-04],\n",
      "        [5.9446e-04],\n",
      "        [4.7983e-04],\n",
      "        [5.2849e-04],\n",
      "        [7.7794e-04],\n",
      "        [4.7027e-04],\n",
      "        [1.7030e-03],\n",
      "        [2.4483e-04],\n",
      "        [4.3976e-04],\n",
      "        [1.3910e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.979218482971191: \n",
      "target probs tensor([[0.0004],\n",
      "        [0.0008],\n",
      "        [0.0013],\n",
      "        [0.0012],\n",
      "        [0.0013],\n",
      "        [0.0012],\n",
      "        [0.0003],\n",
      "        [0.0003],\n",
      "        [0.0007],\n",
      "        [0.0001],\n",
      "        [0.0006],\n",
      "        [0.0006],\n",
      "        [0.0006],\n",
      "        [0.0013],\n",
      "        [0.0004],\n",
      "        [0.0004]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.403365135192871: \n",
      "target probs tensor([[1.4533e-05],\n",
      "        [1.1224e-03],\n",
      "        [7.5872e-04],\n",
      "        [3.1602e-03],\n",
      "        [1.7626e-03],\n",
      "        [9.1520e-05],\n",
      "        [4.2397e-02],\n",
      "        [2.7437e-04],\n",
      "        [9.3315e-05],\n",
      "        [1.8280e-03],\n",
      "        [4.6263e-04],\n",
      "        [1.8628e-04],\n",
      "        [1.7790e-03],\n",
      "        [6.4255e-04],\n",
      "        [3.4819e-04],\n",
      "        [2.6863e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.618391990661621: \n",
      "target probs tensor([[4.4848e-04],\n",
      "        [7.9823e-02],\n",
      "        [3.1955e-04],\n",
      "        [7.3881e-04],\n",
      "        [1.0689e-03],\n",
      "        [1.1528e-03],\n",
      "        [7.0160e-05],\n",
      "        [1.9924e-03],\n",
      "        [9.0905e-05],\n",
      "        [5.9878e-04],\n",
      "        [2.1313e-06],\n",
      "        [2.9265e-04],\n",
      "        [1.4724e-03],\n",
      "        [4.5326e-04],\n",
      "        [1.4040e-03],\n",
      "        [1.3978e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.510577201843262: \n",
      "target probs tensor([[2.1568e-04],\n",
      "        [1.4677e-03],\n",
      "        [5.6948e-04],\n",
      "        [1.4395e-04],\n",
      "        [1.0353e-03],\n",
      "        [2.9912e-04],\n",
      "        [1.8787e-04],\n",
      "        [1.7286e-03],\n",
      "        [2.4837e-04],\n",
      "        [3.6081e-03],\n",
      "        [2.9648e-04],\n",
      "        [4.1313e-03],\n",
      "        [7.1075e-06],\n",
      "        [7.6828e-04],\n",
      "        [1.4817e-04],\n",
      "        [1.6341e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.544089317321777: \n",
      "target probs tensor([[1.6789e-03],\n",
      "        [5.3521e-05],\n",
      "        [9.5460e-02],\n",
      "        [4.2063e-04],\n",
      "        [4.8336e-04],\n",
      "        [2.7038e-04],\n",
      "        [2.0626e-04],\n",
      "        [4.6899e-03],\n",
      "        [9.6652e-04],\n",
      "        [1.4461e-04],\n",
      "        [5.6442e-06],\n",
      "        [3.3776e-06],\n",
      "        [2.7483e-04],\n",
      "        [1.8761e-03],\n",
      "        [2.0558e-03],\n",
      "        [6.1086e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.786193370819092: \n",
      "target probs tensor([[0.0004],\n",
      "        [0.0017],\n",
      "        [0.0007],\n",
      "        [0.0004],\n",
      "        [0.0002],\n",
      "        [0.0025],\n",
      "        [0.0009],\n",
      "        [0.0001],\n",
      "        [0.0005],\n",
      "        [0.0099],\n",
      "        [0.0012],\n",
      "        [0.0001],\n",
      "        [0.0002],\n",
      "        [0.0005],\n",
      "        [0.0002],\n",
      "        [0.0005]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.498083591461182: \n",
      "target probs tensor([[8.1352e-05],\n",
      "        [6.3612e-03],\n",
      "        [4.7439e-04],\n",
      "        [9.3770e-04],\n",
      "        [5.5857e-04],\n",
      "        [3.9569e-03],\n",
      "        [3.0664e-04],\n",
      "        [6.0776e-05],\n",
      "        [7.9829e-02],\n",
      "        [1.1312e-04],\n",
      "        [2.5027e-04],\n",
      "        [3.5160e-04],\n",
      "        [6.4294e-04],\n",
      "        [3.2138e-04],\n",
      "        [2.2237e-04],\n",
      "        [2.5685e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.490738391876221: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[0.0002],\n",
      "        [0.0002],\n",
      "        [0.0018],\n",
      "        [0.0003],\n",
      "        [0.0005],\n",
      "        [0.0030],\n",
      "        [0.0009],\n",
      "        [0.0016]], device='cuda:0'), loss: 7.324398994445801: \n",
      "target probs tensor([[1.1355e-03],\n",
      "        [1.2351e-03],\n",
      "        [8.0358e-04],\n",
      "        [5.4235e-04],\n",
      "        [1.8531e-04],\n",
      "        [6.0217e-04],\n",
      "        [4.8644e-04],\n",
      "        [3.2713e-04],\n",
      "        [3.4025e-04],\n",
      "        [3.7813e-04],\n",
      "        [1.7058e-03],\n",
      "        [5.4160e-04],\n",
      "        [2.4362e-03],\n",
      "        [5.8469e-05],\n",
      "        [1.9193e-04],\n",
      "        [5.1310e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.590243339538574: \n",
      "target probs tensor([[7.9240e-04],\n",
      "        [8.7593e-04],\n",
      "        [4.9588e-04],\n",
      "        [2.3800e-01],\n",
      "        [3.7947e-03],\n",
      "        [2.4613e-04],\n",
      "        [2.0277e-03],\n",
      "        [1.2645e-04],\n",
      "        [4.1078e-03],\n",
      "        [1.1060e-02],\n",
      "        [5.3746e-04],\n",
      "        [1.5786e-03],\n",
      "        [1.7973e-03],\n",
      "        [9.7792e-04],\n",
      "        [7.6106e-04],\n",
      "        [1.7543e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.584090232849121: \n",
      "target probs tensor([[2.0023e-04],\n",
      "        [8.4530e-03],\n",
      "        [5.0543e-04],\n",
      "        [1.1627e-02],\n",
      "        [2.4411e-04],\n",
      "        [2.5920e-04],\n",
      "        [2.0346e-03],\n",
      "        [1.2611e-03],\n",
      "        [1.1041e-03],\n",
      "        [7.3541e-04],\n",
      "        [9.3506e-04],\n",
      "        [9.0035e-05],\n",
      "        [2.6426e-03],\n",
      "        [8.5368e-03],\n",
      "        [7.7620e-04],\n",
      "        [3.1413e-04]], device='cuda:0'), loss: 6.938943386077881: \n",
      "target probs tensor([[8.9447e-04],\n",
      "        [7.3166e-04],\n",
      "        [6.4848e-06],\n",
      "        [2.8354e-03],\n",
      "        [1.2210e-03],\n",
      "        [6.5095e-06],\n",
      "        [2.4969e-03],\n",
      "        [1.7867e-04],\n",
      "        [1.2216e-03],\n",
      "        [1.1254e-03],\n",
      "        [3.0633e-04],\n",
      "        [2.0044e-04],\n",
      "        [2.0367e-04],\n",
      "        [5.7000e-05],\n",
      "        [1.3114e-03],\n",
      "        [2.4476e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.040651321411133: \n",
      "target probs tensor([[1.1217e-03],\n",
      "        [3.4507e-05],\n",
      "        [6.6186e-05],\n",
      "        [7.8494e-05],\n",
      "        [3.6039e-04],\n",
      "        [5.7150e-03],\n",
      "        [1.3123e-03],\n",
      "        [4.8404e-05],\n",
      "        [5.9939e-05],\n",
      "        [2.2741e-04],\n",
      "        [3.2875e-04],\n",
      "        [1.1959e-03],\n",
      "        [5.0077e-04],\n",
      "        [3.6221e-05],\n",
      "        [1.7138e-04],\n",
      "        [1.8771e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.215158462524414: \n",
      "target probs tensor([[1.1199e-03],\n",
      "        [2.9748e-04],\n",
      "        [3.5318e-05],\n",
      "        [1.1288e-05],\n",
      "        [1.4584e-01],\n",
      "        [5.4785e-04],\n",
      "        [5.0548e-04],\n",
      "        [2.9140e-04],\n",
      "        [1.3080e-06],\n",
      "        [1.6637e-03],\n",
      "        [3.9397e-04],\n",
      "        [2.9941e-04],\n",
      "        [6.7740e-04],\n",
      "        [1.1843e-03],\n",
      "        [1.5044e-02],\n",
      "        [6.2752e-04]], device='cuda:0'), loss: 7.701754093170166: \n",
      "target probs tensor([[1.4909e-03],\n",
      "        [6.8414e-04],\n",
      "        [1.2184e-03],\n",
      "        [5.7063e-04],\n",
      "        [1.0043e-03],\n",
      "        [4.1729e-03],\n",
      "        [2.8516e-06],\n",
      "        [1.5281e-05],\n",
      "        [9.1637e-04],\n",
      "        [5.0401e-04],\n",
      "        [3.6708e-05],\n",
      "        [5.7858e-02],\n",
      "        [3.8937e-04],\n",
      "        [2.4676e-04],\n",
      "        [1.4947e-02],\n",
      "        [1.1265e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.438377380371094: \n",
      "target probs tensor([[1.0697e-01],\n",
      "        [6.3368e-04],\n",
      "        [6.9523e-04],\n",
      "        [1.6418e-03],\n",
      "        [6.4811e-04],\n",
      "        [3.4917e-04],\n",
      "        [8.2924e-03],\n",
      "        [7.0536e-04],\n",
      "        [5.8151e-04],\n",
      "        [1.3114e-04],\n",
      "        [4.8462e-04],\n",
      "        [2.2190e-03],\n",
      "        [1.1530e-03],\n",
      "        [5.2645e-04],\n",
      "        [2.1173e-04],\n",
      "        [7.0599e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.212560653686523: \n",
      "target probs tensor([[1.0990e-03],\n",
      "        [1.4760e-04],\n",
      "        [1.1633e-03],\n",
      "        [7.5216e-06],\n",
      "        [3.5303e-04],\n",
      "        [1.5817e-03],\n",
      "        [2.9098e-04],\n",
      "        [8.9585e-04],\n",
      "        [2.5800e-03],\n",
      "        [1.6882e-03],\n",
      "        [5.2002e-04],\n",
      "        [1.1180e-03],\n",
      "        [2.3351e-05],\n",
      "        [8.5827e-05],\n",
      "        [5.2643e-04],\n",
      "        [8.0213e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.822144508361816: \n",
      "target probs tensor([[5.8590e-04],\n",
      "        [6.3900e-04],\n",
      "        [8.3999e-03],\n",
      "        [1.4223e-04],\n",
      "        [1.3206e-04],\n",
      "        [1.4416e-03],\n",
      "        [4.1647e-03],\n",
      "        [9.6321e-05],\n",
      "        [1.0271e-03],\n",
      "        [2.6857e-04],\n",
      "        [3.4213e-03],\n",
      "        [3.8266e-04],\n",
      "        [6.6010e-04],\n",
      "        [3.3065e-04],\n",
      "        [1.7800e-03],\n",
      "        [3.0255e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.1723527908325195: \n",
      "target probs tensor([[1.7430e-04],\n",
      "        [1.9055e-03],\n",
      "        [6.1230e-04],\n",
      "        [1.3280e-04],\n",
      "        [5.2530e-04],\n",
      "        [3.3823e-03],\n",
      "        [2.0721e-02],\n",
      "        [1.4016e-03],\n",
      "        [7.8673e-04],\n",
      "        [1.5775e-03],\n",
      "        [2.5379e-04],\n",
      "        [5.1841e-04],\n",
      "        [1.2467e-04],\n",
      "        [2.8928e-04],\n",
      "        [1.9243e-05],\n",
      "        [1.2794e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.5833563804626465: \n",
      "target probs tensor([[4.1326e-04],\n",
      "        [3.3720e-03],\n",
      "        [7.4168e-04],\n",
      "        [1.7886e-05],\n",
      "        [6.0010e-03],\n",
      "        [5.8334e-04],\n",
      "        [2.2685e-03],\n",
      "        [5.8214e-04],\n",
      "        [1.2597e-03],\n",
      "        [7.4662e-04],\n",
      "        [1.1841e-03],\n",
      "        [3.5500e-04],\n",
      "        [5.0949e-03],\n",
      "        [2.7917e-04],\n",
      "        [9.0125e-04],\n",
      "        [1.4311e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.081567764282227: \n",
      "target probs tensor([[6.4633e-04],\n",
      "        [6.8628e-05],\n",
      "        [7.4336e-03],\n",
      "        [1.2187e-04],\n",
      "        [5.4371e-04],\n",
      "        [3.1776e-03],\n",
      "        [1.5190e-04],\n",
      "        [3.5696e-04],\n",
      "        [1.4031e-03],\n",
      "        [9.7468e-04],\n",
      "        [2.0842e-03],\n",
      "        [7.1738e-05],\n",
      "        [6.3374e-07],\n",
      "        [1.1864e-03],\n",
      "        [1.8813e-04],\n",
      "        [6.6993e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.934844970703125: \n",
      "target probs tensor([[4.3756e-04],\n",
      "        [7.4070e-05],\n",
      "        [1.7485e-04],\n",
      "        [2.0018e-04],\n",
      "        [1.0067e-03],\n",
      "        [2.2113e-03],\n",
      "        [3.5708e-04],\n",
      "        [6.7015e-04],\n",
      "        [8.3049e-04],\n",
      "        [1.3916e-05],\n",
      "        [1.1046e-03],\n",
      "        [1.0931e-03],\n",
      "        [5.7821e-04],\n",
      "        [2.4966e-04],\n",
      "        [3.3362e-03],\n",
      "        [8.0225e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.69740104675293: \n",
      "target probs tensor([[7.6390e-04],\n",
      "        [5.3503e-04],\n",
      "        [1.1469e-03],\n",
      "        [6.6122e-02],\n",
      "        [8.5238e-04],\n",
      "        [3.6836e-04],\n",
      "        [7.3839e-04],\n",
      "        [9.1419e-04],\n",
      "        [1.7843e-04],\n",
      "        [5.7765e-04],\n",
      "        [1.2992e-03],\n",
      "        [5.0963e-04],\n",
      "        [3.3605e-04],\n",
      "        [1.8735e-03],\n",
      "        [6.5942e-04],\n",
      "        [3.6732e-07]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.507151126861572: \n",
      "target probs tensor([[4.0973e-04],\n",
      "        [4.7772e-03],\n",
      "        [2.1629e-03],\n",
      "        [9.0395e-04],\n",
      "        [4.5215e-03],\n",
      "        [9.8210e-05],\n",
      "        [2.2971e-05],\n",
      "        [4.0333e-03],\n",
      "        [4.7012e-04],\n",
      "        [1.3747e-02],\n",
      "        [8.1231e-04],\n",
      "        [3.7436e-04],\n",
      "        [5.9267e-03],\n",
      "        [1.6544e-03],\n",
      "        [8.9471e-04],\n",
      "        [1.3290e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.971474647521973: \n",
      "target probs tensor([[9.1115e-05],\n",
      "        [2.1334e-04],\n",
      "        [5.8851e-04],\n",
      "        [1.5872e-03],\n",
      "        [4.1440e-04],\n",
      "        [3.8406e-04],\n",
      "        [9.5054e-03],\n",
      "        [1.1190e-03],\n",
      "        [8.9181e-06],\n",
      "        [1.0089e-03],\n",
      "        [3.6935e-04],\n",
      "        [1.1967e-03],\n",
      "        [3.2587e-03],\n",
      "        [3.4906e-04],\n",
      "        [3.6931e-04],\n",
      "        [4.8341e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.570487022399902: \n",
      "target probs tensor([[8.2124e-05],\n",
      "        [5.4382e-04],\n",
      "        [6.3905e-04],\n",
      "        [4.9807e-04],\n",
      "        [4.5633e-04],\n",
      "        [6.7169e-04],\n",
      "        [2.3212e-04],\n",
      "        [8.6316e-04],\n",
      "        [6.0926e-04],\n",
      "        [1.5034e-02],\n",
      "        [2.1798e-03],\n",
      "        [3.7906e-04],\n",
      "        [6.6806e-04],\n",
      "        [1.9530e-03],\n",
      "        [3.0889e-04],\n",
      "        [3.5084e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.487657070159912: \n",
      "target probs tensor([[0.0009],\n",
      "        [0.0008],\n",
      "        [0.0021],\n",
      "        [0.0003],\n",
      "        [0.0008],\n",
      "        [0.0014],\n",
      "        [0.0045],\n",
      "        [0.1088],\n",
      "        [0.0004],\n",
      "        [0.0003],\n",
      "        [0.0003],\n",
      "        [0.0165],\n",
      "        [0.0147],\n",
      "        [0.0685],\n",
      "        [0.0002],\n",
      "        [0.0013]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.24730110168457: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[8.6341e-04],\n",
      "        [2.1571e-04],\n",
      "        [6.3357e-04],\n",
      "        [1.5467e-03],\n",
      "        [2.9875e-02],\n",
      "        [2.3712e-06],\n",
      "        [3.8600e-04],\n",
      "        [1.0686e-03],\n",
      "        [3.2113e-04],\n",
      "        [2.7763e-03],\n",
      "        [1.1301e-03],\n",
      "        [5.8536e-04],\n",
      "        [3.5036e-04],\n",
      "        [9.7760e-04],\n",
      "        [4.8015e-04],\n",
      "        [1.5343e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.353933811187744: \n",
      "target probs tensor([[0.0002],\n",
      "        [0.0010],\n",
      "        [0.0006],\n",
      "        [0.0025],\n",
      "        [0.0051],\n",
      "        [0.0012],\n",
      "        [0.1066],\n",
      "        [0.0005],\n",
      "        [0.0003],\n",
      "        [0.0036],\n",
      "        [0.0001],\n",
      "        [0.0011],\n",
      "        [0.0003],\n",
      "        [0.0001],\n",
      "        [0.0007],\n",
      "        [0.0002]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.050826072692871: \n",
      "target probs tensor([[1.1463e-03],\n",
      "        [1.2060e-03],\n",
      "        [3.4506e-04],\n",
      "        [4.8841e-04],\n",
      "        [3.1381e-04],\n",
      "        [5.0383e-03],\n",
      "        [2.0099e-04],\n",
      "        [1.4347e-03],\n",
      "        [1.3729e-05],\n",
      "        [7.6466e-04],\n",
      "        [1.2346e-03],\n",
      "        [8.2338e-04],\n",
      "        [1.8660e-03],\n",
      "        [7.4555e-06],\n",
      "        [1.5726e-03],\n",
      "        [2.4662e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.514120578765869: \n",
      "target probs tensor([[0.0011],\n",
      "        [0.0016],\n",
      "        [0.0004],\n",
      "        [0.0002],\n",
      "        [0.0007],\n",
      "        [0.0007],\n",
      "        [0.0006],\n",
      "        [0.0004],\n",
      "        [0.0009],\n",
      "        [0.0022],\n",
      "        [0.0005],\n",
      "        [0.0009],\n",
      "        [0.0550],\n",
      "        [0.0007],\n",
      "        [0.0084],\n",
      "        [0.0247]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.638540267944336: \n",
      "target probs tensor([[6.6954e-04],\n",
      "        [9.2345e-04],\n",
      "        [4.9397e-04],\n",
      "        [1.6385e-03],\n",
      "        [7.5468e-03],\n",
      "        [4.8557e-04],\n",
      "        [3.3229e-03],\n",
      "        [5.1306e-04],\n",
      "        [1.5630e-03],\n",
      "        [3.8732e-03],\n",
      "        [3.9676e-06],\n",
      "        [4.0238e-03],\n",
      "        [5.2216e-03],\n",
      "        [1.0090e-04],\n",
      "        [1.2077e-06],\n",
      "        [2.0616e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.541233539581299: \n",
      "target probs tensor([[0.0005],\n",
      "        [0.0019],\n",
      "        [0.0010],\n",
      "        [0.0008],\n",
      "        [0.0011],\n",
      "        [0.0130],\n",
      "        [0.0006],\n",
      "        [0.0003]], device='cuda:0'), loss: 6.808271408081055: \n",
      "Better model found at epoch 127 with validation value: 0.7599999904632568.\n",
      "target probs tensor([[6.4293e-04],\n",
      "        [3.7287e-05],\n",
      "        [2.6022e-04],\n",
      "        [5.2287e-04],\n",
      "        [5.3978e-04],\n",
      "        [6.8871e-05],\n",
      "        [1.2283e-03],\n",
      "        [1.0169e-04],\n",
      "        [1.3271e-03],\n",
      "        [3.3714e-03],\n",
      "        [2.6347e-04],\n",
      "        [2.9803e-04],\n",
      "        [6.1812e-01],\n",
      "        [6.1064e-04],\n",
      "        [5.7008e-04],\n",
      "        [5.8294e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.364756107330322: \n",
      "target probs tensor([[0.0003],\n",
      "        [0.0002],\n",
      "        [0.0001],\n",
      "        [0.0004],\n",
      "        [0.0086],\n",
      "        [0.0032],\n",
      "        [0.0015],\n",
      "        [0.0007],\n",
      "        [0.0005],\n",
      "        [0.0027],\n",
      "        [0.0009],\n",
      "        [0.0150],\n",
      "        [0.0062],\n",
      "        [0.0013],\n",
      "        [0.0001],\n",
      "        [0.0008]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.873828411102295: \n",
      "target probs tensor([[1.5880e-04],\n",
      "        [2.9583e-04],\n",
      "        [6.2940e-04],\n",
      "        [1.8807e-03],\n",
      "        [4.9977e-04],\n",
      "        [4.5257e-05],\n",
      "        [3.7397e-02],\n",
      "        [1.7521e-04],\n",
      "        [4.4330e-04],\n",
      "        [1.1903e-03],\n",
      "        [2.5012e-04],\n",
      "        [3.3307e-04],\n",
      "        [3.1579e-03],\n",
      "        [9.5981e-03],\n",
      "        [6.3696e-04],\n",
      "        [4.6210e-04]], device='cuda:0'), loss: 7.266175270080566: \n",
      "target probs tensor([[1.8008e-04],\n",
      "        [1.1202e-03],\n",
      "        [5.3161e-03],\n",
      "        [2.3468e-03],\n",
      "        [2.3533e-05],\n",
      "        [1.2560e-03],\n",
      "        [1.5345e-03],\n",
      "        [5.6838e-04],\n",
      "        [3.1107e-03],\n",
      "        [1.2131e-02],\n",
      "        [8.4926e-04],\n",
      "        [6.7545e-04],\n",
      "        [2.9498e-03],\n",
      "        [5.7511e-04],\n",
      "        [1.1292e-03],\n",
      "        [3.2778e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.915614604949951: \n",
      "target probs tensor([[5.7732e-05],\n",
      "        [2.9362e-04],\n",
      "        [2.2491e-03],\n",
      "        [6.8644e-04],\n",
      "        [1.3080e-05],\n",
      "        [6.8780e-04],\n",
      "        [1.4795e-03],\n",
      "        [3.9758e-04],\n",
      "        [8.2249e-04],\n",
      "        [1.3800e-03],\n",
      "        [7.1610e-04],\n",
      "        [1.0095e-03],\n",
      "        [2.3308e-04],\n",
      "        [5.3240e-03],\n",
      "        [5.1100e-03],\n",
      "        [2.1835e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.167247772216797: \n",
      "target probs tensor([[9.2598e-04],\n",
      "        [1.1767e-03],\n",
      "        [1.5817e-04],\n",
      "        [1.3972e-04],\n",
      "        [1.2637e-03],\n",
      "        [2.7202e-03],\n",
      "        [9.8973e-04],\n",
      "        [2.2875e-03],\n",
      "        [1.6857e-06],\n",
      "        [7.6587e-04],\n",
      "        [3.1375e-04],\n",
      "        [1.4220e-04],\n",
      "        [1.9804e-03],\n",
      "        [9.6918e-04],\n",
      "        [1.0303e-04],\n",
      "        [1.3018e-03]], device='cuda:0'), loss: 7.707352638244629: \n",
      "target probs tensor([[2.9129e-04],\n",
      "        [5.6307e-04],\n",
      "        [5.4151e-04],\n",
      "        [4.1765e-04],\n",
      "        [2.5094e-02],\n",
      "        [5.5117e-03],\n",
      "        [1.2971e-05],\n",
      "        [5.7130e-05],\n",
      "        [2.7750e-03],\n",
      "        [8.8209e-04],\n",
      "        [2.7117e-04],\n",
      "        [1.2511e-03],\n",
      "        [1.0540e-04],\n",
      "        [2.4867e-03],\n",
      "        [6.5479e-04],\n",
      "        [2.1641e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.617356777191162: \n",
      "target probs tensor([[3.2412e-04],\n",
      "        [4.1849e-04],\n",
      "        [4.4957e-05],\n",
      "        [1.7565e-01],\n",
      "        [6.4858e-04],\n",
      "        [1.4790e-03],\n",
      "        [2.7794e-04],\n",
      "        [6.5637e-03],\n",
      "        [7.3263e-04],\n",
      "        [1.9074e-03],\n",
      "        [1.2872e-03],\n",
      "        [6.7249e-04],\n",
      "        [4.8415e-04],\n",
      "        [1.2960e-03],\n",
      "        [1.4114e-02],\n",
      "        [6.2186e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.749958515167236: \n",
      "target probs tensor([[8.2912e-05],\n",
      "        [1.4516e-05],\n",
      "        [9.6496e-05],\n",
      "        [1.3858e-03],\n",
      "        [5.6486e-04],\n",
      "        [1.3461e-03],\n",
      "        [3.5653e-04],\n",
      "        [5.1040e-04],\n",
      "        [7.9795e-04],\n",
      "        [5.5848e-03],\n",
      "        [1.7363e-05],\n",
      "        [1.1740e-03],\n",
      "        [2.4390e-03],\n",
      "        [1.4724e-04],\n",
      "        [3.5698e-06],\n",
      "        [9.9301e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.143837928771973: \n",
      "target probs tensor([[6.2638e-04],\n",
      "        [7.4151e-04],\n",
      "        [3.2815e-04],\n",
      "        [1.6476e-03],\n",
      "        [2.3986e-04],\n",
      "        [3.1581e-04],\n",
      "        [8.3078e-04],\n",
      "        [2.0448e-04],\n",
      "        [6.5329e-04],\n",
      "        [3.2133e-01],\n",
      "        [6.3226e-04],\n",
      "        [1.1664e-04],\n",
      "        [4.4991e-04],\n",
      "        [2.7392e-04],\n",
      "        [9.7409e-04],\n",
      "        [2.8471e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.305967330932617: \n",
      "target probs tensor([[5.3429e-03],\n",
      "        [2.6552e-04],\n",
      "        [1.2787e-03],\n",
      "        [6.7735e-04],\n",
      "        [4.7903e-04],\n",
      "        [8.9228e-06],\n",
      "        [3.9762e-02],\n",
      "        [3.6311e-02],\n",
      "        [3.6568e-04],\n",
      "        [3.7193e-04],\n",
      "        [8.5476e-04],\n",
      "        [7.1986e-04],\n",
      "        [1.0766e-03],\n",
      "        [3.1075e-05],\n",
      "        [1.3308e-02],\n",
      "        [2.9491e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.063071250915527: \n",
      "target probs tensor([[1.7504e-04],\n",
      "        [1.3925e-04],\n",
      "        [8.7579e-04],\n",
      "        [1.3156e-04],\n",
      "        [3.2102e-04],\n",
      "        [2.4244e-04],\n",
      "        [8.3904e-04],\n",
      "        [2.3005e-04],\n",
      "        [4.5526e-05],\n",
      "        [5.5676e-04],\n",
      "        [2.0384e-03],\n",
      "        [2.1128e-04],\n",
      "        [9.8089e-04],\n",
      "        [5.4816e-04],\n",
      "        [1.0465e-04],\n",
      "        [7.9987e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.01348876953125: \n",
      "target probs tensor([[5.1878e-04],\n",
      "        [3.1612e-04],\n",
      "        [3.3522e-04],\n",
      "        [1.9058e-01],\n",
      "        [9.1691e-04],\n",
      "        [3.8751e-03],\n",
      "        [4.3456e-03],\n",
      "        [4.2250e-04],\n",
      "        [2.1269e-04],\n",
      "        [5.2365e-03],\n",
      "        [3.6647e-04],\n",
      "        [4.4694e-04],\n",
      "        [2.9598e-05],\n",
      "        [3.8930e-03],\n",
      "        [7.2470e-05],\n",
      "        [8.2260e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.061351776123047: \n",
      "target probs tensor([[4.1224e-04],\n",
      "        [2.3669e-05],\n",
      "        [8.1531e-04],\n",
      "        [1.4036e-02],\n",
      "        [6.8173e-04],\n",
      "        [5.0878e-04],\n",
      "        [1.3931e-03],\n",
      "        [5.1951e-04],\n",
      "        [2.1466e-03],\n",
      "        [4.0137e-04],\n",
      "        [7.1590e-03],\n",
      "        [2.6790e-04],\n",
      "        [1.7108e-04],\n",
      "        [1.6573e-03],\n",
      "        [3.1016e-04],\n",
      "        [3.8334e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.311642646789551: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[2.8837e-03],\n",
      "        [5.4912e-05],\n",
      "        [4.2693e-05],\n",
      "        [1.5225e-03],\n",
      "        [5.3971e-04],\n",
      "        [1.2774e-02],\n",
      "        [4.1388e-04],\n",
      "        [6.9705e-04],\n",
      "        [4.4536e-04],\n",
      "        [1.3390e-02],\n",
      "        [5.4962e-04],\n",
      "        [3.2415e-04],\n",
      "        [7.1674e-05],\n",
      "        [2.0430e-03],\n",
      "        [4.1313e-04],\n",
      "        [4.2854e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.375299453735352: \n",
      "target probs tensor([[4.7831e-05],\n",
      "        [1.0935e-03],\n",
      "        [1.8400e-03],\n",
      "        [1.8097e-04],\n",
      "        [1.1794e-03],\n",
      "        [1.8322e-04],\n",
      "        [1.2660e-03],\n",
      "        [6.8466e-04],\n",
      "        [8.2869e-04],\n",
      "        [1.0639e-03],\n",
      "        [8.8259e-04],\n",
      "        [5.0027e-05],\n",
      "        [7.7893e-05],\n",
      "        [5.1890e-04],\n",
      "        [8.4187e-05],\n",
      "        [1.6268e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.649338722229004: \n",
      "target probs tensor([[1.5015e-03],\n",
      "        [2.1238e-04],\n",
      "        [2.1595e-04],\n",
      "        [4.6032e-05],\n",
      "        [1.9111e-03],\n",
      "        [1.8458e-03],\n",
      "        [5.9252e-04],\n",
      "        [5.1715e-01],\n",
      "        [3.3926e-03],\n",
      "        [5.4266e-03],\n",
      "        [3.0070e-04],\n",
      "        [5.7715e-04],\n",
      "        [4.5348e-05],\n",
      "        [1.4358e-03],\n",
      "        [4.1153e-03],\n",
      "        [1.0674e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.836418628692627: \n",
      "target probs tensor([[4.4599e-02],\n",
      "        [8.8062e-05],\n",
      "        [4.7717e-04],\n",
      "        [5.0926e-04],\n",
      "        [3.0947e-04],\n",
      "        [8.3793e-05],\n",
      "        [2.3519e-03],\n",
      "        [3.9935e-04],\n",
      "        [1.4723e-03],\n",
      "        [9.2748e-04],\n",
      "        [9.7684e-05],\n",
      "        [5.4229e-04],\n",
      "        [1.5139e-03],\n",
      "        [1.0432e-03],\n",
      "        [7.0375e-03],\n",
      "        [8.3553e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.311641216278076: \n",
      "target probs tensor([[3.0287e-03],\n",
      "        [2.1073e-04],\n",
      "        [1.3350e-03],\n",
      "        [2.9407e-05],\n",
      "        [6.4608e-04],\n",
      "        [3.4147e-04],\n",
      "        [5.3027e-03],\n",
      "        [6.9859e-04],\n",
      "        [8.1427e-04],\n",
      "        [2.1531e-04],\n",
      "        [7.5351e-05],\n",
      "        [3.3269e-04],\n",
      "        [1.1752e-03],\n",
      "        [4.3487e-04],\n",
      "        [3.8172e-04],\n",
      "        [1.1812e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.581725597381592: \n",
      "target probs tensor([[1.3829e-04],\n",
      "        [8.6177e-04],\n",
      "        [2.0001e-04],\n",
      "        [4.6083e-03],\n",
      "        [2.5496e-02],\n",
      "        [2.6346e-04],\n",
      "        [5.7237e-04],\n",
      "        [3.7018e-03],\n",
      "        [1.0745e-03],\n",
      "        [9.7047e-03],\n",
      "        [1.0318e-04],\n",
      "        [7.5290e-05],\n",
      "        [9.0302e-04],\n",
      "        [7.8697e-04],\n",
      "        [3.6678e-04],\n",
      "        [1.0752e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.116410255432129: \n",
      "target probs tensor([[6.2080e-04],\n",
      "        [4.2866e-05],\n",
      "        [4.2037e-04],\n",
      "        [1.3160e-01],\n",
      "        [6.2924e-04],\n",
      "        [8.6281e-03],\n",
      "        [3.7200e-04],\n",
      "        [6.1624e-05],\n",
      "        [1.7210e-03],\n",
      "        [1.2944e-04],\n",
      "        [1.0696e-02],\n",
      "        [1.5828e-04],\n",
      "        [3.6025e-03],\n",
      "        [2.1741e-04],\n",
      "        [7.7364e-04],\n",
      "        [6.0312e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.9937744140625: \n",
      "target probs tensor([[1.6728e-03],\n",
      "        [3.0138e-04],\n",
      "        [5.8228e-03],\n",
      "        [2.1139e-04],\n",
      "        [3.0307e-05],\n",
      "        [1.7887e-03],\n",
      "        [1.4084e-03],\n",
      "        [3.3771e-04],\n",
      "        [3.4065e-03],\n",
      "        [9.2015e-05],\n",
      "        [5.9165e-02],\n",
      "        [9.2461e-04],\n",
      "        [1.3926e-03],\n",
      "        [6.1204e-04],\n",
      "        [1.5055e-03],\n",
      "        [9.3913e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.976930618286133: \n",
      "target probs tensor([[1.3529e-05],\n",
      "        [1.9991e-04],\n",
      "        [4.2371e-04],\n",
      "        [5.2340e-04],\n",
      "        [2.8373e-04],\n",
      "        [4.3850e-04],\n",
      "        [2.1590e-03],\n",
      "        [1.2362e-03],\n",
      "        [2.9858e-04],\n",
      "        [8.7045e-04],\n",
      "        [7.5089e-05],\n",
      "        [1.0481e-03],\n",
      "        [1.3395e-04],\n",
      "        [1.1264e-03],\n",
      "        [1.8041e-03],\n",
      "        [6.9398e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.93150520324707: \n",
      "target probs tensor([[3.3886e-04],\n",
      "        [2.0308e-04],\n",
      "        [4.0215e-02],\n",
      "        [7.2691e-04],\n",
      "        [4.8108e-04],\n",
      "        [1.1177e-04],\n",
      "        [7.5503e-03],\n",
      "        [1.6763e-04],\n",
      "        [1.2349e-03],\n",
      "        [8.4822e-05],\n",
      "        [1.0390e-03],\n",
      "        [2.7308e-03],\n",
      "        [7.8229e-04],\n",
      "        [2.5665e-04],\n",
      "        [3.6374e-03],\n",
      "        [5.8297e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.1612420082092285: \n",
      "target probs tensor([[0.0006],\n",
      "        [0.0036],\n",
      "        [0.0015],\n",
      "        [0.0005],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0057],\n",
      "        [0.0086]], device='cuda:0'), loss: 6.728431701660156: \n",
      "target probs tensor([[2.8044e-03],\n",
      "        [1.9213e-04],\n",
      "        [3.6262e-04],\n",
      "        [6.6394e-04],\n",
      "        [1.5985e-04],\n",
      "        [2.9088e-05],\n",
      "        [7.8013e-04],\n",
      "        [2.0886e-05],\n",
      "        [6.9792e-04],\n",
      "        [1.7810e-03],\n",
      "        [9.7902e-04],\n",
      "        [1.1047e-04],\n",
      "        [1.4083e-03],\n",
      "        [1.0583e-02],\n",
      "        [4.0579e-03],\n",
      "        [3.6628e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.560188293457031: \n",
      "target probs tensor([[7.7487e-04],\n",
      "        [7.6607e-04],\n",
      "        [6.6284e-03],\n",
      "        [4.6769e-04],\n",
      "        [2.4355e-03],\n",
      "        [3.0197e-05],\n",
      "        [1.8889e-03],\n",
      "        [1.2705e-03],\n",
      "        [5.0357e-05],\n",
      "        [1.2913e-03],\n",
      "        [2.8168e-04],\n",
      "        [4.8655e-04],\n",
      "        [2.2154e-03],\n",
      "        [4.7042e-04],\n",
      "        [7.3317e-05],\n",
      "        [6.2055e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.6075263023376465: \n",
      "target probs tensor([[0.0075],\n",
      "        [0.0008],\n",
      "        [0.0002],\n",
      "        [0.0014],\n",
      "        [0.0019],\n",
      "        [0.0009],\n",
      "        [0.0019],\n",
      "        [0.0018],\n",
      "        [0.0040],\n",
      "        [0.0010],\n",
      "        [0.0005],\n",
      "        [0.0021],\n",
      "        [0.0084],\n",
      "        [0.0012],\n",
      "        [0.0034],\n",
      "        [0.0014]], device='cuda:0'), loss: 6.432404041290283: \n",
      "target probs tensor([[9.9983e-05],\n",
      "        [5.3562e-04],\n",
      "        [1.5107e-03],\n",
      "        [6.6835e-03],\n",
      "        [2.0578e-03],\n",
      "        [3.4330e-04],\n",
      "        [1.1061e-04],\n",
      "        [3.8992e-04],\n",
      "        [4.8957e-03],\n",
      "        [1.8248e-04],\n",
      "        [4.1891e-04],\n",
      "        [6.4869e-04],\n",
      "        [1.8590e-04],\n",
      "        [1.5210e-04],\n",
      "        [7.5558e-05],\n",
      "        [1.1834e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.770535469055176: \n",
      "target probs tensor([[1.9752e-03],\n",
      "        [1.8630e-04],\n",
      "        [8.4944e-04],\n",
      "        [2.9569e-04],\n",
      "        [8.5294e-04],\n",
      "        [1.4066e-03],\n",
      "        [4.5743e-04],\n",
      "        [1.8869e-04],\n",
      "        [2.3381e-04],\n",
      "        [1.5094e-03],\n",
      "        [1.7929e-05],\n",
      "        [2.9198e-04],\n",
      "        [1.9706e-04],\n",
      "        [4.6050e-04],\n",
      "        [7.0331e-04],\n",
      "        [7.4008e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.782473564147949: \n",
      "target probs tensor([[3.3158e-04],\n",
      "        [1.5308e-03],\n",
      "        [1.6556e-04],\n",
      "        [1.4359e-04],\n",
      "        [3.5095e-04],\n",
      "        [5.1919e-04],\n",
      "        [3.3143e-04],\n",
      "        [2.1801e-03],\n",
      "        [4.7086e-07],\n",
      "        [1.0466e-03],\n",
      "        [1.2831e-03],\n",
      "        [2.3377e-04],\n",
      "        [7.1499e-03],\n",
      "        [8.8292e-04],\n",
      "        [1.3231e-04],\n",
      "        [6.0973e-04]], device='cuda:0'), loss: 7.9039387702941895: \n",
      "target probs tensor([[0.0005],\n",
      "        [0.0009],\n",
      "        [0.0008],\n",
      "        [0.0002],\n",
      "        [0.0003],\n",
      "        [0.0010],\n",
      "        [0.0012],\n",
      "        [0.0005],\n",
      "        [0.0002],\n",
      "        [0.0011],\n",
      "        [0.0167],\n",
      "        [0.0005],\n",
      "        [0.0017],\n",
      "        [0.0002],\n",
      "        [0.0019],\n",
      "        [0.0026]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.137747764587402: \n",
      "target probs tensor([[1.0638e-02],\n",
      "        [1.4006e-03],\n",
      "        [1.6458e-04],\n",
      "        [8.2464e-04],\n",
      "        [9.1975e-06],\n",
      "        [3.7243e-06],\n",
      "        [1.7566e-05],\n",
      "        [3.1645e-04],\n",
      "        [1.2144e-03],\n",
      "        [3.6115e-03],\n",
      "        [1.0960e-04],\n",
      "        [1.6611e-03],\n",
      "        [3.1293e-04],\n",
      "        [1.2868e-02],\n",
      "        [1.4967e-05],\n",
      "        [1.3353e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.146334648132324: \n",
      "target probs tensor([[6.8234e-05],\n",
      "        [1.3787e-04],\n",
      "        [1.8003e-03],\n",
      "        [5.7347e-05],\n",
      "        [9.7832e-05],\n",
      "        [5.9303e-04],\n",
      "        [6.9519e-04],\n",
      "        [3.5110e-04],\n",
      "        [3.2533e-04],\n",
      "        [1.3186e-03],\n",
      "        [5.4294e-04],\n",
      "        [5.4794e-03],\n",
      "        [9.8308e-04],\n",
      "        [3.3810e-04],\n",
      "        [1.1675e-03],\n",
      "        [7.6878e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.667743682861328: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 138 with validation value: 0.7620000243186951.\n",
      "target probs tensor([[0.0005],\n",
      "        [0.0093],\n",
      "        [0.0011],\n",
      "        [0.0001],\n",
      "        [0.0006],\n",
      "        [0.0010],\n",
      "        [0.0006],\n",
      "        [0.0074],\n",
      "        [0.0009],\n",
      "        [0.0091],\n",
      "        [0.0023],\n",
      "        [0.0012],\n",
      "        [0.0026],\n",
      "        [0.0002],\n",
      "        [0.0015],\n",
      "        [0.0002]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.791839122772217: \n",
      "target probs tensor([[4.6020e-04],\n",
      "        [2.4025e-04],\n",
      "        [9.6039e-04],\n",
      "        [3.8507e-03],\n",
      "        [3.0155e-04],\n",
      "        [9.9245e-04],\n",
      "        [2.7809e-03],\n",
      "        [2.0762e-04],\n",
      "        [1.5158e-03],\n",
      "        [6.9469e-04],\n",
      "        [3.8992e-04],\n",
      "        [3.5352e-05],\n",
      "        [8.3655e-05],\n",
      "        [8.3527e-06],\n",
      "        [6.6711e-04],\n",
      "        [1.4914e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.648458480834961: \n",
      "target probs tensor([[1.9130e-04],\n",
      "        [1.1994e-03],\n",
      "        [3.4582e-04],\n",
      "        [3.8370e-04],\n",
      "        [8.7976e-04],\n",
      "        [1.4113e-03],\n",
      "        [1.3542e-03],\n",
      "        [3.2367e-03],\n",
      "        [2.2429e-04],\n",
      "        [2.9061e-04],\n",
      "        [3.9341e-04],\n",
      "        [1.0718e-02],\n",
      "        [1.3614e-04],\n",
      "        [5.8416e-04],\n",
      "        [2.3099e-05],\n",
      "        [4.7987e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.540441513061523: \n",
      "target probs tensor([[0.0017],\n",
      "        [0.0005],\n",
      "        [0.0032],\n",
      "        [0.0002],\n",
      "        [0.0005],\n",
      "        [0.0009],\n",
      "        [0.0023],\n",
      "        [0.0106],\n",
      "        [0.0003],\n",
      "        [0.0007],\n",
      "        [0.0006],\n",
      "        [0.0004],\n",
      "        [0.0012],\n",
      "        [0.0008],\n",
      "        [0.0003],\n",
      "        [0.0002]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.165175437927246: \n",
      "target probs tensor([[9.2875e-04],\n",
      "        [1.5419e-04],\n",
      "        [6.7661e-04],\n",
      "        [5.5721e-04],\n",
      "        [7.8333e-05],\n",
      "        [6.5178e-04],\n",
      "        [1.2290e-04],\n",
      "        [2.8074e-03],\n",
      "        [3.9682e-04],\n",
      "        [5.3714e-02],\n",
      "        [1.8080e-04],\n",
      "        [2.2135e-04],\n",
      "        [1.5540e-04],\n",
      "        [8.3570e-04],\n",
      "        [2.7706e-03],\n",
      "        [9.2540e-06]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.709113597869873: \n",
      "target probs tensor([[0.0016],\n",
      "        [0.0010],\n",
      "        [0.0002],\n",
      "        [0.0003],\n",
      "        [0.0003],\n",
      "        [0.0006],\n",
      "        [0.0024],\n",
      "        [0.0002],\n",
      "        [0.0013],\n",
      "        [0.0007],\n",
      "        [0.0021],\n",
      "        [0.0001],\n",
      "        [0.0008],\n",
      "        [0.0009],\n",
      "        [0.0006],\n",
      "        [0.0008]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.352874755859375: \n",
      "target probs tensor([[2.2616e-03],\n",
      "        [3.8367e-04],\n",
      "        [2.6097e-03],\n",
      "        [3.4364e-04],\n",
      "        [7.2388e-04],\n",
      "        [7.9008e-04],\n",
      "        [7.0409e-04],\n",
      "        [1.6951e-04],\n",
      "        [9.0712e-04],\n",
      "        [2.4863e-04],\n",
      "        [5.0773e-04],\n",
      "        [1.0978e-05],\n",
      "        [8.0997e-04],\n",
      "        [1.0670e-03],\n",
      "        [1.8185e-02],\n",
      "        [6.7515e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.217113494873047: \n",
      "target probs tensor([[2.1213e-03],\n",
      "        [2.6925e-02],\n",
      "        [4.8786e-05],\n",
      "        [1.9201e-04],\n",
      "        [1.1136e-03],\n",
      "        [7.9835e-04],\n",
      "        [4.7305e-04],\n",
      "        [1.3335e-03],\n",
      "        [2.6922e-04],\n",
      "        [1.4821e-03],\n",
      "        [9.3109e-06],\n",
      "        [3.9737e-04],\n",
      "        [7.3690e-04],\n",
      "        [7.9369e-04],\n",
      "        [4.7795e-04],\n",
      "        [4.5866e-02]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.230932235717773: \n",
      "target probs tensor([[1.3187e-03],\n",
      "        [1.1112e-03],\n",
      "        [1.4320e-04],\n",
      "        [2.6747e-03],\n",
      "        [2.5269e-03],\n",
      "        [2.8092e-05],\n",
      "        [1.5739e-03],\n",
      "        [2.7527e-03],\n",
      "        [6.3249e-04],\n",
      "        [2.2883e-05],\n",
      "        [1.5789e-02],\n",
      "        [3.4842e-04],\n",
      "        [1.6675e-04],\n",
      "        [2.3126e-02],\n",
      "        [7.4754e-04],\n",
      "        [4.6099e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.301846504211426: \n",
      "target probs tensor([[0.0009],\n",
      "        [0.0015],\n",
      "        [0.0044],\n",
      "        [0.0003],\n",
      "        [0.0002],\n",
      "        [0.0024],\n",
      "        [0.0016],\n",
      "        [0.0004],\n",
      "        [0.0022],\n",
      "        [0.0008],\n",
      "        [0.0076],\n",
      "        [0.0044],\n",
      "        [0.0027],\n",
      "        [0.0020],\n",
      "        [0.0002],\n",
      "        [0.0005]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.702552318572998: \n",
      "target probs tensor([[1.6818e-05],\n",
      "        [7.6671e-04],\n",
      "        [8.3457e-04],\n",
      "        [4.4900e-04],\n",
      "        [7.3317e-04],\n",
      "        [6.2708e-04],\n",
      "        [1.0329e-03],\n",
      "        [4.1112e-04],\n",
      "        [5.3702e-04],\n",
      "        [7.0582e-04],\n",
      "        [3.5412e-03],\n",
      "        [1.0330e-03],\n",
      "        [2.0612e-04],\n",
      "        [5.3927e-03],\n",
      "        [1.9518e-04],\n",
      "        [4.7761e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.464399814605713: \n",
      "target probs tensor([[1.4304e-05],\n",
      "        [1.5428e-04],\n",
      "        [6.9913e-04],\n",
      "        [5.5925e-04],\n",
      "        [7.5183e-05],\n",
      "        [4.9547e-04],\n",
      "        [1.2431e-03],\n",
      "        [2.4932e-05],\n",
      "        [8.6045e-04],\n",
      "        [9.7280e-04],\n",
      "        [1.9925e-04],\n",
      "        [3.3389e-03],\n",
      "        [6.1580e-04],\n",
      "        [3.1319e-04],\n",
      "        [3.9018e-04],\n",
      "        [1.1374e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.961689472198486: \n",
      "target probs tensor([[2.5577e-02],\n",
      "        [5.3734e-04],\n",
      "        [2.2066e-03],\n",
      "        [1.4159e-03],\n",
      "        [1.2850e-04],\n",
      "        [1.9497e-04],\n",
      "        [1.9585e-03],\n",
      "        [1.6035e-03],\n",
      "        [1.1876e-03],\n",
      "        [9.8095e-04],\n",
      "        [3.6772e-04],\n",
      "        [6.7070e-05],\n",
      "        [3.8027e-04],\n",
      "        [1.5824e-04],\n",
      "        [7.8233e-04],\n",
      "        [7.3708e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.2635908126831055: \n",
      "target probs tensor([[1.1283e-03],\n",
      "        [6.5514e-04],\n",
      "        [1.4652e-05],\n",
      "        [4.2185e-03],\n",
      "        [3.2741e-04],\n",
      "        [1.6883e-03],\n",
      "        [6.2538e-07],\n",
      "        [4.5980e-04],\n",
      "        [8.2577e-04],\n",
      "        [8.0368e-04],\n",
      "        [1.2520e-03],\n",
      "        [1.0576e-02],\n",
      "        [5.2437e-04],\n",
      "        [7.5206e-01],\n",
      "        [5.7343e-07],\n",
      "        [1.6822e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.571864604949951: \n",
      "target probs tensor([[5.4793e-03],\n",
      "        [3.0687e-03],\n",
      "        [2.7523e-04],\n",
      "        [4.4404e-04],\n",
      "        [5.9431e-04],\n",
      "        [1.6871e-04],\n",
      "        [2.4652e-04],\n",
      "        [1.0129e-03],\n",
      "        [6.3351e-05],\n",
      "        [1.2614e-03],\n",
      "        [4.5634e-03],\n",
      "        [2.7829e-04],\n",
      "        [2.1155e-03],\n",
      "        [1.8596e-03],\n",
      "        [9.1042e-04],\n",
      "        [2.3726e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.246370792388916: \n",
      "target probs tensor([[0.0002],\n",
      "        [0.0005],\n",
      "        [0.0012],\n",
      "        [0.0652],\n",
      "        [0.0019],\n",
      "        [0.0003],\n",
      "        [0.0032],\n",
      "        [0.0009]], device='cuda:0'), loss: 6.625153541564941: \n",
      "target probs tensor([[5.9909e-04],\n",
      "        [2.3504e-05],\n",
      "        [4.4591e-04],\n",
      "        [1.2036e-04],\n",
      "        [8.7297e-04],\n",
      "        [1.2889e-03],\n",
      "        [3.0691e-04],\n",
      "        [6.8277e-04],\n",
      "        [7.1645e-04],\n",
      "        [1.0018e-04],\n",
      "        [2.0736e-03],\n",
      "        [8.5413e-03],\n",
      "        [1.3019e-04],\n",
      "        [2.0713e-03],\n",
      "        [3.6787e-04],\n",
      "        [1.4341e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.554178237915039: \n",
      "target probs tensor([[1.4228e-04],\n",
      "        [1.2586e-03],\n",
      "        [1.5555e-04],\n",
      "        [1.6049e-03],\n",
      "        [6.8185e-05],\n",
      "        [7.0370e-04],\n",
      "        [1.5891e-02],\n",
      "        [3.1855e-04],\n",
      "        [1.6637e-03],\n",
      "        [7.1121e-04],\n",
      "        [3.3429e-05],\n",
      "        [2.7919e-06],\n",
      "        [3.2208e-04],\n",
      "        [6.1041e-03],\n",
      "        [3.3050e-03],\n",
      "        [6.3982e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.526909351348877: \n",
      "target probs tensor([[2.2651e-04],\n",
      "        [3.1430e-04],\n",
      "        [1.2993e-03],\n",
      "        [3.1089e-04],\n",
      "        [1.2848e-03],\n",
      "        [1.4555e-04],\n",
      "        [1.0916e-02],\n",
      "        [2.2676e-04],\n",
      "        [1.0986e-03],\n",
      "        [1.7063e-02],\n",
      "        [1.2576e-03],\n",
      "        [5.1472e-05],\n",
      "        [1.0226e-03],\n",
      "        [4.7296e-04],\n",
      "        [8.1878e-04],\n",
      "        [9.8748e-05]], device='cuda:0'), loss: 7.368210792541504: \n",
      "target probs tensor([[8.0164e-04],\n",
      "        [1.4255e-03],\n",
      "        [1.9976e-03],\n",
      "        [1.2377e-03],\n",
      "        [1.4858e-03],\n",
      "        [8.1868e-04],\n",
      "        [4.7309e-04],\n",
      "        [2.4888e-03],\n",
      "        [4.2449e-04],\n",
      "        [1.7995e-04],\n",
      "        [1.2123e-04],\n",
      "        [5.2558e-04],\n",
      "        [2.7449e-03],\n",
      "        [1.6844e-01],\n",
      "        [5.0447e-04],\n",
      "        [3.3963e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.8799519538879395: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[1.4860e-04],\n",
      "        [6.7531e-04],\n",
      "        [8.3324e-04],\n",
      "        [8.7845e-04],\n",
      "        [1.1213e-03],\n",
      "        [1.4739e-03],\n",
      "        [7.2719e-04],\n",
      "        [2.3594e-04],\n",
      "        [1.3220e-04],\n",
      "        [2.8616e-04],\n",
      "        [1.7170e-04],\n",
      "        [6.5571e-04],\n",
      "        [7.1011e-05],\n",
      "        [4.2842e-06],\n",
      "        [1.5660e-04],\n",
      "        [5.6213e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 8.004966735839844: \n",
      "target probs tensor([[2.6036e-04],\n",
      "        [6.5566e-04],\n",
      "        [2.0427e-03],\n",
      "        [7.1546e-05],\n",
      "        [2.5413e-04],\n",
      "        [8.3061e-04],\n",
      "        [2.3653e-03],\n",
      "        [7.3707e-03],\n",
      "        [1.2024e-06],\n",
      "        [3.9570e-04],\n",
      "        [1.2363e-04],\n",
      "        [1.0496e-03],\n",
      "        [9.5534e-04],\n",
      "        [1.2544e-03],\n",
      "        [1.3207e-04],\n",
      "        [3.9193e-04]], device='cuda:0'), loss: 7.836544990539551: \n",
      "target probs tensor([[9.6310e-04],\n",
      "        [2.9110e-04],\n",
      "        [2.1739e-05],\n",
      "        [1.3372e-03],\n",
      "        [3.4452e-03],\n",
      "        [8.8974e-04],\n",
      "        [4.1846e-04],\n",
      "        [8.6537e-04],\n",
      "        [7.4991e-04],\n",
      "        [3.4817e-01],\n",
      "        [6.5191e-04],\n",
      "        [1.7780e-03],\n",
      "        [7.6511e-04],\n",
      "        [1.1781e-03],\n",
      "        [2.3313e-04],\n",
      "        [1.1082e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.93588924407959: \n",
      "target probs tensor([[2.0075e-02],\n",
      "        [9.1660e-04],\n",
      "        [7.2515e-04],\n",
      "        [1.1372e-03],\n",
      "        [1.0700e-04],\n",
      "        [7.7222e-03],\n",
      "        [3.5913e-04],\n",
      "        [2.6558e-03],\n",
      "        [4.8207e-03],\n",
      "        [4.2838e-04],\n",
      "        [2.2720e-05],\n",
      "        [2.1313e-04],\n",
      "        [1.7688e-04],\n",
      "        [4.0607e-06],\n",
      "        [2.8003e-02],\n",
      "        [9.1432e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.434148788452148: \n",
      "target probs tensor([[3.4457e-04],\n",
      "        [1.1960e-03],\n",
      "        [7.5970e-04],\n",
      "        [8.6079e-04],\n",
      "        [6.0629e-05],\n",
      "        [2.1502e-03],\n",
      "        [2.4331e-03],\n",
      "        [1.4092e-02],\n",
      "        [4.4942e-04],\n",
      "        [3.9816e-04],\n",
      "        [1.0147e-03],\n",
      "        [1.8795e-04],\n",
      "        [9.6476e-05],\n",
      "        [1.3441e-03],\n",
      "        [8.8531e-04],\n",
      "        [2.0377e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.341930389404297: \n",
      "target probs tensor([[1.6709e-02],\n",
      "        [8.0368e-03],\n",
      "        [2.4975e-04],\n",
      "        [1.4470e-03],\n",
      "        [1.0595e-03],\n",
      "        [2.1225e-05],\n",
      "        [3.0561e-04],\n",
      "        [3.2390e-04],\n",
      "        [3.4059e-04],\n",
      "        [3.2494e-04],\n",
      "        [1.2282e-03],\n",
      "        [2.8196e-03],\n",
      "        [1.4720e-03],\n",
      "        [6.4467e-04],\n",
      "        [8.8334e-04],\n",
      "        [1.1052e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.255465507507324: \n",
      "target probs tensor([[2.2469e-04],\n",
      "        [5.0480e-02],\n",
      "        [7.3387e-04],\n",
      "        [1.4049e-03],\n",
      "        [1.1111e-03],\n",
      "        [1.2354e-03],\n",
      "        [6.9076e-03],\n",
      "        [1.0206e-03],\n",
      "        [1.2517e-03],\n",
      "        [1.4447e-03],\n",
      "        [1.6886e-01],\n",
      "        [1.4091e-04],\n",
      "        [5.0283e-04],\n",
      "        [2.6152e-04],\n",
      "        [1.7433e-04],\n",
      "        [6.1729e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.643208026885986: \n",
      "target probs tensor([[1.8104e-03],\n",
      "        [2.7133e-04],\n",
      "        [7.3803e-04],\n",
      "        [5.6331e-04],\n",
      "        [5.8025e-05],\n",
      "        [5.8579e-04],\n",
      "        [1.5322e-04],\n",
      "        [1.9447e-03],\n",
      "        [6.0050e-04],\n",
      "        [7.9535e-04],\n",
      "        [4.4947e-04],\n",
      "        [4.4988e-04],\n",
      "        [2.9951e-04],\n",
      "        [2.4599e-03],\n",
      "        [8.2532e-04],\n",
      "        [6.1024e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.502118110656738: \n",
      "target probs tensor([[6.5013e-04],\n",
      "        [3.9410e-05],\n",
      "        [5.0914e-04],\n",
      "        [4.1684e-04],\n",
      "        [2.8747e-04],\n",
      "        [4.9465e-04],\n",
      "        [9.3659e-03],\n",
      "        [6.3964e-03],\n",
      "        [2.0062e-03],\n",
      "        [9.2146e-04],\n",
      "        [2.7059e-04],\n",
      "        [3.8734e-04],\n",
      "        [2.7176e-05],\n",
      "        [6.3374e-05],\n",
      "        [3.9231e-02],\n",
      "        [7.8534e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.385855674743652: \n",
      "target probs tensor([[4.7407e-03],\n",
      "        [8.0857e-05],\n",
      "        [1.0150e-03],\n",
      "        [9.9057e-05],\n",
      "        [4.9479e-05],\n",
      "        [1.4479e-03],\n",
      "        [5.3375e-04],\n",
      "        [3.6979e-04],\n",
      "        [8.7550e-04],\n",
      "        [4.3336e-04],\n",
      "        [4.6573e-04],\n",
      "        [1.2553e-03],\n",
      "        [7.2531e-04],\n",
      "        [4.2127e-04],\n",
      "        [2.2269e-04],\n",
      "        [3.2075e-05]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.85451078414917: \n",
      "target probs tensor([[1.7865e-04],\n",
      "        [2.8089e-04],\n",
      "        [4.3275e-04],\n",
      "        [5.1828e-03],\n",
      "        [9.4112e-05],\n",
      "        [2.7351e-03],\n",
      "        [1.7584e-03],\n",
      "        [1.7553e-03],\n",
      "        [1.3133e-04],\n",
      "        [1.8643e-01],\n",
      "        [2.5757e-04],\n",
      "        [1.9234e-04],\n",
      "        [2.8126e-04],\n",
      "        [5.1660e-04],\n",
      "        [5.7568e-04],\n",
      "        [2.0384e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.157144546508789: \n",
      "target probs tensor([[2.3802e-02],\n",
      "        [1.5944e-04],\n",
      "        [9.0990e-04],\n",
      "        [1.4865e-04],\n",
      "        [9.3279e-05],\n",
      "        [5.4078e-04],\n",
      "        [3.6932e-04],\n",
      "        [3.9849e-06],\n",
      "        [8.9022e-04],\n",
      "        [1.4386e-01],\n",
      "        [1.5021e-03],\n",
      "        [3.1913e-04],\n",
      "        [3.4650e-04],\n",
      "        [8.5185e-04],\n",
      "        [2.0779e-04],\n",
      "        [1.6690e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.428823947906494: \n",
      "target probs tensor([[2.5261e-03],\n",
      "        [5.4375e-04],\n",
      "        [1.8169e-03],\n",
      "        [1.8696e-02],\n",
      "        [1.2677e-03],\n",
      "        [1.1467e-03],\n",
      "        [4.5426e-04],\n",
      "        [7.5107e-04],\n",
      "        [4.1135e-05],\n",
      "        [1.3554e-03],\n",
      "        [4.5954e-05],\n",
      "        [1.8807e-03],\n",
      "        [3.6670e-03],\n",
      "        [6.0187e-04],\n",
      "        [3.5464e-03],\n",
      "        [9.1935e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.921512603759766: \n",
      "target probs tensor([[2.3996e-05],\n",
      "        [2.9457e-05],\n",
      "        [4.3404e-04],\n",
      "        [8.5651e-03],\n",
      "        [3.3405e-02],\n",
      "        [3.1483e-04],\n",
      "        [2.9138e-04],\n",
      "        [4.4814e-04],\n",
      "        [2.6999e-03],\n",
      "        [1.1230e-04],\n",
      "        [9.1631e-05],\n",
      "        [3.9762e-05],\n",
      "        [2.3987e-04],\n",
      "        [4.3843e-05],\n",
      "        [6.5578e-03],\n",
      "        [1.1423e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.987524509429932: \n",
      "target probs tensor([[5.6026e-04],\n",
      "        [1.5369e-03],\n",
      "        [5.9367e-05],\n",
      "        [8.4311e-04],\n",
      "        [1.4053e-03],\n",
      "        [6.8050e-06],\n",
      "        [1.0694e-03],\n",
      "        [3.7974e-03],\n",
      "        [8.3429e-04],\n",
      "        [8.6050e-06],\n",
      "        [1.4470e-03],\n",
      "        [1.1910e-04],\n",
      "        [2.0579e-03],\n",
      "        [3.4458e-03],\n",
      "        [2.4774e-05],\n",
      "        [3.2448e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.904763221740723: \n",
      "target probs tensor([[7.1141e-04],\n",
      "        [2.0615e-04],\n",
      "        [1.4086e-04],\n",
      "        [9.5581e-04],\n",
      "        [1.9573e-03],\n",
      "        [3.6427e-04],\n",
      "        [4.2495e-03],\n",
      "        [6.0712e-06],\n",
      "        [2.1170e-04],\n",
      "        [5.8275e-04],\n",
      "        [8.4476e-06],\n",
      "        [8.4838e-04],\n",
      "        [5.2007e-05],\n",
      "        [8.7320e-03],\n",
      "        [2.3584e-03],\n",
      "        [7.7067e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.854208946228027: \n",
      "target probs tensor([[0.0005],\n",
      "        [0.0005],\n",
      "        [0.0029],\n",
      "        [0.0019],\n",
      "        [0.0270],\n",
      "        [0.0017],\n",
      "        [0.0039],\n",
      "        [0.0003],\n",
      "        [0.0002],\n",
      "        [0.0014],\n",
      "        [0.0047],\n",
      "        [0.0022],\n",
      "        [0.0045],\n",
      "        [0.0215],\n",
      "        [0.0017],\n",
      "        [0.1514]], device='cuda:0', grad_fn=<GatherBackward>), loss: 5.958183288574219: \n",
      "target probs tensor([[1.7130e-03],\n",
      "        [8.9507e-04],\n",
      "        [2.1846e-03],\n",
      "        [7.5358e-07],\n",
      "        [1.3552e-03],\n",
      "        [4.9800e-03],\n",
      "        [1.9186e-03],\n",
      "        [2.8353e-03],\n",
      "        [4.1686e-04],\n",
      "        [7.0006e-04],\n",
      "        [8.9381e-04],\n",
      "        [9.9059e-04],\n",
      "        [4.8643e-05],\n",
      "        [6.7935e-05],\n",
      "        [7.7408e-04],\n",
      "        [1.7363e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.479545593261719: \n",
      "target probs tensor([[1.4869e-03],\n",
      "        [4.7305e-04],\n",
      "        [1.6040e-03],\n",
      "        [1.1529e-04],\n",
      "        [1.2829e-03],\n",
      "        [1.7515e-04],\n",
      "        [3.6265e-04],\n",
      "        [3.4716e-04],\n",
      "        [8.9906e-05],\n",
      "        [9.3264e-04],\n",
      "        [7.5727e-04],\n",
      "        [9.1682e-05],\n",
      "        [5.4883e-04],\n",
      "        [6.6242e-03],\n",
      "        [6.7786e-04],\n",
      "        [2.6924e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.461426734924316: \n",
      "target probs tensor([[0.0002],\n",
      "        [0.0004],\n",
      "        [0.0008],\n",
      "        [0.0007],\n",
      "        [0.0002],\n",
      "        [0.0003],\n",
      "        [0.0007],\n",
      "        [0.0002]], device='cuda:0'), loss: 7.926172733306885: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target probs tensor([[2.0703e-04],\n",
      "        [2.7401e-04],\n",
      "        [1.2931e-03],\n",
      "        [5.4352e-04],\n",
      "        [2.0994e-03],\n",
      "        [1.1718e-04],\n",
      "        [1.5204e-03],\n",
      "        [1.0604e-03],\n",
      "        [1.0043e-03],\n",
      "        [3.1612e-04],\n",
      "        [4.6699e-04],\n",
      "        [3.5907e-03],\n",
      "        [1.6417e-03],\n",
      "        [1.0589e-03],\n",
      "        [1.9366e-05],\n",
      "        [4.8319e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 7.4637556076049805: \n",
      "target probs tensor([[0.0004],\n",
      "        [0.0014],\n",
      "        [0.0003],\n",
      "        [0.0007],\n",
      "        [0.0132],\n",
      "        [0.0016],\n",
      "        [0.0006],\n",
      "        [0.0019],\n",
      "        [0.0007],\n",
      "        [0.0011],\n",
      "        [0.0003],\n",
      "        [0.0009],\n",
      "        [0.0147],\n",
      "        [0.0057],\n",
      "        [0.0006],\n",
      "        [0.0001]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.830201148986816: \n",
      "target probs tensor([[4.5615e-04],\n",
      "        [1.3772e-04],\n",
      "        [1.1120e-03],\n",
      "        [1.1077e-03],\n",
      "        [3.8092e-04],\n",
      "        [3.1769e-04],\n",
      "        [1.5582e-03],\n",
      "        [6.0047e-04],\n",
      "        [7.7009e-04],\n",
      "        [1.0600e-03],\n",
      "        [1.3343e-03],\n",
      "        [2.1319e-05],\n",
      "        [1.7290e-02],\n",
      "        [4.0806e-04],\n",
      "        [1.8951e-02],\n",
      "        [2.2805e-02]], device='cuda:0'), loss: 6.937602996826172: \n",
      "target probs tensor([[0.0007],\n",
      "        [0.0091],\n",
      "        [0.0004],\n",
      "        [0.0125],\n",
      "        [0.0005],\n",
      "        [0.0004],\n",
      "        [0.0013],\n",
      "        [0.0006],\n",
      "        [0.0007],\n",
      "        [0.0008],\n",
      "        [0.0004],\n",
      "        [0.0008],\n",
      "        [0.0004],\n",
      "        [0.0009],\n",
      "        [0.0034],\n",
      "        [0.0014]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.884914398193359: \n",
      "target probs tensor([[1.5459e-04],\n",
      "        [6.7131e-04],\n",
      "        [3.0809e-03],\n",
      "        [1.3963e-02],\n",
      "        [3.4565e-03],\n",
      "        [3.4653e-04],\n",
      "        [1.8981e-03],\n",
      "        [3.7953e-04],\n",
      "        [2.7469e-03],\n",
      "        [1.4440e-04],\n",
      "        [2.7247e-04],\n",
      "        [4.3210e-02],\n",
      "        [1.6641e-03],\n",
      "        [4.3470e-04],\n",
      "        [8.4001e-05],\n",
      "        [2.1522e-03]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.854285717010498: \n",
      "target probs tensor([[1.4064e-03],\n",
      "        [5.4169e-04],\n",
      "        [1.4400e-04],\n",
      "        [1.8858e-04],\n",
      "        [1.8741e-04],\n",
      "        [1.6903e-03],\n",
      "        [7.3874e-04],\n",
      "        [6.7373e-04],\n",
      "        [1.0141e-06],\n",
      "        [5.2415e-04],\n",
      "        [2.4611e-04],\n",
      "        [6.3491e-04],\n",
      "        [2.1283e-03],\n",
      "        [5.7022e-04],\n",
      "        [8.3942e-05],\n",
      "        [7.6082e-04]], device='cuda:0'), loss: 8.012678146362305: \n",
      "target probs tensor([[8.5939e-03],\n",
      "        [1.4040e-03],\n",
      "        [6.5588e-05],\n",
      "        [1.4532e-04],\n",
      "        [2.2669e-02],\n",
      "        [4.8967e-03],\n",
      "        [1.1102e-03],\n",
      "        [7.9638e-04],\n",
      "        [5.7656e-04],\n",
      "        [2.6381e-04],\n",
      "        [2.5375e-03],\n",
      "        [4.0308e-03],\n",
      "        [5.1658e-04],\n",
      "        [1.2731e-04],\n",
      "        [1.2129e-03],\n",
      "        [3.8303e-04]], device='cuda:0', grad_fn=<GatherBackward>), loss: 6.946674346923828: \n"
     ]
    }
   ],
   "source": [
    "# RUN SITE\n",
    "if 'x' in env.save_filename:\n",
    "  raise ValueError('save_filename contains x')\n",
    "\n",
    "saver_best = SaveModelCallback(learn, every='improvement', monitor='validation', name=env.save_filename + \"-best\")\n",
    "saver_every_epoch = SaveModelCallback(learn, every='epoch', name=env.save_filename)\n",
    "fooling_weight_scheduler = FoolingWeightScheduler(learn, fooling_loss_index = 3)\n",
    "lr_anneal = LRAnneal(learn, 1e-4)\n",
    "\n",
    "callbacks = [saver_best, saver_every_epoch]\n",
    "callbacks.append(lr_anneal)\n",
    "# callbacks.append(fooling_weight_scheduler)\n",
    "\n",
    "learn.fit(300, lr=1e-2, wd = 0., callbacks=callbacks)\n",
    "\n",
    "# with Hooks(gen, append_stats_normal) as hooks:\n",
    "#   learn.fit(1, lr=5e-03, wd = 0., callbacks=[saver_best, saver_every_epoch])\n",
    "\n",
    "# for i in range(10):\n",
    "#   learn.fit_one_cycle(7, wd = 0.,max_lr=1., div_factor = 1000.) \n",
    "  \n",
    "shutil.copyfile(env.temp_csv_path + '/' + env.save_filename + \".csv\", env.get_csv_path() + '.csv')\n",
    "shutil.copytree(env.data_path/env.get_learner_models_dir(), env.get_models_path())\n",
    "shutil.rmtree(env.data_path/env.get_learner_models_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanup import cleanup_models_folder\n",
    "cleanup_models_folder('/root/Derakhshani/adversarial/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "percent_total: \n",
      "[(7, 99.999999999, 10.0), (61, 99.999999999, 10.0), (90, 99.999999999, 10.0), (163, 99.999999999, 10.0), (274, 99.999999999, 10.0), (376, 99.999999999, 10.0), (398, 99.999999999, 10.0), (411, 99.999999999, 10.0), (464, 99.999999999, 10.0), (474, 99.999999999, 10.0), (483, 99.999999999, 10.0), (533, 99.999999999, 10.0), (566, 99.999999999, 10.0), (580, 99.999999999, 10.0), (625, 99.999999999, 10.0), (637, 99.999999999, 10.0), (661, 99.999999999, 10.0), (738, 99.999999999, 10.0), (783, 99.999999999, 10.0), (787, 99.999999999, 10.0), (791, 99.999999999, 10.0), (816, 99.999999999, 10.0), (937, 99.999999999, 10.0), (987, 99.999999999, 10.0), (292, 89.99999999955001, 20.0), (489, 89.99999999955001, 20.0), (37, 89.9999999991, 10.0), (45, 89.9999999991, 10.0), (66, 89.9999999991, 10.0), (96, 89.9999999991, 10.0), (102, 89.9999999991, 10.0), (242, 89.9999999991, 10.0), (291, 89.9999999991, 10.0), (331, 89.9999999991, 10.0), (498, 89.9999999991, 10.0), (506, 89.9999999991, 10.0), (581, 89.9999999991, 10.0), (612, 89.9999999991, 10.0), (694, 89.9999999991, 10.0), (709, 89.9999999991, 10.0), (33, 79.9999999992, 10.0), (189, 79.9999999992, 10.0), (235, 79.9999999992, 10.0), (360, 79.9999999992, 10.0), (375, 79.9999999992, 10.0), (401, 79.9999999992, 10.0), (409, 79.9999999992, 10.0), (417, 79.9999999992, 10.0), (443, 79.9999999992, 10.0), (468, 79.9999999992, 10.0), (507, 79.9999999992, 10.0), (570, 79.9999999992, 10.0), (579, 79.9999999992, 10.0), (746, 79.9999999992, 10.0), (822, 79.9999999992, 10.0), (870, 79.9999999992, 10.0), (955, 79.9999999992, 10.0), (984, 79.9999999992, 10.0), (25, 74.999999999625, 20.0), (538, 69.99999999965, 20.0), (872, 69.99999999965, 20.0), (52, 69.9999999993, 10.0), (86, 69.9999999993, 10.0), (92, 69.9999999993, 10.0), (160, 69.9999999993, 10.0), (164, 69.9999999993, 10.0), (198, 69.9999999993, 10.0), (260, 69.9999999993, 10.0), (321, 69.9999999993, 10.0), (347, 69.9999999993, 10.0), (349, 69.9999999993, 10.0), (423, 69.9999999993, 10.0), (425, 69.9999999993, 10.0), (528, 69.9999999993, 10.0), (555, 69.9999999993, 10.0), (582, 69.9999999993, 10.0), (586, 69.9999999993, 10.0), (629, 69.9999999993, 10.0), (668, 69.9999999993, 10.0), (716, 69.9999999993, 10.0), (762, 69.9999999993, 10.0), (776, 69.9999999993, 10.0), (781, 69.9999999993, 10.0), (803, 69.9999999993, 10.0), (939, 69.9999999993, 10.0), (992, 69.9999999993, 10.0), (72, 64.99999999967501, 20.0), (828, 64.99999999967501, 20.0), (230, 59.99999999970001, 20.0), (858, 59.99999999970001, 20.0), (15, 59.9999999994, 10.0), (21, 59.9999999994, 10.0), (42, 59.9999999994, 10.0), (120, 59.9999999994, 10.0), (252, 59.9999999994, 10.0), (300, 59.9999999994, 10.0), (307, 59.9999999994, 10.0), (341, 59.9999999994, 10.0), (369, 59.9999999994, 10.0), (430, 59.9999999994, 10.0), (518, 59.9999999994, 10.0), (568, 59.9999999994, 10.0), (576, 59.9999999994, 10.0), (585, 59.9999999994, 10.0), (609, 59.9999999994, 10.0), (658, 59.9999999994, 10.0), (684, 59.9999999994, 10.0), (698, 59.9999999994, 10.0), (712, 59.9999999994, 10.0), (729, 59.9999999994, 10.0), (779, 59.9999999994, 10.0), (789, 59.9999999994, 10.0), (806, 59.9999999994, 10.0), (847, 59.9999999994, 10.0), (864, 59.9999999994, 10.0), (873, 59.9999999994, 10.0), (900, 59.9999999994, 10.0), (906, 59.9999999994, 10.0), (920, 59.9999999994, 10.0), (944, 59.9999999994, 10.0), (57, 49.99999999975, 20.0), (91, 49.99999999975, 20.0), (454, 49.99999999975, 20.0), (953, 49.99999999975, 20.0), (8, 49.9999999995, 10.0), (24, 49.9999999995, 10.0), (28, 49.9999999995, 10.0), (83, 49.9999999995, 10.0), (87, 49.9999999995, 10.0), (115, 49.9999999995, 10.0), (151, 49.9999999995, 10.0), (183, 49.9999999995, 10.0), (186, 49.9999999995, 10.0), (188, 49.9999999995, 10.0), (197, 49.9999999995, 10.0), (232, 49.9999999995, 10.0), (286, 49.9999999995, 10.0), (289, 49.9999999995, 10.0), (293, 49.9999999995, 10.0), (303, 49.9999999995, 10.0), (306, 49.9999999995, 10.0), (313, 49.9999999995, 10.0), (314, 49.9999999995, 10.0), (316, 49.9999999995, 10.0), (378, 49.9999999995, 10.0), (393, 49.9999999995, 10.0), (399, 49.9999999995, 10.0), (407, 49.9999999995, 10.0), (429, 49.9999999995, 10.0), (496, 49.9999999995, 10.0), (509, 49.9999999995, 10.0), (511, 49.9999999995, 10.0), (534, 49.9999999995, 10.0), (567, 49.9999999995, 10.0), (572, 49.9999999995, 10.0), (619, 49.9999999995, 10.0), (621, 49.9999999995, 10.0), (646, 49.9999999995, 10.0), (734, 49.9999999995, 10.0), (751, 49.9999999995, 10.0), (753, 49.9999999995, 10.0), (765, 49.9999999995, 10.0), (905, 49.9999999995, 10.0), (919, 49.9999999995, 10.0), (128, 44.99999999977501, 20.0), (281, 44.99999999977501, 20.0), (334, 44.99999999977501, 20.0), (721, 44.99999999977501, 20.0), (75, 39.99999999986667, 30.0), (491, 39.9999999998, 20.0), (10, 39.9999999996, 10.0), (31, 39.9999999996, 10.0), (79, 39.9999999996, 10.0), (141, 39.9999999996, 10.0), (145, 39.9999999996, 10.0), (153, 39.9999999996, 10.0), (192, 39.9999999996, 10.0), (196, 39.9999999996, 10.0), (228, 39.9999999996, 10.0), (276, 39.9999999996, 10.0), (342, 39.9999999996, 10.0), (465, 39.9999999996, 10.0), (495, 39.9999999996, 10.0), (584, 39.9999999996, 10.0), (593, 39.9999999996, 10.0), (614, 39.9999999996, 10.0), (620, 39.9999999996, 10.0), (727, 39.9999999996, 10.0), (755, 39.9999999996, 10.0), (802, 39.9999999996, 10.0), (826, 39.9999999996, 10.0), (981, 39.9999999996, 10.0), (985, 39.9999999996, 10.0), (134, 36.666666666544444, 30.0), (55, 34.999999999825, 20.0), (84, 34.999999999825, 20.0), (343, 34.999999999825, 20.0), (565, 34.999999999825, 20.0), (777, 29.999999999850004, 20.0), (831, 29.999999999850004, 20.0), (47, 29.9999999997, 10.0), (99, 29.9999999997, 10.0), (109, 29.9999999997, 10.0), (116, 29.9999999997, 10.0), (144, 29.9999999997, 10.0), (218, 29.9999999997, 10.0), (319, 29.9999999997, 10.0), (381, 29.9999999997, 10.0), (392, 29.9999999997, 10.0), (424, 29.9999999997, 10.0), (442, 29.9999999997, 10.0), (502, 29.9999999997, 10.0), (651, 29.9999999997, 10.0), (706, 29.9999999997, 10.0), (890, 29.9999999997, 10.0), (926, 29.9999999997, 10.0), (517, 26.66666666657778, 30.0), (575, 26.66666666657778, 30.0), (68, 24.999999999875, 20.0), (205, 24.999999999875, 20.0), (437, 24.999999999875, 20.0), (547, 24.999999999875, 20.0), (558, 24.999999999875, 20.0), (607, 24.999999999875, 20.0), (722, 24.999999999875, 20.0), (918, 24.999999999875, 20.0), (238, 23.333333333255556, 30.0), (344, 19.9999999999, 20.0), (416, 19.9999999999, 20.0), (110, 19.9999999998, 10.0), (158, 19.9999999998, 10.0), (176, 19.9999999998, 10.0), (202, 19.9999999998, 10.0), (233, 19.9999999998, 10.0), (290, 19.9999999998, 10.0), (315, 19.9999999998, 10.0), (388, 19.9999999998, 10.0), (402, 19.9999999998, 10.0), (452, 19.9999999998, 10.0), (477, 19.9999999998, 10.0), (520, 19.9999999998, 10.0), (522, 19.9999999998, 10.0), (574, 19.9999999998, 10.0), (624, 19.9999999998, 10.0), (645, 19.9999999998, 10.0), (819, 19.9999999998, 10.0), (825, 19.9999999998, 10.0), (839, 19.9999999998, 10.0), (854, 19.9999999998, 10.0), (904, 19.9999999998, 10.0), (917, 19.9999999998, 10.0), (463, 16.66666666661111, 30.0), (472, 16.66666666661111, 30.0), (652, 14.9999999999625, 40.0), (38, 14.999999999925002, 20.0), (384, 14.999999999925002, 20.0), (405, 14.999999999925002, 20.0), (492, 14.999999999925002, 20.0), (848, 14.999999999925002, 20.0), (850, 14.999999999925002, 20.0), (893, 14.999999999925002, 20.0), (554, 13.33333333328889, 30.0), (921, 9.999999999966667, 30.0), (595, 9.99999999995, 20.0), (656, 9.99999999995, 20.0), (869, 9.99999999995, 20.0), (909, 9.99999999995, 20.0), (1, 9.9999999999, 10.0), (18, 9.9999999999, 10.0), (59, 9.9999999999, 10.0), (60, 9.9999999999, 10.0), (98, 9.9999999999, 10.0), (123, 9.9999999999, 10.0), (139, 9.9999999999, 10.0), (150, 9.9999999999, 10.0), (182, 9.9999999999, 10.0), (193, 9.9999999999, 10.0), (204, 9.9999999999, 10.0), (213, 9.9999999999, 10.0), (236, 9.9999999999, 10.0), (253, 9.9999999999, 10.0), (258, 9.9999999999, 10.0), (264, 9.9999999999, 10.0), (275, 9.9999999999, 10.0), (294, 9.9999999999, 10.0), (301, 9.9999999999, 10.0), (302, 9.9999999999, 10.0), (326, 9.9999999999, 10.0), (330, 9.9999999999, 10.0), (355, 9.9999999999, 10.0), (406, 9.9999999999, 10.0), (420, 9.9999999999, 10.0), (439, 9.9999999999, 10.0), (459, 9.9999999999, 10.0), (486, 9.9999999999, 10.0), (490, 9.9999999999, 10.0), (505, 9.9999999999, 10.0), (530, 9.9999999999, 10.0), (535, 9.9999999999, 10.0), (543, 9.9999999999, 10.0), (560, 9.9999999999, 10.0), (571, 9.9999999999, 10.0), (577, 9.9999999999, 10.0), (599, 9.9999999999, 10.0), (605, 9.9999999999, 10.0), (606, 9.9999999999, 10.0), (611, 9.9999999999, 10.0), (669, 9.9999999999, 10.0), (707, 9.9999999999, 10.0), (735, 9.9999999999, 10.0), (739, 9.9999999999, 10.0), (796, 9.9999999999, 10.0), (808, 9.9999999999, 10.0), (813, 9.9999999999, 10.0), (821, 9.9999999999, 10.0), (833, 9.9999999999, 10.0), (842, 9.9999999999, 10.0), (871, 9.9999999999, 10.0), (901, 9.9999999999, 10.0), (910, 9.9999999999, 10.0), (915, 9.9999999999, 10.0), (932, 9.9999999999, 10.0), (934, 9.9999999999, 10.0), (973, 9.9999999999, 10.0), (979, 9.9999999999, 10.0), (271, 4.999999999975, 20.0), (284, 4.999999999975, 20.0), (305, 4.999999999975, 20.0), (377, 4.999999999975, 20.0), (962, 4.999999999975, 20.0), (996, 4.999999999975, 20.0)]\n"
     ]
    }
   ],
   "source": [
    "total_histogram = fooled_histogram + unfooled_histogram\n",
    "percent_total = [(i, 100. * u / (total_histogram[i] + 1e-10), total_histogram[i]) for i, u in enumerate(unfooled_histogram)]\n",
    "sorted_percent_total = sorted(percent_total, key =lambda x: x[1], reverse = True)\n",
    "print('\\npercent_total: ')\n",
    "print(list(filter(lambda x: x[1] > 0.0, sorted_percent_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram\n",
    "fig, axes = plt.subplots(len(hooks),1, figsize=(30,12))\n",
    "for ax,h in zip(axes.flatten(), hooks):\n",
    "  ax.imshow(get_hist(h), origin='lower')\n",
    "  ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mean and std\n",
    "fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "for h in hooks:\n",
    "  ms, ss, _ = h.stats\n",
    "  ax0.plot(ms[:100])\n",
    "  ax1.plot(ss[:100])\n",
    "plt.legend(range(len(hooks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))\n",
    "for h in hooks:\n",
    "  ms, ss, _ = h.stats\n",
    "  ax0.plot(ms)\n",
    "  ax1.plot(ss)\n",
    "plt.legend(range(len(hooks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero precentage:\n",
    "fig,axes = plt.subplots(len(hooks),1, figsize=(30,30))\n",
    "for ax,h in zip(axes.flatten(), hooks):\n",
    "    ax.plot(get_min(h))\n",
    "    ax.set_ylim(0,1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MO2fZ-hSSUzJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# z1 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "# z2 = torch.empty(10).uniform_(-1,1).cuda()\n",
    "z1 = torch.tensor([0.8, -0.5] * 5).cuda()\n",
    "z2 = torch.tensor([-1.] * 10).cuda()\n",
    "print(\"z1: \", z1)\n",
    "print(\"z2: \", z2)\n",
    "print(\"distance: \", torch.norm(z1-z2,p=2))\n",
    "model = learn.model.eval()\n",
    "\n",
    "z_s = interpolate(z1, z2, 0.1)\n",
    "print(len(z_s))\n",
    "\n",
    "for i,z in enumerate(z_s):\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n",
    "  #img.save('./pics/' + str(i) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea : have 200 noises (1 for each class), then start iterating the dataset, and for each image, randomly apply one noise and record the result\n",
    "def targeted_diversity(learn, n_perturbations = 200, percentage = 95):\n",
    "  model = learn.model.eval()\n",
    "\n",
    "  one_hot_conditions = [torch.empty(z_dim).uniform_(0,1).cuda().detach() for _ in range(n_perturbations)]\n",
    "#   for i in range(z_dim):\n",
    "#     one_hot_conditions[i][i] = 1.\n",
    "\n",
    "  perturbations = [model.forward_single_z(z) for z in one_hot_conditions]\n",
    "\n",
    "  hist = [0.] * 1000\n",
    "  batch_no = -1\n",
    "  for batch, _ in learn.data.valid_dl:\n",
    "    batch_no += 1\n",
    "    if batch_no % 100 == 0 : print(\"at batch_no {}\".format(batch_no))\n",
    "    perturbed_batch = batch + perturbations[np.random.randint(0,len(perturbations))][None]\n",
    "    preds = arch(perturbed_batch).argmax(1)\n",
    "    for pred in preds:\n",
    "      hist[pred] += 1\n",
    "\n",
    "  pred_histogram_sum = np.sum(hist)\n",
    "  indexed_pred_histogram = [(i, hist_element) for i,hist_element in  \n",
    "                            enumerate(hist)]\n",
    "\n",
    "  indexed_pred_histogram.sort(key=lambda x: x[1], reverse = True)\n",
    "\n",
    "  cumulative_percent = 0\n",
    "  n_used_classes = 0\n",
    "  while cumulative_percent < percentage:\n",
    "    hist_elem = indexed_pred_histogram[n_used_classes]\n",
    "    cumulative_percent += (hist_elem[1] / pred_histogram_sum) * 100.\n",
    "    n_used_classes += 1\n",
    "\n",
    "  return n_used_classes, indexed_pred_histogram\n",
    "\n",
    "def targeted_diversity_average(learn, n_perturbations = 200, percentage = 95, average_over = 4):\n",
    "  results = []\n",
    "  for i in range(average_over):\n",
    "    n, _ = targeted_diversity(learn, n_perturbations, percentage)\n",
    "    print(f'done with the {i}th calculation: {n}')\n",
    "    results.append(n)\n",
    "  return np.mean(results)\n",
    "\n",
    "def diversity_average(learn, n_perturbations = 10, percentage = 95, average_over = 4):\n",
    "  results = []\n",
    "  for i in range(average_over):\n",
    "    n, _ = diversity(learn, n_perturbations, percentage, verbose = False)\n",
    "    print(f'done with the {i}th calculation: {n}')\n",
    "    results.append(n)\n",
    "  return np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 5\n",
      "at batch no 10\n",
      "at batch no 15\n",
      "at batch no 20\n",
      "at batch no 25\n",
      "at batch no 30\n",
      "at batch no 35\n",
      "at batch no 40\n",
      "at batch no 45\n",
      "at batch no 50\n",
      "at batch no 55\n",
      "at batch no 60\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(566,\n",
       " [(794, 50.099998474121094),\n",
       "  (599, 21.100000381469727),\n",
       "  (668, 20.200000762939453),\n",
       "  (904, 15.0),\n",
       "  (973, 13.600000381469727),\n",
       "  (490, 12.899999618530273),\n",
       "  (39, 12.699999809265137),\n",
       "  (770, 12.600000381469727),\n",
       "  (741, 11.300000190734863),\n",
       "  (828, 11.100000381469727),\n",
       "  (109, 9.199999809265137),\n",
       "  (556, 8.600000381469727),\n",
       "  (489, 8.399999618530273),\n",
       "  (955, 8.399999618530273),\n",
       "  (887, 8.300000190734863),\n",
       "  (669, 8.100000381469727),\n",
       "  (84, 7.400000095367432),\n",
       "  (855, 7.0),\n",
       "  (538, 6.800000190734863),\n",
       "  (108, 6.599999904632568),\n",
       "  (124, 6.0),\n",
       "  (397, 5.900000095367432),\n",
       "  (48, 5.800000190734863),\n",
       "  (61, 5.5),\n",
       "  (777, 4.900000095367432),\n",
       "  (721, 4.800000190734863),\n",
       "  (401, 4.5),\n",
       "  (971, 4.400000095367432),\n",
       "  (893, 4.300000190734863),\n",
       "  (857, 4.099999904632568),\n",
       "  (591, 4.0),\n",
       "  (711, 3.9000000953674316),\n",
       "  (709, 3.799999952316284),\n",
       "  (455, 3.700000047683716),\n",
       "  (55, 3.5999999046325684),\n",
       "  (414, 3.5),\n",
       "  (906, 3.5),\n",
       "  (151, 3.4000000953674316),\n",
       "  (389, 3.4000000953674316),\n",
       "  (406, 3.4000000953674316),\n",
       "  (865, 3.299999952316284),\n",
       "  (750, 3.200000047683716),\n",
       "  (581, 3.0),\n",
       "  (0, 2.9000000953674316),\n",
       "  (1, 2.9000000953674316),\n",
       "  (476, 2.9000000953674316),\n",
       "  (915, 2.9000000953674316),\n",
       "  (363, 2.799999952316284),\n",
       "  (837, 2.799999952316284),\n",
       "  (982, 2.799999952316284),\n",
       "  (110, 2.700000047683716),\n",
       "  (46, 2.5999999046325684),\n",
       "  (62, 2.5999999046325684),\n",
       "  (593, 2.5999999046325684),\n",
       "  (640, 2.5999999046325684),\n",
       "  (735, 2.5999999046325684),\n",
       "  (819, 2.5999999046325684),\n",
       "  (94, 2.5),\n",
       "  (126, 2.5),\n",
       "  (431, 2.5),\n",
       "  (611, 2.4000000953674316),\n",
       "  (864, 2.4000000953674316),\n",
       "  (868, 2.4000000953674316),\n",
       "  (222, 2.299999952316284),\n",
       "  (558, 2.299999952316284),\n",
       "  (572, 2.299999952316284),\n",
       "  (850, 2.299999952316284),\n",
       "  (115, 2.200000047683716),\n",
       "  (410, 2.200000047683716),\n",
       "  (698, 2.200000047683716),\n",
       "  (763, 2.200000047683716),\n",
       "  (772, 2.200000047683716),\n",
       "  (779, 2.200000047683716),\n",
       "  (97, 2.0999999046325684),\n",
       "  (238, 2.0999999046325684),\n",
       "  (440, 2.0999999046325684),\n",
       "  (447, 2.0999999046325684),\n",
       "  (464, 2.0999999046325684),\n",
       "  (872, 2.0999999046325684),\n",
       "  (898, 2.0999999046325684),\n",
       "  (68, 2.0),\n",
       "  (118, 2.0),\n",
       "  (182, 2.0),\n",
       "  (242, 2.0),\n",
       "  (292, 2.0),\n",
       "  (393, 2.0),\n",
       "  (520, 2.0),\n",
       "  (621, 2.0),\n",
       "  (60, 1.899999976158142),\n",
       "  (188, 1.899999976158142),\n",
       "  (189, 1.899999976158142),\n",
       "  (192, 1.899999976158142),\n",
       "  (342, 1.899999976158142),\n",
       "  (411, 1.899999976158142),\n",
       "  (472, 1.899999976158142),\n",
       "  (570, 1.899999976158142),\n",
       "  (619, 1.899999976158142),\n",
       "  (620, 1.899999976158142),\n",
       "  (724, 1.899999976158142),\n",
       "  (791, 1.899999976158142),\n",
       "  (800, 1.899999976158142),\n",
       "  (199, 1.7999999523162842),\n",
       "  (290, 1.7999999523162842),\n",
       "  (348, 1.7999999523162842),\n",
       "  (423, 1.7999999523162842),\n",
       "  (761, 1.7999999523162842),\n",
       "  (762, 1.7999999523162842),\n",
       "  (870, 1.7999999523162842),\n",
       "  (920, 1.7999999523162842),\n",
       "  (72, 1.7000000476837158),\n",
       "  (128, 1.7000000476837158),\n",
       "  (155, 1.7000000476837158),\n",
       "  (195, 1.7000000476837158),\n",
       "  (334, 1.7000000476837158),\n",
       "  (526, 1.7000000476837158),\n",
       "  (547, 1.7000000476837158),\n",
       "  (671, 1.7000000476837158),\n",
       "  (725, 1.7000000476837158),\n",
       "  (775, 1.7000000476837158),\n",
       "  (783, 1.7000000476837158),\n",
       "  (824, 1.7000000476837158),\n",
       "  (871, 1.7000000476837158),\n",
       "  (123, 1.600000023841858),\n",
       "  (375, 1.600000023841858),\n",
       "  (457, 1.600000023841858),\n",
       "  (468, 1.600000023841858),\n",
       "  (492, 1.600000023841858),\n",
       "  (508, 1.600000023841858),\n",
       "  (550, 1.600000023841858),\n",
       "  (562, 1.600000023841858),\n",
       "  (803, 1.600000023841858),\n",
       "  (817, 1.600000023841858),\n",
       "  (953, 1.600000023841858),\n",
       "  (963, 1.600000023841858),\n",
       "  (107, 1.5),\n",
       "  (119, 1.5),\n",
       "  (202, 1.5),\n",
       "  (230, 1.5),\n",
       "  (420, 1.5),\n",
       "  (477, 1.5),\n",
       "  (564, 1.5),\n",
       "  (748, 1.5),\n",
       "  (815, 1.5),\n",
       "  (907, 1.5),\n",
       "  (76, 1.399999976158142),\n",
       "  (83, 1.399999976158142),\n",
       "  (193, 1.399999976158142),\n",
       "  (231, 1.399999976158142),\n",
       "  (274, 1.399999976158142),\n",
       "  (293, 1.399999976158142),\n",
       "  (305, 1.399999976158142),\n",
       "  (314, 1.399999976158142),\n",
       "  (336, 1.399999976158142),\n",
       "  (552, 1.399999976158142),\n",
       "  (565, 1.399999976158142),\n",
       "  (579, 1.399999976158142),\n",
       "  (597, 1.399999976158142),\n",
       "  (624, 1.399999976158142),\n",
       "  (679, 1.399999976158142),\n",
       "  (784, 1.399999976158142),\n",
       "  (786, 1.399999976158142),\n",
       "  (801, 1.399999976158142),\n",
       "  (891, 1.399999976158142),\n",
       "  (902, 1.399999976158142),\n",
       "  (33, 1.2999999523162842),\n",
       "  (57, 1.2999999523162842),\n",
       "  (96, 1.2999999523162842),\n",
       "  (120, 1.2999999523162842),\n",
       "  (247, 1.2999999523162842),\n",
       "  (275, 1.2999999523162842),\n",
       "  (328, 1.2999999523162842),\n",
       "  (355, 1.2999999523162842),\n",
       "  (409, 1.2999999523162842),\n",
       "  (441, 1.2999999523162842),\n",
       "  (505, 1.2999999523162842),\n",
       "  (586, 1.2999999523162842),\n",
       "  (588, 1.2999999523162842),\n",
       "  (633, 1.2999999523162842),\n",
       "  (638, 1.2999999523162842),\n",
       "  (821, 1.2999999523162842),\n",
       "  (842, 1.2999999523162842),\n",
       "  (892, 1.2999999523162842),\n",
       "  (58, 1.2000000476837158),\n",
       "  (65, 1.2000000476837158),\n",
       "  (171, 1.2000000476837158),\n",
       "  (204, 1.2000000476837158),\n",
       "  (205, 1.2000000476837158),\n",
       "  (219, 1.2000000476837158),\n",
       "  (307, 1.2000000476837158),\n",
       "  (308, 1.2000000476837158),\n",
       "  (327, 1.2000000476837158),\n",
       "  (331, 1.2000000476837158),\n",
       "  (353, 1.2000000476837158),\n",
       "  (366, 1.2000000476837158),\n",
       "  (443, 1.2000000476837158),\n",
       "  (454, 1.2000000476837158),\n",
       "  (474, 1.2000000476837158),\n",
       "  (495, 1.2000000476837158),\n",
       "  (563, 1.2000000476837158),\n",
       "  (574, 1.2000000476837158),\n",
       "  (602, 1.2000000476837158),\n",
       "  (641, 1.2000000476837158),\n",
       "  (654, 1.2000000476837158),\n",
       "  (658, 1.2000000476837158),\n",
       "  (781, 1.2000000476837158),\n",
       "  (823, 1.2000000476837158),\n",
       "  (848, 1.2000000476837158),\n",
       "  (854, 1.2000000476837158),\n",
       "  (858, 1.2000000476837158),\n",
       "  (883, 1.2000000476837158),\n",
       "  (24, 1.100000023841858),\n",
       "  (41, 1.100000023841858),\n",
       "  (51, 1.100000023841858),\n",
       "  (113, 1.100000023841858),\n",
       "  (116, 1.100000023841858),\n",
       "  (236, 1.100000023841858),\n",
       "  (249, 1.100000023841858),\n",
       "  (253, 1.100000023841858),\n",
       "  (271, 1.100000023841858),\n",
       "  (281, 1.100000023841858),\n",
       "  (300, 1.100000023841858),\n",
       "  (310, 1.100000023841858),\n",
       "  (317, 1.100000023841858),\n",
       "  (318, 1.100000023841858),\n",
       "  (319, 1.100000023841858),\n",
       "  (350, 1.100000023841858),\n",
       "  (381, 1.100000023841858),\n",
       "  (395, 1.100000023841858),\n",
       "  (396, 1.100000023841858),\n",
       "  (491, 1.100000023841858),\n",
       "  (496, 1.100000023841858),\n",
       "  (497, 1.100000023841858),\n",
       "  (506, 1.100000023841858),\n",
       "  (507, 1.100000023841858),\n",
       "  (527, 1.100000023841858),\n",
       "  (544, 1.100000023841858),\n",
       "  (575, 1.100000023841858),\n",
       "  (609, 1.100000023841858),\n",
       "  (626, 1.100000023841858),\n",
       "  (759, 1.100000023841858),\n",
       "  (787, 1.100000023841858),\n",
       "  (796, 1.100000023841858),\n",
       "  (806, 1.100000023841858),\n",
       "  (820, 1.100000023841858),\n",
       "  (834, 1.100000023841858),\n",
       "  (863, 1.100000023841858),\n",
       "  (882, 1.100000023841858),\n",
       "  (894, 1.100000023841858),\n",
       "  (918, 1.100000023841858),\n",
       "  (981, 1.100000023841858),\n",
       "  (996, 1.100000023841858),\n",
       "  (7, 1.0),\n",
       "  (8, 1.0),\n",
       "  (10, 1.0),\n",
       "  (15, 1.0),\n",
       "  (17, 1.0),\n",
       "  (19, 1.0),\n",
       "  (25, 1.0),\n",
       "  (28, 1.0),\n",
       "  (37, 1.0),\n",
       "  (42, 1.0),\n",
       "  (45, 1.0),\n",
       "  (49, 1.0),\n",
       "  (52, 1.0),\n",
       "  (53, 1.0),\n",
       "  (63, 1.0),\n",
       "  (70, 1.0),\n",
       "  (75, 1.0),\n",
       "  (79, 1.0),\n",
       "  (86, 1.0),\n",
       "  (87, 1.0),\n",
       "  (90, 1.0),\n",
       "  (91, 1.0),\n",
       "  (92, 1.0),\n",
       "  (98, 1.0),\n",
       "  (102, 1.0),\n",
       "  (105, 1.0),\n",
       "  (117, 1.0),\n",
       "  (134, 1.0),\n",
       "  (139, 1.0),\n",
       "  (140, 1.0),\n",
       "  (141, 1.0),\n",
       "  (144, 1.0),\n",
       "  (158, 1.0),\n",
       "  (161, 1.0),\n",
       "  (162, 1.0),\n",
       "  (163, 1.0),\n",
       "  (164, 1.0),\n",
       "  (173, 1.0),\n",
       "  (183, 1.0),\n",
       "  (186, 1.0),\n",
       "  (196, 1.0),\n",
       "  (197, 1.0),\n",
       "  (198, 1.0),\n",
       "  (206, 1.0),\n",
       "  (213, 1.0),\n",
       "  (218, 1.0),\n",
       "  (228, 1.0),\n",
       "  (235, 1.0),\n",
       "  (260, 1.0),\n",
       "  (273, 1.0),\n",
       "  (284, 1.0),\n",
       "  (289, 1.0),\n",
       "  (291, 1.0),\n",
       "  (301, 1.0),\n",
       "  (304, 1.0),\n",
       "  (306, 1.0),\n",
       "  (312, 1.0),\n",
       "  (313, 1.0),\n",
       "  (316, 1.0),\n",
       "  (321, 1.0),\n",
       "  (323, 1.0),\n",
       "  (337, 1.0),\n",
       "  (347, 1.0),\n",
       "  (360, 1.0),\n",
       "  (376, 1.0),\n",
       "  (378, 1.0),\n",
       "  (387, 1.0),\n",
       "  (392, 1.0),\n",
       "  (398, 1.0),\n",
       "  (417, 1.0),\n",
       "  (425, 1.0),\n",
       "  (428, 1.0),\n",
       "  (429, 1.0),\n",
       "  (433, 1.0),\n",
       "  (445, 1.0),\n",
       "  (451, 1.0),\n",
       "  (483, 1.0),\n",
       "  (488, 1.0),\n",
       "  (498, 1.0),\n",
       "  (518, 1.0),\n",
       "  (528, 1.0),\n",
       "  (530, 1.0),\n",
       "  (531, 1.0),\n",
       "  (533, 1.0),\n",
       "  (566, 1.0),\n",
       "  (580, 1.0),\n",
       "  (608, 1.0),\n",
       "  (612, 1.0),\n",
       "  (616, 1.0),\n",
       "  (625, 1.0),\n",
       "  (629, 1.0),\n",
       "  (637, 1.0),\n",
       "  (645, 1.0),\n",
       "  (646, 1.0),\n",
       "  (651, 1.0),\n",
       "  (655, 1.0),\n",
       "  (661, 1.0),\n",
       "  (684, 1.0),\n",
       "  (687, 1.0),\n",
       "  (691, 1.0),\n",
       "  (692, 1.0),\n",
       "  (694, 1.0),\n",
       "  (716, 1.0),\n",
       "  (719, 1.0),\n",
       "  (734, 1.0),\n",
       "  (738, 1.0),\n",
       "  (746, 1.0),\n",
       "  (753, 1.0),\n",
       "  (768, 1.0),\n",
       "  (793, 1.0),\n",
       "  (802, 1.0),\n",
       "  (816, 1.0),\n",
       "  (826, 1.0),\n",
       "  (830, 1.0),\n",
       "  (831, 1.0),\n",
       "  (847, 1.0),\n",
       "  (873, 1.0),\n",
       "  (884, 1.0),\n",
       "  (905, 1.0),\n",
       "  (923, 1.0),\n",
       "  (932, 1.0),\n",
       "  (934, 1.0),\n",
       "  (937, 1.0),\n",
       "  (939, 1.0),\n",
       "  (944, 1.0),\n",
       "  (946, 1.0),\n",
       "  (957, 1.0),\n",
       "  (959, 1.0),\n",
       "  (984, 1.0),\n",
       "  (985, 1.0),\n",
       "  (987, 1.0),\n",
       "  (989, 1.0),\n",
       "  (992, 1.0),\n",
       "  (9, 0.8999999761581421),\n",
       "  (18, 0.8999999761581421),\n",
       "  (21, 0.8999999761581421),\n",
       "  (36, 0.8999999761581421),\n",
       "  (47, 0.8999999761581421),\n",
       "  (85, 0.8999999761581421),\n",
       "  (135, 0.8999999761581421),\n",
       "  (142, 0.8999999761581421),\n",
       "  (145, 0.8999999761581421),\n",
       "  (176, 0.8999999761581421),\n",
       "  (187, 0.8999999761581421),\n",
       "  (263, 0.8999999761581421),\n",
       "  (266, 0.8999999761581421),\n",
       "  (303, 0.8999999761581421),\n",
       "  (315, 0.8999999761581421),\n",
       "  (326, 0.8999999761581421),\n",
       "  (365, 0.8999999761581421),\n",
       "  (384, 0.8999999761581421),\n",
       "  (390, 0.8999999761581421),\n",
       "  (391, 0.8999999761581421),\n",
       "  (408, 0.8999999761581421),\n",
       "  (459, 0.8999999761581421),\n",
       "  (463, 0.8999999761581421),\n",
       "  (482, 0.8999999761581421),\n",
       "  (503, 0.8999999761581421),\n",
       "  (534, 0.8999999761581421),\n",
       "  (535, 0.8999999761581421),\n",
       "  (555, 0.8999999761581421),\n",
       "  (577, 0.8999999761581421),\n",
       "  (635, 0.8999999761581421),\n",
       "  (663, 0.8999999761581421),\n",
       "  (674, 0.8999999761581421),\n",
       "  (702, 0.8999999761581421),\n",
       "  (703, 0.8999999761581421),\n",
       "  (712, 0.8999999761581421),\n",
       "  (743, 0.8999999761581421),\n",
       "  (757, 0.8999999761581421),\n",
       "  (764, 0.8999999761581421),\n",
       "  (776, 0.8999999761581421),\n",
       "  (788, 0.8999999761581421),\n",
       "  (808, 0.8999999761581421),\n",
       "  (832, 0.8999999761581421),\n",
       "  (833, 0.8999999761581421),\n",
       "  (900, 0.8999999761581421),\n",
       "  (968, 0.8999999761581421),\n",
       "  (988, 0.8999999761581421),\n",
       "  (997, 0.8999999761581421),\n",
       "  (38, 0.800000011920929),\n",
       "  (77, 0.800000011920929),\n",
       "  (93, 0.800000011920929),\n",
       "  (100, 0.800000011920929),\n",
       "  (160, 0.800000011920929),\n",
       "  (246, 0.800000011920929),\n",
       "  (254, 0.800000011920929),\n",
       "  (280, 0.800000011920929),\n",
       "  (294, 0.800000011920929),\n",
       "  (344, 0.800000011920929),\n",
       "  (372, 0.800000011920929),\n",
       "  (377, 0.800000011920929),\n",
       "  (399, 0.800000011920929),\n",
       "  (407, 0.800000011920929),\n",
       "  (415, 0.800000011920929),\n",
       "  (430, 0.800000011920929),\n",
       "  (432, 0.800000011920929),\n",
       "  (439, 0.800000011920929),\n",
       "  (458, 0.800000011920929),\n",
       "  (514, 0.800000011920929),\n",
       "  (545, 0.800000011920929),\n",
       "  (546, 0.800000011920929),\n",
       "  (595, 0.800000011920929),\n",
       "  (603, 0.800000011920929),\n",
       "  (643, 0.800000011920929),\n",
       "  (644, 0.800000011920929),\n",
       "  (672, 0.800000011920929),\n",
       "  (696, 0.800000011920929),\n",
       "  (729, 0.800000011920929),\n",
       "  (732, 0.800000011920929),\n",
       "  (809, 0.800000011920929),\n",
       "  (822, 0.800000011920929),\n",
       "  (829, 0.800000011920929),\n",
       "  (838, 0.800000011920929),\n",
       "  (843, 0.800000011920929),\n",
       "  (852, 0.800000011920929),\n",
       "  (885, 0.800000011920929),\n",
       "  (889, 0.800000011920929),\n",
       "  (901, 0.800000011920929),\n",
       "  (956, 0.800000011920929),\n",
       "  (34, 0.699999988079071),\n",
       "  (50, 0.699999988079071),\n",
       "  (99, 0.699999988079071),\n",
       "  (112, 0.699999988079071),\n",
       "  (168, 0.699999988079071),\n",
       "  (184, 0.699999988079071),\n",
       "  (214, 0.699999988079071),\n",
       "  (216, 0.699999988079071),\n",
       "  (217, 0.699999988079071),\n",
       "  (232, 0.699999988079071),\n",
       "  (270, 0.699999988079071),\n",
       "  (320, 0.699999988079071),\n",
       "  (330, 0.699999988079071),\n",
       "  (335, 0.699999988079071),\n",
       "  (345, 0.699999988079071),\n",
       "  (361, 0.699999988079071),\n",
       "  (388, 0.699999988079071),\n",
       "  (412, 0.699999988079071),\n",
       "  (512, 0.699999988079071),\n",
       "  (561, 0.699999988079071),\n",
       "  (576, 0.699999988079071),\n",
       "  (632, 0.699999988079071),\n",
       "  (695, 0.699999988079071),\n",
       "  (805, 0.699999988079071),\n",
       "  (879, 0.699999988079071),\n",
       "  (886, 0.699999988079071),\n",
       "  (952, 0.699999988079071),\n",
       "  (32, 0.6000000238418579),\n",
       "  (88, 0.6000000238418579),\n",
       "  (146, 0.6000000238418579),\n",
       "  (148, 0.6000000238418579),\n",
       "  (209, 0.6000000238418579),\n",
       "  (211, 0.6000000238418579),\n",
       "  (322, 0.6000000238418579),\n",
       "  (340, 0.6000000238418579),\n",
       "  (343, 0.6000000238418579),\n",
       "  (442, 0.6000000238418579),\n",
       "  (450, 0.6000000238418579),\n",
       "  (470, 0.6000000238418579),\n",
       "  (532, 0.6000000238418579),\n",
       "  (539, 0.6000000238418579),\n",
       "  (554, 0.6000000238418579),\n",
       "  (560, 0.6000000238418579),\n",
       "  (571, 0.6000000238418579),\n",
       "  (584, 0.6000000238418579),\n",
       "  (589, 0.6000000238418579),\n",
       "  (656, 0.6000000238418579),\n",
       "  (699, 0.6000000238418579),\n",
       "  (736, 0.6000000238418579),\n",
       "  (754, 0.6000000238418579),\n",
       "  (758, 0.6000000238418579),\n",
       "  (765, 0.6000000238418579),\n",
       "  (790, 0.6000000238418579),\n",
       "  (888, 0.6000000238418579),\n",
       "  (890, 0.6000000238418579),\n",
       "  (972, 0.6000000238418579),\n",
       "  (979, 0.6000000238418579),\n",
       "  (12, 0.5),\n",
       "  (16, 0.5),\n",
       "  (22, 0.5),\n",
       "  (143, 0.5),\n",
       "  (157, 0.5),\n",
       "  (166, 0.5),\n",
       "  (203, 0.5),\n",
       "  (212, 0.5),\n",
       "  (221, 0.5),\n",
       "  (224, 0.5),\n",
       "  (229, 0.5),\n",
       "  (234, 0.5),\n",
       "  (237, 0.5),\n",
       "  (252, 0.5),\n",
       "  (276, 0.5),\n",
       "  (282, 0.5),\n",
       "  (288, 0.5),\n",
       "  (295, 0.5),\n",
       "  (296, 0.5),\n",
       "  (309, 0.5),\n",
       "  (329, 0.5),\n",
       "  (402, 0.5),\n",
       "  (413, 0.5),\n",
       "  (436, 0.5),\n",
       "  (444, 0.5),\n",
       "  (448, 0.5),\n",
       "  (471, 0.5),\n",
       "  (540, 0.5),\n",
       "  (604, 0.5),\n",
       "  (639, 0.5),\n",
       "  (647, 0.5),\n",
       "  (683, 0.5),\n",
       "  (697, 0.5),\n",
       "  (715, 0.5),\n",
       "  (752, 0.5),\n",
       "  (755, 0.5),\n",
       "  (795, 0.5),\n",
       "  (811, 0.5),\n",
       "  (839, 0.5),\n",
       "  (853, 0.5),\n",
       "  (862, 0.5),\n",
       "  (877, 0.5),\n",
       "  (897, 0.5),\n",
       "  (910, 0.5),\n",
       "  (911, 0.5),\n",
       "  (927, 0.5),\n",
       "  (977, 0.5),\n",
       "  (20, 0.4000000059604645),\n",
       "  (29, 0.4000000059604645),\n",
       "  (69, 0.4000000059604645),\n",
       "  (122, 0.4000000059604645),\n",
       "  (125, 0.4000000059604645),\n",
       "  (130, 0.4000000059604645),\n",
       "  (132, 0.4000000059604645),\n",
       "  (178, 0.4000000059604645),\n",
       "  (180, 0.4000000059604645),\n",
       "  (256, 0.4000000059604645),\n",
       "  (264, 0.4000000059604645),\n",
       "  (298, 0.4000000059604645),\n",
       "  (370, 0.4000000059604645),\n",
       "  (419, 0.4000000059604645),\n",
       "  (424, 0.4000000059604645),\n",
       "  (480, 0.4000000059604645),\n",
       "  (484, 0.4000000059604645),\n",
       "  (509, 0.4000000059604645),\n",
       "  (511, 0.4000000059604645),\n",
       "  (537, 0.4000000059604645),\n",
       "  (582, 0.4000000059604645),\n",
       "  (587, 0.4000000059604645),\n",
       "  (590, 0.4000000059604645),\n",
       "  (606, 0.4000000059604645),\n",
       "  (615, 0.4000000059604645),\n",
       "  (652, 0.4000000059604645),\n",
       "  (670, 0.4000000059604645),\n",
       "  (689, 0.4000000059604645),\n",
       "  (700, 0.4000000059604645),\n",
       "  (707, 0.4000000059604645),\n",
       "  (720, 0.4000000059604645),\n",
       "  (727, 0.4000000059604645),\n",
       "  (745, 0.4000000059604645),\n",
       "  (804, 0.4000000059604645),\n",
       "  (844, 0.4000000059604645),\n",
       "  (866, 0.4000000059604645),\n",
       "  (878, 0.4000000059604645),\n",
       "  (881, 0.4000000059604645),\n",
       "  (896, 0.4000000059604645),\n",
       "  (925, 0.4000000059604645),\n",
       "  (2, 0.30000001192092896),\n",
       "  (5, 0.30000001192092896),\n",
       "  (74, 0.30000001192092896),\n",
       "  (78, 0.30000001192092896),\n",
       "  (81, 0.30000001192092896),\n",
       "  (89, 0.30000001192092896),\n",
       "  (101, 0.30000001192092896),\n",
       "  (136, 0.30000001192092896),\n",
       "  (138, 0.30000001192092896),\n",
       "  (169, 0.30000001192092896),\n",
       "  (190, 0.30000001192092896),\n",
       "  (207, 0.30000001192092896),\n",
       "  (208, 0.30000001192092896),\n",
       "  (215, 0.30000001192092896),\n",
       "  (243, 0.30000001192092896),\n",
       "  (250, 0.30000001192092896),\n",
       "  (285, 0.30000001192092896),\n",
       "  (286, 0.30000001192092896),\n",
       "  (302, 0.30000001192092896),\n",
       "  (332, 0.30000001192092896),\n",
       "  (341, 0.30000001192092896),\n",
       "  (357, 0.30000001192092896),\n",
       "  (364, 0.30000001192092896),\n",
       "  (369, 0.30000001192092896),\n",
       "  (386, 0.30000001192092896),\n",
       "  (403, 0.30000001192092896),\n",
       "  (438, 0.30000001192092896),\n",
       "  (456, 0.30000001192092896),\n",
       "  (461, 0.30000001192092896),\n",
       "  (487, 0.30000001192092896),\n",
       "  (523, 0.30000001192092896),\n",
       "  (541, 0.30000001192092896),\n",
       "  (542, 0.30000001192092896),\n",
       "  (607, 0.30000001192092896),\n",
       "  (613, 0.30000001192092896),\n",
       "  (636, 0.30000001192092896),\n",
       "  (650, 0.30000001192092896),\n",
       "  (704, 0.30000001192092896),\n",
       "  (740, 0.30000001192092896),\n",
       "  (818, 0.30000001192092896),\n",
       "  (867, 0.30000001192092896),\n",
       "  (6, 0.20000000298023224),\n",
       "  (11, 0.20000000298023224),\n",
       "  (14, 0.20000000298023224),\n",
       "  (23, 0.20000000298023224),\n",
       "  (35, 0.20000000298023224),\n",
       "  (40, 0.20000000298023224),\n",
       "  (44, 0.20000000298023224),\n",
       "  (56, 0.20000000298023224),\n",
       "  (71, 0.20000000298023224),\n",
       "  (114, 0.20000000298023224),\n",
       "  (159, 0.20000000298023224),\n",
       "  (201, 0.20000000298023224),\n",
       "  (223, 0.20000000298023224),\n",
       "  (241, 0.20000000298023224),\n",
       "  (248, 0.20000000298023224),\n",
       "  (261, 0.20000000298023224),\n",
       "  (265, 0.20000000298023224),\n",
       "  (269, 0.20000000298023224),\n",
       "  (272, 0.20000000298023224),\n",
       "  (352, 0.20000000298023224),\n",
       "  (394, 0.20000000298023224),\n",
       "  (453, 0.20000000298023224),\n",
       "  (486, 0.20000000298023224),\n",
       "  (502, 0.20000000298023224),\n",
       "  (513, 0.20000000298023224),\n",
       "  (516, 0.20000000298023224),\n",
       "  (524, 0.20000000298023224),\n",
       "  (529, 0.20000000298023224),\n",
       "  (567, 0.20000000298023224),\n",
       "  (592, 0.20000000298023224),\n",
       "  (601, 0.20000000298023224),\n",
       "  (614, 0.20000000298023224),\n",
       "  (634, 0.20000000298023224),\n",
       "  (642, 0.20000000298023224),\n",
       "  (649, 0.20000000298023224),\n",
       "  (662, 0.20000000298023224),\n",
       "  (664, 0.20000000298023224),\n",
       "  (667, 0.20000000298023224),\n",
       "  (678, 0.20000000298023224),\n",
       "  (680, 0.20000000298023224),\n",
       "  (688, 0.20000000298023224),\n",
       "  (706, 0.20000000298023224),\n",
       "  (751, 0.20000000298023224),\n",
       "  (766, 0.20000000298023224),\n",
       "  (773, 0.20000000298023224),\n",
       "  (799, 0.20000000298023224),\n",
       "  (827, 0.20000000298023224),\n",
       "  (859, 0.20000000298023224),\n",
       "  (962, 0.20000000298023224),\n",
       "  (976, 0.20000000298023224),\n",
       "  (978, 0.20000000298023224),\n",
       "  (994, 0.20000000298023224),\n",
       "  (4, 0.10000000149011612),\n",
       "  (31, 0.10000000149011612),\n",
       "  (67, 0.10000000149011612),\n",
       "  (95, 0.10000000149011612),\n",
       "  (121, 0.10000000149011612),\n",
       "  (150, 0.10000000149011612),\n",
       "  (170, 0.10000000149011612),\n",
       "  (181, 0.10000000149011612),\n",
       "  (226, 0.10000000149011612),\n",
       "  (227, 0.10000000149011612),\n",
       "  (245, 0.10000000149011612),\n",
       "  (257, 0.10000000149011612),\n",
       "  (259, 0.10000000149011612),\n",
       "  (267, 0.10000000149011612),\n",
       "  (279, 0.10000000149011612),\n",
       "  (297, 0.10000000149011612),\n",
       "  (311, 0.10000000149011612),\n",
       "  (324, 0.10000000149011612),\n",
       "  (354, 0.10000000149011612),\n",
       "  (358, 0.10000000149011612),\n",
       "  (359, 0.10000000149011612),\n",
       "  (362, 0.10000000149011612),\n",
       "  (379, 0.10000000149011612),\n",
       "  (380, 0.10000000149011612),\n",
       "  (382, 0.10000000149011612),\n",
       "  (383, 0.10000000149011612),\n",
       "  (385, 0.10000000149011612),\n",
       "  (400, 0.10000000149011612),\n",
       "  (422, 0.10000000149011612),\n",
       "  (434, 0.10000000149011612),\n",
       "  (452, 0.10000000149011612),\n",
       "  (501, 0.10000000149011612),\n",
       "  (517, 0.10000000149011612),\n",
       "  (568, 0.10000000149011612),\n",
       "  (578, 0.10000000149011612),\n",
       "  (585, 0.10000000149011612),\n",
       "  (605, 0.10000000149011612),\n",
       "  (618, 0.10000000149011612),\n",
       "  (628, 0.10000000149011612),\n",
       "  (657, 0.10000000149011612),\n",
       "  (665, 0.10000000149011612),\n",
       "  (676, 0.10000000149011612),\n",
       "  (685, 0.10000000149011612),\n",
       "  (690, 0.10000000149011612),\n",
       "  (701, 0.10000000149011612),\n",
       "  (710, 0.10000000149011612),\n",
       "  (749, 0.10000000149011612),\n",
       "  (771, 0.10000000149011612),\n",
       "  (774, 0.10000000149011612),\n",
       "  (778, 0.10000000149011612),\n",
       "  (782, 0.10000000149011612),\n",
       "  (797, 0.10000000149011612),\n",
       "  (814, 0.10000000149011612),\n",
       "  (825, 0.10000000149011612),\n",
       "  (849, 0.10000000149011612),\n",
       "  (869, 0.10000000149011612),\n",
       "  (874, 0.10000000149011612),\n",
       "  (875, 0.10000000149011612),\n",
       "  (903, 0.10000000149011612),\n",
       "  (919, 0.10000000149011612),\n",
       "  (921, 0.10000000149011612),\n",
       "  (933, 0.10000000149011612),\n",
       "  (938, 0.10000000149011612),\n",
       "  (943, 0.10000000149011612),\n",
       "  (949, 0.10000000149011612),\n",
       "  (983, 0.10000000149011612),\n",
       "  (990, 0.10000000149011612),\n",
       "  (3, 0.0),\n",
       "  (13, 0.0),\n",
       "  (26, 0.0),\n",
       "  (27, 0.0),\n",
       "  (30, 0.0),\n",
       "  (43, 0.0),\n",
       "  (54, 0.0),\n",
       "  (59, 0.0),\n",
       "  (64, 0.0),\n",
       "  (66, 0.0),\n",
       "  (73, 0.0),\n",
       "  (80, 0.0),\n",
       "  (82, 0.0),\n",
       "  (103, 0.0),\n",
       "  (104, 0.0),\n",
       "  (106, 0.0),\n",
       "  (111, 0.0),\n",
       "  (127, 0.0),\n",
       "  (129, 0.0),\n",
       "  (131, 0.0),\n",
       "  (133, 0.0),\n",
       "  (137, 0.0),\n",
       "  (147, 0.0),\n",
       "  (149, 0.0),\n",
       "  (152, 0.0),\n",
       "  (153, 0.0),\n",
       "  (154, 0.0),\n",
       "  (156, 0.0),\n",
       "  (165, 0.0),\n",
       "  (167, 0.0),\n",
       "  (172, 0.0),\n",
       "  (174, 0.0),\n",
       "  (175, 0.0),\n",
       "  (177, 0.0),\n",
       "  (179, 0.0),\n",
       "  (185, 0.0),\n",
       "  (191, 0.0),\n",
       "  (194, 0.0),\n",
       "  (200, 0.0),\n",
       "  (210, 0.0),\n",
       "  (220, 0.0),\n",
       "  (225, 0.0),\n",
       "  (233, 0.0),\n",
       "  (239, 0.0),\n",
       "  (240, 0.0),\n",
       "  (244, 0.0),\n",
       "  (251, 0.0),\n",
       "  (255, 0.0),\n",
       "  (258, 0.0),\n",
       "  (262, 0.0),\n",
       "  (268, 0.0),\n",
       "  (277, 0.0),\n",
       "  (278, 0.0),\n",
       "  (283, 0.0),\n",
       "  (287, 0.0),\n",
       "  (299, 0.0),\n",
       "  (325, 0.0),\n",
       "  (333, 0.0),\n",
       "  (338, 0.0),\n",
       "  (339, 0.0),\n",
       "  (346, 0.0),\n",
       "  (349, 0.0),\n",
       "  (351, 0.0),\n",
       "  (356, 0.0),\n",
       "  (367, 0.0),\n",
       "  (368, 0.0),\n",
       "  (371, 0.0),\n",
       "  (373, 0.0),\n",
       "  (374, 0.0),\n",
       "  (404, 0.0),\n",
       "  (405, 0.0),\n",
       "  (416, 0.0),\n",
       "  (418, 0.0),\n",
       "  (421, 0.0),\n",
       "  (426, 0.0),\n",
       "  (427, 0.0),\n",
       "  (435, 0.0),\n",
       "  (437, 0.0),\n",
       "  (446, 0.0),\n",
       "  (449, 0.0),\n",
       "  (460, 0.0),\n",
       "  (462, 0.0),\n",
       "  (465, 0.0),\n",
       "  (466, 0.0),\n",
       "  (467, 0.0),\n",
       "  (469, 0.0),\n",
       "  (473, 0.0),\n",
       "  (475, 0.0),\n",
       "  (478, 0.0),\n",
       "  (479, 0.0),\n",
       "  (481, 0.0),\n",
       "  (485, 0.0),\n",
       "  (493, 0.0),\n",
       "  (494, 0.0),\n",
       "  (499, 0.0),\n",
       "  (500, 0.0),\n",
       "  (504, 0.0),\n",
       "  (510, 0.0),\n",
       "  (515, 0.0),\n",
       "  (519, 0.0),\n",
       "  (521, 0.0),\n",
       "  (522, 0.0),\n",
       "  (525, 0.0),\n",
       "  (536, 0.0),\n",
       "  (543, 0.0),\n",
       "  (548, 0.0),\n",
       "  (549, 0.0),\n",
       "  (551, 0.0),\n",
       "  (553, 0.0),\n",
       "  (557, 0.0),\n",
       "  (559, 0.0),\n",
       "  (569, 0.0),\n",
       "  (573, 0.0),\n",
       "  (583, 0.0),\n",
       "  (594, 0.0),\n",
       "  (596, 0.0),\n",
       "  (598, 0.0),\n",
       "  (600, 0.0),\n",
       "  (610, 0.0),\n",
       "  (617, 0.0),\n",
       "  (622, 0.0),\n",
       "  (623, 0.0),\n",
       "  (627, 0.0),\n",
       "  (630, 0.0),\n",
       "  (631, 0.0),\n",
       "  (648, 0.0),\n",
       "  (653, 0.0),\n",
       "  (659, 0.0),\n",
       "  (660, 0.0),\n",
       "  (666, 0.0),\n",
       "  (673, 0.0),\n",
       "  (675, 0.0),\n",
       "  (677, 0.0),\n",
       "  (681, 0.0),\n",
       "  (682, 0.0),\n",
       "  (686, 0.0),\n",
       "  (693, 0.0),\n",
       "  (705, 0.0),\n",
       "  (708, 0.0),\n",
       "  (713, 0.0),\n",
       "  (714, 0.0),\n",
       "  (717, 0.0),\n",
       "  (718, 0.0),\n",
       "  (722, 0.0),\n",
       "  (723, 0.0),\n",
       "  (726, 0.0),\n",
       "  (728, 0.0),\n",
       "  (730, 0.0),\n",
       "  (731, 0.0),\n",
       "  (733, 0.0),\n",
       "  (737, 0.0),\n",
       "  (739, 0.0),\n",
       "  (742, 0.0),\n",
       "  (744, 0.0),\n",
       "  (747, 0.0),\n",
       "  (756, 0.0),\n",
       "  (760, 0.0),\n",
       "  (767, 0.0),\n",
       "  (769, 0.0),\n",
       "  (780, 0.0),\n",
       "  (785, 0.0),\n",
       "  (789, 0.0),\n",
       "  (792, 0.0),\n",
       "  (798, 0.0),\n",
       "  (807, 0.0),\n",
       "  (810, 0.0),\n",
       "  (812, 0.0),\n",
       "  (813, 0.0),\n",
       "  (835, 0.0),\n",
       "  (836, 0.0),\n",
       "  (840, 0.0),\n",
       "  (841, 0.0),\n",
       "  (845, 0.0),\n",
       "  (846, 0.0),\n",
       "  (851, 0.0),\n",
       "  (856, 0.0),\n",
       "  (860, 0.0),\n",
       "  (861, 0.0),\n",
       "  (876, 0.0),\n",
       "  (880, 0.0),\n",
       "  (895, 0.0),\n",
       "  (899, 0.0),\n",
       "  (908, 0.0),\n",
       "  (909, 0.0),\n",
       "  (912, 0.0),\n",
       "  (913, 0.0),\n",
       "  (914, 0.0),\n",
       "  (916, 0.0),\n",
       "  (917, 0.0),\n",
       "  (922, 0.0),\n",
       "  (924, 0.0),\n",
       "  (926, 0.0),\n",
       "  (928, 0.0),\n",
       "  (929, 0.0),\n",
       "  (930, 0.0),\n",
       "  (931, 0.0),\n",
       "  (935, 0.0),\n",
       "  (936, 0.0),\n",
       "  (940, 0.0),\n",
       "  (941, 0.0),\n",
       "  (942, 0.0),\n",
       "  (945, 0.0),\n",
       "  (947, 0.0),\n",
       "  (948, 0.0),\n",
       "  (950, 0.0),\n",
       "  (951, 0.0),\n",
       "  (954, 0.0),\n",
       "  (958, 0.0),\n",
       "  (960, 0.0),\n",
       "  (961, 0.0),\n",
       "  (964, 0.0),\n",
       "  (965, 0.0),\n",
       "  (966, 0.0),\n",
       "  (967, 0.0),\n",
       "  (969, 0.0),\n",
       "  (970, 0.0),\n",
       "  (974, 0.0),\n",
       "  (975, 0.0),\n",
       "  (980, 0.0),\n",
       "  (986, 0.0),\n",
       "  (991, 0.0),\n",
       "  (993, 0.0),\n",
       "  (995, 0.0),\n",
       "  (998, 0.0),\n",
       "  (999, 0.0)])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch_no 0\n",
      "done with the 0th calculation: 717\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 718\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 720\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 718\n",
      "result for n_pert: 10 is 718.25\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 713\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 724\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 715\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 719\n",
      "result for n_pert: 20 is 717.75\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 712\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 722\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 720\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 717\n",
      "result for n_pert: 30 is 717.75\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 716\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 717\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 713\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 714\n",
      "result for n_pert: 40 is 715.0\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 717\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 723\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 722\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 720\n",
      "result for n_pert: 50 is 720.5\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 719\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 712\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 712\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 720\n",
      "result for n_pert: 60 is 715.75\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 723\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 721\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 722\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 711\n",
      "result for n_pert: 70 is 719.25\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 715\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 715\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 717\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 715\n",
      "result for n_pert: 80 is 715.5\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 721\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 716\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 716\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 714\n",
      "result for n_pert: 90 is 716.75\n",
      "at batch_no 0\n",
      "done with the 0th calculation: 718\n",
      "at batch_no 0\n",
      "done with the 1th calculation: 719\n",
      "at batch_no 0\n",
      "done with the 2th calculation: 724\n",
      "at batch_no 0\n",
      "done with the 3th calculation: 723\n",
      "result for n_pert: 100 is 721.0\n"
     ]
    }
   ],
   "source": [
    "results_1 = []\n",
    "for n_pert in range(10, 110, 10):\n",
    "  n = targeted_diversity_average(learn, n_pert, 95, 4)\n",
    "  print(f'result for n_pert: {n_pert} is {n}')\n",
    "  results_1.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7feac1262b70>]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(10, 110, 10))\n",
    "plt.ylim(0, 800)\n",
    "plt.plot(x, results_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 548\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 524\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 519\n",
      "result for n_pert: 10 is 530.3333333333334\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 524\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 546\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 518\n",
      "result for n_pert: 20 is 529.3333333333334\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 561\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 543\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 523\n",
      "result for n_pert: 30 is 542.3333333333334\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 571\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 548\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 548\n",
      "result for n_pert: 40 is 555.6666666666666\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 552\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 535\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 538\n",
      "result for n_pert: 50 is 541.6666666666666\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 558\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 540\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 540\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 534\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 534\n",
      "result for n_pert: 70 is 536.0\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 547\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 521\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 542\n",
      "result for n_pert: 80 is 536.6666666666666\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 539\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 564\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 530\n",
      "result for n_pert: 90 is 544.3333333333334\n",
      "finished creating the prediction histogram\n",
      "done with the 0th calculation: 550\n",
      "finished creating the prediction histogram\n",
      "done with the 1th calculation: 540\n",
      "finished creating the prediction histogram\n",
      "done with the 2th calculation: 540\n",
      "result for n_pert: 100 is 543.3333333333334\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for n_pert in range(10, 110, 10):\n",
    "  n = diversity_average(learn, n_pert, 95, 3)\n",
    "  print(f'result for n_pert: {n_pert} is {n}')\n",
    "  results.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fea6868afd0>]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFYxJREFUeJzt3X+MXeWd3/H3dzwe/xhjbI8Hx3iMbYg3QCtBqEWd0D/SsLsNNFpQFdRE22IhKv9Dt2yz1Zbdf1Yr9Y9EqpYNaoUWhWxIlSZBbLZYKM0WOUTdqgqNXbIEcBCOMXhqg8c/8I8ZY3tmvv3jPjO+Y4+ZO7885pn3S7o65zznufc+9/jcz3nmOedcR2YiSapX21w3QJI0uwx6SaqcQS9JlTPoJalyBr0kVc6gl6TKtRT0EbEiIp6LiF9FxJ6I+ExErIqIFyPirTJdWepGRDwREXsj4tWIuGN2P4Ik6aO02qP/BvDjzLwZuA3YAzwG7MzMzcDOsgxwD7C5PLYDT85oiyVJkxIT3TAVEcuBvwNuzKbKEfEm8LnMPBQRa4GfZuanIuIvyvz3Lq43a59CknRZ7S3UuRHoA/4yIm4DdgOPAmtGwruE/XWl/jrgQNPze0vZmKCPiO00evx0dnb+g5tvvnk6n0OS5p3du3cfyczuieq1EvTtwB3A72XmyxHxDS4M04wnxim75M+GzHwKeApgy5YtuWvXrhaaIkkaERHvtFKvlTH6XqA3M18uy8/RCP73y5ANZXq4qf76puf3AAdbaYwkaeZNGPSZ+R5wICI+VYruBt4AdgDbStk24PkyvwN4sFx9sxU44fi8JM2dVoZuAH4P+G5EdAD7gIdoHCSejYiHgXeBB0rdHwH3AnuBgVJXkjRHWgr6zPwFsGWcVXePUzeBR6bZLknSDPHOWEmqnEEvSZUz6CWpcga9JFXOoJekyhn0klQ5g16SKmfQS1LlDHpJqlyrP4EgTejs4BAHjp3hwLEBli9ZyI2rO1nZ2THXzZLmPYNekzIS5vuP9LP/aOPxztEB3j7Sz8EPzjB80Q9Sr1y6kE2rO7mxe1ljWuY3dC1l8cIFc/MhZtng0DCHTnxI7/Ez9B4f4MSZ89x03TL+3trldF+ziIjxfslbmj0GvS4xmTBfvridTas7ueOGlfyzO3rYtHop61cu5cSZ87x9pJ9f9/Xz9pHT/O1bfTy3u3f0eRGwbsWSMeHfOCB0cv21S2hru3rD8OIgb0wvzL938kOGLj7iFV2dHdx6/XJuWbucW9c2pjd2d7JwgaOomj0G/Tx1bnCYd48NTDnMN3R1sqmrkxVLF7bcQz19dpD9R/r5dd9p3j7Sz76+ft4+0s9zu3vpPzc0Wm9RexubVneOPkYOAjd1d7Ji6ewPBU02yCPgE8sX07NyCXduWkXPyiXlsZSelUu4ZvFC3nr/FHsOneSNQyfZc+gU3/7f+zk3OAxAR3sbv7FmGbc2hf8t1y9n+eKFs/5ZNT9M+H/GXgn+D1OzYyTM3znaCNRWwnxDVycbV3dOOcynIjPpO3WWfaPhf+FA8O6xAQabGnrxUNBN3Z1sWj25oaDpBPlIeDfPr712CR3tk+uRnx8aZl9ff1P4n+SNgyc52n9utE7PyiWjwX/r9Y2DQM/KJQ79TMOH54c41n+OY/3nGBxOFkQQAQvaggVtQVsEbWW5LYK2tmBBBG1tNKYjZW2Nem3RmB95nSv9bxMRuzNzvF8WHlvPoP94+/D8EAeODbD/aCPQr9Ywn6rzQ8McODYwGvz7jjQOBPv6+jl86uxoveahoJvKQaBn5RKO9Z+bkyCfipED3utNwb/n0En2Heln5Gt6zeL20WGfkYPA5jXLqj3f8VEyk4FzjeA+2n+OY/1nOXr63GiQHx0zPcux0+fG/OU4G0YOEhGN8B89IIweMMqBpOmA8dXf+g3uu33dlN7PoK/I6bODvFMCfP/Rft4t03eODvDeyQ/Jj3mYT9Xps4O83dfPviNjh4L29Z0e84W+WoJ8qs6cG+LN90+NBv/IXwAD5TMuaAtu6u4c0/u/Ze1yVi9bNMctn5zM5NTZQY6dvhDSx/rPNuZPXxTepxvlZ8vw18U6FrSxqrODVZ0ddC3ruDDf2cGqzkWs6uygoz0YHoahTIaHk+G8MD80nAxn4zFU6mQ2yoeGkyx1h4Yvfe5wZtPr0PQ65TWHx9Z9YMt67vrk6iltM4P+YyQz+WDgfCPEjw2w/0ijd/5OGXY5cvrcmPqrl3WwoauTDV1L2bCqk40lzDesWlpdmE/FSM/4wPEzdHV2sHbFYha119XjHR5O3j02MGbYZ8+hkxw88eFoneuuWTTmxO8Nq5Y2nptZ/tJrTEeCKkt5jpSXcBsuYZVcqDPyGpmN0BvOi56TjJZf/JyBs4NNQX6hx328/zznhsYP7iULF1w2tLvK8qplHaPzyxa1z4vvgUF/lRkJn/3j9MrfOdrPyQ8Hx9S//trF3NC1lI1dnRdCvasR6MsWeQ5d4zvef449740E/yneOHSSvYdPcX5o7r/nzZYtah8N7tGgvkxod3UuYklHXQfqmdJq0H+sE+ODgXMcHzg/9qTIxSdURsbGmk6ojIyhzbSh4eTgB2cavfKmEG9MBzhz/sJwwoK2oGflEjZ0dXL7+hVsGA31paxfVe815ppdKzs7+OxNq/nsTReGAs4NDrP38GkOfnCGtrbGCcOA8j1pfFfiMtOROo0TjZc+50IZo/XGm44+h2BxR1t1f2Fd7T7WQf/9nx/ga//9V1N+/oKmM+ptEWNOkDQfPJoPEmMPHMGCUn7q7CAHjg2M6Tl1tLexYVWjJ37XJ1eP9sg3di3l+hVLvHZaV0RHe1vjqp3rl891UzRHPtZBf/fN17Fm+aIxJzeG8sK448UnVMaeELm07lAZbxyZHz2xcvFzR0/KNIZkhjJZt3IJv33rJ9hYwnxD11I+sXzxVX3jj6T54WMd9JvXXMPmNdfMdTMk6arm2IEkVc6gl6TKGfSSVDmDXpIqZ9BLUuVaCvqI2B8Rv4yIX0TErlK2KiJejIi3ynRlKY+IeCIi9kbEqxFxx2x+AEnSR5tMj/4fZ+btTbfbPgbszMzNwM6yDHAPsLk8tgNPzlRjJUmTN52hm/uAZ8r8M8D9TeXfyYafASsiYu003keSNA2tBn0C/yMidkfE9lK2JjMPAZTpdaV8HXCg6bm9pUySNAdavTP2rsw8GBHXAS9GxEf9wMx49/xf8tN55YCxHeCGG25osRmSpMlqqUefmQfL9DDw18CdwPsjQzJlerhU7wXWNz29Bzg4zms+lZlbMnNLd3f31D+BJOkjTRj0EdEZEdeMzAO/DbwG7AC2lWrbgOfL/A7gwXL1zVbgxMgQjyTpymtl6GYN8Nfl99vbgf+amT+OiJ8Dz0bEw8C7wAOl/o+Ae4G9wADw0Iy3WpLUsgmDPjP3AbeNU34UuHuc8gQemZHWSZKmzTtjJalyBr0kVc6gl6TKGfSSVDmDXpIqZ9BLUuUMekmqnEEvSZUz6CWpcga9JFXOoJekyhn0klQ5g16SKmfQS1LlDHpJqpxBL0mVM+glqXIGvSRVzqCXpMoZ9JJUOYNekipn0EtS5Qx6SaqcQS9JlTPoJalyBr0kVc6gl6TKGfSSVLmWgz4iFkTEKxHxQlneFBEvR8RbEfGDiOgo5YvK8t6yfuPsNF2S1IrJ9OgfBfY0LX8deDwzNwPHgYdL+cPA8cz8JPB4qSdJmiMtBX1E9AD/FPhmWQ7g88BzpcozwP1l/r6yTFl/d6kvSZoDrfbo/xz4Q2C4LHcBH2TmYFnuBdaV+XXAAYCy/kSpP0ZEbI+IXRGxq6+vb4rNlyRNZMKgj4gvAoczc3dz8ThVs4V1Fwoyn8rMLZm5pbu7u6XGSpImr72FOncBvxMR9wKLgeU0evgrIqK99Np7gIOlfi+wHuiNiHbgWuDYjLdcktSSCXv0mflHmdmTmRuBLwM/yczfBV4CvlSqbQOeL/M7yjJl/U8y85IevSTpypjOdfT/HvhqROylMQb/dCl/Gugq5V8FHpteEyVJ09HK0M2ozPwp8NMyvw+4c5w6HwIPzEDbJEkzwDtjJalyBr0kVc6gl6TKGfSSVDmDXpIqZ9BLUuUMekmqnEEvSZUz6CWpcga9JFXOoJekyhn0klQ5g16SKmfQS1LlDHpJqpxBL0mVM+glqXIGvSRVzqCXpMoZ9JJUOYNekipn0EtS5Qx6SaqcQS9JlTPoJalyBr0kVc6gl6TKTRj0EbE4Iv5PRPxdRLweEX9ayjdFxMsR8VZE/CAiOkr5orK8t6zfOLsfQZL0UVrp0Z8FPp+ZtwG3A1+IiK3A14HHM3MzcBx4uNR/GDiemZ8EHi/1JElzZMKgz4bTZXFheSTweeC5Uv4McH+Zv68sU9bfHRExYy2WJE1KS2P0EbEgIn4BHAZeBH4NfJCZg6VKL7CuzK8DDgCU9SeArnFec3tE7IqIXX19fdP7FJKky2op6DNzKDNvB3qAO4FbxqtWpuP13vOSgsynMnNLZm7p7u5utb2SpEma1FU3mfkB8FNgK7AiItrLqh7gYJnvBdYDlPXXAsdmorGSpMlr5aqb7ohYUeaXAL8J7AFeAr5Uqm0Dni/zO8oyZf1PMvOSHr0k6cpon7gKa4FnImIBjQPDs5n5QkS8AXw/Iv4D8ArwdKn/NPBfImIvjZ78l2eh3ZKkFk0Y9Jn5KvDpccr30Rivv7j8Q+CBGWmdJGnavDNWkipn0EtS5Qx6SaqcQS9JlTPoJalyBr0kVc6gl6TKGfSSVDmDXpIqZ9BLUuUMekmqnEEvSZUz6CWpcga9JFXOoJekyhn0klQ5g16SKmfQS1LlDHpJqpxBL0mVM+glqXIGvSRVzqCXpMoZ9JJUOYNekipn0EtS5Qx6SarchEEfEesj4qWI2BMRr0fEo6V8VUS8GBFvlenKUh4R8URE7I2IVyPijtn+EJKky2ulRz8I/EFm3gJsBR6JiFuBx4CdmbkZ2FmWAe4BNpfHduDJGW+1JKllEwZ9Zh7KzP9b5k8Be4B1wH3AM6XaM8D9Zf4+4DvZ8DNgRUSsnfGWS5JaMqkx+ojYCHwaeBlYk5mHoHEwAK4r1dYBB5qe1lvKLn6t7RGxKyJ29fX1Tb7lkqSWtBz0EbEM+Cvg9zPz5EdVHacsLynIfCozt2Tmlu7u7labIUmapJaCPiIW0gj572bmD0vx+yNDMmV6uJT3Auubnt4DHJyZ5kqSJquVq24CeBrYk5l/1rRqB7CtzG8Dnm8qf7BcfbMVODEyxCNJuvLaW6hzF/AvgV9GxC9K2R8DXwOejYiHgXeBB8q6HwH3AnuBAeChGW2xJGlSJgz6zPxfjD/uDnD3OPUTeGSa7ZIkzRDvjJWkyhn0klQ5g16SKmfQS1LlDHpJqpxBL0mVM+glqXIGvSRVzqCXpMoZ9JJUOYNekipn0EtS5Qx6SaqcQS9JlTPoJalyBr0kVc6gl6TKGfSSVDmDXpIqZ9BLUuUMekmqnEEvSZUz6CWpcga9JFXOoJekyhn0klQ5g16SKjdh0EfEtyLicES81lS2KiJejIi3ynRlKY+IeCIi9kbEqxFxx2w2XpI0sVZ69N8GvnBR2WPAzszcDOwsywD3AJvLYzvw5Mw0U5I0VRMGfWb+T+DYRcX3Ac+U+WeA+5vKv5MNPwNWRMTamWqsJGnypjpGvyYzDwGU6XWlfB1woKlebymTJM2RmT4ZG+OU5bgVI7ZHxK6I2NXX1zfDzZAkjZhq0L8/MiRTpodLeS+wvqleD3BwvBfIzKcyc0tmbunu7p5iMyRJE5lq0O8AtpX5bcDzTeUPlqtvtgInRoZ4JElzo32iChHxPeBzwOqI6AX+BPga8GxEPAy8CzxQqv8IuBfYCwwAD81CmyVJkzBh0GfmVy6z6u5x6ibwyHQbJUmaOd4ZK0mVM+glqXIGvSRVzqCXpMoZ9JJUOYNekipn0EtS5Qx6SaqcQS9JlTPoJalyBr0kVc6gl6TKGfSSVDmDXpIqZ9BLUuUMekmqnEEvSZUz6CWpcga9JFXOoJekyhn0klQ5g16SKmfQS1LlDHpJqpxBL0mVM+glqXIGvSRVzqCXpMrNStBHxBci4s2I2BsRj83Ge0iSWjPjQR8RC4D/DNwD3Ap8JSJunen3kSS1ZjZ69HcCezNzX2aeA74P3DcL7yNJakH7LLzmOuBA03Iv8A8vrhQR24HtZfF0RLw5C225klYDR+a6EVcRt8cFboux3B5jTWd7bGil0mwEfYxTlpcUZD4FPDUL7z8nImJXZm6Z63ZcLdweF7gtxnJ7jHUltsdsDN30AuublnuAg7PwPpKkFsxG0P8c2BwRmyKiA/gysGMW3keS1IIZH7rJzMGI+NfA3wALgG9l5usz/T5XoWqGoWaI2+MCt8VYbo+xZn17ROYlw+eSpIp4Z6wkVc6gl6TKGfRTEBHrI+KliNgTEa9HxKOlfFVEvBgRb5Xpyrlu65USEQsi4pWIeKEsb4qIl8u2+EE5MT8vRMSKiHguIn5V9pHPzNd9IyL+bfmOvBYR34uIxfNp34iIb0XE4Yh4rals3H0hGp4oPx3zakTcMVPtMOinZhD4g8y8BdgKPFJ+5uExYGdmbgZ2luX54lFgT9Py14HHy7Y4Djw8J62aG98AfpyZNwO30dgu827fiIh1wL8BtmTm36dxccaXmV/7xreBL1xUdrl94R5gc3lsB56csVZkpo9pPoDngd8C3gTWlrK1wJtz3bYr9Pl7yg77eeAFGjfNHQHay/rPAH8z1+28QttiOfA25UKHpvJ5t29w4S75VTSu8HsB+Cfzbd8ANgKvTbQvAH8BfGW8etN92KOfpojYCHwaeBlYk5mHAMr0urlr2RX158AfAsNluQv4IDMHy3IvjS/9fHAj0Af8ZRnK+mZEdDIP943M/H/AfwTeBQ4BJ4DdzN99Y8Tl9oXxfj5mRraNQT8NEbEM+Cvg9zPz5Fy3Zy5ExBeBw5m5u7l4nKrz5TreduAO4MnM/DTQzzwYphlPGXu+D9gEXA900hieuNh82TcmMmvfG4N+iiJiIY2Q/25m/rAUvx8Ra8v6tcDhuWrfFXQX8DsRsZ/GL5V+nkYPf0VEjNyQN59+BqMX6M3Ml8vyczSCfz7uG78JvJ2ZfZl5Hvgh8Fnm774x4nL7wqz9fIxBPwUREcDTwJ7M/LOmVTuAbWV+G42x+6pl5h9lZk9mbqRxou0nmfm7wEvAl0q1ebEtADLzPeBARHyqFN0NvME83DdoDNlsjYil5Tszsi3m5b7R5HL7wg7gwXL1zVbgxMgQz3R5Z+wURMQ/Av4W+CUXxqX/mMY4/bPADTR28gcy89icNHIORMTngH+XmV+MiBtp9PBXAa8A/yIzz85l+66UiLgd+CbQAewDHqLRqZp3+0ZE/Cnwz2lcqfYK8K9ojDvPi30jIr4HfI7GTxG/D/wJ8N8YZ18oB8P/ROMqnQHgoczcNSPtMOglqW4O3UhS5Qx6SaqcQS9JlTPoJalyBr0kVc6gl6TKGfSSVLn/D2PlQa0O9Lz0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(10, 110, 10))\n",
    "plt.ylim(0, 600)\n",
    "plt.plot(x, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#targeted:\n",
    "targeted_div_metrics = results_1\n",
    "div_metrics = results\n",
    "\n",
    "#non-targeted:\n",
    "n_targeted_div_metrics = [244.0, 247.0, 265.3333333333333, 246.66666666666666, 241.0, 231.33333333333334, \n",
    "                          247.66666666666666, 229.0, 222.33333333333334, 236.0]\n",
    "n_div_metrics = [132, 118, 122, 135, 133, 129, 136, 132, 124, 143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, targeted_div_metrics)\n",
    "plt.plot(x, n_targeted_div_metrics)\n",
    "plt.title('targeted_div_metric')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, div_metrics)\n",
    "plt.plot(x, n_div_metrics)\n",
    "plt.title('div_metric')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch no 0\n",
      "at batch no 5\n",
      "at batch no 10\n",
      "at batch no 15\n",
      "at batch no 20\n",
      "at batch no 25\n",
      "at batch no 30\n",
      "at batch no 35\n",
      "at batch no 40\n",
      "at batch no 45\n",
      "at batch no 50\n",
      "at batch no 55\n",
      "at batch no 60\n",
      "finished creating the prediction histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(364,\n",
       " [(721, 143.20),\n",
       "  (750, 49.40),\n",
       "  (971, 48.40),\n",
       "  (794, 47.10),\n",
       "  (431, 35.10),\n",
       "  (669, 35.10),\n",
       "  (414, 30.60),\n",
       "  (588, 28.40),\n",
       "  (520, 24.90),\n",
       "  (61, 24.20),\n",
       "  (904, 18.30),\n",
       "  (411, 14.30),\n",
       "  (828, 13.50),\n",
       "  (39, 11.70),\n",
       "  (556, 11.40),\n",
       "  (581, 9.20),\n",
       "  (651, 8.30),\n",
       "  (489, 7.80),\n",
       "  (599, 6.60),\n",
       "  (84, 5.50),\n",
       "  (572, 4.80),\n",
       "  (907, 4.70),\n",
       "  (987, 4.40),\n",
       "  (401, 4.30),\n",
       "  (490, 4.30),\n",
       "  (614, 4.30),\n",
       "  (60, 4.00),\n",
       "  (955, 3.50),\n",
       "  (711, 3.30),\n",
       "  (48, 3.20),\n",
       "  (691, 3.10),\n",
       "  (709, 3.00),\n",
       "  (419, 2.90),\n",
       "  (770, 2.90),\n",
       "  (815, 2.90),\n",
       "  (864, 2.80),\n",
       "  (879, 2.80),\n",
       "  (108, 2.70),\n",
       "  (441, 2.70),\n",
       "  (632, 2.70),\n",
       "  (56, 2.50),\n",
       "  (604, 2.50),\n",
       "  (55, 2.40),\n",
       "  (464, 2.30),\n",
       "  (549, 2.30),\n",
       "  (575, 2.20),\n",
       "  (893, 2.20),\n",
       "  (973, 2.20),\n",
       "  (96, 2.10),\n",
       "  (496, 2.10),\n",
       "  (518, 2.10),\n",
       "  (412, 2.00),\n",
       "  (641, 2.00),\n",
       "  (762, 2.00),\n",
       "  (801, 2.00),\n",
       "  (872, 2.00),\n",
       "  (858, 1.90),\n",
       "  (871, 1.90),\n",
       "  (580, 1.80),\n",
       "  (621, 1.80),\n",
       "  (746, 1.80),\n",
       "  (806, 1.80),\n",
       "  (46, 1.70),\n",
       "  (151, 1.70),\n",
       "  (171, 1.70),\n",
       "  (633, 1.70),\n",
       "  (781, 1.70),\n",
       "  (790, 1.70),\n",
       "  (850, 1.70),\n",
       "  (424, 1.60),\n",
       "  (443, 1.60),\n",
       "  (453, 1.60),\n",
       "  (646, 1.60),\n",
       "  (981, 1.60),\n",
       "  (128, 1.50),\n",
       "  (360, 1.50),\n",
       "  (457, 1.50),\n",
       "  (545, 1.50),\n",
       "  (680, 1.50),\n",
       "  (722, 1.50),\n",
       "  (743, 1.50),\n",
       "  (1, 1.40),\n",
       "  (94, 1.40),\n",
       "  (230, 1.40),\n",
       "  (292, 1.40),\n",
       "  (406, 1.40),\n",
       "  (410, 1.40),\n",
       "  (440, 1.40),\n",
       "  (506, 1.40),\n",
       "  (847, 1.40),\n",
       "  (897, 1.40),\n",
       "  (67, 1.30),\n",
       "  (68, 1.30),\n",
       "  (124, 1.30),\n",
       "  (417, 1.30),\n",
       "  (538, 1.30),\n",
       "  (706, 1.30),\n",
       "  (779, 1.30),\n",
       "  (791, 1.30),\n",
       "  (826, 1.30),\n",
       "  (865, 1.30),\n",
       "  (898, 1.30),\n",
       "  (937, 1.30),\n",
       "  (953, 1.30),\n",
       "  (242, 1.20),\n",
       "  (310, 1.20),\n",
       "  (408, 1.20),\n",
       "  (582, 1.20),\n",
       "  (591, 1.20),\n",
       "  (698, 1.20),\n",
       "  (857, 1.20),\n",
       "  (896, 1.20),\n",
       "  (963, 1.20),\n",
       "  (83, 1.10),\n",
       "  (90, 1.10),\n",
       "  (92, 1.10),\n",
       "  (123, 1.10),\n",
       "  (293, 1.10),\n",
       "  (300, 1.10),\n",
       "  (316, 1.10),\n",
       "  (393, 1.10),\n",
       "  (407, 1.10),\n",
       "  (468, 1.10),\n",
       "  (612, 1.10),\n",
       "  (619, 1.10),\n",
       "  (645, 1.10),\n",
       "  (656, 1.10),\n",
       "  (738, 1.10),\n",
       "  (783, 1.10),\n",
       "  (805, 1.10),\n",
       "  (817, 1.10),\n",
       "  (906, 1.10),\n",
       "  (7, 1.00),\n",
       "  (25, 1.00),\n",
       "  (31, 1.00),\n",
       "  (33, 1.00),\n",
       "  (37, 1.00),\n",
       "  (57, 1.00),\n",
       "  (75, 1.00),\n",
       "  (88, 1.00),\n",
       "  (91, 1.00),\n",
       "  (102, 1.00),\n",
       "  (115, 1.00),\n",
       "  (120, 1.00),\n",
       "  (163, 1.00),\n",
       "  (176, 1.00),\n",
       "  (186, 1.00),\n",
       "  (195, 1.00),\n",
       "  (218, 1.00),\n",
       "  (231, 1.00),\n",
       "  (247, 1.00),\n",
       "  (260, 1.00),\n",
       "  (274, 1.00),\n",
       "  (281, 1.00),\n",
       "  (290, 1.00),\n",
       "  (306, 1.00),\n",
       "  (307, 1.00),\n",
       "  (314, 1.00),\n",
       "  (318, 1.00),\n",
       "  (334, 1.00),\n",
       "  (376, 1.00),\n",
       "  (392, 1.00),\n",
       "  (429, 1.00),\n",
       "  (474, 1.00),\n",
       "  (483, 1.00),\n",
       "  (488, 1.00),\n",
       "  (492, 1.00),\n",
       "  (498, 1.00),\n",
       "  (507, 1.00),\n",
       "  (516, 1.00),\n",
       "  (528, 1.00),\n",
       "  (530, 1.00),\n",
       "  (533, 1.00),\n",
       "  (562, 1.00),\n",
       "  (566, 1.00),\n",
       "  (602, 1.00),\n",
       "  (625, 1.00),\n",
       "  (637, 1.00),\n",
       "  (661, 1.00),\n",
       "  (684, 1.00),\n",
       "  (694, 1.00),\n",
       "  (716, 1.00),\n",
       "  (725, 1.00),\n",
       "  (734, 1.00),\n",
       "  (787, 1.00),\n",
       "  (816, 1.00),\n",
       "  (822, 1.00),\n",
       "  (824, 1.00),\n",
       "  (837, 1.00),\n",
       "  (873, 1.00),\n",
       "  (923, 1.00),\n",
       "  (934, 1.00),\n",
       "  (944, 1.00),\n",
       "  (992, 1.00),\n",
       "  (997, 1.00),\n",
       "  (40, 0.90),\n",
       "  (62, 0.90),\n",
       "  (105, 0.90),\n",
       "  (164, 0.90),\n",
       "  (301, 0.90),\n",
       "  (347, 0.90),\n",
       "  (369, 0.90),\n",
       "  (397, 0.90),\n",
       "  (425, 0.90),\n",
       "  (444, 0.90),\n",
       "  (753, 0.90),\n",
       "  (819, 0.90),\n",
       "  (868, 0.90),\n",
       "  (889, 0.90),\n",
       "  (982, 0.90),\n",
       "  (47, 0.80),\n",
       "  (72, 0.80),\n",
       "  (86, 0.80),\n",
       "  (97, 0.80),\n",
       "  (118, 0.80),\n",
       "  (254, 0.80),\n",
       "  (275, 0.80),\n",
       "  (304, 0.80),\n",
       "  (331, 0.80),\n",
       "  (463, 0.80),\n",
       "  (509, 0.80),\n",
       "  (532, 0.80),\n",
       "  (576, 0.80),\n",
       "  (586, 0.80),\n",
       "  (606, 0.80),\n",
       "  (638, 0.80),\n",
       "  (703, 0.80),\n",
       "  (732, 0.80),\n",
       "  (772, 0.80),\n",
       "  (786, 0.80),\n",
       "  (803, 0.80),\n",
       "  (829, 0.80),\n",
       "  (878, 0.80),\n",
       "  (899, 0.80),\n",
       "  (905, 0.80),\n",
       "  (915, 0.80),\n",
       "  (984, 0.80),\n",
       "  (8, 0.70),\n",
       "  (15, 0.70),\n",
       "  (38, 0.70),\n",
       "  (42, 0.70),\n",
       "  (109, 0.70),\n",
       "  (116, 0.70),\n",
       "  (144, 0.70),\n",
       "  (155, 0.70),\n",
       "  (189, 0.70),\n",
       "  (235, 0.70),\n",
       "  (313, 0.70),\n",
       "  (341, 0.70),\n",
       "  (353, 0.70),\n",
       "  (355, 0.70),\n",
       "  (387, 0.70),\n",
       "  (438, 0.70),\n",
       "  (476, 0.70),\n",
       "  (497, 0.70),\n",
       "  (508, 0.70),\n",
       "  (517, 0.70),\n",
       "  (526, 0.70),\n",
       "  (547, 0.70),\n",
       "  (552, 0.70),\n",
       "  (564, 0.70),\n",
       "  (570, 0.70),\n",
       "  (579, 0.70),\n",
       "  (620, 0.70),\n",
       "  (629, 0.70),\n",
       "  (727, 0.70),\n",
       "  (741, 0.70),\n",
       "  (752, 0.70),\n",
       "  (778, 0.70),\n",
       "  (788, 0.70),\n",
       "  (814, 0.70),\n",
       "  (925, 0.70),\n",
       "  (17, 0.60),\n",
       "  (19, 0.60),\n",
       "  (87, 0.60),\n",
       "  (289, 0.60),\n",
       "  (291, 0.60),\n",
       "  (294, 0.60),\n",
       "  (327, 0.60),\n",
       "  (375, 0.60),\n",
       "  (398, 0.60),\n",
       "  (409, 0.60),\n",
       "  (433, 0.60),\n",
       "  (445, 0.60),\n",
       "  (472, 0.60),\n",
       "  (482, 0.60),\n",
       "  (535, 0.60),\n",
       "  (544, 0.60),\n",
       "  (565, 0.60),\n",
       "  (671, 0.60),\n",
       "  (679, 0.60),\n",
       "  (696, 0.60),\n",
       "  (701, 0.60),\n",
       "  (751, 0.60),\n",
       "  (760, 0.60),\n",
       "  (796, 0.60),\n",
       "  (797, 0.60),\n",
       "  (820, 0.60),\n",
       "  (867, 0.60),\n",
       "  (892, 0.60),\n",
       "  (902, 0.60),\n",
       "  (920, 0.60),\n",
       "  (998, 0.60),\n",
       "  (0, 0.50),\n",
       "  (45, 0.50),\n",
       "  (50, 0.50),\n",
       "  (52, 0.50),\n",
       "  (107, 0.50),\n",
       "  (149, 0.50),\n",
       "  (159, 0.50),\n",
       "  (336, 0.50),\n",
       "  (342, 0.50),\n",
       "  (348, 0.50),\n",
       "  (391, 0.50),\n",
       "  (495, 0.50),\n",
       "  (515, 0.50),\n",
       "  (523, 0.50),\n",
       "  (527, 0.50),\n",
       "  (539, 0.50),\n",
       "  (555, 0.50),\n",
       "  (605, 0.50),\n",
       "  (607, 0.50),\n",
       "  (609, 0.50),\n",
       "  (626, 0.50),\n",
       "  (654, 0.50),\n",
       "  (655, 0.50),\n",
       "  (664, 0.50),\n",
       "  (674, 0.50),\n",
       "  (705, 0.50),\n",
       "  (719, 0.50),\n",
       "  (745, 0.50),\n",
       "  (754, 0.50),\n",
       "  (759, 0.50),\n",
       "  (768, 0.50),\n",
       "  (823, 0.50),\n",
       "  (853, 0.50),\n",
       "  (900, 0.50),\n",
       "  (917, 0.50),\n",
       "  (985, 0.50),\n",
       "  (9, 0.40),\n",
       "  (23, 0.40),\n",
       "  (24, 0.40),\n",
       "  (28, 0.40),\n",
       "  (63, 0.40),\n",
       "  (77, 0.40),\n",
       "  (126, 0.40),\n",
       "  (134, 0.40),\n",
       "  (140, 0.40),\n",
       "  (168, 0.40),\n",
       "  (192, 0.40),\n",
       "  (197, 0.40),\n",
       "  (205, 0.40),\n",
       "  (219, 0.40),\n",
       "  (224, 0.40),\n",
       "  (236, 0.40),\n",
       "  (249, 0.40),\n",
       "  (305, 0.40),\n",
       "  (319, 0.40),\n",
       "  (321, 0.40),\n",
       "  (332, 0.40),\n",
       "  (363, 0.40),\n",
       "  (381, 0.40),\n",
       "  (383, 0.40),\n",
       "  (388, 0.40),\n",
       "  (389, 0.40),\n",
       "  (396, 0.40),\n",
       "  (473, 0.40),\n",
       "  (481, 0.40),\n",
       "  (491, 0.40),\n",
       "  (522, 0.40),\n",
       "  (546, 0.40),\n",
       "  (574, 0.40),\n",
       "  (584, 0.40),\n",
       "  (672, 0.40),\n",
       "  (692, 0.40),\n",
       "  (697, 0.40),\n",
       "  (712, 0.40),\n",
       "  (802, 0.40),\n",
       "  (843, 0.40),\n",
       "  (854, 0.40),\n",
       "  (870, 0.40),\n",
       "  (880, 0.40),\n",
       "  (882, 0.40),\n",
       "  (890, 0.40),\n",
       "  (918, 0.40),\n",
       "  (938, 0.40),\n",
       "  (991, 0.40),\n",
       "  (41, 0.30),\n",
       "  (65, 0.30),\n",
       "  (71, 0.30),\n",
       "  (74, 0.30),\n",
       "  (113, 0.30),\n",
       "  (183, 0.30),\n",
       "  (191, 0.30),\n",
       "  (196, 0.30),\n",
       "  (202, 0.30),\n",
       "  (228, 0.30),\n",
       "  (232, 0.30),\n",
       "  (253, 0.30),\n",
       "  (303, 0.30),\n",
       "  (320, 0.30),\n",
       "  (337, 0.30),\n",
       "  (350, 0.30),\n",
       "  (358, 0.30),\n",
       "  (364, 0.30),\n",
       "  (399, 0.30),\n",
       "  (452, 0.30),\n",
       "  (454, 0.30),\n",
       "  (477, 0.30),\n",
       "  (480, 0.30),\n",
       "  (512, 0.30),\n",
       "  (537, 0.30),\n",
       "  (550, 0.30),\n",
       "  (554, 0.30),\n",
       "  (560, 0.30),\n",
       "  (593, 0.30),\n",
       "  (640, 0.30),\n",
       "  (644, 0.30),\n",
       "  (683, 0.30),\n",
       "  (707, 0.30),\n",
       "  (720, 0.30),\n",
       "  (729, 0.30),\n",
       "  (748, 0.30),\n",
       "  (757, 0.30),\n",
       "  (758, 0.30),\n",
       "  (777, 0.30),\n",
       "  (811, 0.30),\n",
       "  (831, 0.30),\n",
       "  (840, 0.30),\n",
       "  (846, 0.30),\n",
       "  (875, 0.30),\n",
       "  (883, 0.30),\n",
       "  (932, 0.30),\n",
       "  (939, 0.30),\n",
       "  (951, 0.30),\n",
       "  (957, 0.30),\n",
       "  (968, 0.30),\n",
       "  (995, 0.30),\n",
       "  (58, 0.20),\n",
       "  (82, 0.20),\n",
       "  (93, 0.20),\n",
       "  (98, 0.20),\n",
       "  (99, 0.20),\n",
       "  (100, 0.20),\n",
       "  (117, 0.20),\n",
       "  (122, 0.20),\n",
       "  (131, 0.20),\n",
       "  (138, 0.20),\n",
       "  (153, 0.20),\n",
       "  (161, 0.20),\n",
       "  (178, 0.20),\n",
       "  (188, 0.20),\n",
       "  (198, 0.20),\n",
       "  (206, 0.20),\n",
       "  (238, 0.20),\n",
       "  (283, 0.20),\n",
       "  (284, 0.20),\n",
       "  (317, 0.20),\n",
       "  (323, 0.20),\n",
       "  (326, 0.20),\n",
       "  (340, 0.20),\n",
       "  (344, 0.20),\n",
       "  (379, 0.20),\n",
       "  (395, 0.20),\n",
       "  (420, 0.20),\n",
       "  (432, 0.20),\n",
       "  (435, 0.20),\n",
       "  (436, 0.20),\n",
       "  (455, 0.20),\n",
       "  (484, 0.20),\n",
       "  (485, 0.20),\n",
       "  (487, 0.20),\n",
       "  (502, 0.20),\n",
       "  (505, 0.20),\n",
       "  (514, 0.20),\n",
       "  (534, 0.20),\n",
       "  (557, 0.20),\n",
       "  (585, 0.20),\n",
       "  (595, 0.20),\n",
       "  (618, 0.20),\n",
       "  (635, 0.20),\n",
       "  (643, 0.20),\n",
       "  (681, 0.20),\n",
       "  (700, 0.20),\n",
       "  (717, 0.20),\n",
       "  (723, 0.20),\n",
       "  (728, 0.20),\n",
       "  (736, 0.20),\n",
       "  (747, 0.20),\n",
       "  (785, 0.20),\n",
       "  (800, 0.20),\n",
       "  (808, 0.20),\n",
       "  (809, 0.20),\n",
       "  (818, 0.20),\n",
       "  (821, 0.20),\n",
       "  (825, 0.20),\n",
       "  (832, 0.20),\n",
       "  (844, 0.20),\n",
       "  (852, 0.20),\n",
       "  (876, 0.20),\n",
       "  (881, 0.20),\n",
       "  (926, 0.20),\n",
       "  (962, 0.20),\n",
       "  (966, 0.20),\n",
       "  (996, 0.20),\n",
       "  (11, 0.10),\n",
       "  (18, 0.10),\n",
       "  (21, 0.10),\n",
       "  (22, 0.10),\n",
       "  (26, 0.10),\n",
       "  (66, 0.10),\n",
       "  (70, 0.10),\n",
       "  (76, 0.10),\n",
       "  (79, 0.10),\n",
       "  (95, 0.10),\n",
       "  (111, 0.10),\n",
       "  (114, 0.10),\n",
       "  (121, 0.10),\n",
       "  (125, 0.10),\n",
       "  (129, 0.10),\n",
       "  (132, 0.10),\n",
       "  (139, 0.10),\n",
       "  (141, 0.10),\n",
       "  (142, 0.10),\n",
       "  (146, 0.10),\n",
       "  (158, 0.10),\n",
       "  (173, 0.10),\n",
       "  (179, 0.10),\n",
       "  (193, 0.10),\n",
       "  (214, 0.10),\n",
       "  (222, 0.10),\n",
       "  (229, 0.10),\n",
       "  (237, 0.10),\n",
       "  (252, 0.10),\n",
       "  (256, 0.10),\n",
       "  (268, 0.10),\n",
       "  (273, 0.10),\n",
       "  (276, 0.10),\n",
       "  (278, 0.10),\n",
       "  (282, 0.10),\n",
       "  (295, 0.10),\n",
       "  (298, 0.10),\n",
       "  (308, 0.10),\n",
       "  (309, 0.10),\n",
       "  (311, 0.10),\n",
       "  (315, 0.10),\n",
       "  (330, 0.10),\n",
       "  (343, 0.10),\n",
       "  (346, 0.10),\n",
       "  (351, 0.10),\n",
       "  (352, 0.10),\n",
       "  (356, 0.10),\n",
       "  (361, 0.10),\n",
       "  (365, 0.10),\n",
       "  (366, 0.10),\n",
       "  (377, 0.10),\n",
       "  (413, 0.10),\n",
       "  (423, 0.10),\n",
       "  (427, 0.10),\n",
       "  (428, 0.10),\n",
       "  (439, 0.10),\n",
       "  (442, 0.10),\n",
       "  (447, 0.10),\n",
       "  (448, 0.10),\n",
       "  (450, 0.10),\n",
       "  (451, 0.10),\n",
       "  (459, 0.10),\n",
       "  (475, 0.10),\n",
       "  (486, 0.10),\n",
       "  (493, 0.10),\n",
       "  (504, 0.10),\n",
       "  (521, 0.10),\n",
       "  (540, 0.10),\n",
       "  (551, 0.10),\n",
       "  (563, 0.10),\n",
       "  (569, 0.10),\n",
       "  (577, 0.10),\n",
       "  (603, 0.10),\n",
       "  (611, 0.10),\n",
       "  (613, 0.10),\n",
       "  (615, 0.10),\n",
       "  (616, 0.10),\n",
       "  (636, 0.10),\n",
       "  (639, 0.10),\n",
       "  (650, 0.10),\n",
       "  (665, 0.10),\n",
       "  (668, 0.10),\n",
       "  (673, 0.10),\n",
       "  (730, 0.10),\n",
       "  (735, 0.10),\n",
       "  (755, 0.10),\n",
       "  (756, 0.10),\n",
       "  (761, 0.10),\n",
       "  (763, 0.10),\n",
       "  (764, 0.10),\n",
       "  (765, 0.10),\n",
       "  (775, 0.10),\n",
       "  (792, 0.10),\n",
       "  (804, 0.10),\n",
       "  (813, 0.10),\n",
       "  (830, 0.10),\n",
       "  (833, 0.10),\n",
       "  (834, 0.10),\n",
       "  (836, 0.10),\n",
       "  (839, 0.10),\n",
       "  (842, 0.10),\n",
       "  (855, 0.10),\n",
       "  (856, 0.10),\n",
       "  (863, 0.10),\n",
       "  (866, 0.10),\n",
       "  (884, 0.10),\n",
       "  (885, 0.10),\n",
       "  (901, 0.10),\n",
       "  (927, 0.10),\n",
       "  (946, 0.10),\n",
       "  (950, 0.10),\n",
       "  (952, 0.10),\n",
       "  (954, 0.10),\n",
       "  (978, 0.10),\n",
       "  (983, 0.10),\n",
       "  (988, 0.10),\n",
       "  (999, 0.10),\n",
       "  (2, 0.00),\n",
       "  (3, 0.00),\n",
       "  (4, 0.00),\n",
       "  (5, 0.00),\n",
       "  (6, 0.00),\n",
       "  (10, 0.00),\n",
       "  (12, 0.00),\n",
       "  (13, 0.00),\n",
       "  (14, 0.00),\n",
       "  (16, 0.00),\n",
       "  (20, 0.00),\n",
       "  (27, 0.00),\n",
       "  (29, 0.00),\n",
       "  (30, 0.00),\n",
       "  (32, 0.00),\n",
       "  (34, 0.00),\n",
       "  (35, 0.00),\n",
       "  (36, 0.00),\n",
       "  (43, 0.00),\n",
       "  (44, 0.00),\n",
       "  (49, 0.00),\n",
       "  (51, 0.00),\n",
       "  (53, 0.00),\n",
       "  (54, 0.00),\n",
       "  (59, 0.00),\n",
       "  (64, 0.00),\n",
       "  (69, 0.00),\n",
       "  (73, 0.00),\n",
       "  (78, 0.00),\n",
       "  (80, 0.00),\n",
       "  (81, 0.00),\n",
       "  (85, 0.00),\n",
       "  (89, 0.00),\n",
       "  (101, 0.00),\n",
       "  (103, 0.00),\n",
       "  (104, 0.00),\n",
       "  (106, 0.00),\n",
       "  (110, 0.00),\n",
       "  (112, 0.00),\n",
       "  (119, 0.00),\n",
       "  (127, 0.00),\n",
       "  (130, 0.00),\n",
       "  (133, 0.00),\n",
       "  (135, 0.00),\n",
       "  (136, 0.00),\n",
       "  (137, 0.00),\n",
       "  (143, 0.00),\n",
       "  (145, 0.00),\n",
       "  (147, 0.00),\n",
       "  (148, 0.00),\n",
       "  (150, 0.00),\n",
       "  (152, 0.00),\n",
       "  (154, 0.00),\n",
       "  (156, 0.00),\n",
       "  (157, 0.00),\n",
       "  (160, 0.00),\n",
       "  (162, 0.00),\n",
       "  (165, 0.00),\n",
       "  (166, 0.00),\n",
       "  (167, 0.00),\n",
       "  (169, 0.00),\n",
       "  (170, 0.00),\n",
       "  (172, 0.00),\n",
       "  (174, 0.00),\n",
       "  (175, 0.00),\n",
       "  (177, 0.00),\n",
       "  (180, 0.00),\n",
       "  (181, 0.00),\n",
       "  (182, 0.00),\n",
       "  (184, 0.00),\n",
       "  (185, 0.00),\n",
       "  (187, 0.00),\n",
       "  (190, 0.00),\n",
       "  (194, 0.00),\n",
       "  (199, 0.00),\n",
       "  (200, 0.00),\n",
       "  (201, 0.00),\n",
       "  (203, 0.00),\n",
       "  (204, 0.00),\n",
       "  (207, 0.00),\n",
       "  (208, 0.00),\n",
       "  (209, 0.00),\n",
       "  (210, 0.00),\n",
       "  (211, 0.00),\n",
       "  (212, 0.00),\n",
       "  (213, 0.00),\n",
       "  (215, 0.00),\n",
       "  (216, 0.00),\n",
       "  (217, 0.00),\n",
       "  (220, 0.00),\n",
       "  (221, 0.00),\n",
       "  (223, 0.00),\n",
       "  (225, 0.00),\n",
       "  (226, 0.00),\n",
       "  (227, 0.00),\n",
       "  (233, 0.00),\n",
       "  (234, 0.00),\n",
       "  (239, 0.00),\n",
       "  (240, 0.00),\n",
       "  (241, 0.00),\n",
       "  (243, 0.00),\n",
       "  (244, 0.00),\n",
       "  (245, 0.00),\n",
       "  (246, 0.00),\n",
       "  (248, 0.00),\n",
       "  (250, 0.00),\n",
       "  (251, 0.00),\n",
       "  (255, 0.00),\n",
       "  (257, 0.00),\n",
       "  (258, 0.00),\n",
       "  (259, 0.00),\n",
       "  (261, 0.00),\n",
       "  (262, 0.00),\n",
       "  (263, 0.00),\n",
       "  (264, 0.00),\n",
       "  (265, 0.00),\n",
       "  (266, 0.00),\n",
       "  (267, 0.00),\n",
       "  (269, 0.00),\n",
       "  (270, 0.00),\n",
       "  (271, 0.00),\n",
       "  (272, 0.00),\n",
       "  (277, 0.00),\n",
       "  (279, 0.00),\n",
       "  (280, 0.00),\n",
       "  (285, 0.00),\n",
       "  (286, 0.00),\n",
       "  (287, 0.00),\n",
       "  (288, 0.00),\n",
       "  (296, 0.00),\n",
       "  (297, 0.00),\n",
       "  (299, 0.00),\n",
       "  (302, 0.00),\n",
       "  (312, 0.00),\n",
       "  (322, 0.00),\n",
       "  (324, 0.00),\n",
       "  (325, 0.00),\n",
       "  (328, 0.00),\n",
       "  (329, 0.00),\n",
       "  (333, 0.00),\n",
       "  (335, 0.00),\n",
       "  (338, 0.00),\n",
       "  (339, 0.00),\n",
       "  (345, 0.00),\n",
       "  (349, 0.00),\n",
       "  (354, 0.00),\n",
       "  (357, 0.00),\n",
       "  (359, 0.00),\n",
       "  (362, 0.00),\n",
       "  (367, 0.00),\n",
       "  (368, 0.00),\n",
       "  (370, 0.00),\n",
       "  (371, 0.00),\n",
       "  (372, 0.00),\n",
       "  (373, 0.00),\n",
       "  (374, 0.00),\n",
       "  (378, 0.00),\n",
       "  (380, 0.00),\n",
       "  (382, 0.00),\n",
       "  (384, 0.00),\n",
       "  (385, 0.00),\n",
       "  (386, 0.00),\n",
       "  (390, 0.00),\n",
       "  (394, 0.00),\n",
       "  (400, 0.00),\n",
       "  (402, 0.00),\n",
       "  (403, 0.00),\n",
       "  (404, 0.00),\n",
       "  (405, 0.00),\n",
       "  (415, 0.00),\n",
       "  (416, 0.00),\n",
       "  (418, 0.00),\n",
       "  (421, 0.00),\n",
       "  (422, 0.00),\n",
       "  (426, 0.00),\n",
       "  (430, 0.00),\n",
       "  (434, 0.00),\n",
       "  (437, 0.00),\n",
       "  (446, 0.00),\n",
       "  (449, 0.00),\n",
       "  (456, 0.00),\n",
       "  (458, 0.00),\n",
       "  (460, 0.00),\n",
       "  (461, 0.00),\n",
       "  (462, 0.00),\n",
       "  (465, 0.00),\n",
       "  (466, 0.00),\n",
       "  (467, 0.00),\n",
       "  (469, 0.00),\n",
       "  (470, 0.00),\n",
       "  (471, 0.00),\n",
       "  (478, 0.00),\n",
       "  (479, 0.00),\n",
       "  (494, 0.00),\n",
       "  (499, 0.00),\n",
       "  (500, 0.00),\n",
       "  (501, 0.00),\n",
       "  (503, 0.00),\n",
       "  (510, 0.00),\n",
       "  (511, 0.00),\n",
       "  (513, 0.00),\n",
       "  (519, 0.00),\n",
       "  (524, 0.00),\n",
       "  (525, 0.00),\n",
       "  (529, 0.00),\n",
       "  (531, 0.00),\n",
       "  (536, 0.00),\n",
       "  (541, 0.00),\n",
       "  (542, 0.00),\n",
       "  (543, 0.00),\n",
       "  (548, 0.00),\n",
       "  (553, 0.00),\n",
       "  (558, 0.00),\n",
       "  (559, 0.00),\n",
       "  (561, 0.00),\n",
       "  (567, 0.00),\n",
       "  (568, 0.00),\n",
       "  (571, 0.00),\n",
       "  (573, 0.00),\n",
       "  (578, 0.00),\n",
       "  (583, 0.00),\n",
       "  (587, 0.00),\n",
       "  (589, 0.00),\n",
       "  (590, 0.00),\n",
       "  (592, 0.00),\n",
       "  (594, 0.00),\n",
       "  (596, 0.00),\n",
       "  (597, 0.00),\n",
       "  (598, 0.00),\n",
       "  (600, 0.00),\n",
       "  (601, 0.00),\n",
       "  (608, 0.00),\n",
       "  (610, 0.00),\n",
       "  (617, 0.00),\n",
       "  (622, 0.00),\n",
       "  (623, 0.00),\n",
       "  (624, 0.00),\n",
       "  (627, 0.00),\n",
       "  (628, 0.00),\n",
       "  (630, 0.00),\n",
       "  (631, 0.00),\n",
       "  (634, 0.00),\n",
       "  (642, 0.00),\n",
       "  (647, 0.00),\n",
       "  (648, 0.00),\n",
       "  (649, 0.00),\n",
       "  (652, 0.00),\n",
       "  (653, 0.00),\n",
       "  (657, 0.00),\n",
       "  (658, 0.00),\n",
       "  (659, 0.00),\n",
       "  (660, 0.00),\n",
       "  (662, 0.00),\n",
       "  (663, 0.00),\n",
       "  (666, 0.00),\n",
       "  (667, 0.00),\n",
       "  (670, 0.00),\n",
       "  (675, 0.00),\n",
       "  (676, 0.00),\n",
       "  (677, 0.00),\n",
       "  (678, 0.00),\n",
       "  (682, 0.00),\n",
       "  (685, 0.00),\n",
       "  (686, 0.00),\n",
       "  (687, 0.00),\n",
       "  (688, 0.00),\n",
       "  (689, 0.00),\n",
       "  (690, 0.00),\n",
       "  (693, 0.00),\n",
       "  (695, 0.00),\n",
       "  (699, 0.00),\n",
       "  (702, 0.00),\n",
       "  (704, 0.00),\n",
       "  (708, 0.00),\n",
       "  (710, 0.00),\n",
       "  (713, 0.00),\n",
       "  (714, 0.00),\n",
       "  (715, 0.00),\n",
       "  (718, 0.00),\n",
       "  (724, 0.00),\n",
       "  (726, 0.00),\n",
       "  (731, 0.00),\n",
       "  (733, 0.00),\n",
       "  (737, 0.00),\n",
       "  (739, 0.00),\n",
       "  (740, 0.00),\n",
       "  (742, 0.00),\n",
       "  (744, 0.00),\n",
       "  (749, 0.00),\n",
       "  (766, 0.00),\n",
       "  (767, 0.00),\n",
       "  (769, 0.00),\n",
       "  (771, 0.00),\n",
       "  (773, 0.00),\n",
       "  (774, 0.00),\n",
       "  (776, 0.00),\n",
       "  (780, 0.00),\n",
       "  (782, 0.00),\n",
       "  (784, 0.00),\n",
       "  (789, 0.00),\n",
       "  (793, 0.00),\n",
       "  (795, 0.00),\n",
       "  (798, 0.00),\n",
       "  (799, 0.00),\n",
       "  (807, 0.00),\n",
       "  (810, 0.00),\n",
       "  (812, 0.00),\n",
       "  (827, 0.00),\n",
       "  (835, 0.00),\n",
       "  (838, 0.00),\n",
       "  (841, 0.00),\n",
       "  (845, 0.00),\n",
       "  (848, 0.00),\n",
       "  (849, 0.00),\n",
       "  (851, 0.00),\n",
       "  (859, 0.00),\n",
       "  (860, 0.00),\n",
       "  (861, 0.00),\n",
       "  (862, 0.00),\n",
       "  (869, 0.00),\n",
       "  (874, 0.00),\n",
       "  (877, 0.00),\n",
       "  (886, 0.00),\n",
       "  (887, 0.00),\n",
       "  (888, 0.00),\n",
       "  (891, 0.00),\n",
       "  (894, 0.00),\n",
       "  (895, 0.00),\n",
       "  (903, 0.00),\n",
       "  (908, 0.00),\n",
       "  (909, 0.00),\n",
       "  (910, 0.00),\n",
       "  (911, 0.00),\n",
       "  (912, 0.00),\n",
       "  (913, 0.00),\n",
       "  (914, 0.00),\n",
       "  (916, 0.00),\n",
       "  (919, 0.00),\n",
       "  (921, 0.00),\n",
       "  (922, 0.00),\n",
       "  (924, 0.00),\n",
       "  (928, 0.00),\n",
       "  (929, 0.00),\n",
       "  (930, 0.00),\n",
       "  (931, 0.00),\n",
       "  (933, 0.00),\n",
       "  (935, 0.00),\n",
       "  (936, 0.00),\n",
       "  (940, 0.00),\n",
       "  (941, 0.00),\n",
       "  (942, 0.00),\n",
       "  (943, 0.00),\n",
       "  (945, 0.00),\n",
       "  (947, 0.00),\n",
       "  (948, 0.00),\n",
       "  (949, 0.00),\n",
       "  (956, 0.00),\n",
       "  (958, 0.00),\n",
       "  (959, 0.00),\n",
       "  (960, 0.00),\n",
       "  (961, 0.00),\n",
       "  (964, 0.00),\n",
       "  (965, 0.00),\n",
       "  (967, 0.00),\n",
       "  (969, 0.00),\n",
       "  (970, 0.00),\n",
       "  (972, 0.00),\n",
       "  (974, 0.00),\n",
       "  (975, 0.00),\n",
       "  (976, 0.00),\n",
       "  (977, 0.00),\n",
       "  (979, 0.00),\n",
       "  (980, 0.00),\n",
       "  (986, 0.00),\n",
       "  (989, 0.00),\n",
       "  (990, 0.00),\n",
       "  (993, 0.00),\n",
       "  (994, 0.00)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on validation\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = diversity(learn, 10, 95)\n",
    "n, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f59cf2afe48>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_hist = sorted(hist, key=lambda x: x[0], reverse = False)\n",
    "values = [elem[1] for elem in sorted_hist]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.6468)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch_no 0\n",
      "at batch_no 100\n",
      "at batch_no 200\n",
      "at batch_no 300\n",
      "at batch_no 400\n",
      "at batch_no 500\n",
      "at batch_no 600\n",
      "at batch_no 700\n",
      "at batch_no 800\n",
      "at batch_no 900\n",
      "at batch_no 1000\n",
      "at batch_no 1100\n",
      "at batch_no 1200\n",
      "at batch_no 1300\n",
      "at batch_no 1400\n",
      "at batch_no 1500\n",
      "at batch_no 1600\n",
      "at batch_no 1700\n",
      "at batch_no 1800\n",
      "at batch_no 1900\n",
      "at batch_no 2000\n",
      "at batch_no 2100\n",
      "at batch_no 2200\n",
      "at batch_no 2300\n",
      "at batch_no 2400\n",
      "at batch_no 2500\n",
      "at batch_no 2600\n",
      "at batch_no 2700\n",
      "at batch_no 2800\n",
      "at batch_no 2900\n",
      "at batch_no 3000\n",
      "at batch_no 3100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(885,\n",
       " [(815, 234.00),\n",
       "  (652, 177.00),\n",
       "  (646, 163.00),\n",
       "  (580, 160.00),\n",
       "  (611, 156.00),\n",
       "  (489, 145.00),\n",
       "  (591, 144.00),\n",
       "  (621, 141.00),\n",
       "  (737, 139.00),\n",
       "  (904, 130.00),\n",
       "  (94, 129.00),\n",
       "  (868, 127.00),\n",
       "  (582, 125.00),\n",
       "  (497, 123.00),\n",
       "  (893, 120.00),\n",
       "  (794, 118.00),\n",
       "  (116, 115.00),\n",
       "  (955, 115.00),\n",
       "  (979, 114.00),\n",
       "  (679, 112.00),\n",
       "  (721, 109.00),\n",
       "  (39, 108.00),\n",
       "  (565, 108.00),\n",
       "  (741, 106.00),\n",
       "  (491, 105.00),\n",
       "  (562, 103.00),\n",
       "  (839, 102.00),\n",
       "  (109, 101.00),\n",
       "  (162, 101.00),\n",
       "  (549, 99.00),\n",
       "  (46, 97.00),\n",
       "  (48, 96.00),\n",
       "  (84, 95.00),\n",
       "  (750, 95.00),\n",
       "  (82, 94.00),\n",
       "  (973, 94.00),\n",
       "  (151, 93.00),\n",
       "  (492, 93.00),\n",
       "  (695, 93.00),\n",
       "  (199, 91.00),\n",
       "  (843, 89.00),\n",
       "  (51, 88.00),\n",
       "  (971, 88.00),\n",
       "  (640, 87.00),\n",
       "  (424, 86.00),\n",
       "  (669, 86.00),\n",
       "  (692, 86.00),\n",
       "  (879, 86.00),\n",
       "  (281, 85.00),\n",
       "  (47, 84.00),\n",
       "  (783, 84.00),\n",
       "  (203, 83.00),\n",
       "  (310, 83.00),\n",
       "  (382, 83.00),\n",
       "  (411, 83.00),\n",
       "  (866, 83.00),\n",
       "  (743, 82.00),\n",
       "  (364, 81.00),\n",
       "  (577, 81.00),\n",
       "  (197, 80.00),\n",
       "  (318, 80.00),\n",
       "  (319, 80.00),\n",
       "  (406, 80.00),\n",
       "  (725, 80.00),\n",
       "  (828, 80.00),\n",
       "  (180, 79.00),\n",
       "  (189, 79.00),\n",
       "  (208, 79.00),\n",
       "  (298, 79.00),\n",
       "  (703, 79.00),\n",
       "  (754, 79.00),\n",
       "  (905, 79.00),\n",
       "  (956, 79.00),\n",
       "  (847, 78.00),\n",
       "  (76, 77.00),\n",
       "  (182, 77.00),\n",
       "  (342, 77.00),\n",
       "  (455, 77.00),\n",
       "  (572, 77.00),\n",
       "  (711, 77.00),\n",
       "  (762, 77.00),\n",
       "  (896, 77.00),\n",
       "  (963, 77.00),\n",
       "  (217, 76.00),\n",
       "  (440, 76.00),\n",
       "  (824, 76.00),\n",
       "  (830, 76.00),\n",
       "  (55, 75.00),\n",
       "  (219, 75.00),\n",
       "  (270, 75.00),\n",
       "  (700, 75.00),\n",
       "  (730, 75.00),\n",
       "  (791, 75.00),\n",
       "  (128, 74.00),\n",
       "  (849, 74.00),\n",
       "  (938, 74.00),\n",
       "  (982, 74.00),\n",
       "  (237, 73.00),\n",
       "  (343, 73.00),\n",
       "  (363, 73.00),\n",
       "  (454, 73.00),\n",
       "  (570, 73.00),\n",
       "  (645, 73.00),\n",
       "  (735, 73.00),\n",
       "  (778, 73.00),\n",
       "  (825, 73.00),\n",
       "  (222, 72.00),\n",
       "  (304, 72.00),\n",
       "  (436, 72.00),\n",
       "  (483, 72.00),\n",
       "  (800, 72.00),\n",
       "  (826, 72.00),\n",
       "  (61, 71.00),\n",
       "  (74, 71.00),\n",
       "  (471, 71.00),\n",
       "  (472, 71.00),\n",
       "  (805, 71.00),\n",
       "  (806, 71.00),\n",
       "  (916, 71.00),\n",
       "  (30, 70.00),\n",
       "  (496, 70.00),\n",
       "  (716, 70.00),\n",
       "  (23, 69.00),\n",
       "  (341, 69.00),\n",
       "  (425, 69.00),\n",
       "  (775, 69.00),\n",
       "  (808, 69.00),\n",
       "  (878, 69.00),\n",
       "  (556, 68.00),\n",
       "  (620, 68.00),\n",
       "  (845, 68.00),\n",
       "  (184, 67.00),\n",
       "  (192, 67.00),\n",
       "  (195, 67.00),\n",
       "  (300, 67.00),\n",
       "  (361, 67.00),\n",
       "  (671, 67.00),\n",
       "  (887, 67.00),\n",
       "  (912, 67.00),\n",
       "  (37, 66.00),\n",
       "  (178, 66.00),\n",
       "  (313, 66.00),\n",
       "  (458, 66.00),\n",
       "  (654, 66.00),\n",
       "  (772, 66.00),\n",
       "  (820, 66.00),\n",
       "  (863, 66.00),\n",
       "  (870, 66.00),\n",
       "  (77, 65.00),\n",
       "  (124, 65.00),\n",
       "  (423, 65.00),\n",
       "  (655, 65.00),\n",
       "  (949, 65.00),\n",
       "  (988, 65.00),\n",
       "  (6, 64.00),\n",
       "  (211, 64.00),\n",
       "  (506, 64.00),\n",
       "  (688, 64.00),\n",
       "  (926, 64.00),\n",
       "  (972, 64.00),\n",
       "  (234, 63.00),\n",
       "  (272, 63.00),\n",
       "  (293, 63.00),\n",
       "  (481, 63.00),\n",
       "  (595, 63.00),\n",
       "  (852, 63.00),\n",
       "  (871, 63.00),\n",
       "  (895, 63.00),\n",
       "  (953, 63.00),\n",
       "  (975, 63.00),\n",
       "  (992, 63.00),\n",
       "  (90, 62.00),\n",
       "  (463, 62.00),\n",
       "  (505, 62.00),\n",
       "  (561, 62.00),\n",
       "  (697, 62.00),\n",
       "  (809, 62.00),\n",
       "  (864, 62.00),\n",
       "  (944, 62.00),\n",
       "  (987, 62.00),\n",
       "  (97, 61.00),\n",
       "  (99, 61.00),\n",
       "  (118, 61.00),\n",
       "  (125, 61.00),\n",
       "  (135, 61.00),\n",
       "  (155, 61.00),\n",
       "  (238, 61.00),\n",
       "  (292, 61.00),\n",
       "  (331, 61.00),\n",
       "  (468, 61.00),\n",
       "  (474, 61.00),\n",
       "  (597, 61.00),\n",
       "  (788, 61.00),\n",
       "  (834, 61.00),\n",
       "  (913, 61.00),\n",
       "  (50, 60.00),\n",
       "  (311, 60.00),\n",
       "  (401, 60.00),\n",
       "  (404, 60.00),\n",
       "  (420, 60.00),\n",
       "  (457, 60.00),\n",
       "  (476, 60.00),\n",
       "  (515, 60.00),\n",
       "  (532, 60.00),\n",
       "  (586, 60.00),\n",
       "  (594, 60.00),\n",
       "  (635, 60.00),\n",
       "  (649, 60.00),\n",
       "  (781, 60.00),\n",
       "  (850, 60.00),\n",
       "  (880, 60.00),\n",
       "  (892, 60.00),\n",
       "  (937, 60.00),\n",
       "  (946, 60.00),\n",
       "  (33, 59.00),\n",
       "  (129, 59.00),\n",
       "  (263, 59.00),\n",
       "  (372, 59.00),\n",
       "  (519, 59.00),\n",
       "  (564, 59.00),\n",
       "  (607, 59.00),\n",
       "  (724, 59.00),\n",
       "  (766, 59.00),\n",
       "  (770, 59.00),\n",
       "  (875, 59.00),\n",
       "  (957, 59.00),\n",
       "  (85, 58.00),\n",
       "  (119, 58.00),\n",
       "  (161, 58.00),\n",
       "  (181, 58.00),\n",
       "  (249, 58.00),\n",
       "  (259, 58.00),\n",
       "  (280, 58.00),\n",
       "  (348, 58.00),\n",
       "  (383, 58.00),\n",
       "  (441, 58.00),\n",
       "  (522, 58.00),\n",
       "  (539, 58.00),\n",
       "  (552, 58.00),\n",
       "  (563, 58.00),\n",
       "  (601, 58.00),\n",
       "  (619, 58.00),\n",
       "  (696, 58.00),\n",
       "  (748, 58.00),\n",
       "  (816, 58.00),\n",
       "  (21, 57.00),\n",
       "  (69, 57.00),\n",
       "  (89, 57.00),\n",
       "  (113, 57.00),\n",
       "  (115, 57.00),\n",
       "  (218, 57.00),\n",
       "  (232, 57.00),\n",
       "  (250, 57.00),\n",
       "  (284, 57.00),\n",
       "  (316, 57.00),\n",
       "  (407, 57.00),\n",
       "  (428, 57.00),\n",
       "  (488, 57.00),\n",
       "  (581, 57.00),\n",
       "  (603, 57.00),\n",
       "  (614, 57.00),\n",
       "  (626, 57.00),\n",
       "  (698, 57.00),\n",
       "  (774, 57.00),\n",
       "  (777, 57.00),\n",
       "  (784, 57.00),\n",
       "  (790, 57.00),\n",
       "  (842, 57.00),\n",
       "  (962, 57.00),\n",
       "  (8, 56.00),\n",
       "  (31, 56.00),\n",
       "  (57, 56.00),\n",
       "  (60, 56.00),\n",
       "  (171, 56.00),\n",
       "  (225, 56.00),\n",
       "  (275, 56.00),\n",
       "  (317, 56.00),\n",
       "  (334, 56.00),\n",
       "  (391, 56.00),\n",
       "  (443, 56.00),\n",
       "  (490, 56.00),\n",
       "  (527, 56.00),\n",
       "  (569, 56.00),\n",
       "  (593, 56.00),\n",
       "  (609, 56.00),\n",
       "  (642, 56.00),\n",
       "  (792, 56.00),\n",
       "  (819, 56.00),\n",
       "  (857, 56.00),\n",
       "  (924, 56.00),\n",
       "  (952, 56.00),\n",
       "  (24, 55.00),\n",
       "  (25, 55.00),\n",
       "  (67, 55.00),\n",
       "  (229, 55.00),\n",
       "  (231, 55.00),\n",
       "  (283, 55.00),\n",
       "  (291, 55.00),\n",
       "  (308, 55.00),\n",
       "  (328, 55.00),\n",
       "  (386, 55.00),\n",
       "  (396, 55.00),\n",
       "  (410, 55.00),\n",
       "  (509, 55.00),\n",
       "  (512, 55.00),\n",
       "  (612, 55.00),\n",
       "  (661, 55.00),\n",
       "  (822, 55.00),\n",
       "  (858, 55.00),\n",
       "  (884, 55.00),\n",
       "  (950, 55.00),\n",
       "  (985, 55.00),\n",
       "  (986, 55.00),\n",
       "  (991, 55.00),\n",
       "  (88, 54.00),\n",
       "  (159, 54.00),\n",
       "  (170, 54.00),\n",
       "  (206, 54.00),\n",
       "  (228, 54.00),\n",
       "  (241, 54.00),\n",
       "  (269, 54.00),\n",
       "  (276, 54.00),\n",
       "  (285, 54.00),\n",
       "  (327, 54.00),\n",
       "  (487, 54.00),\n",
       "  (547, 54.00),\n",
       "  (657, 54.00),\n",
       "  (709, 54.00),\n",
       "  (768, 54.00),\n",
       "  (780, 54.00),\n",
       "  (801, 54.00),\n",
       "  (832, 54.00),\n",
       "  (855, 54.00),\n",
       "  (936, 54.00),\n",
       "  (990, 54.00),\n",
       "  (995, 54.00),\n",
       "  (3, 53.00),\n",
       "  (35, 53.00),\n",
       "  (58, 53.00),\n",
       "  (70, 53.00),\n",
       "  (104, 53.00),\n",
       "  (138, 53.00),\n",
       "  (177, 53.00),\n",
       "  (251, 53.00),\n",
       "  (254, 53.00),\n",
       "  (274, 53.00),\n",
       "  (307, 53.00),\n",
       "  (367, 53.00),\n",
       "  (444, 53.00),\n",
       "  (452, 53.00),\n",
       "  (477, 53.00),\n",
       "  (508, 53.00),\n",
       "  (524, 53.00),\n",
       "  (526, 53.00),\n",
       "  (528, 53.00),\n",
       "  (533, 53.00),\n",
       "  (641, 53.00),\n",
       "  (653, 53.00),\n",
       "  (665, 53.00),\n",
       "  (668, 53.00),\n",
       "  (739, 53.00),\n",
       "  (818, 53.00),\n",
       "  (835, 53.00),\n",
       "  (888, 53.00),\n",
       "  (903, 53.00),\n",
       "  (922, 53.00),\n",
       "  (1, 52.00),\n",
       "  (92, 52.00),\n",
       "  (164, 52.00),\n",
       "  (176, 52.00),\n",
       "  (216, 52.00),\n",
       "  (239, 52.00),\n",
       "  (431, 52.00),\n",
       "  (448, 52.00),\n",
       "  (478, 52.00),\n",
       "  (701, 52.00),\n",
       "  (738, 52.00),\n",
       "  (752, 52.00),\n",
       "  (779, 52.00),\n",
       "  (787, 52.00),\n",
       "  (829, 52.00),\n",
       "  (833, 52.00),\n",
       "  (840, 52.00),\n",
       "  (877, 52.00),\n",
       "  (917, 52.00),\n",
       "  (939, 52.00),\n",
       "  (943, 52.00),\n",
       "  (133, 51.00),\n",
       "  (160, 51.00),\n",
       "  (188, 51.00),\n",
       "  (196, 51.00),\n",
       "  (212, 51.00),\n",
       "  (221, 51.00),\n",
       "  (286, 51.00),\n",
       "  (362, 51.00),\n",
       "  (377, 51.00),\n",
       "  (416, 51.00),\n",
       "  (419, 51.00),\n",
       "  (514, 51.00),\n",
       "  (545, 51.00),\n",
       "  (592, 51.00),\n",
       "  (636, 51.00),\n",
       "  (637, 51.00),\n",
       "  (746, 51.00),\n",
       "  (757, 51.00),\n",
       "  (764, 51.00),\n",
       "  (776, 51.00),\n",
       "  (902, 51.00),\n",
       "  (927, 51.00),\n",
       "  (13, 50.00),\n",
       "  (36, 50.00),\n",
       "  (102, 50.00),\n",
       "  (114, 50.00),\n",
       "  (126, 50.00),\n",
       "  (261, 50.00),\n",
       "  (277, 50.00),\n",
       "  (289, 50.00),\n",
       "  (294, 50.00),\n",
       "  (295, 50.00),\n",
       "  (301, 50.00),\n",
       "  (352, 50.00),\n",
       "  (358, 50.00),\n",
       "  (373, 50.00),\n",
       "  (449, 50.00),\n",
       "  (467, 50.00),\n",
       "  (604, 50.00),\n",
       "  (608, 50.00),\n",
       "  (618, 50.00),\n",
       "  (639, 50.00),\n",
       "  (659, 50.00),\n",
       "  (685, 50.00),\n",
       "  (765, 50.00),\n",
       "  (997, 50.00),\n",
       "  (0, 49.00),\n",
       "  (9, 49.00),\n",
       "  (123, 49.00),\n",
       "  (172, 49.00),\n",
       "  (267, 49.00),\n",
       "  (325, 49.00),\n",
       "  (375, 49.00),\n",
       "  (376, 49.00),\n",
       "  (378, 49.00),\n",
       "  (387, 49.00),\n",
       "  (388, 49.00),\n",
       "  (398, 49.00),\n",
       "  (523, 49.00),\n",
       "  (535, 49.00),\n",
       "  (541, 49.00),\n",
       "  (566, 49.00),\n",
       "  (583, 49.00),\n",
       "  (602, 49.00),\n",
       "  (667, 49.00),\n",
       "  (704, 49.00),\n",
       "  (763, 49.00),\n",
       "  (771, 49.00),\n",
       "  (874, 49.00),\n",
       "  (11, 48.00),\n",
       "  (12, 48.00),\n",
       "  (14, 48.00),\n",
       "  (15, 48.00),\n",
       "  (16, 48.00),\n",
       "  (18, 48.00),\n",
       "  (41, 48.00),\n",
       "  (71, 48.00),\n",
       "  (78, 48.00),\n",
       "  (134, 48.00),\n",
       "  (137, 48.00),\n",
       "  (141, 48.00),\n",
       "  (156, 48.00),\n",
       "  (194, 48.00),\n",
       "  (209, 48.00),\n",
       "  (214, 48.00),\n",
       "  (255, 48.00),\n",
       "  (264, 48.00),\n",
       "  (279, 48.00),\n",
       "  (288, 48.00),\n",
       "  (290, 48.00),\n",
       "  (312, 48.00),\n",
       "  (336, 48.00),\n",
       "  (340, 48.00),\n",
       "  (349, 48.00),\n",
       "  (354, 48.00),\n",
       "  (413, 48.00),\n",
       "  (451, 48.00),\n",
       "  (517, 48.00),\n",
       "  (628, 48.00),\n",
       "  (683, 48.00),\n",
       "  (684, 48.00),\n",
       "  (758, 48.00),\n",
       "  (797, 48.00),\n",
       "  (865, 48.00),\n",
       "  (872, 48.00),\n",
       "  (881, 48.00),\n",
       "  (890, 48.00),\n",
       "  (891, 48.00),\n",
       "  (915, 48.00),\n",
       "  (951, 48.00),\n",
       "  (994, 48.00),\n",
       "  (40, 47.00),\n",
       "  (53, 47.00),\n",
       "  (100, 47.00),\n",
       "  (101, 47.00),\n",
       "  (110, 47.00),\n",
       "  (130, 47.00),\n",
       "  (136, 47.00),\n",
       "  (227, 47.00),\n",
       "  (243, 47.00),\n",
       "  (253, 47.00),\n",
       "  (256, 47.00),\n",
       "  (265, 47.00),\n",
       "  (321, 47.00),\n",
       "  (333, 47.00),\n",
       "  (335, 47.00),\n",
       "  (337, 47.00),\n",
       "  (350, 47.00),\n",
       "  (351, 47.00),\n",
       "  (395, 47.00),\n",
       "  (426, 47.00),\n",
       "  (503, 47.00),\n",
       "  (518, 47.00),\n",
       "  (560, 47.00),\n",
       "  (576, 47.00),\n",
       "  (606, 47.00),\n",
       "  (707, 47.00),\n",
       "  (732, 47.00),\n",
       "  (759, 47.00),\n",
       "  (769, 47.00),\n",
       "  (807, 47.00),\n",
       "  (886, 47.00),\n",
       "  (5, 46.00),\n",
       "  (22, 46.00),\n",
       "  (56, 46.00),\n",
       "  (63, 46.00),\n",
       "  (65, 46.00),\n",
       "  (72, 46.00),\n",
       "  (79, 46.00),\n",
       "  (87, 46.00),\n",
       "  (95, 46.00),\n",
       "  (108, 46.00),\n",
       "  (149, 46.00),\n",
       "  (157, 46.00),\n",
       "  (201, 46.00),\n",
       "  (215, 46.00),\n",
       "  (266, 46.00),\n",
       "  (320, 46.00),\n",
       "  (323, 46.00),\n",
       "  (324, 46.00),\n",
       "  (366, 46.00),\n",
       "  (370, 46.00),\n",
       "  (397, 46.00),\n",
       "  (422, 46.00),\n",
       "  (432, 46.00),\n",
       "  (433, 46.00),\n",
       "  (434, 46.00),\n",
       "  (530, 46.00),\n",
       "  (616, 46.00),\n",
       "  (658, 46.00),\n",
       "  (664, 46.00),\n",
       "  (751, 46.00),\n",
       "  (753, 46.00),\n",
       "  (796, 46.00),\n",
       "  (823, 46.00),\n",
       "  (848, 46.00),\n",
       "  (867, 46.00),\n",
       "  (882, 46.00),\n",
       "  (918, 46.00),\n",
       "  (983, 46.00),\n",
       "  (989, 46.00),\n",
       "  (993, 46.00),\n",
       "  (2, 45.00),\n",
       "  (28, 45.00),\n",
       "  (66, 45.00),\n",
       "  (96, 45.00),\n",
       "  (105, 45.00),\n",
       "  (121, 45.00),\n",
       "  (139, 45.00),\n",
       "  (144, 45.00),\n",
       "  (169, 45.00),\n",
       "  (191, 45.00),\n",
       "  (247, 45.00),\n",
       "  (273, 45.00),\n",
       "  (299, 45.00),\n",
       "  (326, 45.00),\n",
       "  (332, 45.00),\n",
       "  (339, 45.00),\n",
       "  (344, 45.00),\n",
       "  (355, 45.00),\n",
       "  (365, 45.00),\n",
       "  (389, 45.00),\n",
       "  (392, 45.00),\n",
       "  (445, 45.00),\n",
       "  (571, 45.00),\n",
       "  (599, 45.00),\n",
       "  (625, 45.00),\n",
       "  (674, 45.00),\n",
       "  (734, 45.00),\n",
       "  (755, 45.00),\n",
       "  (817, 45.00),\n",
       "  (853, 45.00),\n",
       "  (900, 45.00),\n",
       "  (910, 45.00),\n",
       "  (75, 44.00),\n",
       "  (131, 44.00),\n",
       "  (186, 44.00),\n",
       "  (223, 44.00),\n",
       "  (244, 44.00),\n",
       "  (252, 44.00),\n",
       "  (405, 44.00),\n",
       "  (412, 44.00),\n",
       "  (417, 44.00),\n",
       "  (473, 44.00),\n",
       "  (475, 44.00),\n",
       "  (486, 44.00),\n",
       "  (579, 44.00),\n",
       "  (613, 44.00),\n",
       "  (682, 44.00),\n",
       "  (702, 44.00),\n",
       "  (706, 44.00),\n",
       "  (727, 44.00),\n",
       "  (821, 44.00),\n",
       "  (941, 44.00),\n",
       "  (968, 44.00),\n",
       "  (984, 44.00),\n",
       "  (44, 43.00),\n",
       "  (83, 43.00),\n",
       "  (107, 43.00),\n",
       "  (142, 43.00),\n",
       "  (198, 43.00),\n",
       "  (268, 43.00),\n",
       "  (330, 43.00),\n",
       "  (353, 43.00),\n",
       "  (357, 43.00),\n",
       "  (360, 43.00),\n",
       "  (381, 43.00),\n",
       "  (384, 43.00),\n",
       "  (414, 43.00),\n",
       "  (495, 43.00),\n",
       "  (537, 43.00),\n",
       "  (542, 43.00),\n",
       "  (717, 43.00),\n",
       "  (782, 43.00),\n",
       "  (844, 43.00),\n",
       "  (873, 43.00),\n",
       "  (907, 43.00),\n",
       "  (920, 43.00),\n",
       "  (959, 43.00),\n",
       "  (998, 43.00),\n",
       "  (7, 42.00),\n",
       "  (17, 42.00),\n",
       "  (19, 42.00),\n",
       "  (42, 42.00),\n",
       "  (80, 42.00),\n",
       "  (93, 42.00),\n",
       "  (112, 42.00),\n",
       "  (132, 42.00),\n",
       "  (154, 42.00),\n",
       "  (174, 42.00),\n",
       "  (193, 42.00),\n",
       "  (245, 42.00),\n",
       "  (260, 42.00),\n",
       "  (306, 42.00),\n",
       "  (309, 42.00),\n",
       "  (421, 42.00),\n",
       "  (429, 42.00),\n",
       "  (437, 42.00),\n",
       "  (447, 42.00),\n",
       "  (464, 42.00),\n",
       "  (485, 42.00),\n",
       "  (507, 42.00),\n",
       "  (510, 42.00),\n",
       "  (538, 42.00),\n",
       "  (575, 42.00),\n",
       "  (633, 42.00),\n",
       "  (656, 42.00),\n",
       "  (666, 42.00),\n",
       "  (690, 42.00),\n",
       "  (710, 42.00),\n",
       "  (799, 42.00),\n",
       "  (889, 42.00),\n",
       "  (894, 42.00),\n",
       "  (919, 42.00),\n",
       "  (932, 42.00),\n",
       "  (933, 42.00),\n",
       "  (981, 42.00),\n",
       "  (999, 42.00),\n",
       "  (117, 41.00),\n",
       "  (127, 41.00),\n",
       "  (153, 41.00),\n",
       "  (190, 41.00),\n",
       "  (224, 41.00),\n",
       "  (258, 41.00),\n",
       "  (278, 41.00),\n",
       "  (322, 41.00),\n",
       "  (390, 41.00),\n",
       "  (393, 41.00),\n",
       "  (430, 41.00),\n",
       "  (574, 41.00),\n",
       "  (712, 41.00),\n",
       "  (723, 41.00),\n",
       "  (795, 41.00),\n",
       "  (854, 41.00),\n",
       "  (898, 41.00),\n",
       "  (929, 41.00),\n",
       "  (27, 40.00),\n",
       "  (230, 40.00),\n",
       "  (242, 40.00),\n",
       "  (257, 40.00),\n",
       "  (287, 40.00),\n",
       "  (418, 40.00),\n",
       "  (500, 40.00),\n",
       "  (546, 40.00),\n",
       "  (548, 40.00),\n",
       "  (558, 40.00),\n",
       "  (573, 40.00),\n",
       "  (584, 40.00),\n",
       "  (600, 40.00),\n",
       "  (610, 40.00),\n",
       "  (713, 40.00),\n",
       "  (736, 40.00),\n",
       "  (749, 40.00),\n",
       "  (837, 40.00),\n",
       "  (862, 40.00),\n",
       "  (921, 40.00),\n",
       "  (925, 40.00),\n",
       "  (996, 40.00),\n",
       "  (173, 39.00),\n",
       "  (183, 39.00),\n",
       "  (262, 39.00),\n",
       "  (296, 39.00),\n",
       "  (297, 39.00),\n",
       "  (302, 39.00),\n",
       "  (329, 39.00),\n",
       "  (368, 39.00),\n",
       "  (435, 39.00),\n",
       "  (470, 39.00),\n",
       "  (480, 39.00),\n",
       "  (513, 39.00),\n",
       "  (520, 39.00),\n",
       "  (529, 39.00),\n",
       "  (553, 39.00),\n",
       "  (670, 39.00),\n",
       "  (672, 39.00),\n",
       "  (677, 39.00),\n",
       "  (694, 39.00),\n",
       "  (722, 39.00),\n",
       "  (726, 39.00),\n",
       "  (756, 39.00),\n",
       "  (812, 39.00),\n",
       "  (945, 39.00),\n",
       "  (43, 38.00),\n",
       "  (91, 38.00),\n",
       "  (210, 38.00),\n",
       "  (235, 38.00),\n",
       "  (236, 38.00),\n",
       "  (305, 38.00),\n",
       "  (314, 38.00),\n",
       "  (439, 38.00),\n",
       "  (442, 38.00),\n",
       "  (456, 38.00),\n",
       "  (462, 38.00),\n",
       "  (466, 38.00),\n",
       "  (501, 38.00),\n",
       "  (540, 38.00),\n",
       "  (555, 38.00),\n",
       "  (559, 38.00),\n",
       "  (643, 38.00),\n",
       "  (687, 38.00),\n",
       "  (699, 38.00),\n",
       "  (708, 38.00),\n",
       "  (719, 38.00),\n",
       "  (38, 37.00),\n",
       "  (52, 37.00),\n",
       "  (111, 37.00),\n",
       "  (140, 37.00),\n",
       "  (175, 37.00),\n",
       "  (204, 37.00),\n",
       "  (248, 37.00),\n",
       "  (338, 37.00),\n",
       "  (346, 37.00),\n",
       "  (379, 37.00),\n",
       "  (450, 37.00),\n",
       "  (494, 37.00),\n",
       "  (554, 37.00),\n",
       "  (615, 37.00),\n",
       "  (630, 37.00),\n",
       "  (650, 37.00),\n",
       "  (678, 37.00),\n",
       "  (693, 37.00),\n",
       "  (714, 37.00),\n",
       "  (761, 37.00),\n",
       "  (814, 37.00),\n",
       "  (831, 37.00),\n",
       "  (934, 37.00),\n",
       "  (10, 36.00),\n",
       "  (26, 36.00),\n",
       "  (45, 36.00),\n",
       "  (62, 36.00),\n",
       "  (98, 36.00),\n",
       "  (120, 36.00),\n",
       "  (145, 36.00),\n",
       "  (200, 36.00),\n",
       "  (207, 36.00),\n",
       "  (408, 36.00),\n",
       "  (427, 36.00),\n",
       "  (453, 36.00),\n",
       "  (511, 36.00),\n",
       "  (588, 36.00),\n",
       "  (827, 36.00),\n",
       "  (851, 36.00),\n",
       "  (20, 35.00),\n",
       "  (143, 35.00),\n",
       "  (148, 35.00),\n",
       "  (220, 35.00),\n",
       "  (347, 35.00),\n",
       "  (374, 35.00),\n",
       "  (380, 35.00),\n",
       "  (409, 35.00),\n",
       "  (415, 35.00),\n",
       "  (543, 35.00),\n",
       "  (605, 35.00),\n",
       "  (627, 35.00),\n",
       "  (745, 35.00),\n",
       "  (760, 35.00),\n",
       "  (793, 35.00),\n",
       "  (798, 35.00),\n",
       "  (846, 35.00),\n",
       "  (958, 35.00),\n",
       "  (73, 34.00),\n",
       "  (226, 34.00),\n",
       "  (399, 34.00),\n",
       "  (465, 34.00),\n",
       "  (720, 34.00),\n",
       "  (786, 34.00),\n",
       "  (802, 34.00),\n",
       "  (861, 34.00),\n",
       "  (897, 34.00),\n",
       "  (928, 34.00),\n",
       "  (81, 33.00),\n",
       "  (152, 33.00),\n",
       "  (158, 33.00),\n",
       "  (205, 33.00),\n",
       "  (213, 33.00),\n",
       "  (385, 33.00),\n",
       "  (647, 33.00),\n",
       "  (803, 33.00),\n",
       "  (859, 33.00),\n",
       "  (964, 33.00),\n",
       "  (59, 32.00),\n",
       "  (86, 32.00),\n",
       "  (146, 32.00),\n",
       "  (356, 32.00),\n",
       "  (359, 32.00),\n",
       "  (371, 32.00),\n",
       "  (402, 32.00),\n",
       "  (629, 32.00),\n",
       "  (883, 32.00),\n",
       "  (947, 32.00),\n",
       "  (980, 32.00),\n",
       "  (106, 31.00),\n",
       "  (179, 31.00),\n",
       "  (233, 31.00),\n",
       "  (403, 31.00),\n",
       "  (459, 31.00),\n",
       "  (502, 31.00),\n",
       "  (521, 31.00),\n",
       "  (744, 31.00),\n",
       "  (914, 31.00),\n",
       "  (923, 31.00),\n",
       "  (954, 31.00),\n",
       "  (64, 30.00),\n",
       "  (166, 30.00),\n",
       "  (202, 30.00),\n",
       "  (345, 30.00),\n",
       "  (469, 30.00),\n",
       "  (484, 30.00),\n",
       "  (531, 30.00),\n",
       "  (551, 30.00),\n",
       "  (568, 30.00),\n",
       "  (578, 30.00),\n",
       "  (589, 30.00),\n",
       "  (634, 30.00),\n",
       "  (663, 30.00),\n",
       "  (705, 30.00),\n",
       "  (948, 30.00),\n",
       "  (187, 29.00),\n",
       "  (461, 29.00),\n",
       "  (498, 29.00),\n",
       "  (557, 29.00),\n",
       "  (585, 29.00),\n",
       "  (617, 29.00),\n",
       "  (860, 29.00),\n",
       "  (966, 29.00),\n",
       "  (4, 28.00),\n",
       "  (29, 28.00),\n",
       "  (122, 28.00),\n",
       "  (596, 28.00),\n",
       "  (632, 28.00),\n",
       "  (676, 28.00),\n",
       "  (691, 28.00),\n",
       "  (733, 28.00),\n",
       "  (747, 28.00),\n",
       "  (49, 27.00),\n",
       "  (54, 27.00),\n",
       "  (147, 27.00),\n",
       "  (150, 27.00),\n",
       "  (271, 27.00),\n",
       "  (303, 27.00),\n",
       "  (369, 27.00),\n",
       "  (567, 27.00),\n",
       "  (587, 27.00),\n",
       "  (624, 27.00),\n",
       "  (644, 27.00),\n",
       "  (660, 27.00),\n",
       "  (718, 27.00),\n",
       "  (767, 27.00),\n",
       "  (789, 27.00),\n",
       "  (32, 26.00),\n",
       "  (516, 26.00),\n",
       "  (544, 26.00),\n",
       "  (631, 26.00),\n",
       "  (731, 26.00),\n",
       "  (804, 26.00),\n",
       "  (869, 26.00),\n",
       "  (965, 26.00),\n",
       "  (163, 25.00),\n",
       "  (168, 25.00),\n",
       "  (394, 25.00),\n",
       "  (479, 25.00),\n",
       "  (482, 25.00),\n",
       "  (536, 25.00),\n",
       "  (590, 25.00),\n",
       "  (623, 25.00),\n",
       "  (686, 25.00),\n",
       "  (838, 25.00),\n",
       "  (908, 25.00),\n",
       "  (165, 24.00),\n",
       "  (785, 24.00),\n",
       "  (901, 24.00),\n",
       "  (931, 24.00),\n",
       "  (942, 24.00),\n",
       "  (185, 23.00),\n",
       "  (240, 23.00),\n",
       "  (499, 23.00),\n",
       "  (638, 23.00),\n",
       "  (680, 23.00),\n",
       "  (876, 23.00),\n",
       "  (974, 23.00),\n",
       "  (68, 22.00),\n",
       "  (525, 22.00),\n",
       "  (740, 22.00),\n",
       "  (773, 22.00),\n",
       "  (811, 22.00),\n",
       "  (856, 22.00),\n",
       "  (909, 22.00),\n",
       "  (911, 22.00),\n",
       "  (246, 21.00),\n",
       "  (438, 21.00),\n",
       "  (675, 21.00),\n",
       "  (885, 21.00),\n",
       "  (315, 20.00),\n",
       "  (400, 20.00),\n",
       "  (662, 20.00),\n",
       "  (715, 20.00),\n",
       "  (977, 20.00),\n",
       "  (598, 19.00),\n",
       "  (729, 19.00),\n",
       "  (622, 18.00),\n",
       "  (651, 18.00),\n",
       "  (103, 17.00),\n",
       "  (550, 17.00),\n",
       "  (648, 17.00),\n",
       "  (728, 17.00),\n",
       "  (841, 17.00),\n",
       "  (976, 17.00),\n",
       "  (460, 16.00),\n",
       "  (504, 16.00),\n",
       "  (836, 16.00),\n",
       "  (930, 16.00),\n",
       "  (970, 16.00),\n",
       "  (282, 15.00),\n",
       "  (446, 15.00),\n",
       "  (493, 15.00),\n",
       "  (534, 15.00),\n",
       "  (906, 15.00),\n",
       "  (813, 14.00),\n",
       "  (969, 13.00),\n",
       "  (742, 12.00),\n",
       "  (940, 12.00),\n",
       "  (34, 11.00),\n",
       "  (167, 11.00),\n",
       "  (689, 11.00),\n",
       "  (967, 11.00),\n",
       "  (978, 11.00),\n",
       "  (681, 9.00),\n",
       "  (899, 9.00),\n",
       "  (960, 9.00),\n",
       "  (673, 8.00),\n",
       "  (810, 7.00),\n",
       "  (935, 6.00),\n",
       "  (961, 2.00)])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = targeted_diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch_no 0\n",
      "at batch_no 100\n",
      "at batch_no 200\n",
      "at batch_no 300\n",
      "at batch_no 400\n",
      "at batch_no 500\n",
      "at batch_no 600\n",
      "at batch_no 700\n",
      "at batch_no 800\n",
      "at batch_no 900\n",
      "at batch_no 1000\n",
      "at batch_no 1100\n",
      "at batch_no 1200\n",
      "at batch_no 1300\n",
      "at batch_no 1400\n",
      "at batch_no 1500\n",
      "at batch_no 1600\n",
      "at batch_no 1700\n",
      "at batch_no 1800\n",
      "at batch_no 1900\n",
      "at batch_no 2000\n",
      "at batch_no 2100\n",
      "at batch_no 2200\n",
      "at batch_no 2300\n",
      "at batch_no 2400\n",
      "at batch_no 2500\n",
      "at batch_no 2600\n",
      "at batch_no 2700\n",
      "at batch_no 2800\n",
      "at batch_no 2900\n",
      "at batch_no 3000\n",
      "at batch_no 3100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(885,\n",
       " [(815, 239.00),\n",
       "  (652, 181.00),\n",
       "  (646, 168.00),\n",
       "  (580, 163.00),\n",
       "  (611, 155.00),\n",
       "  (591, 148.00),\n",
       "  (737, 143.00),\n",
       "  (489, 142.00),\n",
       "  (621, 139.00),\n",
       "  (904, 134.00),\n",
       "  (794, 130.00),\n",
       "  (497, 127.00),\n",
       "  (893, 127.00),\n",
       "  (582, 125.00),\n",
       "  (94, 123.00),\n",
       "  (955, 120.00),\n",
       "  (116, 115.00),\n",
       "  (868, 115.00),\n",
       "  (39, 112.00),\n",
       "  (679, 112.00),\n",
       "  (565, 110.00),\n",
       "  (162, 109.00),\n",
       "  (491, 108.00),\n",
       "  (721, 108.00),\n",
       "  (979, 108.00),\n",
       "  (741, 105.00),\n",
       "  (839, 105.00),\n",
       "  (109, 104.00),\n",
       "  (562, 104.00),\n",
       "  (84, 103.00),\n",
       "  (549, 102.00),\n",
       "  (48, 99.00),\n",
       "  (82, 99.00),\n",
       "  (492, 99.00),\n",
       "  (51, 98.00),\n",
       "  (973, 96.00),\n",
       "  (199, 95.00),\n",
       "  (750, 95.00),\n",
       "  (46, 93.00),\n",
       "  (640, 92.00),\n",
       "  (695, 92.00),\n",
       "  (151, 88.00),\n",
       "  (971, 88.00),\n",
       "  (783, 87.00),\n",
       "  (843, 87.00),\n",
       "  (203, 86.00),\n",
       "  (669, 86.00),\n",
       "  (692, 86.00),\n",
       "  (424, 84.00),\n",
       "  (956, 84.00),\n",
       "  (281, 83.00),\n",
       "  (577, 83.00),\n",
       "  (828, 82.00),\n",
       "  (866, 82.00),\n",
       "  (47, 81.00),\n",
       "  (310, 81.00),\n",
       "  (743, 81.00),\n",
       "  (847, 81.00),\n",
       "  (61, 80.00),\n",
       "  (208, 80.00),\n",
       "  (703, 80.00),\n",
       "  (180, 79.00),\n",
       "  (182, 79.00),\n",
       "  (382, 79.00),\n",
       "  (830, 79.00),\n",
       "  (189, 78.00),\n",
       "  (219, 78.00),\n",
       "  (319, 78.00),\n",
       "  (343, 78.00),\n",
       "  (411, 78.00),\n",
       "  (725, 78.00),\n",
       "  (879, 78.00),\n",
       "  (342, 77.00),\n",
       "  (406, 77.00),\n",
       "  (440, 77.00),\n",
       "  (454, 77.00),\n",
       "  (778, 77.00),\n",
       "  (128, 76.00),\n",
       "  (197, 76.00),\n",
       "  (270, 76.00),\n",
       "  (298, 76.00),\n",
       "  (570, 76.00),\n",
       "  (824, 76.00),\n",
       "  (982, 76.00),\n",
       "  (711, 75.00),\n",
       "  (754, 75.00),\n",
       "  (762, 75.00),\n",
       "  (791, 75.00),\n",
       "  (963, 75.00),\n",
       "  (55, 74.00),\n",
       "  (217, 74.00),\n",
       "  (237, 74.00),\n",
       "  (364, 74.00),\n",
       "  (455, 74.00),\n",
       "  (572, 74.00),\n",
       "  (671, 74.00),\n",
       "  (805, 74.00),\n",
       "  (905, 74.00),\n",
       "  (318, 73.00),\n",
       "  (436, 73.00),\n",
       "  (735, 73.00),\n",
       "  (30, 72.00),\n",
       "  (304, 72.00),\n",
       "  (363, 72.00),\n",
       "  (472, 72.00),\n",
       "  (645, 72.00),\n",
       "  (730, 72.00),\n",
       "  (800, 72.00),\n",
       "  (845, 72.00),\n",
       "  (76, 71.00),\n",
       "  (483, 71.00),\n",
       "  (496, 71.00),\n",
       "  (716, 71.00),\n",
       "  (878, 71.00),\n",
       "  (938, 71.00),\n",
       "  (23, 70.00),\n",
       "  (50, 70.00),\n",
       "  (313, 70.00),\n",
       "  (471, 70.00),\n",
       "  (654, 70.00),\n",
       "  (700, 70.00),\n",
       "  (806, 70.00),\n",
       "  (826, 70.00),\n",
       "  (192, 69.00),\n",
       "  (820, 69.00),\n",
       "  (825, 69.00),\n",
       "  (896, 69.00),\n",
       "  (195, 68.00),\n",
       "  (425, 68.00),\n",
       "  (849, 68.00),\n",
       "  (916, 68.00),\n",
       "  (74, 67.00),\n",
       "  (222, 67.00),\n",
       "  (341, 67.00),\n",
       "  (468, 67.00),\n",
       "  (808, 67.00),\n",
       "  (949, 67.00),\n",
       "  (37, 66.00),\n",
       "  (77, 66.00),\n",
       "  (556, 66.00),\n",
       "  (688, 66.00),\n",
       "  (850, 66.00),\n",
       "  (912, 66.00),\n",
       "  (944, 66.00),\n",
       "  (972, 66.00),\n",
       "  (655, 65.00),\n",
       "  (775, 65.00),\n",
       "  (926, 65.00),\n",
       "  (97, 64.00),\n",
       "  (124, 64.00),\n",
       "  (178, 64.00),\n",
       "  (293, 64.00),\n",
       "  (772, 64.00),\n",
       "  (863, 64.00),\n",
       "  (870, 64.00),\n",
       "  (880, 64.00),\n",
       "  (992, 64.00),\n",
       "  (125, 63.00),\n",
       "  (181, 63.00),\n",
       "  (184, 63.00),\n",
       "  (211, 63.00),\n",
       "  (238, 63.00),\n",
       "  (272, 63.00),\n",
       "  (300, 63.00),\n",
       "  (311, 63.00),\n",
       "  (331, 63.00),\n",
       "  (420, 63.00),\n",
       "  (458, 63.00),\n",
       "  (506, 63.00),\n",
       "  (515, 63.00),\n",
       "  (620, 63.00),\n",
       "  (864, 63.00),\n",
       "  (892, 63.00),\n",
       "  (953, 63.00),\n",
       "  (90, 62.00),\n",
       "  (135, 62.00),\n",
       "  (155, 62.00),\n",
       "  (161, 62.00),\n",
       "  (234, 62.00),\n",
       "  (505, 62.00),\n",
       "  (561, 62.00),\n",
       "  (619, 62.00),\n",
       "  (784, 62.00),\n",
       "  (819, 62.00),\n",
       "  (871, 62.00),\n",
       "  (975, 62.00),\n",
       "  (988, 62.00),\n",
       "  (6, 61.00),\n",
       "  (249, 61.00),\n",
       "  (292, 61.00),\n",
       "  (476, 61.00),\n",
       "  (527, 61.00),\n",
       "  (635, 61.00),\n",
       "  (724, 61.00),\n",
       "  (788, 61.00),\n",
       "  (895, 61.00),\n",
       "  (937, 61.00),\n",
       "  (946, 61.00),\n",
       "  (99, 60.00),\n",
       "  (225, 60.00),\n",
       "  (423, 60.00),\n",
       "  (463, 60.00),\n",
       "  (481, 60.00),\n",
       "  (586, 60.00),\n",
       "  (595, 60.00),\n",
       "  (626, 60.00),\n",
       "  (748, 60.00),\n",
       "  (781, 60.00),\n",
       "  (792, 60.00),\n",
       "  (875, 60.00),\n",
       "  (884, 60.00),\n",
       "  (887, 60.00),\n",
       "  (231, 59.00),\n",
       "  (316, 59.00),\n",
       "  (327, 59.00),\n",
       "  (361, 59.00),\n",
       "  (391, 59.00),\n",
       "  (401, 59.00),\n",
       "  (474, 59.00),\n",
       "  (509, 59.00),\n",
       "  (593, 59.00),\n",
       "  (597, 59.00),\n",
       "  (641, 59.00),\n",
       "  (697, 59.00),\n",
       "  (790, 59.00),\n",
       "  (952, 59.00),\n",
       "  (8, 58.00),\n",
       "  (115, 58.00),\n",
       "  (118, 58.00),\n",
       "  (348, 58.00),\n",
       "  (404, 58.00),\n",
       "  (410, 58.00),\n",
       "  (478, 58.00),\n",
       "  (532, 58.00),\n",
       "  (594, 58.00),\n",
       "  (609, 58.00),\n",
       "  (614, 58.00),\n",
       "  (770, 58.00),\n",
       "  (809, 58.00),\n",
       "  (957, 58.00),\n",
       "  (962, 58.00),\n",
       "  (991, 58.00),\n",
       "  (24, 57.00),\n",
       "  (33, 57.00),\n",
       "  (113, 57.00),\n",
       "  (129, 57.00),\n",
       "  (275, 57.00),\n",
       "  (308, 57.00),\n",
       "  (407, 57.00),\n",
       "  (441, 57.00),\n",
       "  (457, 57.00),\n",
       "  (477, 57.00),\n",
       "  (522, 57.00),\n",
       "  (539, 57.00),\n",
       "  (564, 57.00),\n",
       "  (581, 57.00),\n",
       "  (607, 57.00),\n",
       "  (698, 57.00),\n",
       "  (774, 57.00),\n",
       "  (816, 57.00),\n",
       "  (834, 57.00),\n",
       "  (842, 57.00),\n",
       "  (877, 57.00),\n",
       "  (913, 57.00),\n",
       "  (25, 56.00),\n",
       "  (57, 56.00),\n",
       "  (119, 56.00),\n",
       "  (171, 56.00),\n",
       "  (280, 56.00),\n",
       "  (284, 56.00),\n",
       "  (317, 56.00),\n",
       "  (334, 56.00),\n",
       "  (396, 56.00),\n",
       "  (428, 56.00),\n",
       "  (443, 56.00),\n",
       "  (488, 56.00),\n",
       "  (601, 56.00),\n",
       "  (642, 56.00),\n",
       "  (649, 56.00),\n",
       "  (661, 56.00),\n",
       "  (696, 56.00),\n",
       "  (852, 56.00),\n",
       "  (950, 56.00),\n",
       "  (987, 56.00),\n",
       "  (21, 55.00),\n",
       "  (60, 55.00),\n",
       "  (69, 55.00),\n",
       "  (85, 55.00),\n",
       "  (88, 55.00),\n",
       "  (104, 55.00),\n",
       "  (170, 55.00),\n",
       "  (232, 55.00),\n",
       "  (250, 55.00),\n",
       "  (254, 55.00),\n",
       "  (259, 55.00),\n",
       "  (274, 55.00),\n",
       "  (283, 55.00),\n",
       "  (291, 55.00),\n",
       "  (328, 55.00),\n",
       "  (367, 55.00),\n",
       "  (372, 55.00),\n",
       "  (375, 55.00),\n",
       "  (452, 55.00),\n",
       "  (512, 55.00),\n",
       "  (547, 55.00),\n",
       "  (563, 55.00),\n",
       "  (569, 55.00),\n",
       "  (603, 55.00),\n",
       "  (608, 55.00),\n",
       "  (618, 55.00),\n",
       "  (653, 55.00),\n",
       "  (766, 55.00),\n",
       "  (780, 55.00),\n",
       "  (797, 55.00),\n",
       "  (801, 55.00),\n",
       "  (822, 55.00),\n",
       "  (829, 55.00),\n",
       "  (840, 55.00),\n",
       "  (857, 55.00),\n",
       "  (903, 55.00),\n",
       "  (936, 55.00),\n",
       "  (206, 54.00),\n",
       "  (228, 54.00),\n",
       "  (251, 54.00),\n",
       "  (263, 54.00),\n",
       "  (285, 54.00),\n",
       "  (307, 54.00),\n",
       "  (386, 54.00),\n",
       "  (419, 54.00),\n",
       "  (448, 54.00),\n",
       "  (524, 54.00),\n",
       "  (701, 54.00),\n",
       "  (768, 54.00),\n",
       "  (777, 54.00),\n",
       "  (832, 54.00),\n",
       "  (835, 54.00),\n",
       "  (888, 54.00),\n",
       "  (902, 54.00),\n",
       "  (924, 54.00),\n",
       "  (939, 54.00),\n",
       "  (943, 54.00),\n",
       "  (985, 54.00),\n",
       "  (986, 54.00),\n",
       "  (31, 53.00),\n",
       "  (58, 53.00),\n",
       "  (159, 53.00),\n",
       "  (256, 53.00),\n",
       "  (294, 53.00),\n",
       "  (383, 53.00),\n",
       "  (487, 53.00),\n",
       "  (533, 53.00),\n",
       "  (541, 53.00),\n",
       "  (612, 53.00),\n",
       "  (657, 53.00),\n",
       "  (664, 53.00),\n",
       "  (667, 53.00),\n",
       "  (709, 53.00),\n",
       "  (858, 53.00),\n",
       "  (917, 53.00),\n",
       "  (922, 53.00),\n",
       "  (990, 53.00),\n",
       "  (995, 53.00),\n",
       "  (997, 53.00),\n",
       "  (0, 52.00),\n",
       "  (89, 52.00),\n",
       "  (164, 52.00),\n",
       "  (218, 52.00),\n",
       "  (229, 52.00),\n",
       "  (269, 52.00),\n",
       "  (276, 52.00),\n",
       "  (289, 52.00),\n",
       "  (352, 52.00),\n",
       "  (451, 52.00),\n",
       "  (490, 52.00),\n",
       "  (528, 52.00),\n",
       "  (545, 52.00),\n",
       "  (665, 52.00),\n",
       "  (752, 52.00),\n",
       "  (771, 52.00),\n",
       "  (927, 52.00),\n",
       "  (13, 51.00),\n",
       "  (36, 51.00),\n",
       "  (67, 51.00),\n",
       "  (70, 51.00),\n",
       "  (78, 51.00),\n",
       "  (126, 51.00),\n",
       "  (133, 51.00),\n",
       "  (177, 51.00),\n",
       "  (194, 51.00),\n",
       "  (196, 51.00),\n",
       "  (209, 51.00),\n",
       "  (239, 51.00),\n",
       "  (241, 51.00),\n",
       "  (261, 51.00),\n",
       "  (264, 51.00),\n",
       "  (286, 51.00),\n",
       "  (295, 51.00),\n",
       "  (362, 51.00),\n",
       "  (388, 51.00),\n",
       "  (431, 51.00),\n",
       "  (508, 51.00),\n",
       "  (526, 51.00),\n",
       "  (602, 51.00),\n",
       "  (616, 51.00),\n",
       "  (685, 51.00),\n",
       "  (738, 51.00),\n",
       "  (739, 51.00),\n",
       "  (1, 50.00),\n",
       "  (3, 50.00),\n",
       "  (92, 50.00),\n",
       "  (100, 50.00),\n",
       "  (138, 50.00),\n",
       "  (172, 50.00),\n",
       "  (216, 50.00),\n",
       "  (277, 50.00),\n",
       "  (325, 50.00),\n",
       "  (340, 50.00),\n",
       "  (358, 50.00),\n",
       "  (373, 50.00),\n",
       "  (378, 50.00),\n",
       "  (433, 50.00),\n",
       "  (519, 50.00),\n",
       "  (552, 50.00),\n",
       "  (639, 50.00),\n",
       "  (668, 50.00),\n",
       "  (683, 50.00),\n",
       "  (684, 50.00),\n",
       "  (746, 50.00),\n",
       "  (757, 50.00),\n",
       "  (779, 50.00),\n",
       "  (833, 50.00),\n",
       "  (865, 50.00),\n",
       "  (9, 49.00),\n",
       "  (14, 49.00),\n",
       "  (15, 49.00),\n",
       "  (35, 49.00),\n",
       "  (63, 49.00),\n",
       "  (66, 49.00),\n",
       "  (96, 49.00),\n",
       "  (123, 49.00),\n",
       "  (160, 49.00),\n",
       "  (176, 49.00),\n",
       "  (212, 49.00),\n",
       "  (214, 49.00),\n",
       "  (267, 49.00),\n",
       "  (326, 49.00),\n",
       "  (333, 49.00),\n",
       "  (336, 49.00),\n",
       "  (349, 49.00),\n",
       "  (376, 49.00),\n",
       "  (377, 49.00),\n",
       "  (387, 49.00),\n",
       "  (416, 49.00),\n",
       "  (444, 49.00),\n",
       "  (467, 49.00),\n",
       "  (583, 49.00),\n",
       "  (604, 49.00),\n",
       "  (606, 49.00),\n",
       "  (628, 49.00),\n",
       "  (659, 49.00),\n",
       "  (704, 49.00),\n",
       "  (732, 49.00),\n",
       "  (765, 49.00),\n",
       "  (776, 49.00),\n",
       "  (787, 49.00),\n",
       "  (855, 49.00),\n",
       "  (11, 48.00),\n",
       "  (16, 48.00),\n",
       "  (18, 48.00),\n",
       "  (22, 48.00),\n",
       "  (41, 48.00),\n",
       "  (95, 48.00),\n",
       "  (102, 48.00),\n",
       "  (114, 48.00),\n",
       "  (188, 48.00),\n",
       "  (191, 48.00),\n",
       "  (243, 48.00),\n",
       "  (253, 48.00),\n",
       "  (265, 48.00),\n",
       "  (290, 48.00),\n",
       "  (337, 48.00),\n",
       "  (344, 48.00),\n",
       "  (365, 48.00),\n",
       "  (397, 48.00),\n",
       "  (432, 48.00),\n",
       "  (449, 48.00),\n",
       "  (514, 48.00),\n",
       "  (535, 48.00),\n",
       "  (566, 48.00),\n",
       "  (576, 48.00),\n",
       "  (592, 48.00),\n",
       "  (633, 48.00),\n",
       "  (674, 48.00),\n",
       "  (706, 48.00),\n",
       "  (707, 48.00),\n",
       "  (764, 48.00),\n",
       "  (853, 48.00),\n",
       "  (874, 48.00),\n",
       "  (890, 48.00),\n",
       "  (891, 48.00),\n",
       "  (915, 48.00),\n",
       "  (5, 47.00),\n",
       "  (12, 47.00),\n",
       "  (53, 47.00),\n",
       "  (72, 47.00),\n",
       "  (79, 47.00),\n",
       "  (121, 47.00),\n",
       "  (134, 47.00),\n",
       "  (144, 47.00),\n",
       "  (156, 47.00),\n",
       "  (169, 47.00),\n",
       "  (193, 47.00),\n",
       "  (201, 47.00),\n",
       "  (255, 47.00),\n",
       "  (279, 47.00),\n",
       "  (288, 47.00),\n",
       "  (301, 47.00),\n",
       "  (321, 47.00),\n",
       "  (350, 47.00),\n",
       "  (351, 47.00),\n",
       "  (366, 47.00),\n",
       "  (413, 47.00),\n",
       "  (503, 47.00),\n",
       "  (560, 47.00),\n",
       "  (571, 47.00),\n",
       "  (753, 47.00),\n",
       "  (769, 47.00),\n",
       "  (796, 47.00),\n",
       "  (823, 47.00),\n",
       "  (867, 47.00),\n",
       "  (886, 47.00),\n",
       "  (889, 47.00),\n",
       "  (951, 47.00),\n",
       "  (989, 47.00),\n",
       "  (28, 46.00),\n",
       "  (65, 46.00),\n",
       "  (87, 46.00),\n",
       "  (101, 46.00),\n",
       "  (110, 46.00),\n",
       "  (130, 46.00),\n",
       "  (136, 46.00),\n",
       "  (139, 46.00),\n",
       "  (149, 46.00),\n",
       "  (221, 46.00),\n",
       "  (227, 46.00),\n",
       "  (273, 46.00),\n",
       "  (312, 46.00),\n",
       "  (323, 46.00),\n",
       "  (392, 46.00),\n",
       "  (395, 46.00),\n",
       "  (398, 46.00),\n",
       "  (414, 46.00),\n",
       "  (486, 46.00),\n",
       "  (523, 46.00),\n",
       "  (530, 46.00),\n",
       "  (625, 46.00),\n",
       "  (727, 46.00),\n",
       "  (751, 46.00),\n",
       "  (758, 46.00),\n",
       "  (795, 46.00),\n",
       "  (872, 46.00),\n",
       "  (881, 46.00),\n",
       "  (882, 46.00),\n",
       "  (907, 46.00),\n",
       "  (959, 46.00),\n",
       "  (983, 46.00),\n",
       "  (994, 46.00),\n",
       "  (2, 45.00),\n",
       "  (56, 45.00),\n",
       "  (71, 45.00),\n",
       "  (131, 45.00),\n",
       "  (137, 45.00),\n",
       "  (141, 45.00),\n",
       "  (157, 45.00),\n",
       "  (247, 45.00),\n",
       "  (320, 45.00),\n",
       "  (324, 45.00),\n",
       "  (335, 45.00),\n",
       "  (339, 45.00),\n",
       "  (354, 45.00),\n",
       "  (384, 45.00),\n",
       "  (417, 45.00),\n",
       "  (422, 45.00),\n",
       "  (426, 45.00),\n",
       "  (475, 45.00),\n",
       "  (517, 45.00),\n",
       "  (599, 45.00),\n",
       "  (613, 45.00),\n",
       "  (666, 45.00),\n",
       "  (702, 45.00),\n",
       "  (759, 45.00),\n",
       "  (763, 45.00),\n",
       "  (818, 45.00),\n",
       "  (821, 45.00),\n",
       "  (848, 45.00),\n",
       "  (873, 45.00),\n",
       "  (900, 45.00),\n",
       "  (918, 45.00),\n",
       "  (44, 44.00),\n",
       "  (75, 44.00),\n",
       "  (105, 44.00),\n",
       "  (107, 44.00),\n",
       "  (112, 44.00),\n",
       "  (215, 44.00),\n",
       "  (244, 44.00),\n",
       "  (252, 44.00),\n",
       "  (258, 44.00),\n",
       "  (332, 44.00),\n",
       "  (390, 44.00),\n",
       "  (445, 44.00),\n",
       "  (456, 44.00),\n",
       "  (518, 44.00),\n",
       "  (579, 44.00),\n",
       "  (658, 44.00),\n",
       "  (690, 44.00),\n",
       "  (717, 44.00),\n",
       "  (755, 44.00),\n",
       "  (807, 44.00),\n",
       "  (920, 44.00),\n",
       "  (941, 44.00),\n",
       "  (984, 44.00),\n",
       "  (993, 44.00),\n",
       "  (80, 43.00),\n",
       "  (108, 43.00),\n",
       "  (142, 43.00),\n",
       "  (174, 43.00),\n",
       "  (198, 43.00),\n",
       "  (242, 43.00),\n",
       "  (268, 43.00),\n",
       "  (287, 43.00),\n",
       "  (306, 43.00),\n",
       "  (330, 43.00),\n",
       "  (355, 43.00),\n",
       "  (357, 43.00),\n",
       "  (381, 43.00),\n",
       "  (421, 43.00),\n",
       "  (434, 43.00),\n",
       "  (473, 43.00),\n",
       "  (495, 43.00),\n",
       "  (510, 43.00),\n",
       "  (637, 43.00),\n",
       "  (672, 43.00),\n",
       "  (812, 43.00),\n",
       "  (817, 43.00),\n",
       "  (894, 43.00),\n",
       "  (929, 43.00),\n",
       "  (933, 43.00),\n",
       "  (981, 43.00),\n",
       "  (998, 43.00),\n",
       "  (7, 42.00),\n",
       "  (42, 42.00),\n",
       "  (153, 42.00),\n",
       "  (173, 42.00),\n",
       "  (257, 42.00),\n",
       "  (266, 42.00),\n",
       "  (299, 42.00),\n",
       "  (360, 42.00),\n",
       "  (370, 42.00),\n",
       "  (389, 42.00),\n",
       "  (418, 42.00),\n",
       "  (442, 42.00),\n",
       "  (520, 42.00),\n",
       "  (538, 42.00),\n",
       "  (573, 42.00),\n",
       "  (723, 42.00),\n",
       "  (734, 42.00),\n",
       "  (756, 42.00),\n",
       "  (837, 42.00),\n",
       "  (898, 42.00),\n",
       "  (910, 42.00),\n",
       "  (968, 42.00),\n",
       "  (19, 41.00),\n",
       "  (38, 41.00),\n",
       "  (40, 41.00),\n",
       "  (83, 41.00),\n",
       "  (132, 41.00),\n",
       "  (223, 41.00),\n",
       "  (260, 41.00),\n",
       "  (302, 41.00),\n",
       "  (305, 41.00),\n",
       "  (353, 41.00),\n",
       "  (405, 41.00),\n",
       "  (429, 41.00),\n",
       "  (430, 41.00),\n",
       "  (437, 41.00),\n",
       "  (537, 41.00),\n",
       "  (558, 41.00),\n",
       "  (574, 41.00),\n",
       "  (575, 41.00),\n",
       "  (636, 41.00),\n",
       "  (694, 41.00),\n",
       "  (710, 41.00),\n",
       "  (712, 41.00),\n",
       "  (851, 41.00),\n",
       "  (919, 41.00),\n",
       "  (932, 41.00),\n",
       "  (996, 41.00),\n",
       "  (93, 40.00),\n",
       "  (117, 40.00),\n",
       "  (190, 40.00),\n",
       "  (224, 40.00),\n",
       "  (236, 40.00),\n",
       "  (245, 40.00),\n",
       "  (278, 40.00),\n",
       "  (322, 40.00),\n",
       "  (380, 40.00),\n",
       "  (447, 40.00),\n",
       "  (485, 40.00),\n",
       "  (500, 40.00),\n",
       "  (507, 40.00),\n",
       "  (542, 40.00),\n",
       "  (555, 40.00),\n",
       "  (559, 40.00),\n",
       "  (584, 40.00),\n",
       "  (643, 40.00),\n",
       "  (682, 40.00),\n",
       "  (699, 40.00),\n",
       "  (713, 40.00),\n",
       "  (726, 40.00),\n",
       "  (799, 40.00),\n",
       "  (862, 40.00),\n",
       "  (925, 40.00),\n",
       "  (945, 40.00),\n",
       "  (999, 40.00),\n",
       "  (10, 39.00),\n",
       "  (17, 39.00),\n",
       "  (27, 39.00),\n",
       "  (91, 39.00),\n",
       "  (111, 39.00),\n",
       "  (127, 39.00),\n",
       "  (140, 39.00),\n",
       "  (145, 39.00),\n",
       "  (148, 39.00),\n",
       "  (186, 39.00),\n",
       "  (329, 39.00),\n",
       "  (338, 39.00),\n",
       "  (379, 39.00),\n",
       "  (393, 39.00),\n",
       "  (439, 39.00),\n",
       "  (480, 39.00),\n",
       "  (501, 39.00),\n",
       "  (546, 39.00),\n",
       "  (670, 39.00),\n",
       "  (722, 39.00),\n",
       "  (749, 39.00),\n",
       "  (761, 39.00),\n",
       "  (883, 39.00),\n",
       "  (204, 38.00),\n",
       "  (230, 38.00),\n",
       "  (297, 38.00),\n",
       "  (309, 38.00),\n",
       "  (314, 38.00),\n",
       "  (368, 38.00),\n",
       "  (412, 38.00),\n",
       "  (470, 38.00),\n",
       "  (529, 38.00),\n",
       "  (540, 38.00),\n",
       "  (548, 38.00),\n",
       "  (553, 38.00),\n",
       "  (656, 38.00),\n",
       "  (687, 38.00),\n",
       "  (708, 38.00),\n",
       "  (714, 38.00),\n",
       "  (736, 38.00),\n",
       "  (782, 38.00),\n",
       "  (831, 38.00),\n",
       "  (844, 38.00),\n",
       "  (854, 38.00),\n",
       "  (20, 37.00),\n",
       "  (52, 37.00),\n",
       "  (152, 37.00),\n",
       "  (154, 37.00),\n",
       "  (175, 37.00),\n",
       "  (183, 37.00),\n",
       "  (210, 37.00),\n",
       "  (235, 37.00),\n",
       "  (262, 37.00),\n",
       "  (296, 37.00),\n",
       "  (347, 37.00),\n",
       "  (435, 37.00),\n",
       "  (462, 37.00),\n",
       "  (466, 37.00),\n",
       "  (494, 37.00),\n",
       "  (554, 37.00),\n",
       "  (600, 37.00),\n",
       "  (610, 37.00),\n",
       "  (630, 37.00),\n",
       "  (678, 37.00),\n",
       "  (814, 37.00),\n",
       "  (921, 37.00),\n",
       "  (98, 36.00),\n",
       "  (200, 36.00),\n",
       "  (205, 36.00),\n",
       "  (207, 36.00),\n",
       "  (399, 36.00),\n",
       "  (408, 36.00),\n",
       "  (453, 36.00),\n",
       "  (511, 36.00),\n",
       "  (513, 36.00),\n",
       "  (650, 36.00),\n",
       "  (693, 36.00),\n",
       "  (719, 36.00),\n",
       "  (720, 36.00),\n",
       "  (861, 36.00),\n",
       "  (928, 36.00),\n",
       "  (934, 36.00),\n",
       "  (120, 35.00),\n",
       "  (143, 35.00),\n",
       "  (248, 35.00),\n",
       "  (346, 35.00),\n",
       "  (356, 35.00),\n",
       "  (605, 35.00),\n",
       "  (802, 35.00),\n",
       "  (43, 34.00),\n",
       "  (45, 34.00),\n",
       "  (62, 34.00),\n",
       "  (86, 34.00),\n",
       "  (220, 34.00),\n",
       "  (359, 34.00),\n",
       "  (374, 34.00),\n",
       "  (450, 34.00),\n",
       "  (464, 34.00),\n",
       "  (615, 34.00),\n",
       "  (677, 34.00),\n",
       "  (745, 34.00),\n",
       "  (793, 34.00),\n",
       "  (798, 34.00),\n",
       "  (827, 34.00),\n",
       "  (947, 34.00),\n",
       "  (958, 34.00),\n",
       "  (964, 34.00),\n",
       "  (59, 33.00),\n",
       "  (158, 33.00),\n",
       "  (213, 33.00),\n",
       "  (226, 33.00),\n",
       "  (233, 33.00),\n",
       "  (402, 33.00),\n",
       "  (403, 33.00),\n",
       "  (409, 33.00),\n",
       "  (415, 33.00),\n",
       "  (588, 33.00),\n",
       "  (691, 33.00),\n",
       "  (954, 33.00),\n",
       "  (26, 32.00),\n",
       "  (64, 32.00),\n",
       "  (73, 32.00),\n",
       "  (106, 32.00),\n",
       "  (385, 32.00),\n",
       "  (427, 32.00),\n",
       "  (465, 32.00),\n",
       "  (543, 32.00),\n",
       "  (627, 32.00),\n",
       "  (760, 32.00),\n",
       "  (786, 32.00),\n",
       "  (859, 32.00),\n",
       "  (897, 32.00),\n",
       "  (948, 32.00),\n",
       "  (980, 32.00),\n",
       "  (122, 31.00),\n",
       "  (371, 31.00),\n",
       "  (461, 31.00),\n",
       "  (469, 31.00),\n",
       "  (484, 31.00),\n",
       "  (502, 31.00),\n",
       "  (521, 31.00),\n",
       "  (531, 31.00),\n",
       "  (551, 31.00),\n",
       "  (568, 31.00),\n",
       "  (578, 31.00),\n",
       "  (644, 31.00),\n",
       "  (647, 31.00),\n",
       "  (705, 31.00),\n",
       "  (744, 31.00),\n",
       "  (789, 31.00),\n",
       "  (846, 31.00),\n",
       "  (914, 31.00),\n",
       "  (81, 30.00),\n",
       "  (146, 30.00),\n",
       "  (166, 30.00),\n",
       "  (179, 30.00),\n",
       "  (345, 30.00),\n",
       "  (459, 30.00),\n",
       "  (557, 30.00),\n",
       "  (589, 30.00),\n",
       "  (617, 30.00),\n",
       "  (629, 30.00),\n",
       "  (632, 30.00),\n",
       "  (803, 30.00),\n",
       "  (966, 30.00),\n",
       "  (303, 29.00),\n",
       "  (498, 29.00),\n",
       "  (585, 29.00),\n",
       "  (634, 29.00),\n",
       "  (4, 28.00),\n",
       "  (29, 28.00),\n",
       "  (54, 28.00),\n",
       "  (567, 28.00),\n",
       "  (587, 28.00),\n",
       "  (631, 28.00),\n",
       "  (663, 28.00),\n",
       "  (718, 28.00),\n",
       "  (731, 28.00),\n",
       "  (767, 28.00),\n",
       "  (860, 28.00),\n",
       "  (876, 28.00),\n",
       "  (909, 28.00),\n",
       "  (923, 28.00),\n",
       "  (147, 27.00),\n",
       "  (187, 27.00),\n",
       "  (202, 27.00),\n",
       "  (479, 27.00),\n",
       "  (482, 27.00),\n",
       "  (660, 27.00),\n",
       "  (733, 27.00),\n",
       "  (965, 27.00),\n",
       "  (49, 26.00),\n",
       "  (150, 26.00),\n",
       "  (271, 26.00),\n",
       "  (624, 26.00),\n",
       "  (676, 26.00),\n",
       "  (686, 26.00),\n",
       "  (785, 26.00),\n",
       "  (869, 26.00),\n",
       "  (931, 26.00),\n",
       "  (32, 25.00),\n",
       "  (163, 25.00),\n",
       "  (240, 25.00),\n",
       "  (369, 25.00),\n",
       "  (544, 25.00),\n",
       "  (804, 25.00),\n",
       "  (885, 25.00),\n",
       "  (516, 24.00),\n",
       "  (536, 24.00),\n",
       "  (590, 24.00),\n",
       "  (596, 24.00),\n",
       "  (747, 24.00),\n",
       "  (856, 24.00),\n",
       "  (901, 24.00),\n",
       "  (168, 23.00),\n",
       "  (185, 23.00),\n",
       "  (315, 23.00),\n",
       "  (394, 23.00),\n",
       "  (438, 23.00),\n",
       "  (623, 23.00),\n",
       "  (680, 23.00),\n",
       "  (811, 23.00),\n",
       "  (838, 23.00),\n",
       "  (908, 23.00),\n",
       "  (911, 23.00),\n",
       "  (974, 23.00),\n",
       "  (165, 22.00),\n",
       "  (550, 22.00),\n",
       "  (638, 22.00),\n",
       "  (675, 22.00),\n",
       "  (942, 22.00),\n",
       "  (499, 21.00),\n",
       "  (525, 21.00),\n",
       "  (773, 21.00),\n",
       "  (68, 20.00),\n",
       "  (662, 20.00),\n",
       "  (715, 20.00),\n",
       "  (729, 20.00),\n",
       "  (740, 20.00),\n",
       "  (246, 19.00),\n",
       "  (400, 19.00),\n",
       "  (598, 19.00),\n",
       "  (622, 19.00),\n",
       "  (648, 18.00),\n",
       "  (651, 18.00),\n",
       "  (728, 18.00),\n",
       "  (836, 18.00),\n",
       "  (977, 18.00),\n",
       "  (504, 17.00),\n",
       "  (813, 17.00),\n",
       "  (906, 17.00),\n",
       "  (976, 17.00),\n",
       "  (460, 16.00),\n",
       "  (534, 16.00),\n",
       "  (841, 16.00),\n",
       "  (930, 16.00),\n",
       "  (970, 15.00),\n",
       "  (103, 14.00),\n",
       "  (282, 14.00),\n",
       "  (742, 14.00),\n",
       "  (969, 14.00),\n",
       "  (34, 13.00),\n",
       "  (446, 13.00),\n",
       "  (493, 13.00),\n",
       "  (967, 12.00),\n",
       "  (167, 11.00),\n",
       "  (940, 11.00),\n",
       "  (978, 11.00),\n",
       "  (689, 10.00),\n",
       "  (899, 9.00),\n",
       "  (960, 9.00),\n",
       "  (673, 8.00),\n",
       "  (681, 8.00),\n",
       "  (810, 8.00),\n",
       "  (935, 8.00),\n",
       "  (961, 2.00)])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = targeted_diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch_no 0\n",
      "at batch_no 100\n",
      "at batch_no 200\n",
      "at batch_no 300\n",
      "at batch_no 400\n",
      "at batch_no 500\n",
      "at batch_no 600\n",
      "at batch_no 700\n",
      "at batch_no 800\n",
      "at batch_no 900\n",
      "at batch_no 1000\n",
      "at batch_no 1100\n",
      "at batch_no 1200\n",
      "at batch_no 1300\n",
      "at batch_no 1400\n",
      "at batch_no 1500\n",
      "at batch_no 1600\n",
      "at batch_no 1700\n",
      "at batch_no 1800\n",
      "at batch_no 1900\n",
      "at batch_no 2000\n",
      "at batch_no 2100\n",
      "at batch_no 2200\n",
      "at batch_no 2300\n",
      "at batch_no 2400\n",
      "at batch_no 2500\n",
      "at batch_no 2600\n",
      "at batch_no 2700\n",
      "at batch_no 2800\n",
      "at batch_no 2900\n",
      "at batch_no 3000\n",
      "at batch_no 3100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(885,\n",
       " [(815, 227.00),\n",
       "  (652, 183.00),\n",
       "  (646, 172.00),\n",
       "  (580, 155.00),\n",
       "  (591, 149.00),\n",
       "  (621, 148.00),\n",
       "  (737, 148.00),\n",
       "  (611, 147.00),\n",
       "  (489, 136.00),\n",
       "  (904, 129.00),\n",
       "  (497, 125.00),\n",
       "  (868, 125.00),\n",
       "  (979, 124.00),\n",
       "  (582, 122.00),\n",
       "  (794, 122.00),\n",
       "  (94, 118.00),\n",
       "  (955, 118.00),\n",
       "  (893, 117.00),\n",
       "  (721, 116.00),\n",
       "  (116, 114.00),\n",
       "  (491, 108.00),\n",
       "  (565, 108.00),\n",
       "  (741, 107.00),\n",
       "  (562, 106.00),\n",
       "  (679, 106.00),\n",
       "  (39, 105.00),\n",
       "  (839, 104.00),\n",
       "  (109, 102.00),\n",
       "  (162, 101.00),\n",
       "  (750, 100.00),\n",
       "  (46, 98.00),\n",
       "  (695, 98.00),\n",
       "  (549, 97.00),\n",
       "  (82, 95.00),\n",
       "  (199, 95.00),\n",
       "  (492, 95.00),\n",
       "  (84, 94.00),\n",
       "  (973, 94.00),\n",
       "  (51, 93.00),\n",
       "  (151, 91.00),\n",
       "  (48, 90.00),\n",
       "  (424, 89.00),\n",
       "  (843, 89.00),\n",
       "  (203, 87.00),\n",
       "  (692, 87.00),\n",
       "  (971, 87.00),\n",
       "  (640, 86.00),\n",
       "  (669, 86.00),\n",
       "  (197, 84.00),\n",
       "  (440, 84.00),\n",
       "  (879, 84.00),\n",
       "  (47, 83.00),\n",
       "  (180, 83.00),\n",
       "  (577, 83.00),\n",
       "  (208, 81.00),\n",
       "  (743, 81.00),\n",
       "  (783, 81.00),\n",
       "  (847, 81.00),\n",
       "  (189, 80.00),\n",
       "  (281, 80.00),\n",
       "  (382, 80.00),\n",
       "  (406, 80.00),\n",
       "  (703, 80.00),\n",
       "  (824, 80.00),\n",
       "  (905, 80.00),\n",
       "  (61, 79.00),\n",
       "  (318, 79.00),\n",
       "  (319, 79.00),\n",
       "  (411, 79.00),\n",
       "  (762, 79.00),\n",
       "  (866, 79.00),\n",
       "  (219, 78.00),\n",
       "  (310, 78.00),\n",
       "  (938, 78.00),\n",
       "  (76, 77.00),\n",
       "  (270, 77.00),\n",
       "  (364, 77.00),\n",
       "  (849, 77.00),\n",
       "  (182, 76.00),\n",
       "  (298, 76.00),\n",
       "  (791, 76.00),\n",
       "  (828, 76.00),\n",
       "  (896, 76.00),\n",
       "  (956, 76.00),\n",
       "  (963, 76.00),\n",
       "  (55, 75.00),\n",
       "  (217, 75.00),\n",
       "  (342, 75.00),\n",
       "  (343, 75.00),\n",
       "  (730, 75.00),\n",
       "  (778, 75.00),\n",
       "  (830, 75.00),\n",
       "  (455, 74.00),\n",
       "  (483, 74.00),\n",
       "  (800, 74.00),\n",
       "  (982, 74.00),\n",
       "  (304, 73.00),\n",
       "  (363, 73.00),\n",
       "  (436, 73.00),\n",
       "  (471, 73.00),\n",
       "  (472, 73.00),\n",
       "  (645, 73.00),\n",
       "  (805, 73.00),\n",
       "  (128, 72.00),\n",
       "  (725, 72.00),\n",
       "  (754, 72.00),\n",
       "  (878, 71.00),\n",
       "  (192, 70.00),\n",
       "  (341, 70.00),\n",
       "  (454, 70.00),\n",
       "  (556, 70.00),\n",
       "  (572, 70.00),\n",
       "  (735, 70.00),\n",
       "  (806, 70.00),\n",
       "  (825, 70.00),\n",
       "  (826, 70.00),\n",
       "  (178, 69.00),\n",
       "  (195, 69.00),\n",
       "  (570, 69.00),\n",
       "  (671, 69.00),\n",
       "  (716, 69.00),\n",
       "  (23, 68.00),\n",
       "  (50, 68.00),\n",
       "  (654, 68.00),\n",
       "  (700, 68.00),\n",
       "  (775, 68.00),\n",
       "  (30, 67.00),\n",
       "  (77, 67.00),\n",
       "  (425, 67.00),\n",
       "  (496, 67.00),\n",
       "  (845, 67.00),\n",
       "  (916, 67.00),\n",
       "  (949, 67.00),\n",
       "  (37, 66.00),\n",
       "  (124, 66.00),\n",
       "  (420, 66.00),\n",
       "  (586, 66.00),\n",
       "  (620, 66.00),\n",
       "  (784, 66.00),\n",
       "  (895, 66.00),\n",
       "  (926, 66.00),\n",
       "  (155, 65.00),\n",
       "  (234, 65.00),\n",
       "  (238, 65.00),\n",
       "  (293, 65.00),\n",
       "  (313, 65.00),\n",
       "  (649, 65.00),\n",
       "  (688, 65.00),\n",
       "  (820, 65.00),\n",
       "  (863, 65.00),\n",
       "  (864, 65.00),\n",
       "  (870, 65.00),\n",
       "  (871, 65.00),\n",
       "  (892, 65.00),\n",
       "  (988, 65.00),\n",
       "  (6, 64.00),\n",
       "  (74, 64.00),\n",
       "  (222, 64.00),\n",
       "  (237, 64.00),\n",
       "  (361, 64.00),\n",
       "  (463, 64.00),\n",
       "  (468, 64.00),\n",
       "  (711, 64.00),\n",
       "  (772, 64.00),\n",
       "  (887, 64.00),\n",
       "  (944, 64.00),\n",
       "  (97, 63.00),\n",
       "  (119, 63.00),\n",
       "  (331, 63.00),\n",
       "  (474, 63.00),\n",
       "  (819, 63.00),\n",
       "  (913, 63.00),\n",
       "  (972, 63.00),\n",
       "  (8, 62.00),\n",
       "  (90, 62.00),\n",
       "  (99, 62.00),\n",
       "  (125, 62.00),\n",
       "  (161, 62.00),\n",
       "  (184, 62.00),\n",
       "  (272, 62.00),\n",
       "  (300, 62.00),\n",
       "  (348, 62.00),\n",
       "  (506, 62.00),\n",
       "  (515, 62.00),\n",
       "  (593, 62.00),\n",
       "  (788, 62.00),\n",
       "  (808, 62.00),\n",
       "  (842, 62.00),\n",
       "  (875, 62.00),\n",
       "  (880, 62.00),\n",
       "  (992, 62.00),\n",
       "  (135, 61.00),\n",
       "  (225, 61.00),\n",
       "  (280, 61.00),\n",
       "  (292, 61.00),\n",
       "  (311, 61.00),\n",
       "  (316, 61.00),\n",
       "  (476, 61.00),\n",
       "  (505, 61.00),\n",
       "  (561, 61.00),\n",
       "  (609, 61.00),\n",
       "  (698, 61.00),\n",
       "  (809, 61.00),\n",
       "  (850, 61.00),\n",
       "  (937, 61.00),\n",
       "  (946, 61.00),\n",
       "  (953, 61.00),\n",
       "  (181, 60.00),\n",
       "  (372, 60.00),\n",
       "  (401, 60.00),\n",
       "  (532, 60.00),\n",
       "  (539, 60.00),\n",
       "  (619, 60.00),\n",
       "  (697, 60.00),\n",
       "  (781, 60.00),\n",
       "  (884, 60.00),\n",
       "  (113, 59.00),\n",
       "  (327, 59.00),\n",
       "  (391, 59.00),\n",
       "  (404, 59.00),\n",
       "  (481, 59.00),\n",
       "  (490, 59.00),\n",
       "  (522, 59.00),\n",
       "  (581, 59.00),\n",
       "  (607, 59.00),\n",
       "  (655, 59.00),\n",
       "  (724, 59.00),\n",
       "  (774, 59.00),\n",
       "  (912, 59.00),\n",
       "  (952, 59.00),\n",
       "  (987, 59.00),\n",
       "  (60, 58.00),\n",
       "  (211, 58.00),\n",
       "  (249, 58.00),\n",
       "  (284, 58.00),\n",
       "  (317, 58.00),\n",
       "  (407, 58.00),\n",
       "  (410, 58.00),\n",
       "  (423, 58.00),\n",
       "  (458, 58.00),\n",
       "  (563, 58.00),\n",
       "  (594, 58.00),\n",
       "  (595, 58.00),\n",
       "  (597, 58.00),\n",
       "  (603, 58.00),\n",
       "  (641, 58.00),\n",
       "  (696, 58.00),\n",
       "  (709, 58.00),\n",
       "  (790, 58.00),\n",
       "  (857, 58.00),\n",
       "  (21, 57.00),\n",
       "  (307, 57.00),\n",
       "  (419, 57.00),\n",
       "  (441, 57.00),\n",
       "  (457, 57.00),\n",
       "  (612, 57.00),\n",
       "  (618, 57.00),\n",
       "  (636, 57.00),\n",
       "  (801, 57.00),\n",
       "  (69, 56.00),\n",
       "  (118, 56.00),\n",
       "  (231, 56.00),\n",
       "  (232, 56.00),\n",
       "  (263, 56.00),\n",
       "  (275, 56.00),\n",
       "  (308, 56.00),\n",
       "  (334, 56.00),\n",
       "  (367, 56.00),\n",
       "  (452, 56.00),\n",
       "  (477, 56.00),\n",
       "  (527, 56.00),\n",
       "  (547, 56.00),\n",
       "  (564, 56.00),\n",
       "  (635, 56.00),\n",
       "  (642, 56.00),\n",
       "  (653, 56.00),\n",
       "  (657, 56.00),\n",
       "  (661, 56.00),\n",
       "  (780, 56.00),\n",
       "  (816, 56.00),\n",
       "  (822, 56.00),\n",
       "  (957, 56.00),\n",
       "  (962, 56.00),\n",
       "  (975, 56.00),\n",
       "  (25, 55.00),\n",
       "  (31, 55.00),\n",
       "  (33, 55.00),\n",
       "  (57, 55.00),\n",
       "  (88, 55.00),\n",
       "  (104, 55.00),\n",
       "  (115, 55.00),\n",
       "  (129, 55.00),\n",
       "  (228, 55.00),\n",
       "  (328, 55.00),\n",
       "  (383, 55.00),\n",
       "  (396, 55.00),\n",
       "  (428, 55.00),\n",
       "  (509, 55.00),\n",
       "  (614, 55.00),\n",
       "  (738, 55.00),\n",
       "  (748, 55.00),\n",
       "  (777, 55.00),\n",
       "  (834, 55.00),\n",
       "  (852, 55.00),\n",
       "  (877, 55.00),\n",
       "  (985, 55.00),\n",
       "  (24, 54.00),\n",
       "  (58, 54.00),\n",
       "  (67, 54.00),\n",
       "  (70, 54.00),\n",
       "  (85, 54.00),\n",
       "  (170, 54.00),\n",
       "  (250, 54.00),\n",
       "  (259, 54.00),\n",
       "  (274, 54.00),\n",
       "  (444, 54.00),\n",
       "  (448, 54.00),\n",
       "  (451, 54.00),\n",
       "  (519, 54.00),\n",
       "  (524, 54.00),\n",
       "  (552, 54.00),\n",
       "  (569, 54.00),\n",
       "  (601, 54.00),\n",
       "  (766, 54.00),\n",
       "  (770, 54.00),\n",
       "  (818, 54.00),\n",
       "  (829, 54.00),\n",
       "  (855, 54.00),\n",
       "  (858, 54.00),\n",
       "  (950, 54.00),\n",
       "  (991, 54.00),\n",
       "  (995, 54.00),\n",
       "  (89, 53.00),\n",
       "  (171, 53.00),\n",
       "  (196, 53.00),\n",
       "  (216, 53.00),\n",
       "  (218, 53.00),\n",
       "  (256, 53.00),\n",
       "  (269, 53.00),\n",
       "  (276, 53.00),\n",
       "  (283, 53.00),\n",
       "  (289, 53.00),\n",
       "  (352, 53.00),\n",
       "  (375, 53.00),\n",
       "  (608, 53.00),\n",
       "  (626, 53.00),\n",
       "  (701, 53.00),\n",
       "  (768, 53.00),\n",
       "  (832, 53.00),\n",
       "  (903, 53.00),\n",
       "  (924, 53.00),\n",
       "  (936, 53.00),\n",
       "  (986, 53.00),\n",
       "  (0, 52.00),\n",
       "  (3, 52.00),\n",
       "  (36, 52.00),\n",
       "  (92, 52.00),\n",
       "  (96, 52.00),\n",
       "  (164, 52.00),\n",
       "  (172, 52.00),\n",
       "  (209, 52.00),\n",
       "  (241, 52.00),\n",
       "  (251, 52.00),\n",
       "  (254, 52.00),\n",
       "  (277, 52.00),\n",
       "  (291, 52.00),\n",
       "  (467, 52.00),\n",
       "  (528, 52.00),\n",
       "  (685, 52.00),\n",
       "  (757, 52.00),\n",
       "  (765, 52.00),\n",
       "  (792, 52.00),\n",
       "  (840, 52.00),\n",
       "  (865, 52.00),\n",
       "  (888, 52.00),\n",
       "  (922, 52.00),\n",
       "  (939, 52.00),\n",
       "  (990, 52.00),\n",
       "  (997, 52.00),\n",
       "  (15, 51.00),\n",
       "  (78, 51.00),\n",
       "  (114, 51.00),\n",
       "  (206, 51.00),\n",
       "  (229, 51.00),\n",
       "  (239, 51.00),\n",
       "  (285, 51.00),\n",
       "  (286, 51.00),\n",
       "  (336, 51.00),\n",
       "  (358, 51.00),\n",
       "  (378, 51.00),\n",
       "  (386, 51.00),\n",
       "  (395, 51.00),\n",
       "  (413, 51.00),\n",
       "  (431, 51.00),\n",
       "  (487, 51.00),\n",
       "  (488, 51.00),\n",
       "  (512, 51.00),\n",
       "  (526, 51.00),\n",
       "  (533, 51.00),\n",
       "  (535, 51.00),\n",
       "  (592, 51.00),\n",
       "  (667, 51.00),\n",
       "  (752, 51.00),\n",
       "  (753, 51.00),\n",
       "  (835, 51.00),\n",
       "  (886, 51.00),\n",
       "  (902, 51.00),\n",
       "  (1, 50.00),\n",
       "  (41, 50.00),\n",
       "  (126, 50.00),\n",
       "  (138, 50.00),\n",
       "  (176, 50.00),\n",
       "  (177, 50.00),\n",
       "  (194, 50.00),\n",
       "  (253, 50.00),\n",
       "  (261, 50.00),\n",
       "  (267, 50.00),\n",
       "  (294, 50.00),\n",
       "  (362, 50.00),\n",
       "  (373, 50.00),\n",
       "  (443, 50.00),\n",
       "  (449, 50.00),\n",
       "  (508, 50.00),\n",
       "  (514, 50.00),\n",
       "  (523, 50.00),\n",
       "  (545, 50.00),\n",
       "  (602, 50.00),\n",
       "  (606, 50.00),\n",
       "  (616, 50.00),\n",
       "  (665, 50.00),\n",
       "  (746, 50.00),\n",
       "  (771, 50.00),\n",
       "  (779, 50.00),\n",
       "  (797, 50.00),\n",
       "  (874, 50.00),\n",
       "  (917, 50.00),\n",
       "  (9, 49.00),\n",
       "  (12, 49.00),\n",
       "  (13, 49.00),\n",
       "  (35, 49.00),\n",
       "  (71, 49.00),\n",
       "  (102, 49.00),\n",
       "  (121, 49.00),\n",
       "  (159, 49.00),\n",
       "  (160, 49.00),\n",
       "  (214, 49.00),\n",
       "  (266, 49.00),\n",
       "  (301, 49.00),\n",
       "  (340, 49.00),\n",
       "  (387, 49.00),\n",
       "  (388, 49.00),\n",
       "  (541, 49.00),\n",
       "  (604, 49.00),\n",
       "  (639, 49.00),\n",
       "  (664, 49.00),\n",
       "  (739, 49.00),\n",
       "  (764, 49.00),\n",
       "  (776, 49.00),\n",
       "  (833, 49.00),\n",
       "  (890, 49.00),\n",
       "  (915, 49.00),\n",
       "  (927, 49.00),\n",
       "  (11, 48.00),\n",
       "  (14, 48.00),\n",
       "  (18, 48.00),\n",
       "  (72, 48.00),\n",
       "  (95, 48.00),\n",
       "  (100, 48.00),\n",
       "  (133, 48.00),\n",
       "  (134, 48.00),\n",
       "  (149, 48.00),\n",
       "  (156, 48.00),\n",
       "  (169, 48.00),\n",
       "  (221, 48.00),\n",
       "  (247, 48.00),\n",
       "  (255, 48.00),\n",
       "  (295, 48.00),\n",
       "  (320, 48.00),\n",
       "  (321, 48.00),\n",
       "  (325, 48.00),\n",
       "  (337, 48.00),\n",
       "  (376, 48.00),\n",
       "  (398, 48.00),\n",
       "  (432, 48.00),\n",
       "  (433, 48.00),\n",
       "  (478, 48.00),\n",
       "  (503, 48.00),\n",
       "  (517, 48.00),\n",
       "  (583, 48.00),\n",
       "  (628, 48.00),\n",
       "  (668, 48.00),\n",
       "  (683, 48.00),\n",
       "  (704, 48.00),\n",
       "  (727, 48.00),\n",
       "  (787, 48.00),\n",
       "  (848, 48.00),\n",
       "  (881, 48.00),\n",
       "  (943, 48.00),\n",
       "  (951, 48.00),\n",
       "  (989, 48.00),\n",
       "  (994, 48.00),\n",
       "  (22, 47.00),\n",
       "  (56, 47.00),\n",
       "  (79, 47.00),\n",
       "  (87, 47.00),\n",
       "  (123, 47.00),\n",
       "  (137, 47.00),\n",
       "  (139, 47.00),\n",
       "  (141, 47.00),\n",
       "  (193, 47.00),\n",
       "  (273, 47.00),\n",
       "  (288, 47.00),\n",
       "  (290, 47.00),\n",
       "  (326, 47.00),\n",
       "  (333, 47.00),\n",
       "  (335, 47.00),\n",
       "  (344, 47.00),\n",
       "  (351, 47.00),\n",
       "  (354, 47.00),\n",
       "  (366, 47.00),\n",
       "  (416, 47.00),\n",
       "  (422, 47.00),\n",
       "  (445, 47.00),\n",
       "  (518, 47.00),\n",
       "  (571, 47.00),\n",
       "  (576, 47.00),\n",
       "  (625, 47.00),\n",
       "  (637, 47.00),\n",
       "  (666, 47.00),\n",
       "  (684, 47.00),\n",
       "  (707, 47.00),\n",
       "  (751, 47.00),\n",
       "  (763, 47.00),\n",
       "  (769, 47.00),\n",
       "  (807, 47.00),\n",
       "  (889, 47.00),\n",
       "  (891, 47.00),\n",
       "  (918, 47.00),\n",
       "  (983, 47.00),\n",
       "  (5, 46.00),\n",
       "  (28, 46.00),\n",
       "  (53, 46.00),\n",
       "  (66, 46.00),\n",
       "  (130, 46.00),\n",
       "  (136, 46.00),\n",
       "  (144, 46.00),\n",
       "  (157, 46.00),\n",
       "  (191, 46.00),\n",
       "  (201, 46.00),\n",
       "  (212, 46.00),\n",
       "  (243, 46.00),\n",
       "  (257, 46.00),\n",
       "  (264, 46.00),\n",
       "  (279, 46.00),\n",
       "  (312, 46.00),\n",
       "  (323, 46.00),\n",
       "  (350, 46.00),\n",
       "  (377, 46.00),\n",
       "  (384, 46.00),\n",
       "  (392, 46.00),\n",
       "  (560, 46.00),\n",
       "  (566, 46.00),\n",
       "  (579, 46.00),\n",
       "  (659, 46.00),\n",
       "  (702, 46.00),\n",
       "  (706, 46.00),\n",
       "  (732, 46.00),\n",
       "  (823, 46.00),\n",
       "  (872, 46.00),\n",
       "  (993, 46.00),\n",
       "  (16, 45.00),\n",
       "  (40, 45.00),\n",
       "  (75, 45.00),\n",
       "  (105, 45.00),\n",
       "  (110, 45.00),\n",
       "  (131, 45.00),\n",
       "  (188, 45.00),\n",
       "  (215, 45.00),\n",
       "  (252, 45.00),\n",
       "  (324, 45.00),\n",
       "  (339, 45.00),\n",
       "  (355, 45.00),\n",
       "  (365, 45.00),\n",
       "  (381, 45.00),\n",
       "  (389, 45.00),\n",
       "  (397, 45.00),\n",
       "  (426, 45.00),\n",
       "  (475, 45.00),\n",
       "  (537, 45.00),\n",
       "  (613, 45.00),\n",
       "  (674, 45.00),\n",
       "  (758, 45.00),\n",
       "  (759, 45.00),\n",
       "  (796, 45.00),\n",
       "  (853, 45.00),\n",
       "  (867, 45.00),\n",
       "  (882, 45.00),\n",
       "  (900, 45.00),\n",
       "  (968, 45.00),\n",
       "  (981, 45.00),\n",
       "  (2, 44.00),\n",
       "  (17, 44.00),\n",
       "  (42, 44.00),\n",
       "  (44, 44.00),\n",
       "  (101, 44.00),\n",
       "  (107, 44.00),\n",
       "  (132, 44.00),\n",
       "  (227, 44.00),\n",
       "  (245, 44.00),\n",
       "  (287, 44.00),\n",
       "  (370, 44.00),\n",
       "  (412, 44.00),\n",
       "  (414, 44.00),\n",
       "  (447, 44.00),\n",
       "  (817, 44.00),\n",
       "  (821, 44.00),\n",
       "  (929, 44.00),\n",
       "  (941, 44.00),\n",
       "  (959, 44.00),\n",
       "  (63, 43.00),\n",
       "  (65, 43.00),\n",
       "  (83, 43.00),\n",
       "  (93, 43.00),\n",
       "  (174, 43.00),\n",
       "  (244, 43.00),\n",
       "  (299, 43.00),\n",
       "  (330, 43.00),\n",
       "  (332, 43.00),\n",
       "  (349, 43.00),\n",
       "  (437, 43.00),\n",
       "  (456, 43.00),\n",
       "  (485, 43.00),\n",
       "  (520, 43.00),\n",
       "  (574, 43.00),\n",
       "  (575, 43.00),\n",
       "  (599, 43.00),\n",
       "  (658, 43.00),\n",
       "  (682, 43.00),\n",
       "  (710, 43.00),\n",
       "  (717, 43.00),\n",
       "  (723, 43.00),\n",
       "  (755, 43.00),\n",
       "  (756, 43.00),\n",
       "  (761, 43.00),\n",
       "  (782, 43.00),\n",
       "  (795, 43.00),\n",
       "  (873, 43.00),\n",
       "  (898, 43.00),\n",
       "  (907, 43.00),\n",
       "  (910, 43.00),\n",
       "  (919, 43.00),\n",
       "  (984, 43.00),\n",
       "  (999, 43.00),\n",
       "  (108, 42.00),\n",
       "  (112, 42.00),\n",
       "  (153, 42.00),\n",
       "  (223, 42.00),\n",
       "  (260, 42.00),\n",
       "  (265, 42.00),\n",
       "  (268, 42.00),\n",
       "  (305, 42.00),\n",
       "  (329, 42.00),\n",
       "  (353, 42.00),\n",
       "  (429, 42.00),\n",
       "  (430, 42.00),\n",
       "  (434, 42.00),\n",
       "  (473, 42.00),\n",
       "  (495, 42.00),\n",
       "  (510, 42.00),\n",
       "  (558, 42.00),\n",
       "  (633, 42.00),\n",
       "  (643, 42.00),\n",
       "  (672, 42.00),\n",
       "  (690, 42.00),\n",
       "  (996, 42.00),\n",
       "  (7, 41.00),\n",
       "  (19, 41.00),\n",
       "  (80, 41.00),\n",
       "  (142, 41.00),\n",
       "  (154, 41.00),\n",
       "  (173, 41.00),\n",
       "  (190, 41.00),\n",
       "  (258, 41.00),\n",
       "  (302, 41.00),\n",
       "  (306, 41.00),\n",
       "  (322, 41.00),\n",
       "  (360, 41.00),\n",
       "  (390, 41.00),\n",
       "  (393, 41.00),\n",
       "  (421, 41.00),\n",
       "  (462, 41.00),\n",
       "  (486, 41.00),\n",
       "  (501, 41.00),\n",
       "  (507, 41.00),\n",
       "  (530, 41.00),\n",
       "  (573, 41.00),\n",
       "  (712, 41.00),\n",
       "  (714, 41.00),\n",
       "  (734, 41.00),\n",
       "  (837, 41.00),\n",
       "  (844, 41.00),\n",
       "  (862, 41.00),\n",
       "  (933, 41.00),\n",
       "  (998, 41.00),\n",
       "  (27, 40.00),\n",
       "  (117, 40.00),\n",
       "  (127, 40.00),\n",
       "  (186, 40.00),\n",
       "  (210, 40.00),\n",
       "  (297, 40.00),\n",
       "  (368, 40.00),\n",
       "  (380, 40.00),\n",
       "  (417, 40.00),\n",
       "  (442, 40.00),\n",
       "  (466, 40.00),\n",
       "  (538, 40.00),\n",
       "  (630, 40.00),\n",
       "  (670, 40.00),\n",
       "  (713, 40.00),\n",
       "  (722, 40.00),\n",
       "  (749, 40.00),\n",
       "  (799, 40.00),\n",
       "  (812, 40.00),\n",
       "  (851, 40.00),\n",
       "  (920, 40.00),\n",
       "  (925, 40.00),\n",
       "  (52, 39.00),\n",
       "  (91, 39.00),\n",
       "  (140, 39.00),\n",
       "  (183, 39.00),\n",
       "  (224, 39.00),\n",
       "  (236, 39.00),\n",
       "  (242, 39.00),\n",
       "  (248, 39.00),\n",
       "  (309, 39.00),\n",
       "  (346, 39.00),\n",
       "  (357, 39.00),\n",
       "  (405, 39.00),\n",
       "  (500, 39.00),\n",
       "  (548, 39.00),\n",
       "  (559, 39.00),\n",
       "  (694, 39.00),\n",
       "  (699, 39.00),\n",
       "  (708, 39.00),\n",
       "  (726, 39.00),\n",
       "  (894, 39.00),\n",
       "  (932, 39.00),\n",
       "  (10, 38.00),\n",
       "  (43, 38.00),\n",
       "  (262, 38.00),\n",
       "  (347, 38.00),\n",
       "  (427, 38.00),\n",
       "  (464, 38.00),\n",
       "  (480, 38.00),\n",
       "  (540, 38.00),\n",
       "  (542, 38.00),\n",
       "  (555, 38.00),\n",
       "  (584, 38.00),\n",
       "  (656, 38.00),\n",
       "  (719, 38.00),\n",
       "  (736, 38.00),\n",
       "  (814, 38.00),\n",
       "  (859, 38.00),\n",
       "  (921, 38.00),\n",
       "  (928, 38.00),\n",
       "  (38, 37.00),\n",
       "  (98, 37.00),\n",
       "  (111, 37.00),\n",
       "  (198, 37.00),\n",
       "  (200, 37.00),\n",
       "  (207, 37.00),\n",
       "  (230, 37.00),\n",
       "  (235, 37.00),\n",
       "  (278, 37.00),\n",
       "  (296, 37.00),\n",
       "  (338, 37.00),\n",
       "  (374, 37.00),\n",
       "  (379, 37.00),\n",
       "  (418, 37.00),\n",
       "  (439, 37.00),\n",
       "  (513, 37.00),\n",
       "  (529, 37.00),\n",
       "  (553, 37.00),\n",
       "  (554, 37.00),\n",
       "  (786, 37.00),\n",
       "  (854, 37.00),\n",
       "  (945, 37.00),\n",
       "  (45, 36.00),\n",
       "  (62, 36.00),\n",
       "  (143, 36.00),\n",
       "  (145, 36.00),\n",
       "  (148, 36.00),\n",
       "  (152, 36.00),\n",
       "  (175, 36.00),\n",
       "  (204, 36.00),\n",
       "  (205, 36.00),\n",
       "  (314, 36.00),\n",
       "  (385, 36.00),\n",
       "  (399, 36.00),\n",
       "  (408, 36.00),\n",
       "  (435, 36.00),\n",
       "  (453, 36.00),\n",
       "  (470, 36.00),\n",
       "  (546, 36.00),\n",
       "  (588, 36.00),\n",
       "  (605, 36.00),\n",
       "  (610, 36.00),\n",
       "  (615, 36.00),\n",
       "  (678, 36.00),\n",
       "  (687, 36.00),\n",
       "  (720, 36.00),\n",
       "  (793, 36.00),\n",
       "  (802, 36.00),\n",
       "  (934, 36.00),\n",
       "  (20, 35.00),\n",
       "  (120, 35.00),\n",
       "  (213, 35.00),\n",
       "  (226, 35.00),\n",
       "  (359, 35.00),\n",
       "  (600, 35.00),\n",
       "  (632, 35.00),\n",
       "  (827, 35.00),\n",
       "  (831, 35.00),\n",
       "  (846, 35.00),\n",
       "  (883, 35.00),\n",
       "  (958, 35.00),\n",
       "  (26, 34.00),\n",
       "  (73, 34.00),\n",
       "  (220, 34.00),\n",
       "  (356, 34.00),\n",
       "  (409, 34.00),\n",
       "  (415, 34.00),\n",
       "  (450, 34.00),\n",
       "  (494, 34.00),\n",
       "  (511, 34.00),\n",
       "  (543, 34.00),\n",
       "  (585, 34.00),\n",
       "  (627, 34.00),\n",
       "  (677, 34.00),\n",
       "  (693, 34.00),\n",
       "  (861, 34.00),\n",
       "  (897, 34.00),\n",
       "  (923, 34.00),\n",
       "  (947, 34.00),\n",
       "  (86, 33.00),\n",
       "  (403, 33.00),\n",
       "  (465, 33.00),\n",
       "  (650, 33.00),\n",
       "  (798, 33.00),\n",
       "  (964, 33.00),\n",
       "  (980, 33.00),\n",
       "  (158, 32.00),\n",
       "  (371, 32.00),\n",
       "  (402, 32.00),\n",
       "  (461, 32.00),\n",
       "  (629, 32.00),\n",
       "  (634, 32.00),\n",
       "  (644, 32.00),\n",
       "  (647, 32.00),\n",
       "  (760, 32.00),\n",
       "  (948, 32.00),\n",
       "  (954, 32.00),\n",
       "  (59, 31.00),\n",
       "  (64, 31.00),\n",
       "  (81, 31.00),\n",
       "  (459, 31.00),\n",
       "  (498, 31.00),\n",
       "  (521, 31.00),\n",
       "  (531, 31.00),\n",
       "  (578, 31.00),\n",
       "  (589, 31.00),\n",
       "  (745, 31.00),\n",
       "  (803, 31.00),\n",
       "  (860, 31.00),\n",
       "  (966, 31.00),\n",
       "  (146, 30.00),\n",
       "  (179, 30.00),\n",
       "  (303, 30.00),\n",
       "  (567, 30.00),\n",
       "  (676, 30.00),\n",
       "  (744, 30.00),\n",
       "  (789, 30.00),\n",
       "  (4, 29.00),\n",
       "  (122, 29.00),\n",
       "  (166, 29.00),\n",
       "  (187, 29.00),\n",
       "  (233, 29.00),\n",
       "  (345, 29.00),\n",
       "  (484, 29.00),\n",
       "  (551, 29.00),\n",
       "  (557, 29.00),\n",
       "  (568, 29.00),\n",
       "  (617, 29.00),\n",
       "  (733, 29.00),\n",
       "  (914, 29.00),\n",
       "  (29, 28.00),\n",
       "  (32, 28.00),\n",
       "  (106, 28.00),\n",
       "  (147, 28.00),\n",
       "  (469, 28.00),\n",
       "  (482, 28.00),\n",
       "  (502, 28.00),\n",
       "  (544, 28.00),\n",
       "  (663, 28.00),\n",
       "  (686, 28.00),\n",
       "  (691, 28.00),\n",
       "  (747, 28.00),\n",
       "  (804, 28.00),\n",
       "  (931, 28.00),\n",
       "  (54, 27.00),\n",
       "  (150, 27.00),\n",
       "  (271, 27.00),\n",
       "  (587, 27.00),\n",
       "  (596, 27.00),\n",
       "  (624, 27.00),\n",
       "  (660, 27.00),\n",
       "  (705, 27.00),\n",
       "  (869, 27.00),\n",
       "  (369, 26.00),\n",
       "  (718, 26.00),\n",
       "  (731, 26.00),\n",
       "  (942, 26.00),\n",
       "  (965, 26.00),\n",
       "  (202, 25.00),\n",
       "  (240, 25.00),\n",
       "  (536, 25.00),\n",
       "  (767, 25.00),\n",
       "  (911, 25.00),\n",
       "  (974, 25.00),\n",
       "  (163, 24.00),\n",
       "  (168, 24.00),\n",
       "  (479, 24.00),\n",
       "  (516, 24.00),\n",
       "  (590, 24.00),\n",
       "  (631, 24.00),\n",
       "  (680, 24.00),\n",
       "  (876, 24.00),\n",
       "  (901, 24.00),\n",
       "  (909, 24.00),\n",
       "  (49, 23.00),\n",
       "  (165, 23.00),\n",
       "  (185, 23.00),\n",
       "  (623, 23.00),\n",
       "  (785, 23.00),\n",
       "  (811, 23.00),\n",
       "  (838, 23.00),\n",
       "  (856, 23.00),\n",
       "  (68, 22.00),\n",
       "  (315, 22.00),\n",
       "  (438, 22.00),\n",
       "  (499, 22.00),\n",
       "  (773, 22.00),\n",
       "  (394, 21.00),\n",
       "  (622, 21.00),\n",
       "  (638, 21.00),\n",
       "  (675, 21.00),\n",
       "  (729, 21.00),\n",
       "  (976, 21.00),\n",
       "  (525, 20.00),\n",
       "  (598, 20.00),\n",
       "  (662, 20.00),\n",
       "  (728, 20.00),\n",
       "  (908, 20.00),\n",
       "  (103, 19.00),\n",
       "  (246, 19.00),\n",
       "  (400, 19.00),\n",
       "  (550, 19.00),\n",
       "  (715, 19.00),\n",
       "  (740, 19.00),\n",
       "  (885, 19.00),\n",
       "  (282, 18.00),\n",
       "  (648, 18.00),\n",
       "  (836, 18.00),\n",
       "  (970, 18.00),\n",
       "  (651, 17.00),\n",
       "  (906, 17.00),\n",
       "  (930, 17.00),\n",
       "  (977, 17.00),\n",
       "  (446, 16.00),\n",
       "  (841, 16.00),\n",
       "  (460, 15.00),\n",
       "  (534, 15.00),\n",
       "  (969, 15.00),\n",
       "  (493, 14.00),\n",
       "  (504, 14.00),\n",
       "  (742, 14.00),\n",
       "  (813, 14.00),\n",
       "  (689, 13.00),\n",
       "  (34, 12.00),\n",
       "  (940, 12.00),\n",
       "  (967, 12.00),\n",
       "  (167, 11.00),\n",
       "  (899, 11.00),\n",
       "  (673, 10.00),\n",
       "  (960, 10.00),\n",
       "  (978, 9.00),\n",
       "  (681, 8.00),\n",
       "  (935, 8.00),\n",
       "  (810, 7.00),\n",
       "  (961, 2.00)])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "%precision 2\n",
    "# n, hist = targeted_diversity(learn, 150, 95)\n",
    "# n, hist\n",
    "n, hist = targeted_diversity(learn, 10, 95)\n",
    "n, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f59b994b4e0>]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# on test\n",
    "sorted_hist = sorted(hist, key=lambda x: x[0], reverse = False)\n",
    "values = [elem[1] for elem in sorted_hist]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.8493)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test\n",
    "entropy(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fCvFG0VMKts"
   },
   "outputs": [],
   "source": [
    "def make_triplet_samples(z, margin, r2, r3):\n",
    "  positive_sample = z + random_vector_volume(z.shape, 0, margin).cuda() \n",
    "  negative_sample = z + random_vector_volume(z.shape, r2, r3).cuda()\n",
    "  return positive_sample, negative_sample\n",
    "\n",
    "def random_vector_surface(shape, r = 1.):\n",
    "  mat = torch.randn(size=shape).cuda()\n",
    "  norm = torch.norm(mat, p=2, dim=1, keepdim = True).cuda()\n",
    "  return (mat/norm) * r\n",
    "\n",
    "def random_vector_volume(shape, inner_r, outer_r):\n",
    "  fraction = torch.empty(shape[0]).uniform_(inner_r, outer_r).cuda()\n",
    "  fraction = ((fraction / outer_r) ** (1 / shape[1])) * outer_r # volume-normalize the fraction\n",
    "  fraction.unsqueeze_(-1)\n",
    "  return random_vector_surface(shape, 1) * fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PizmBkGqMKtu"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_frequent(x):\n",
    "  return Counter(x).most_common(1)[0]\n",
    "\n",
    "def preds_around(center, radius, n_preds, model, dummy_img):\n",
    "  z_s = random_vector_volume([n_preds, 10], radius, radius + 0.01) + center[None]\n",
    "  noises = model.forward_z(z_s)\n",
    "  perturbed_imgs = noises + dummy_img \n",
    "  return torch.argmax(arch(perturbed_imgs), 1)\n",
    "  \n",
    "def most_freq_pred_around(center, radius, n_preds, model, dummy_img):\n",
    "  preds = preds_around(center, radius, n_preds, model, dummy_img)\n",
    "  most_freq = most_frequent(preds.tolist())\n",
    "  return (class_index_to_label(most_freq[0]), most_freq[1]/n_preds)\n",
    "\n",
    "def investigate_neighborhood(z, step, model, dummy_img):\n",
    "  with torch.no_grad():\n",
    "    result = []\n",
    "    for radius in np.arange(0.1, 6., step):\n",
    "#       print(\"creating {} more preds\".format(int(10 + 5 * (radius ** 2))))\n",
    "      most_freq_pred = most_freq_pred_around(z, radius, int(10 + 5 * (radius ** 2)), model, dummy_img)\n",
    "      result.append((radius, most_freq_pred))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-btRW4qMKtw",
    "outputId": "90e81f6a-1b9e-45a4-ae82-bda370319bd9"
   },
   "outputs": [],
   "source": [
    "#experiment 1\n",
    "\n",
    "z = torch.tensor([0.5] * 10).cuda()\n",
    "# z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "# z_s = z[None]\n",
    "\n",
    "model = learn.model.eval()\n",
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "  \n",
    "for i in range(6):\n",
    "  z = torch.empty(10).uniform_(-1, 1).cuda()\n",
    "  print(\"investigation for: \", z)\n",
    "  for elem in investigate_neighborhood(z, 0.5, model, x_img):\n",
    "    print(elem)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 1-1: modified investigate_z\n",
    "z_investigate_path = '/root/Derakhshani/adversarial/textual_notes/investigate_z_{}.txt'.format(env.save_filename)\n",
    "if Path(z_investigate_path).exists(): raise FileExistsError(\"file already exists\")\n",
    "file = open(str(z_investigate_path), 'w')\n",
    "        \n",
    "for i, (z, noise) in enumerate(zip(pruned_z_s, pruned_noises)):\n",
    "  hist = compute_prediction_histogram(learn, noise)\n",
    "  indexed_hist = [(i, val) for i, val in enumerate(hist)]\n",
    "  sorted_hist = sorted(indexed_hist, key=lambda x: x[1], reverse=True)\n",
    "  labeled_hist = [(class_index_to_label(i), count) for i, count in sorted_hist]\n",
    "  print(\"result {}:\".format(i))\n",
    "  print(big_vector_to_str(z))\n",
    "  print(labeled_hist[:6])\n",
    "  print(\"\\n\\n\")\n",
    "  \n",
    "  file.write(\"result {}:\\n\".format(i))\n",
    "  file.write(big_vector_to_str(z) + \"\\n\")\n",
    "  file.write(str(labeled_hist[:6]))\n",
    "  file.write(\"\\n\\n\\n\")\n",
    "  file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bp6YOnipMKtz"
   },
   "outputs": [],
   "source": [
    "#experiment 2\n",
    "import itertools\n",
    "z_s = [torch.tensor(t).cuda() for t in itertools.product( *([[-0.33, 0.33]] * 10) )]\n",
    "model = learn.model.eval()\n",
    "noises = []\n",
    "with torch.no_grad():\n",
    "  for z in z_s:\n",
    "    noises.append(model.forward_single_z(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "55lErWDyMKt1",
    "outputId": "93d8fb71-3fd5-44a5-d3ec-8013e13f17ba"
   },
   "outputs": [],
   "source": [
    "x_img = normalize(learn.data.train_ds[50][0].data.cuda())\n",
    "\n",
    "preds = []\n",
    "for noise in noises:\n",
    "  perturbed_img = x_img + noise\n",
    "  preds.append(torch.argmax(arch(perturbed_img[None]), 1)[0].item())\n",
    "\n",
    "from collections import Counter\n",
    "result = [(class_index_to_label(index), count) for index, count in Counter(preds).most_common(5)]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4WSg-wBFMKt5"
   },
   "outputs": [],
   "source": [
    "#experiment 3\n",
    "import itertools\n",
    "dimension_values = [[-0.9, 0.9]] * z_dim\n",
    "for i in range(z_dim):\n",
    "  if i % 100 != 0:\n",
    "    dimension_values[i] = [0.]\n",
    "# dimension_values[0] = [0.]\n",
    "# dimension_values[3] = [0.]\n",
    "# dimension_values[6] = [0.]\n",
    "# dimension_values[9] = [0.]\n",
    "pruned_z_s = [torch.tensor(t).cuda() for t in itertools.product(*dimension_values)]\n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 3: for the targeted-attack case\n",
    "pruned_z_s = []\n",
    "for i in range(z_dim):\n",
    "  new_z = torch.zeros(z_dim).cuda()\n",
    "  new_z[i] = 1.\n",
    "  pruned_z_s.append(new_z)\n",
    "  \n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 3-1: noises for \n",
    "pruned_z_s = []\n",
    "# for i in range(z_dim):\n",
    "#   new_z = torch.empty(z_dim).uniform_(0,1).cuda().detach()\n",
    "#   pruned_z_s.append(new_z)\n",
    "\n",
    "for i in range(z_dim):\n",
    "  new_z = torch.zeros(z_dim).cuda().detach()\n",
    "  new_z[i] = 1.\n",
    "  pruned_z_s.append(new_z)\n",
    "  \n",
    "model = learn.model.eval()\n",
    "with torch.no_grad():\n",
    "  pruned_noises = [model.forward_single_z(z) for z in pruned_z_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise in pruned_noises[0:200]:\n",
    "  img = noise_to_image(noise)\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider web\n",
    "z_values = [\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33],\n",
    "  [-0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33,  0.33, -0.33],\n",
    "  [-0.33,  0.33, -0.33,  0.33,  0.33, -0.33,  0.33,  0.33,  0.33,  0.33],\n",
    "  [-0.33,  0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [ 0.33, -0.33,  0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33, -0.33,  0.33, -0.33, -0.33,  0.33,  0.33, -0.33,  0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33, -0.33, -0.33, -0.33,  0.33, -0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33,  0.33, -0.33],\n",
    "  [ 0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_values = [\n",
    "  # window screen\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33,  0.33, -0.33, -0.33, -0.33, -0.33],\n",
    "  [-0.33,  0.33,  0.33, -0.33, -0.33, -0.33, -0.33, -0.33,  0.33,  0.33],\n",
    "]\n",
    "\n",
    "if any(z_values.count(x) > 1 for x in z_values):\n",
    "  raise Exception(\"duplicate\")\n",
    "  \n",
    "z_s = [torch.tensor(z).cuda() for z in z_values]\n",
    "model = learn.model.eval()\n",
    "\n",
    "for z in z_s:\n",
    "  img = noise_to_image(model.forward_single_z(z))\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuAVZzmKMKt9",
    "outputId": "7e6643e0-ce55-438b-e0ae-79bde3ee4cef"
   },
   "outputs": [],
   "source": [
    "#vgg-16_12 most repeated labels:\n",
    "l = [(611, 215.0),\n",
    "  (474, 194.1),\n",
    "  (398, 120.3),\n",
    "  (721, 79.6),\n",
    "  (741, 73.5),\n",
    "  (510, 62.5)]\n",
    "\n",
    "[(class_index_to_label(index), count) for index, count in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAgk-YyWc3rG"
   },
   "outputs": [],
   "source": [
    "# learn.recorder.plot_losses()\n",
    "# learn.recorder.plot_lr()\n",
    "# learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTHG4Bt7VDYp"
   },
   "outputs": [],
   "source": [
    "fooling_rates = []\n",
    "model = learn.model.eval()\n",
    "learn.metrics = [validation_single_perturbation]\n",
    "for i in range(10):\n",
    "  global_perturbations = model(torch.rand(1, 3, 224, 244).cuda())[0]\n",
    "  nag_util.global_perturbations = global_perturbations\n",
    "  fooling_rates.append(learn.validate()[1].cpu().item())\n",
    "  print(\"%d : %f\"%(i, fooling_rates[-1]))\n",
    "\n",
    "mean = np.mean(fooling_rates)\n",
    "stddev = np.std(fooling_rates)\n",
    "print(mean, stddev); print(fooling_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "OFCjzI7UaY3C",
    "outputId": "740185b4-dd54-46f4-b0af-79ee452568e1"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[200][0]\n",
    "x = normalize(x_img.data.cuda())\n",
    "z = torch.tensor([-0.33,  0.33, -0.33, -0.33, -0.33,  0.33,  0.33, -0.33, -0.33, -0.33], dtype=torch.float32).cuda()\n",
    "# z = torch.empty(z_dim).uniform_(-1,1).cuda()\n",
    "p = model.forward_single_z(z).detach()\n",
    "\n",
    "p_x = x + p\n",
    "# print(\"img range, noise range\")\n",
    "# print_range(x); print_range(p)\n",
    "adv_label = class_index_to_label(arch(p_x[None]).argmax(1).item())\n",
    "print_big_vector(arch(p_x[None])[0])\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0., 1.])\n",
    "p_img = Image(p)\n",
    "x_img.show()\n",
    "p_img.show()\n",
    "p_x_img.show()\n",
    "\n",
    "\n",
    "# print_range(p)\n",
    "# print_range(denormalize(x))\n",
    "# print_range(p_x)\n",
    "\n",
    "benign_label = class_index_to_label(arch(x[None]).argmax(1).item())\n",
    "\n",
    "print_big_vector(arch(x[None])[0])\n",
    "print(benign_label, adv_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzwsI2P1ZANz"
   },
   "outputs": [],
   "source": [
    "z1 = torch.tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p1 = model.forward_single_z(z1)\n",
    "\n",
    "z2 = torch.tensor([1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p2 = model.forward_single_z(z2)\n",
    "\n",
    "z3 = torch.tensor([1, 1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.float32).cuda()\n",
    "p3 = model.forward_single_z(z3)\n",
    "\n",
    "l2_distance(p1, p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eroI82OKSnAL"
   },
   "outputs": [],
   "source": [
    "#the Image works good for floats in range [0..1]\n",
    "model = learn.model.eval()\n",
    "\n",
    "x_img = learn.data.train_ds[4][0]\n",
    "x = x_img.data[None].cuda()\n",
    "p = model(x)[0].squeeze().detach() \n",
    "x = x.squeeze()\n",
    "x = normalize(x)\n",
    "\n",
    "p_x = x + p\n",
    "p_x = denormalize(p_x)\n",
    "p_x.clamp_(0,1)\n",
    "\n",
    "\n",
    "#prepare images\n",
    "p_x_img = Image(p_x)\n",
    "p = scale_to_range(p, [0.,1.])\n",
    "p_img = Image(p)\n",
    "# x_img.show()\n",
    "p_img.show()\n",
    "# p_x_img.show()\n",
    "\n",
    "print_range(p)\n",
    "print_range(x)\n",
    "print_range(p_x)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NAG-tripletLossExperiment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
